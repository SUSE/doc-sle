<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-libvirt-storage">
 <title>Advanced storage topics</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces advanced topics about manipulating storage from the
  perspective of the &vmhost;.
 </para>
 <sect1 xml:id="sec-libvirt-storage-locking">
  <title>Locking disk files and block devices with <systemitem class="daemon">virtlockd</systemitem></title>

  <para>
   Locking block devices and disk files prevents concurrent writes to these
   resources from different VM Guests. It provides protection against starting
   the same &vmguest; twice, or adding the same disk to two different virtual
   machines. This will reduce the risk of a virtual machine's disk image
   becoming corrupted because of a wrong configuration.
  </para>

  <para>
   The locking is controlled by a daemon called
   <systemitem
   class="daemon">virtlockd</systemitem>. Since it operates
   independently from the &libvirtd; daemon, locks will endure a crash or a
   restart of &libvirtd;. Locks will even persist in the case of an update of
   the <systemitem class="daemon">virtlockd</systemitem> itself, since it can
   re-execute itself. This ensures that &vmguest;s do <emphasis>not</emphasis>
   need to be restarted upon a
   <systemitem
   class="daemon">virtlockd</systemitem> update.
   <systemitem
   class="daemon">virtlockd</systemitem> is supported for &kvm;,
   &qemu;, and &xen;.
  </para>

  <sect2 xml:id="sec-libvirt-storage-locking-enable">
   <title>Enable locking</title>
   <para>
    Locking virtual disks is not enabled by default on &productname;. To enable
    and automatically start it upon rebooting, perform the following steps:
   </para>
   <procedure>
    <step>
     <para>
      Edit <filename>/etc/libvirt/qemu.conf</filename> and set
     </para>
<screen>lock_manager = "lockd"</screen>
    </step>
    <step>
     <para>
      Start the <systemitem class="daemon">virtlockd</systemitem> daemon with
      the following command:
     </para>
<screen>&prompt.sudo;systemctl start virtlockd</screen>
    </step>
    <step>
     <para>
      Restart the &libvirtd; daemon with:
     </para>
<screen>&prompt.sudo;systemctl restart libvirtd</screen>
    </step>
    <step>
     <para>
      Make sure <systemitem class="daemon">virtlockd</systemitem> is
      automatically started when booting the system:
     </para>
<screen>&prompt.sudo;systemctl enable virtlockd</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-libvirt-storage-locking-configure">
   <title>Configure locking</title>
   <para>
    By default <systemitem class="daemon">virtlockd</systemitem> is configured
    to automatically lock all disks configured for your &vmguest;s. The default
    setting uses a "direct" lockspace, where the locks are acquired against the
    actual file paths associated with the VM Guest &lt;disk&gt; devices. For
    example, <literal>flock(2)</literal> will be called directly on
    <filename>/var/lib/libvirt/images/my-server/disk0.raw</filename> when the
    &vmguest; contains the following &lt;disk&gt; device:
   </para>
<screen>&lt;disk type='file' device='disk'&gt;
 &lt;driver name='qemu' type='raw'/&gt;
 &lt;source file='/var/lib/libvirt/images/my-server/disk0.raw'/&gt;
 &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</screen>
   <para>
    The <systemitem class="daemon">virtlockd</systemitem> configuration can be
    changed by editing the file
    <filename>/etc/libvirt/qemu-lockd.conf</filename>. It also contains
    detailed comments with further information. Make sure to activate
    configuration changes by reloading
    <systemitem class="daemon">virtlockd</systemitem>:
   </para>
<screen>&prompt.sudo;systemctl reload virtlockd</screen>
   <!-- fs 2014-08-05: FIXME Check if tru &sle; 11 SP3
    <note>
    <title>Locking currently only available for all disks</title>
    <para>
    Currently, locking can only be activated globally, so that all virtual
    disks are locked. Support for locking selected disks is planned for future
    releases.
    </para>
    </note>
    <sect3 id="sec-libvirt-storage-locking-configure-noauto">
    <title>Manually managing locks</title>
    <para>
    As stated above, all virtual disks are locked by default. To
    restrict locking to selected disks, you need to turn off auto locking by
    setting
    </para>
    <screen>auto_disk_leases = 0</screen>
    <para>
    If auto locking is turned off you need to add &lt;lease> statements to
    the &lt;devices> section of the &vmguest;s XML definitions by editing
    them with the <command>virsh edit</command> command. A sample entry will
    look like the following example:
    </para>
    see https://www.redhat.com/archives/libvir-list/2013-April/msg01714.html
    <screen>TBD</screen>
    <para>
    Device leases
    When using a lock manager, it may be desirable to record device leases against a VM. The lock manager will ensure the VM won't start unless the leases can be acquired.
    ...
    <devices>
    ...
    <lease>
    <lockspace>somearea</lockspace>
    <key>somekey</key>
    <target path='/some/lease/path' offset='1024'/>
    </lease>
    ...
    </devices>
    ...
    lockspace
    This is an arbitrary string, identifying the lockspace within which the key is held. Lock managers may impose extra restrictions on the format, or length of the lockspace name.
    key
    This is an arbitrary string, uniquely identifying the lease to be acquired. Lock managers may impose extra restrictions on the format, or length of the key.
    target
    This is the fully qualified path of the file associated with the
    lockspace. The offset specifies where the lease is stored within the file. If
    the lock manager does not require a offset, just pass 0.
    </para>
    <important>
    <title>Starting &vmguest;s without locking</title>
    <para>
    If auto locking is disabled, <systemitem
    class="daemon">virtlockd</systemitem> automatically prevents all
    &vmguest;s that have no proper <quote>lease-configuration</quote> from
    being started. This ensures that only &vmguest;s with disks that are
    locked can be started.
    </para>
    <para>
    Disable this behavior by setting
    </para>
    <screen>require_lease_for_disks = 0</screen>
    </important>
    </sect3>
    -->
   <sect3 xml:id="sec-libvirt-storage-locking-configure-shared-fs">
    <title>Enabling an indirect lockspace</title>
    <para>
     The default configuration of
     <systemitem class="daemon">virtlockd</systemitem> uses a
     <quote>direct</quote> lockspace. This means that the locks are acquired
     against the actual file paths associated with the &lt;disk&gt; devices.
    </para>
    <para>
     If the disk file paths are not accessible to all hosts,
     <systemitem class="daemon">virtlockd</systemitem> can be configured to
     allow an <quote>indirect</quote> lockspace. This means that a hash of the
     disk file path is used to create a file in the indirect lockspace
     directory. The locks are then held on these hash files instead of the
     actual disk file paths. Indirect lockspace is also useful if the file
     system containing the disk files does not support
     <literal>fcntl()</literal> locks. An indirect lockspace is specified with
     the <option>file_lockspace_dir</option> setting:
    </para>
<screen>file_lockspace_dir = "<replaceable>/MY_LOCKSPACE_DIRECTORY</replaceable>"</screen>
   </sect3>
   <sect3 xml:id="sec-libvirt-storage-locking-configure-lvm-iscsi">
    <title>Enable locking on LVM or iSCSI volumes</title>
    <para>
     When wanting to lock virtual disks placed on LVM or iSCSI volumes shared
     by several hosts, locking needs to be done by UUID rather than by path
     (which is used by default). Furthermore, the lockspace directory needs to
     be placed on a shared file system accessible by all hosts sharing the
     volume. Set the following options for LVM and/or iSCSI:
    </para>
<screen>lvm_lockspace_dir = "<replaceable>/MY_LOCKSPACE_DIRECTORY</replaceable>"
iscsi_lockspace_dir = "<replaceable>/MY_LOCKSPACE_DIRECTORY</replaceable>"</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-storage-resize">
  <title>Online resizing of guest block devices</title>

  <para>
   Sometimes you need to change&mdash;extend or shrink&mdash;the size of the
   block device used by your guest system. For example, when the disk space
   originally allocated is no longer enough, it is time to increase its size.
   If the guest disk resides on a <emphasis>logical volume</emphasis>, you can
   resize it while the guest system is running. This is a big advantage over an
   offline disk resizing (see the <command>virt-resize</command> command from
   the <xref linkend="sec-guestfs-tools"/> package) as the service provided by
   the guest is not interrupted by the resizing process. To resize a &vmguest;
   disk, follow these steps:
  </para>

  <procedure>
   <title>Online resizing of guest disk</title>
   <step>
    <para>
     Inside the guest system, check the current size of the disk (for example
     <filename>/dev/vda</filename>).
    </para>
<screen>&prompt.root;fdisk -l /dev/vda
Disk /dev/sda: 160.0 GB, 160041885696 bytes, 312581808 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes</screen>
   </step>
   <step>
    <para>
     On the host, resize the logical volume holding the
     <filename>/dev/vda</filename> disk of the guest to the required size, for
     example 200 GB.
    </para>
<screen>&prompt.root;lvresize -L 200G /dev/mapper/vg00-home
Extending logical volume home to 200 GiB
Logical volume home successfully resized</screen>
   </step>
   <step>
    <para>
     On the host, resize the block device related to the disk
     <filename>/dev/mapper/vg00-home</filename> of the guest. Note that you can
     find the <replaceable>DOMAIN_ID</replaceable> with <command>virsh
     list</command>.
    </para>
<screen>&prompt.root;virsh blockresize  --path /dev/vg00/home --size 200G <replaceable>DOMAIN_ID</replaceable>
Block device '/dev/vg00/home' is resized</screen>
   </step>
   <step>
    <para>
     Check that the new disk size is accepted by the guest.
    </para>
<screen>&prompt.root;fdisk -l /dev/vda
Disk /dev/sda: 200.0 GB, 200052357120 bytes, 390727260 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-libvirt-storage-share">
  <title>Sharing directories between host and guests (file system pass-through)</title>

  <para>
   libvirt allows to share directories between host and guests using &qemu;'s
   file system pass-through (also called VirtFS) feature. Such a directory can
   be also be accessed by several &vmguest;s at once and therefore be used to
   exchange files between &vmguest;s.
  </para>

  <note>
   <title>Windows guests and file system pass-through</title>
   <para>
    Note that sharing directories between &vmhost; and Windows guests via File
    System Pass-Through does not work, because Windows lacks the drivers
    required to mount the shared directory.
   </para>
  </note>

  <para>
   To make a shared directory available on a &vmguest;, proceed as follows:
  </para>

  <procedure>
   <step>
    <para>
     Open the guest's console in &vmm; and either choose
     <menuchoice><guimenu>View</guimenu>
     <guimenu>Details</guimenu></menuchoice> from the menu or click
     <guimenu>Show virtual hardware details</guimenu> in the toolbar. Choose
     <menuchoice> <guimenu>Add Hardware</guimenu> <guimenu>Filesystem</guimenu>
     </menuchoice> to open the <guimenu>Filesystem Passthrough</guimenu>
     dialog.
    </para>
   </step>
   <step>
    <para>
     <guimenu>Driver</guimenu> allows you to choose between a
     <guimenu>Handle</guimenu> or <guimenu>Path</guimenu> base driver. The
     default setting is <guimenu>Path</guimenu>. <guimenu>Mode</guimenu> lets
     you choose the security model, which influences the way file permissions
     are set on the host. Three options are available:
    </para>
    <variablelist>
     <varlistentry>
      <term><guimenu>Passthrough</guimenu> (default)</term>
      <listitem>
       <para>
        Files on the file system are directly created with the client-user's
        credentials. This is very similar to what NFSv3 is using.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>Squash</guimenu></term>
      <listitem>
       <para>
        Same as <guimenu>Passthrough</guimenu>, but failure of privileged
        operations like <command>chown</command> are ignored. This is required
        when &kvm; is not run with
        <systemitem
        class="username">root</systemitem> privileges.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>Mapped</guimenu></term>
      <listitem>
       <para>
        Files are created with the file server's credentials
        (<literal>qemu.qemu</literal>). The user credentials and the
        client-user's credentials are saved in extended attributes. This model
        is recommended when host and guest domains should be kept completely
        isolated.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </step>
   <step>
    <para>
     Specify the path to the directory on the &vmhost; with <guimenu>Source
     Path</guimenu>. Enter a string at <guimenu>Target Path</guimenu> that will
     be used as a tag to mount the shared directory. Note that the string of
     this field is a tag only, not a path on the &vmguest;.
    </para>
   </step>
   <step>
    <para>
     <guimenu>Apply</guimenu> the setting. If the &vmguest; is currently
     running, you need to shut it down to apply the new setting (rebooting the
     guest is not sufficient).
    </para>
   </step>
   <step>
    <para>
     Boot the &vmguest;. To mount the shared directory, enter the following
     command:
    </para>
<screen>&prompt.sudo;mount -t 9p -o trans=virtio,version=9p2000.L,rw <replaceable>TAG</replaceable> /<replaceable>MOUNT_POINT</replaceable></screen>
    <para>
     To make the shared directory permanently available, add the following line
     to the <filename>/etc/fstab</filename> file:
    </para>
<screen><replaceable>TAG</replaceable>   /<replaceable>MOUNT_POINT</replaceable>    9p  trans=virtio,version=9p2000.L,rw    0   0</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="libvirt-storage-rbd">
  <title>Using RADOS block devices with &libvirt;</title>

  <para>
   RADOS Block Devices (RBD) store data in a Ceph cluster. They allow
   snapshotting, replication, and data consistency. You can use an RBD from
   your &libvirt;-managed &vmguest;s similarly to how you use other block
   devices.
  </para>

  <para os="sles;sled">
   For more details, refer to the &ses; <citetitle>&admin;</citetitle>, chapter
   <citetitle>Using libvirt with Ceph</citetitle>. The &ses; documentation is
   available from <link xlink:href="https://documentation.suse.com/ses/"/>.
  </para>
 </sect1>
</chapter>
