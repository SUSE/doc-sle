<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-nfs">
 <title>Sharing file systems with NFS</title>
 <info>
  <abstract>
   <para>
    The <emphasis>Network File System</emphasis> (<emphasis>NFS</emphasis>) is
    a protocol that allows access to files on a server in a manner similar to
    accessing local files.
   </para>
   <para>
    &productname; installs NFS v4.2, which introduces support for sparse
    files, file pre-allocation, server-side clone and copy, application data
    block (ADB), and labeled NFS for mandatory access control (MAC) (requires
    MAC on both client and server).
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec-nfs-overview">
  <title>Overview</title>
  <para>
   The <emphasis>Network File System</emphasis> (NFS) is a standardized,
   well-proven, and widely-supported network protocol that allows files to
   be shared between separate hosts.
  </para>
  <para>
   The <emphasis>Network Information Service</emphasis> (NIS) can be used
   to have centralized user management in the network. Combining NFS
   and NIS allows using file and directory permissions for access control
   in the network. NFS with NIS makes a network transparent to the user.
  </para>
  <para>
   In the default configuration, NFS completely trusts the network and
   thus any machine that is connected to a trusted network. Any user
   with administrator privileges on any computer with physical access to
   any network the NFS server trusts can access any files that the server
   makes available.
  </para>
  <para>
   Often, this level of security is perfectly satisfactory, such
   as when the network that is trusted is truly private, often localized
   to a single cabinet or machine room, and no unauthorized access is
   possible. In other cases, the need to trust a whole subnet as a unit
   is restrictive, and there is a need for more fine-grained trust. To
   meet the need in these cases, NFS supports various security levels
   using the <emphasis>&krb;</emphasis> infrastructure. &krb; requires
   NFSv4, which is used by default. For details, see
   <xref linkend="cha-security-kerberos"/>.
  </para>

  <para>
   The following are terms used in the &yast; module.
  </para>

  <variablelist>
   <varlistentry>
    <term>Exports</term>
    <listitem>
     <para>
      A directory <emphasis>exported</emphasis> by an NFS server, which clients
      can integrate into their systems.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS client</term>
    <listitem>
     <para>
      The NFS client is a system that uses NFS services from an NFS server over
      the Network File System protocol. The TCP/IP protocol is already
      integrated into the Linux kernel; there is no need to install any
      additional software.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS server</term>
    <listitem>
     <para>
      The NFS server provides NFS services to clients. A running server depends
      on the following daemons: <systemitem class="daemon">nfsd</systemitem>
      (worker), <systemitem class="daemon">idmapd</systemitem> (ID-to-name
      mapping for NFSv4, needed for certain scenarios only), <systemitem
      class="daemon">statd</systemitem> (file locking), and <systemitem
      class="daemon">mountd</systemitem> (mount requests).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFSv3</term>
    <listitem>
     <para>
      NFSv3 is the version 3 implementation, the <quote>old</quote> stateless
      NFS that supports client authentication.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFSv4</term>
    <listitem>
     <para>
      NFSv4 is the new version 4 implementation that supports secure user
      authentication via &krb;. NFSv4 requires one single port only and thus
      is better suited for environments behind a firewall than NFSv3.
     </para>
     <para>
      The protocol is specified as
      <link xlink:href="https://datatracker.ietf.org/doc/html/rfc3530"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>pNFS</term>
    <listitem>
     <para>
      Parallel NFS, a protocol extension of NFSv4. Any pNFS clients can
      directly access the data on an NFS server.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <important os="sles;osuse">
   <title>Need for DNS</title>
   <para>
    In principle, all exports can be made using IP addresses only. To avoid
    timeouts, you need a working DNS system. DNS is necessary at least for
    logging purposes, because the <systemitem class="daemon">mountd</systemitem>
    daemon does reverse lookups.
   </para>
  </important>
 </sect1>
<!-- == SLED ======================================================= -->
 <sect1 os="sled" xml:id="sec-nfs-installation-sled">
<!-- For the moment, use different IDs -->

  <title>Installing NFS server</title>

  <para>
   For installing and configuring an NFS server, see the &sls; documentation.
  </para>
 </sect1>
<!-- == osuse and SLES ============================================ -->
 <sect1 os="sles;osuse" xml:id="sec-nfs-installation">
  <title>Installing NFS server</title>

   <para>
   The NFS server is not part of the default installation. To install the NFS
   server using &yast;, choose <menuchoice> <guimenu>Software</guimenu>
   <guimenu>Software Management</guimenu></menuchoice>, select
   <guimenu>Patterns</guimenu>, and enable the <guimenu>File Server</guimenu>
   option in the <guimenu>Server Functions</guimenu> section. Click
   <guimenu>Accept</guimenu> to install the required packages.
  </para>
  <para>
   Like NIS, NFS is a client/server system. However, a machine can be
   both&mdash;it can supply file systems over the network (export) and mount
   file systems from other hosts (import).
  </para>

  <note>
<!-- bnc#870129 -->
   <title>Mounting NFS volumes locally on the exporting server</title>
   <para>
    Mounting NFS volumes locally on the exporting server is not supported on
    &productname;.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec-nfs-configuring-nfs-server" os="sles;osuse">
  <title>Configuring NFS server</title>

  <para>
   Configuring an NFS server can be done either through &yast; or manually. For
   authentication, NFS can also be combined with &krb;.
  </para>

  <sect2 xml:id="sec-nfs-export-yast2">
   <title>Exporting file systems with &yast;</title>
   <para>
    With &yast;, turn a host in your network into an NFS server&mdash;a server
    that exports directories and files to all hosts granted access to it or to
    all members of a group. Thus, the server can also provide applications
    without installing the applications locally on every host.
   </para>
   <para>
    To set up such a server, proceed as follows:
   </para>
   <procedure xml:id="pro-nfs-export-yast2-nfs">
    <title>Setting up an NFS server</title>
    <step>
     <para>
      Start &yast; and select <menuchoice> <guimenu>Network Services</guimenu>
      <guimenu>NFS Server</guimenu> </menuchoice>; see
      <xref linkend="fig-inst-nfsserver1"/>. You may be prompted to install
      additional software.
     </para>

     <figure xml:id="fig-inst-nfsserver1">
      <title>NFS server configuration tool</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_inst_nfsserver1.png" width="75%"
                   os="sles;sled"/>
        <imagedata fileref="yast2_inst_nfsserver1_kde.png" width="75%"
                   os="osuse"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_inst_nfsserver1.png" width="75%"
                   os="sles;sled"/>
        <imagedata fileref="yast2_inst_nfsserver1_kde.png" width="75%"
                   os="osuse"/>
       </imageobject>
      </mediaobject>
     </figure>

    </step>
    <step>
     <para>
      Click the <guimenu>Start</guimenu> radio button.
     </para>
    </step>
    <step>
     <para>
      If &firewalld; is active on your system, configure it separately for NFS
      (see <xref linkend="sec-security-firewall-firewalld"/>). &yast; does not
      yet have complete support for &firewalld;, so ignore the "Firewall not
      configurable" message and continue.
     </para>
    </step>
    <step>
     <para>
      Check whether you want to <guimenu>Enable NFSv4</guimenu>. If you
      deactivate NFSv4, &yast; will only support NFSv3. For information about
      enabling NFSv2, see <xref
      linkend="sec-nfs-export-manual-nsfv2"/>.
     </para>
     <substeps performance="required">
      <step>
       <para>
        If NFSv4 is selected, additionally enter the appropriate NFSv4 domain
        name. This parameter is used by the <systemitem
        class="daemon">idmapd</systemitem> daemon that is required for &krb;
        setups or if clients cannot work with numeric user names. Leave it as
        <literal>localdomain</literal> (the default) if you do not run
        <systemitem class="daemon">idmapd</systemitem> or do not have any
        special requirements. For more information on the <systemitem
        class="daemon">idmapd</systemitem> daemon, see <xref
        linkend="var-nfs-export-manual-idmapd"/>.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Click <guimenu>Enable GSS Security</guimenu> if you need secure access to
      the server. A prerequisite for this is to have &krb; installed on your
      domain and to have both the server and the clients kerberized.
      <remark>emap 2011-0824: A reference to a &krb; chapter
      would be good.</remark>
      Click <guimenu>Next</guimenu> to proceed with the next configuration
      dialog.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Add Directory</guimenu> in the upper half of the dialog to
      export your directory.
     </para>
    </step>
    <step>
     <para>
      If you have not configured the allowed hosts already, another dialog for
      entering the client information and options pops up automatically. Enter
      the host wild card (usually you can leave the default settings as they
      are).
     </para>
<!--<para>The default settings allow every host in your network to
         connect. In case you want to restrict to a set of allowed
         hosts, enter the IP addresses or host names in the
         <guimenu>Host Wild Card</guimenu> textfield. See
         <citetitle>man exports</citetitle> for a detailed view of all
         possible variants.</para>-->
     <para>
      There are four possible types of host wild cards that can be set for each
      host: a single host (name or IP address), netgroups, wild cards (such as
      <literal>*</literal> indicating all machines can access the server), and
      IP networks.
     </para>
     <para>
      For more information about these options, see the
      <literal>exports</literal> man page.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Finish</guimenu> to complete the configuration.
     </para>
    </step>
   </procedure>
  </sect2>

<!-- ============================================================== -->

  <sect2 xml:id="sec-nfs-export-manual" os="sles;osuse">
   <title>Exporting file systems manually</title>
   <para>
    The configuration files for the NFS export service are
    <filename>/etc/exports</filename> and
    <filename>/etc/sysconfig/nfs</filename>. In addition to these files,
    <filename>/etc/idmapd.conf</filename> is needed for the NFSv4 server
    configuration with kerberized NFS or if the clients cannot work with
    numeric user names.
   </para>
   <para>
    To start or restart the services, run the command
    <command>systemctl restart nfsserver</command>. This also restarts the
    RPC port mapper that is required by the NFS server.
   </para>
   <para>
    To make sure the NFS server always starts at boot time, run <command>sudo
    systemctl enable nfsserver</command>.
   </para>
   <note>
    <title>NFSv4</title>
    <para>
     NFSv4 is the latest version of the NFS protocol available on &productname;.
     Configuring directories for export with NFSv4 is now the same as with
     NFSv3.
    </para>
    <para>
     On <phrase os="sles;sled">&sls; 11</phrase><phrase os="osuse">&opensuse;
     prior to Leap</phrase>, the bind mount in
     <filename>/etc/exports</filename> was mandatory. It is still supported,
     but now deprecated.
    </para>
   </note>
   <variablelist>
    <varlistentry>
     <term><filename>/etc/exports</filename></term>
     <listitem>
      <para>
       The <filename>/etc/exports</filename> file contains a list of entries.
       Each entry indicates a directory that is shared and how it is shared. A
       typical entry in <filename>/etc/exports</filename> consists of:
      </para>
<screen>/<replaceable>SHARED</replaceable>/<replaceable>DIRECTORY</replaceable>   <replaceable>HOST</replaceable>(<replaceable>OPTION_LIST</replaceable>)</screen>
      <para>
       For example:
      </para>
<screen>/export/data   192.168.1.2(rw,sync)</screen>
      <para>
       Here the IP address <literal>192.168.1.2</literal> is used to identify
       the allowed client. You can also use the name of the host, a wild card
       indicating a set of hosts (<literal>*.abc.com</literal>,
       <literal>*</literal>, etc.), or netgroups
       (<literal>@my-hosts</literal>).
      </para>
      <para>
       For a detailed explanation of all options and their meanings, refer to
       the <literal>man</literal> page of <filename>/etc/exports</filename>:
       (<command>man exports</command>).
      </para>
      <para>
       In case you have modified <filename>/etc/exports</filename> while the
       NFS server was running, you need to restart it for the changes to become
       active: <command>sudo systemctl restart nfsserver</command>.
      </para>
<!--
     <para>
      When clients mount from this server, they just mount
      <literal>servername:/</literal> rather than
      <literal>servername:/export</literal>. It is not necessary to mount
      <literal>servername:/data</literal>, because it will automatically
      appear beneath wherever <literal>servername:/</literal> was mounted.
     </para>
     -->
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/etc/sysconfig/nfs</filename></term>
     <listitem>
      <para>
       The <filename>/etc/sysconfig/nfs</filename> file contains a few
       parameters that determine NFSv4 server daemon behavior. It is important
       to set the parameter <systemitem>NFS4_SUPPORT</systemitem> to
       <literal>yes</literal> (default). <systemitem>NFS4_SUPPORT</systemitem>
       determines whether the NFS server supports NFSv4 exports and clients.
      </para>
      <para>
       In case you have modified <filename>/etc/sysconfig/nfs</filename> while
       the NFS server was running, you need to restart it for the changes to
       become active: <command>sudo systemctl restart nfsserver</command>.
      </para>
      <tip>
       <title>Mount options</title>
       <para>
        On <phrase os="sles;sled">&sls; 11</phrase><phrase
        os="osuse">&opensuse; prior to Leap</phrase>, the
        <option>--bind</option> mount in <filename>/etc/exports</filename> was
        mandatory. It is still supported, but now deprecated. Configuring
        directories for export with NFSv4 is now the same as with NFSv3.
       </para>
      </tip>
      <note xml:id="sec-nfs-export-manual-nsfv2">
<!-- bsc#919708 -->
       <title>NFSv2</title>
       <para>
        If NFS clients still depend on NFSv2, enable it on the server in
        <filename>/etc/sysconfig/nfs</filename> by setting:
       </para>
<screen>NFSD_OPTIONS="-V2"
MOUNTD_OPTIONS="-V2"</screen>
       <para>
        After restarting the service, check whether version 2 is available
        with the command:
       </para>
<screen>&prompt.user;cat /proc/fs/nfsd/versions
+2 +3 +4 +4.1 +4.2</screen>
      </note>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="var-nfs-export-manual-idmapd">
     <term><filename>/etc/idmapd.conf</filename></term>
     <listitem>
      <para>
       The <systemitem class="daemon">idmapd</systemitem> daemon is only
       required if &krb; authentication is used or if clients cannot work
       with numeric user names. Linux clients can work with numeric user names
       since Linux kernel 2.6.39. The <systemitem
       class="daemon">idmapd</systemitem> daemon does the name-to-ID mapping
       for NFSv4 requests to the server and replies to the client.
      </para>
      <para>
       If required, <systemitem class="daemon">idmapd</systemitem>
       needs to run on the NFSv4 server. Name-to-ID mapping on the client will
       be done by <command>nfsidmap</command> provided by the package
       <package>nfs-client</package>.
      </para>
      <para>
       Make sure that there is a uniform way in which user names and IDs (UIDs)
       are assigned to users across machines that might be sharing file systems
       using NFS. This can be achieved by using NIS, LDAP, or any uniform domain
       authentication mechanism in your domain.
      </para>
      <para>
       The parameter <literal>Domain</literal> must be set the same for both
       client and server in the <filename>/etc/idmapd.conf</filename> file. If
       you are not sure, leave the domain as <literal>localdomain</literal> in
       the server and client files. A sample configuration file looks like the
       following:
      </para>
<screen>[General]
Verbosity = 0
Pipefs-Directory = /var/lib/nfs/rpc_pipefs
Domain = localdomain

[Mapping]
Nobody-User = nobody
Nobody-Group = nobody</screen>
      <para>
       To start the <systemitem class="daemon">idmapd</systemitem> daemon, run
       <command>systemctl start nfs-idmapd</command>. In case you have modified
       <filename>/etc/idmapd.conf</filename> while the daemon was running, you
       need to restart it for the changes to become active: <command>systemctl
       start nfs-idmapd</command>.
      </para>
      <para>
       For more information, see the man pages of <literal>idmapd</literal> and
       <literal>idmapd.conf</literal> (<literal>man idmapd</literal> and
       <literal>man idmapd.conf</literal>).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

<!-- ============================================================== -->

  <sect2 xml:id="sec-nfs-kerberos">
   <title>NFS with &krb;</title>
   <para>
    To use &krb; authentication for NFS, Generic Security Services (GSS) must be enabled.
    Select <guimenu>Enable GSS Security</guimenu> in the initial &yast; NFS
    Server dialog. You must have a working &krb; server to use this feature.
    &yast; does not set up the server but only uses the provided functionality.
    To use &krb; authentication in addition to the &yast;
    configuration, complete at least the following steps before running the NFS
    configuration:
   </para>
   <procedure>
    <step>
     <para>
      Make sure that both the server and the client are in the same &krb;
      domain. They must access the same KDC (Key Distribution Center) server
      and share their <filename>krb5.keytab</filename> file (the default
      location on any machine is <filename>/etc/krb5.keytab</filename>). For
      more information about &krb;, see
      <xref linkend="cha-security-kerberos"/>.
     </para>
    </step>
    <step>
     <para>
      Start the gssd service on the client with <command>systemctl start
      rpc-gssd.service</command>.
     </para>
    </step>
    <step os="sles;osuse">
     <para>
      Start the svcgssd service on the server with <command>systemctl start
      rpc-svcgssd.service</command>.
     </para>
    </step>
   </procedure>
   <para>
    &krb; authentication also requires the <systemitem
    class="daemon">idmapd</systemitem> daemon to run on the server. For more
    information, refer to <xref linkend="var-nfs-export-manual-idmapd"/>.
   </para>
   <para>
    For more information about configuring kerberized NFS, refer to the links
    in <xref linkend="sec-nfs-info" xrefstyle="SectTitleOnPage"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-nfs-configuring-nfs-clients">
  <title>Configuring clients</title>

  <para>
   To configure your host as an NFS client, you do not need to install
   additional software. All needed packages are installed by default.
  </para>

  <sect2 xml:id="sec-nfs-import-yast2">
   <title>Importing file systems with &yast;</title>
   <para>
    Authorized users can mount NFS directories from an NFS server into the
    local file tree using the &yast; NFS client module. Proceed as follows:
   </para>
   <procedure xml:id="pro-nfs-import-yast2">
    <title>Importing NFS directories</title>
    <step>
     <para>
      Start the &yast; NFS client module.
     </para>
    </step>
<!-- Start: <guimenu>NFS Shares</guimenu> -->
    <step>
     <para>
      Click <guimenu>Add</guimenu> in the <guimenu>NFS Shares</guimenu> tab.
      Enter the host name of the NFS server, the directory to import, and the
      mount point at which to mount this directory locally.
     </para>
    </step>
<!-- End: NFS Shares -->
<!-- Start: <guimenu>NFS Settings</guimenu> -->
    <step>
     <para>
      When using NFSv4, select <guimenu>Enable NFSv4</guimenu> in the
      <guimenu>NFS Settings</guimenu> tab. Additionally, the <guimenu>NFSv4
      Domain Name</guimenu> must contain the same value as used by the NFSv4
      server. The default domain is <literal>localdomain</literal>.
     </para>
    </step>
    <step>
     <para>
      To use &krb; authentication for NFS, GSS security must be enabled.
      Select <guimenu>Enable GSS Security</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enable <guimenu>Open Port in Firewall</guimenu> in the <guimenu>NFS
      Settings</guimenu> tab if you use a firewall and want to allow access to
      the service from remote computers. The firewall status is displayed next
      to the check box.
     </para>
    </step>
<!-- End: NFS Settings -->
    <step>
     <para>
      Click <guimenu>OK</guimenu> to save your changes.
     </para>
    </step>
   </procedure>
   <para>
    The configuration is written to <filename>/etc/fstab</filename> and the
    specified file systems are mounted. When you start the &yast; configuration
    client at a later time, it also reads the existing configuration from this
    file.
   </para>
   <tip>
    <title>NFS as a root file system</title>
    <para>
     On (diskless) systems where the root partition is mounted via network as
     an NFS share, you need to be careful when configuring the network device
     with which the NFS share is accessible.
    </para>
    <para>
     When shutting down or rebooting the system, the default processing order
     is to turn off network connections then unmount the root partition. With
     NFS root, this order causes problems as the root partition cannot be
     cleanly unmounted as the network connection to the NFS share is already
     deactivated. To prevent the system from deactivating the relevant
     network device, open the network device configuration tab as described in
     <xref linkend="sec-network-yast-change-start"/> and choose <guimenu>On
     NFSroot</guimenu> in the <guimenu>Device Activation</guimenu> pane.
    </para>
   </tip>
<!--
   <figure id="fig-yast2-nfs-client">
    <title>NFS client configuration with YaST</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast2_nfsclient.png" width="75%"
             />
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast2_nfsclient.png" width="75%"
             />
     </imageobject>
    </mediaobject>
   </figure>
-->
  </sect2>

  <sect2 xml:id="sec-nfs-import">
   <title>Importing file systems manually</title>

<!-- mention nfs.service according to bnc#884690 -->
   <para>
    The prerequisite for importing file systems manually from an NFS server is
    a running RPC port mapper. The <option>nfs</option> service takes care to
    start it properly; thus, start it by entering <command>systemctl start
    nfs</command> as <systemitem class="username">root</systemitem>. Then
    remote file systems can be mounted in the file system just like local
    partitions, using the <command>mount</command>:
   </para>
<screen>&prompt.sudo;mount <replaceable>HOST</replaceable>:<replaceable>REMOTE-PATH</replaceable><replaceable>LOCAL-PATH</replaceable></screen>
   <para>
    To import user directories from the <systemitem>&nfsname;</systemitem>
    machine, for example, use:
   </para>
<screen>&prompt.sudo;mount &nfsname;:/home /home</screen>
   <para>
    To define a count of TCP connections that the clients make to the NFS
    server, you can use the <literal>nconnect</literal> option of the
    <command>mount</command> command. You can specify any number between 1 and
    16, where 1 is the default value if the mount option has not been specified.
   </para>
   <para>
    The <literal>nconnect</literal> setting is applied only during the first
    mount process to the particular NFS server. If the same client executes the
    mount command to the same NFS server, all already established connections
    will be shared&mdash;no new connection will be established. To change the
    <literal>nconnect</literal> setting, you have to unmount
    <emphasis role="bold">all</emphasis> client connections to the particular
    NFS server. Then you can define a new value for the
    <literal>nconnect</literal> option.
   </para>
   <para>
    You can find the value of <literal>nconnect</literal> that is in currently
    in effect in the output of the <command>mount</command>, or in the file
    <filename>/proc/mounts</filename>. If there is no value for the mount
    option, then the option has not been used during mounting and the default
    value of <emphasis>1</emphasis> is in use.
   </para>
   <note>
    <title>Different number of connections than defined by <literal>nconnect</literal></title>
    <para>
     As you can close and open connections after the first mount, the actual
     count of connections does not necessarily have to be the same as the value
     of <literal>nconnect</literal>.
    </para>
   </note>
   
   <sect3 xml:id="sec-nfs-automount">
    <title>Using the automount service</title>
    <para>
     The autofs daemon can be used to mount remote file systems automatically.
     Add the following entry to the <filename>/etc/auto.master</filename> file:
    </para>
<screen>/nfsmounts /etc/auto.nfs</screen>
    <para>
     Now the <filename>/nfsmounts</filename> directory acts as the root for all
     the NFS mounts on the client if the <filename>auto.nfs</filename> file is
     filled appropriately. The name <filename>auto.nfs</filename> is chosen for
     the sake of convenience&mdash;you can choose any name. In
     <filename>auto.nfs</filename> add entries for all the NFS mounts as
     follows:
    </para>
<screen>localdata -fstype=nfs server1:/data
nfs4mount -fstype=nfs4 server2:/</screen>
    <para>
     Activate the settings with <command>systemctl start autofs</command> as
     &rootuser;. In this example, <filename>/nfsmounts/localdata</filename>,
     the <filename>/data</filename> directory of
     <systemitem>server1</systemitem>, is mounted with NFS and
     <filename>/nfsmounts/nfs4mount</filename> from
     <systemitem>server2</systemitem> is mounted with NFSv4.
    </para>
    <para>
     If the <filename>/etc/auto.master</filename> file is edited while the
     service autofs is running, the automounter must be restarted for the
     changes to take effect with <command>systemctl restart autofs</command>.
    </para>
   </sect3>
   <sect3 xml:id="sec-nfs-fstab">
    <title>Manually editing <filename>/etc/fstab</filename></title>
    <para>
     A typical NFSv3 mount entry in <filename>/etc/fstab</filename> looks like
     this:
    </para>
<screen>&nfsname;:/data /local/path nfs rw,noauto 0 0</screen>
    <para>
     For NFSv4 mounts, use <literal>nfs4</literal> instead of
     <literal>nfs</literal> in the third column:
    </para>
<screen>&nfsname;:/data /local/pathv4 nfs4 rw,noauto 0 0</screen>
    <para>
     The <literal>noauto</literal> option prevents the file system from being
     mounted automatically at start-up. If you want to mount the respective
     file system manually, it is possible to shorten the mount command
     specifying the mount point only:
    </para>
<screen>&prompt.sudo;mount /local/path</screen>
    <note>
     <title>Mounting at start-up</title>
     <para>
      If you do not enter the <literal>noauto</literal> option, the init
      scripts of the system will handle the mount of those file systems at
      start-up.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-nfs-pnfs">
   <title>Parallel NFS (pNFS)</title>
   <para>
    NFS is one of the oldest protocols, developed in the 1980s. As such, NFS is
    usually sufficient if you want to share small files. However, when you want
    to transfer big files or many clients want to access data, an NFS server
    becomes a bottleneck and has a significant impact on the system performance.
    This is because files are quickly getting bigger, whereas the relative speed
    of Ethernet has not fully kept pace.
   </para>
   <para>
    When you request a file from a regular NFS server, the server
    looks up the file metadata, collects all the data, and transfers it over the
    network to your client. However, the performance bottleneck becomes
    apparent no matter how small or big the files are:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      With small files, most of the time is spent collecting the metadata.
     </para>
    </listitem>
    <listitem>
     <para>
      With big files, most of the time is spent on transferring the data from
      server to client.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    pNFS, or parallel NFS, overcomes this limitation as it separates the file
    system metadata from the location of the data. As such, pNFS requires two
    types of servers:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      A <emphasis>metadata</emphasis> or <emphasis>control server</emphasis>
      that handles all the non-data traffic
     </para>
    </listitem>
    <listitem>
     <para>
      One or more <emphasis>storage server(s)</emphasis> that hold(s) the data
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The metadata and the storage servers form a single, logical NFS server.
    When a client wants to read or write, the metadata server tells the NFSv4
    client which storage server to use to access the file chunks. The client
    can access the data directly on the server.
   </para>
   <para>
    &productname; supports pNFS on the client side only.
   </para>
   <sect3 xml:id="sec-nfs-pnfs-yast">
    <title>Configuring pNFS client with &yast;</title>
    <para>
     Proceed as described in <xref linkend="pro-nfs-import-yast2"/>, but click
     the <guimenu>pNFS (v4.2)</guimenu> check box and optionally <guimenu>NFSv4
     share</guimenu>. &yast; will do all the necessary steps and will write all
     the required options in the file <filename>/etc/exports</filename>.
    </para>
   </sect3>
   <sect3 xml:id="sec-nfs-pnfs-manual">
    <title>Configuring pNFS client manually</title>
    <para>
     Refer to <xref linkend="sec-nfs-import"/> to start. Most of the
     configuration is done by the NFSv4 server. For pNFS, the only difference
     is to add the <option>minorversion</option> option and the metadata server
     <replaceable>MDS_SERVER</replaceable> to your <command>mount</command>
     command:
    </para>
<screen>&prompt.sudo;mount -t nfs4 -o minorversion=1 <replaceable>MDS_SERVER</replaceable> <replaceable>MOUNTPOINT</replaceable></screen>
    <para>
     To help with debugging, change the value in the <filename>/proc</filename>
     file system:
    </para>
<screen>&prompt.sudo;echo 32767 &gt; /proc/sys/sunrpc/nfsd_debug
&prompt.sudo;echo 32767 &gt; /proc/sys/sunrpc/nfs_debug</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="nfs4-acls" os="sles;osuse">
  <title>Managing Access Control Lists over NFSv4</title>
  <para>
   There is no single standard for Access Control Lists (ACLs) in Linux beyond
   the simple read, write, and execute (<literal>rwx</literal>) flags for user,
   group, and others (<literal>ugo</literal>). One option for finer control is
   the <citetitle>Draft POSIX ACLs</citetitle>, which were never formally
   standardized by POSIX. Another is the NFSv4 ACLs, which were designed to be
   part of the NFSv4 network file system with the goal of making something that
   provided reasonable compatibility between POSIX systems on Linux and WIN32
   systems on Microsoft Windows.
  </para>
  <para>
   NFSv4 ACLs are not sufficient to correctly implement Draft POSIX ACLs so no
   attempt has been made to map ACL accesses on an NFSv4 client (such as using
   <command>setfacl</command>).
  </para>
  <para>
   When using NFSv4, Draft POSIX ACLs cannot be used even in emulation and NFSv4
   ACLs need to be used directly; that means while <command>setfacl</command>
   can work on NFSv3, it cannot work on NFSv4. To allow NFSv4 ACLs to be used on
   an NFSv4 file system, SUSE Linux Enterprise Server provides the
   <filename>nfs4-acl-tools</filename> package, which contains the following:
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <command>nfs4-getfacl</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>nfs4-setfacl</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>nfs4-editacl</command>
    </para>
   </listitem>
  </itemizedlist>
  <para>
   These operate in a generally similar way to <command>getfacl</command> and
   <command>setfacl</command> for examining and modifying NFSv4 ACLs. These
   commands are effective only if the file system on the NFS server provides
   full support for NFSv4 ACLs. Any limitation imposed by the server will affect
   programs running on the client in that some particular combinations of Access
   Control Entries (ACEs) might not be possible.
  </para>
  <para>
   It is not supported to mount NFS volumes locally on the exporting NFS server.
  </para>
  <bridgehead>Additional Information</bridgehead>
  <para>
   For information, see <citetitle>Introduction to NFSv4 ACLs</citetitle> at
   <link xlink:href="http://wiki.linux-nfs.org/wiki/index.php/ACLs#Introduction_to_NFSv4_ACLs"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec-nfs-info">
  <title>More information</title>
  <para>
   In addition to the man pages of <command>exports</command>,
   <command>nfs</command>, and <command>mount</command>, information about
   configuring an NFS server and client is available in
   <filename>/usr/share/doc/packages/nfsidmap/README</filename>. For further
   documentation online, refer to the following Web sites:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     For general information about network security, refer to
     <xref linkend="cha-security-firewall"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Refer to <xref linkend="sec-autofs-nfs"/> if you need to automatically
     mount NFS exports.
    </para>
   </listitem>
   <listitem>
    <para>
     For more details about configuring NFS by using &ay;, refer to
     <xref linkend="ay-nfs"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     For instructions about securing NFS exports with &krb;, refer to
     <xref linkend="sec-security-kerberos-nfs"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Find the detailed technical documentation online at
     <link xlink:href="http://nfs.sourceforge.net/">SourceForge</link>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 
 <sect1 xml:id="sec-nfs-troubleshooting">
  <title>Gathering information for NFS troubleshooting</title>
   <sect2 xml:id="sec-nfs-common-troubleshooting">
    <title>Common troubleshooting</title>
     <para>
      In some cases, you can understand the problem in your NFS by reading the
      error messages produced and looking into the
      <filename>/var/log/messages</filename> file. However, in many cases,
      the information provided by the error messages and in
      <filename>/var/log/messages</filename> is not detailed enough. In these
      cases, most NFS problems can be best understood through capturing network
      packets while reproducing the problem.
     </para>
 
     <para>
      Clearly define the problem. Examine the problem by testing the system in a
      variety of ways and determining when the problem occurs. Isolate the
      simplest steps that lead to the problem. Then try to reproduce the
      problem as described in the procedure below.
     </para>
 
     <procedure>
      <title>Reproducing the problem</title>
       <step>
        <para>
         Capture network packets.  On Linux, you can use the
         <command>tcpdump</command> command, which is supplied by the
         <package>tcpdump</package> package.
        </para>
        <para>
         An example of <command>tcpdump</command> syntax follows:
        </para>
<screen>tcpdump -s0 -i <replaceable>eth0</replaceable> -w <replaceable>/tmp/nfs-demo.cap</replaceable> host <replaceable>x.x.x.x</replaceable></screen>
        <para>
         Where:
        </para>
        <variablelist>
         <varlistentry>
          <term>s0</term>
          <listitem>
           <para>Prevents packet truncation
           </para> 
          </listitem>
         </varlistentry>
         <varlistentry>
          <term>eth0</term>
          <listitem>
           <para>
            Should be replaced with the name of the local interface which the
            packets will pass through. You can use the <literal>any</literal>
            value to capture all interfaces at the same time, but usage of this
            attribute often results in inferior data as well as confusion in
            analysis.
           </para>
          </listitem>
         </varlistentry>
         <varlistentry>
          <term>w</term>
         <listitem>
          <para>
           Designates the name of the capture file to write.
          </para>
         </listitem>
         </varlistentry>
         <varlistentry>
          <term>x.x.x.x</term>
          <listitem>
           <para>
            Should be replaced with the IP address of the other end of the NFS
            connection. For example, when taking a <command>tcpdump</command> at
            the NFS client side, specify the IP address of the NFS Server, and
            vice versa.
           </para>
          </listitem>
         </varlistentry>
        </variablelist>
       <note>
        <para>
         In some cases, capturing the data at either the NFS client or NFS
         server is sufficient. However, in cases where end-to-end network
         integrity is in doubt, it is often necessary to capture data at both
         ends.
        </para>
       </note>
       <para>
        Do not shut down the <command>tcpdump</command> process and proceed to
        the next step.
       </para>
      </step>
      <step>
       <para>
        (Optional) If the problem occurs during execution of the
        <command>nfs mount</command> command itself, you can try to use the
        high-verbosity option (<literal>-vvv</literal>) of the
        <command>nfs mount</command> command to get more output.
       </para>
  </step>
  <step>
   <para>
    (Optional) Get an <command>strace</command> of the reproduction method. An
    <command>strace</command> of reproduction steps records exactly what system
    calls were made at exactly what time. This information can be used to
    further determine on which events in the <literal>tcpdump</literal> you
    should focus.
   </para>
   <para>
    For example, if you found out that executing the command
    <emphasis>mycommand --param</emphasis> was failing on an NFS mount, then you
    could <command>strace</command> the command with:
   </para>
<screen>strace -ttf -s128 -o/tmp/nfs-strace.out mycommand --param</screen>
   <para>
    In case you do not get any <command>strace</command> of the reproduction
    step, note the time when the problem was reproduced. Check the
    <filename>/var/log/messages</filename> log file to isolate the problem.
   </para>
  </step>
  <step>
   <para>
    Once the problem has been reproduced, stop <command>tcpdump</command>
    running in your terminal by pressing
    <keycombo><keycap>CTRL</keycap><keycap>c</keycap></keycombo>. If the 
    <command>strace</command> command resulted in a hang, also terminate the
    <command>strace</command> command.
   </para>
  </step>
  <step>
   <para>
    An administrator with experience in analyzing packet traces and
    <command>strace</command> data can now inspect data in
    <filename>/tmp/nfs-demo.cap</filename> and
    <filename>/tmp/nfs-strace.out</filename>.
   </para>
  </step>  
  </procedure>
 </sect2>
 
 <sect2 xml:id="sec-advance-debugging">
  <title>Advanced NFS debugging</title>
   <important>
    <title>Advanced debugging is for experts</title>
    <para>
     Please bear in mind that the following section is intended only for skilled
     NFS administrators who understand the NFS code. Therefore, perform the
     first steps described in <xref linkend="sec-nfs-common-troubleshooting"/>
     to help narrow down the problem and to inform an expert about which areas
     of debug code (if any) might be needed to learn deeper details.</para>
   </important>
   <para>
    There are various areas of debug code that can be enabled to gather
    additional NFS-related information. However, the debug messages are quite
    cryptic and the volume of them can be so large that the use of debug code
    can affect system performance. It may even impact the system enough to
    prevent the problem from occurring. In the majority of cases, the debug code
    output is not needed, nor is it typically useful to anyone who is not highly
    familiar with the NFS code.  
   </para>
 
   <sect3 xml:id="sec-rpcdebug">
    <title>Activating debugging with <command>rpcdebug</command></title>
    <para>
     The <command>rpcdebug</command> tool allows you to set and clear NFS
     client and server debug flags. In case the <command>rpcdebug</command> tool
     is not accessible in your &slea;, you can install it from the package
     <package>nfs-client</package> or <package>nfs-kernel-server</package> for
     the NFS server.
    </para>
    <para>
     To set debug flags, run:
    </para>
<screen>rpcdebug -m <replaceable>module</replaceable> -s flags</screen>
    <para>
     To clear the debug flags, run:
    </para>
<screen>rpcdebug -m <replaceable>module</replaceable> -c flags</screen>
    <para>
     where <replaceable>module</replaceable> can be:
    </para>
    <variablelist>
     <varlistentry>
      <term>nfsd</term>
      <listitem>
      <para>
       Debug for the NFS server code
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nfs</term>
      <listitem>
      <para>
       Debug for the NFS client code
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nlm</term>
      <listitem>
       <para>
        Debug for the NFS Lock Manager, at either the NFS client or NFS server.
        This only applies to NFS v2/v3.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>rpc</term>
      <listitem>
      <para>
       Debug for the Remote Procedure Call module, at either the NFS client or
       NFS server.
      </para>
     </listitem>
    </varlistentry> 
   </variablelist>
   <para>
    For information on detailed usage of the <command>rpcdebug</command>
    command, refer to the manual page:
   </para>
<screen>man 8 rpcdebug</screen>
  </sect3>
 
 <sect3 xml:id="sec-other-nfs-debug">
  <title>Activating debug for other code upon which NFS depends</title>
  <para>
   NFS activities may depend on other related services, such as the NFS mount
   daemon&mdash;<command>rpc.mountd</command>. You can set options for related
   services within <filename>/etc/sysconfig/nfs</filename>.
  </para>
  <para>
   For example, <filename>/etc/sysconfig/nfs</filename> contains the parameter:
  </para>
<screen>MOUNTD_OPTIONS=""</screen>
  <para>
   To enable the debug mode, you have to use the <literal>-d</literal> option
   followed by any of the values: <literal>all</literal>,
   <literal>auth</literal>, <literal>call</literal>, <literal>general</literal>,
   or <literal>parse</literal>.
  </para>
  <para>
   For example, the following code enables all forms of
   <command>rpc.mountd</command> logging:
  </para>
<screen>MOUNTD_OPTIONS="-d all"</screen>
  <para>
   For all available options refer to the manual pages:
  </para>
<screen>man 8 rpc.mountd</screen>
  <para>
   After changing <filename>/etc/sysconfig/nfs</filename>, services need to be
   restarted:
  </para>
<screen>systemctl restart nfsserver  # for nfs server related changes
systemctl restart nfs  # for nfs client related changes</screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
