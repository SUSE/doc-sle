<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
                 type="text/xml"
                 title="Profiling step"?>
<article xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="art_virtualization-best-practices.xml" version="5.0" xml:id="article-virtualization-best-practices" xml:lang="zh-cn">
 <title><citetitle>虚拟化最佳实践</citetitle></title>
 <info>
  <productname><phrase role="productname"><phrase os="sles">SUSE Linux Enterprise Server</phrase></phrase></productname>
  <productnumber><phrase role="productnumber"><phrase os="sles;sled">15 SP4</phrase></phrase></productnumber>
  <date><?dbtimestamp format="Y"?> 年 <?dbtimestamp format="B" padding="0"?> 月 <?dbtimestamp format="d" padding="0"?> 日
</date>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec-vt-best-scenario">
  <title>虚拟化场景</title>

  <para>
   虚拟化能为您的环境提供许多功能。该技术可以用于多种场景。如需更多相关细节，请参考<xref linkend="book-virtualization"/>（尤其是以下章节）：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <xref linkend="sec-virtualization-scenarios-capabilities"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec-virtualization-introduction-benefits"/>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   本最佳实践指南提供有助于在您的环境中做出正确选择的建议。指南将根据您的工作负载建议或不建议使用某些选项。修复配置问题和执行微调任务会将 VM Guest 的性能提高到近乎裸机的水平。
  </para>

  <remark>
   Bruce 20150806: There is no mention of caching considerations, of migration
   inhibitors, or basic strategies for how to do your storage or networking
   infrastructure, and only a little bit about how to map your virtualization
   requirements to host capabilities.  I see that the doc doesn't address the
   issue of when to use Xen PV vs. Xen FV vs. KVM at all.
  </remark>


 </sect1>
 <sect1 xml:id="sec-vt-best-intro">
  <title>实施修改之前</title>

  <sect2 xml:id="sec-vt-best-intro-backup">
   <title>先备份</title>
   <para>
    更改 VM Guest 或 VM 主机服务器的配置可能会导致数据丢失或状态不稳定。在执行任何更改之前，请务必先对文件、数据、映像等进行备份。如不备份，您将无法在数据丢失或配置错误后恢复原始状态。请勿在生产系统上执行测试或试验。
   </para>
  </sect2>

  <sect2 xml:id="sec-vt-best-intro-testing">
   <title>测试您的工作负载</title>
   <para>
    虚拟化环境的效率取决于众多因素。本指南将提供一些参考，帮助您在生产环境中配置虚拟化时做出正确选择。做任何事情都无法<emphasis>一劳永逸</emphasis>。在规划、测试和部署虚拟化基础结构时，应将硬件、工作负载、资源容量等因素全部考虑在内。测试您的虚拟化工作负载对成功实施虚拟化至关重要。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-vt-best-reco">
  <title>建议</title>

  <sect2 xml:id="sec-vt-best-intro-libvirt">
   <title>建议使用 <systemitem class="library">libvirt</systemitem> 框架</title>
   <remark>add some explanation about how to enable/disable kvm with
   qemu-system-arch</remark>
   <para>
    SUSE 强烈建议使用 <systemitem class="library">libvirt</systemitem> 框架来配置、管理和操作 VM 主机服务器与 VM Guest。该框架提供了一个适用于所有支持的虚拟化技术的统一界面（GUI 和外壳），因此比特定于超级管理程序的工具更容易使用。
   </para>
   <para>
    我们不建议同时使用 libvirt 和特定于超级管理程序的工具，因为 libvirt 工具集可能无法识别使用特定于超级管理程序的工具进行的更改。有关 libvirt 的详细信息，请参见<xref linkend="cha-libvirt-overview"/>。
   </para>
  </sect2>

  <sect2 xml:id="sec-vt-best-intro-qemu">
   <title>qemu-system-i386 与 qemu-system-x86_64 的对比</title>
   <remark>Lin 20150808: Should we add some explanation about how to
   enable/disable kvm with qemu-system-arch</remark>
   <para>
    <command>qemu-system-x86_64</command> 与真正的 64 位 PC 硬件类似，支持运行 32 位或 64 位操作系统的 VM Guest。由于 <command>qemu-system-x86_64</command> 通常也可为 32 位 Guest 提供更佳性能，因此 SUSE 一般建议对 KVM 上的 32 位和 64 位 VM Guest 都使用 <command>qemu-system-x86_64</command>。已知 <command>qemu-system-i386</command> 可提供更佳性能的方案不受 SUSE 支持。
   </para>
   <para>
    Xen 也使用 qemu 软件包中的二进制文件，但更倾向于使用 <command>qemu-system-i386</command>，qemu-system-i386 也可用于 32 位和 64 位 Xen VM Guest。为保持与上游 Xen 社区的兼容性，SUSE 建议对 Xen VM Guest 使用 <command>qemu-system-i386</command>。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-vt-best-hostlevel">
  <title>VM 主机服务器配置和资源分配</title>

  <para>
   为 VM Guest 分配资源是管理虚拟机时至关重要的一点。在向 VM Guest 分配资源时，需注意过量使用资源可能会影响 VM 主机服务器和 VM Guest 的性能。如果所有 VM Guest 同时请求它们的所有资源，主机需要能够提供所有这些资源。如果不能，主机的性能将受到负面影响，进而也会对 VM Guest 的性能产生负面影响。
  </para>

  <sect2 xml:id="sec-vt-best-mem">
   <title>内存</title>
   <para>
    Linux 以称为页的单元来管理内存。在大多数系统上，默认页大小为 4 KB。Linux 和 CPU 需要知道哪些页属于哪些进程。该信息储存在页表中。如果有大量进程在运行，则需要花更多时间来查找内存映射的位置，因为搜索页表需要时间。为加快搜索速度，发明了 TLB（转换后援缓冲区）技术。但在具有大量内存的系统上，TLB 也无法完全满足需求。为避免回退到普通页表（会导致发生缓存不命中情况，这将耗费大量时间），可以使用大页。使用大页将减少 TLB 开销和找不到 TLB 的情况 (pagewalk)。对于内存为 32 GB (32*1014*1024 = 33,554,432 KB)、页大小为 4 KB 的主机，TLB 具有 <emphasis>33,554,432/4 = 8,388,608</emphasis> 个项。使用 2 MB (2048 KB) 的页大小时，TLB 只有 <emphasis>33554432/2048 = 16384</emphasis> 个项，可大大减少找不到 TLB 的情况。
   </para>
   <sect3 xml:id="sec-vt-best-mem-huge-pages">
    <title>将 VM 主机服务器和 VM Guest 配置为使用大页</title>
    <para>
     AMD64/Intel 64 CPU 体系结构支持大于 4 KB 的页，即大页。要确定系统上可用大页的大小（可以是 2 MB 或 1 GB），请查看 <filename>/proc/cpuinfo</filename> 输出中的 <literal>flags</literal> 行中是否包含 <literal>pse</literal> 和/或 <literal>pdpe1gb</literal>。
    </para>
    <table>
     <title>确定可用的大页大小</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="30%"/>
      <colspec colnum="2" colname="2" colwidth="70%"/>
      <thead>
       <row>
        <entry>
         <para>
          CPU 标志
         </para>
        </entry>
        <entry>
         <para>
          可用的大页大小
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <para>
          空字符串
         </para>
        </entry>
        <entry>
         <para>
          没有可用的大页
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          pse
         </para>
        </entry>
        <entry>
         <para>
          2 MB
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          pdpe1gb
         </para>
        </entry>
        <entry>
         <para>
          1 GB
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para>
     使用大页可以提升 VM Guest 的性能并减少主机内存的消耗。
    </para>
    <para>
     默认情况下，系统使用 THP。要使系统上可以使用大页，请在引导时使用 <option>hugepages=1</option> 来激活大页，您也可以选择使用 <option>hugepagesz=2MB</option>（举例而言）来添加大页大小。
    </para>
    <note>
     <title>1 GB 大页</title>
     <para>
      1 GB 的页只能在引导时分配，之后将无法释放。
     </para>
    </note>
    <para>
     要分配和使用大页表 (HugeTlbPage)，您需要使用正确的权限挂载 <filename>hugetlbfs</filename>。
    </para>
    <note>
     <title>大页的相关限制</title>
     <para>
      大页虽然能够提供最佳性能，但也存在一些缺点。使用大页时，内存气球（请参见<xref linkend="sec-vt-best-vmguests-virtio-balloon"/>）、KSM（请参见<xref linkend="sec-vt-best-perf-ksm"/>）等功能将不可用，并且大页无法交换。
     </para>
    </note>
    <procedure>
     <title>配置大页的使用</title>
     <step>
      <para>
       将 <literal>hugetlbfs</literal> 挂载 <filename>/dev/hugepages</filename>：
      </para>
<screen><prompt>&gt; </prompt><command>sudo</command> mount -t hugetlbfs hugetlbfs /dev/hugepages</screen>
     </step>
     <step>
      <para>
       要为大页保留内存，请使用 <command>sysctl</command> 命令。如果系统的大页大小为 2 MB (2048 KB)，而您想要为 VM Guest 保留 1 GB (1,048,576 KB)，那么池中需要有 <emphasis>1,048,576/2048=512</emphasis> 个页：
      </para>
<screen><prompt>&gt; </prompt><command>sudo</command> sysctl vm.nr_hugepages=<replaceable>512</replaceable></screen>
      <para>
       该值将写入 <filename>/proc/sys/vm/nr_hugepages</filename>，表示内核的大页池中当前<emphasis>持续存在</emphasis>的大页数量。被任务释放后，<emphasis>持续存在</emphasis>的大页将返回到大页池。
      </para>
     </step>
     <step>
      <para>
       通过运行 <command>virsh edit <replaceable>CONFIGURATION</replaceable></command> 将 <literal>memoryBacking</literal> 元素添加到 VM Guest 配置文件中。
      </para>
<screen>&lt;memoryBacking&gt;
  &lt;hugepages/&gt;
&lt;/memoryBacking&gt;</screen>
     </step>
     <step>
      <para>
       启动 VM Guest，检查主机是否使用了大页：
      </para>
<screen><prompt>&gt; </prompt>cat /proc/meminfo | grep HugePages_
HugePages_Total:<co xml:id="co-hp-total"/>     512
HugePages_Free:<co xml:id="co-hp-free"/>       92
HugePages_Rsvd:<co xml:id="co-hp-rsvd"/>        0
HugePages_Surp:<co xml:id="co-hp-surp"/>        0</screen>
      <calloutlist>
       <callout arearefs="co-hp-total">
        <para>
         大页池的大小
        </para>
       </callout>
       <callout arearefs="co-hp-free">
        <para>
         池中尚未分配的大页数量
        </para>
       </callout>
       <callout arearefs="co-hp-rsvd">
        <para>
         已承诺从池中分配但尚未进行分配的大页数量
        </para>
       </callout>
       <callout arearefs="co-hp-surp">
        <para>
         池中大于 <filename>/proc/sys/vm/nr_hugepages</filename> 中的值的大页数量。最大剩余大页数由 <filename>/proc/sys/vm/nr_overcommit_hugepages</filename> 控制
        </para>
       </callout>
      </calloutlist>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec-vt-best-mem-thp">
    <title>透明大页</title>
    <para>
     利用透明大页 (THP)，您可以使用 <command>khugepaged</command> 内核线程动态分配大页，无需再手动管理大页的分配和使用。THP 对于采用连续内存访问模式的工作负载很有用。在运行采用连续内存访问模式的合成工作负载时，页错误可减少 1000 倍。反之，对于采用稀疏内存访问模式的工作负载（例如数据库），THP 可能表现较差。在这类情况下，比较好的做法可能是通过添加内核参数 <option>transparent_hugepage=never</option> 来禁用 THP，然后重构建 grub2 配置并重引导。使用以下命令校验是否已禁用 THP：
    </para>
<screen><prompt>&gt; </prompt>cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]</screen>
    <para>
     如已禁用，方括号中将会显示值 <literal>never</literal>，如上例中所示。
    </para>
    <note>
     <title>Xen</title>
     <para>
      THP 在 Xen 下不可用。
     </para>
    </note>
   </sect3>
   <sect3 xml:id="sec-vt-best-mem-xen">
    <title>特定于 Xen 的内存说明</title>
    <sect4 xml:id="sec-vt-best-mem-xen-dom-0">
     <title>管理 Domain-0 内存</title>
     <para>
      在以前的 SUSE Linux Enterprise Server 版本中，Xen 主机的默认内存分配模式是将所有主机物理内存都分配给 Dom0，并启用自动气球式调节功能。当有其他域启动后，内存会自动从 Dom0 进行气球式调节。此行为总是容易出错，因此强烈建议将其禁用。从 SUSE Linux Enterprise Server15 SP1 开始，默认已禁用自动气球式调节，而是为 Dom0 分配 10% 的主机物理内存外加 1GB。例如，在物理内存大小为 32GB 的主机上，将为 Dom0 分配 4.2GB 内存。
     </para>
     <para>
      我们仍支持并建议在 <filename>/etc/default/grub</filename> 中使用 <option>dom0_mem</option> Xen 命令行选项（请参见<xref linkend="sec-vt-best-kernel-parameter"/>了解详细信息）。您可以通过将 <option>dom0_mem</option> 设置为主机物理内存大小，并在 <filename>/etc/xen/xl.conf</filename> 中启用 <option>autoballoon</option> 选项，来恢复旧行为。
     </para>
    </sect4>
   </sect3>
   <sect3 xml:id="sec-vt-best-perf-ksm">
    <title>KSM 和页共享</title>
    <para>
     内核同页合并 (Kernel Samepage Merging) 是一项内核功能，可以通过共享 VM Guest 共有的内存块来减少 VM 主机服务器上消耗的内存。KSM 守护程序 <systemitem class="daemon">ksmd</systemitem> 会定期扫描用户内存，查找包含相同内容、可由单个写保护页替换的页。要启用 KSM 服务，首先需确保已安装 <package>qemu-ksm</package> 软件包，然后运行以下命令： 
    </para>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl enable --now ksm.service</screen>
    <para>
     或者，也可以运行以下命令来启动该服务：
    </para>
<screen><prompt role="root"># </prompt> echo 1 &gt; /sys/kernel/mm/ksm/run</screen>
    <para>
     从 VM Guest 的角度而言，使用 KSM 的优点之一是所有 Guest 内存都受主机匿名内存支持。您可以共享 <emphasis>pagecache</emphasis>、<emphasis>tmpfs</emphasis> 或 Guest 中分配的任何类型的内存。
    </para>
    <para>
     KSM 由 <systemitem>sysfs</systemitem> 控制。您可以在 <filename>/sys/kernel/mm/ksm/</filename> 中查看 KSM 的值：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>pages_shared</literal>：正在使用的共享页数量（只读）。
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pages_sharing</literal>：正在共享页的站点数量（只读）。
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pages_unshared</literal>：为进行合并而反复检查的唯一页数量（只读）。
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pages_volatile</literal>：更改太快而认为无法合并的页数量（只读）。
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>full_scans</literal>：已扫描所有可合并区域的次数（只读）。
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>sleep_millisecs</literal>：<systemitem class="daemon">ksmd</systemitem> 在下次扫描前应休眠的毫秒数。值过低会过度使用 CPU，导致消耗可以用于其他任务的 CPU 时间。我们建议将该值设为 <literal>1000</literal> 以上。
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pages_to_scan</literal>：在 ksmd 进入睡眠状态之前要扫描的当前页数。值过高会过度使用 CPU。我们建议先将该值设为 <literal>1000</literal>，然后根据在测试部署时观察到的 KSM 结果进行必要的调整。
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>merge_across_nodes</literal>：默认情况下，系统会跨 NUMA 节点合并页。将此选项设为 <literal>0</literal> 会禁用此行为。
      </para>
     </listitem>
    </itemizedlist>
    <note>
     <title>使用案例</title>
     <para>
      KSM 技术非常适合在运行同一个应用程序或 VM Guest 的多个实例时过量使用主机内存。当应用程序和 VM Guest 类型不一并且不共享任何共用数据时，最好禁用 KSM。为此，请运行：
     </para>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl disable --now ksm.service</screen>
     <para>
      或者，也可以运行以下命令来禁用该服务：
     </para>
     <screen><prompt role="root"># </prompt> echo 0 &gt; /sys/kernel/mm/ksm/run</screen>
     <para>
      在异类和同类混杂的环境中，可以在主机上启用 KSM，但要对每个 VM Guest 禁用 KSM。可以使用 <command>virsh edit</command> 在 VM Guest 的 XML 配置中添加以下内容来禁用 Guest 页共享：
     </para>
<screen>&lt;memoryBacking&gt;
   &lt;nosharepages/&gt;
&lt;/memoryBacking&gt;</screen>
    </note>
    <warning>
     <title>避免出现内存不足的情况</title>
     <para>
      尽管 KSM 可以释放主机系统上的一些内存，但是管理员应当保留足够的交换来避免可共享内存减少时内存不足的情况。如果可共享内存的量减少，就会增加物理内存的使用量。
     </para>
    </warning>
    <warning>
     <title>内存访问延迟</title>
     <para>
      默认情况下，KSM 会跨 NUMA 节点合并共用页。如果合并后的共用页现在位于较远的 NUMA 节点上（相对于运行 VM Guest vCPU 的节点），这样可能会降低 VM Guest 的性能。如果在 VM Guest 中发现内存访问延迟增加，请使用 <literal>merge_across_nodes</literal> sysfs 控制禁用跨节点合并：
     </para>
<screen><prompt role="root"># </prompt> echo 0 &gt; /sys/kernel/mm/ksm/merge_across_nodes</screen>
    </warning>
   </sect3>
   <sect3 xml:id="sec-vt-best-mem-hot">
    <title>VM Guest：内存热插拔</title>
    <para>
     为了优化主机内存的使用，在需要时为运行中 VM Guest 热插入更多内存可能会很有用。要支持内存热插拔，必须先在 VM Guest 的配置文件中配置 <literal>&lt;maxMemory&gt;</literal> 标记：
    </para>
<screen>&lt;maxMemory<co xml:id="co-mem-hot-max"/> slots='16'<co xml:id="co-mem-hot-slots"/> unit='KiB'&gt;20971520<co xml:id="co-mem-hot-size"/>&lt;/maxMemory&gt;
  &lt;memory<co xml:id="co-mem-hot-mem"/> unit='KiB'&gt;1048576&lt;/memory&gt;
&lt;currentMemory<co xml:id="co-mem-hot-curr"/> unit='KiB'&gt;1048576&lt;/currentMemory&gt;</screen>
    <calloutlist>
     <callout arearefs="co-mem-hot-max">
      <para>
       为 Guest 分配的运行时最大内存。
      </para>
     </callout>
     <callout arearefs="co-mem-hot-slots">
      <para>
       可用于向 Guest 添加内存的槽数
      </para>
     </callout>
     <callout arearefs="co-mem-hot-size">
      <para>
       有效单位包括：
      </para>
      <itemizedlist>
       <listitem>
        <para>
         “KB”，代表千字节（1,000 个字节）
        </para>
       </listitem>
       <listitem>
        <para>
         “k”或“KiB”，代表千位二进制字节（1,024 个字节）
        </para>
       </listitem>
       <listitem>
        <para>
         “MB”，代表兆字节（1,000,000 个字节）
        </para>
       </listitem>
       <listitem>
        <para>
         “M”或“MiB”，代表兆位二进制字节（1,048,576 个字节）
        </para>
       </listitem>
       <listitem>
        <para>
         “GB”，代表千兆字节（1,000,000,000 个字节）
        </para>
       </listitem>
       <listitem>
        <para>
         “G”或“GiB”，代表千兆位二进制字节（1,073,741,824 个字节）
        </para>
       </listitem>
       <listitem>
        <para>
         “TB”，代表万亿字节（1,000,000,000,000 个字节）
        </para>
       </listitem>
       <listitem>
        <para>
         “T”或“TiB”，代表万亿位二进制字节（1,099,511,627,776 个字节）
        </para>
       </listitem>
      </itemizedlist>
     </callout>
     <callout arearefs="co-mem-hot-mem">
      <para>
       在引导时为 Guest 分配的最大内存
      </para>
     </callout>
     <callout arearefs="co-mem-hot-curr">
      <para>
       实际为 Guest 分配的内存
      </para>
     </callout>
    </calloutlist>
    <para>
     要将内存设备热插入到槽中，请创建如下所示的 <filename>mem-dev.xml</filename> 文件：
    </para>
<screen>&lt;memory model='dimm'&gt;
  &lt;target&gt;
  &lt;size unit='KiB'&gt;524287&lt;/size&gt;
  &lt;node&gt;0&lt;/node&gt;
  &lt;/target&gt;
&lt;/memory&gt;</screen>
    <para>
     然后使用以下命令附加该文件：
    </para>
<screen><prompt>&gt; </prompt>virsh attach-device vm-name mem-dev.xml</screen>
    <para>
     要使用内存设备热插拔，必须至少为 Guest 定义 1 个 NUMA 单元（请参见<xref linkend="sec-vt-best-perf-numa-vmguest-topo"/>）。
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-perf-swap">
   <title>交换</title>
   <para>
    系统通常使用<emphasis>交换</emphasis>来储存未充分使用的物理内存（用量较低或长时间未访问）。为防止系统内存不足，强烈建议设置最小交换。
   </para>
   <sect3 xml:id="sec-vt-best-perf-swap-swappiness">
    <title><literal>swappiness</literal></title>
    <para>
     <literal>swappiness</literal> 设置控制系统的交换行为。它定义如何将内存页交换到磁盘。<emphasis>swappiness</emphasis> 的值较高会导致系统频繁交换。可用的值的范围从 <literal>0</literal> 到 <literal>100</literal>。值为 <literal>100</literal> 时，系统会查找非活动页并将这些页置于交换模式。值为 <option>0</option> 时会禁用交换。

    </para>
    <para>
     要在在线系统上执行某些测试，可以即时更改 <filename>/proc/sys/vm/swappiness</filename> 的值，然后检查内存使用情况：
    </para>
<screen><prompt role="root"># </prompt> echo 35 &gt; /proc/sys/vm/swappiness</screen>
<screen><prompt>&gt; </prompt>free -h
total       used       free     shared    buffers     cached
Mem:      24616680    4991492   19625188     167056     144340    2152408
-/+ buffers/cache:    2694744   21921936
Swap:      6171644          0    6171644</screen>
    <para>
     要永久设置 swappiness 值，请在 <filename>/etc/systcl.conf</filename> 中添加如下所示的行：
    </para>
<screen>vm.swappiness = 35</screen>
    <para>
     您还可以通过在 VM Guest 的 XML 配置中使用 <literal>swap_hard_limit</literal> 元素来控制交换。设置此参数并将其用于生产环境中之前，请先进行一些测试，因为如果该值过低，主机可能会终止域。
    </para>
<screen>&lt;memtune&gt;<co xml:id="co-mem-1"/>
  &lt;hard_limit unit='G'&gt;1&lt;/hard_limit&gt;<co xml:id="co-mem-hard"/>
  &lt;soft_limit unit='M'&gt;128&lt;/soft_limit&gt;<co xml:id="co-mem-soft"/>
  &lt;swap_hard_limit unit='G'&gt;2&lt;/swap_hard_limit&gt;<co xml:id="co-mem-swap"/>
&lt;/memtune&gt;</screen>
    <calloutlist>
     <callout arearefs="co-mem-1">
      <para>
       此元素为域提供内存可调参数。如果忽略，则默认设为操作系统提供的默认值。
      </para>
     </callout>
     <callout arearefs="co-mem-hard">
      <para>
       Guest 可以使用的最大内存。为避免 VM Guest 上出现任何问题，强烈建议不使用此参数。
      </para>
     </callout>
     <callout arearefs="co-mem-soft">
      <para>
       在内存争用期间强制执行的内存限制。
      </para>
     </callout>
     <callout arearefs="co-mem-swap">
      <para>
       VM Guest 可以使用的最大内存加交换。
      </para>
     </callout>
    </calloutlist>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-io">
   <title>I/O</title>
   <para/>
   <sect3 xml:id="sec-vt-best-perf-io">
    <title>I/O 调度程序</title>
    <para>
     SUSE Linux Enterprise 15 SP2 和更高版本的 I/O 调度程序为预算公平队列 (BFQ)。BFQ 调度程序的主要目标是为请求 I/O 操作的所有进程公平分配磁盘 I/O 带宽。您可以为不同的设备使用不同的 I/O 调度程序。
    </para>
    <para>
     为了在主机和 VM Guest 上获得更好的性能，可以在 VM Guest 中使用 <literal>none</literal>（禁用 I/O 调度程序），而为虚拟化主机使用 <literal>mq-deadline</literal> 调度程序。
    </para>
    <procedure>
     <title>在运行时查看和更改 I/O 调度程序</title>
     <step>
      <para>
       要查看磁盘的当前 I/O 调度程序，请运行以下命令（将 <replaceable>sdX</replaceable> 替换为要查看的磁盘）：
      </para>
<screen><prompt>&gt; </prompt>cat /sys/block/<replaceable>sdX</replaceable>/queue/scheduler
mq-deadline kyber [bfq] none</screen>
      <para>
       方括号中的值为当前所选的调度程序（在上例中为 <literal>bfq</literal>）。
      </para>
     </step>
     <step>
      <para>
       您可以在运行时更改调度程序，具体做法是以 <systemitem class="username">root</systemitem> 身份运行以下命令：
      </para>
<screen><prompt role="root"># </prompt> echo mq-deadline &gt; /sys/block/<replaceable>sdX</replaceable>/queue/scheduler</screen>
     </step>
    </procedure>
    <para>
     如果您需要为每个磁盘指定不同的 I/O 调度程序，请创建内容类似于下例的 <filename>/usr/lib/tmpfiles.d/IO_ioscheduler.conf</filename> 文件。它为 <filename>/dev/sda</filename> 定义了 <literal>mq-deadline</literal> 调度程序，为 <filename>/dev/sdb</filename> 定义了 <literal>none</literal> 调度程序。请注意，设备名称可能因设备类型而异。此功能仅在 SLE 12 和更高版本上可用。
    </para>
<screen>
w /sys/block/sda/queue/scheduler - - - - mq-deadline
w /sys/block/sdb/queue/scheduler - - - - none
</screen>
   </sect3>
   <sect3 xml:id="sec-vt-best-io-async">
    <title>异步 I/O</title>
    <para>
     许多虚拟磁盘后端在实施中都使用 Linux 异步 I/O (aio)。默认情况下，aio 环境的最大数量设置为 65536，但如果运行的 VM Guest 达数百个，而它们的虚拟磁盘的输入输出由 Linux 异步 I/O 处理，aio 环境数量可能会超过该上限。在 VM 主机服务器上运行大量 VM Guest 时，请考虑增加 /proc/sys/fs/aio-max-nr。
    </para>
    <procedure>
     <title>在运行时查看和更改 aio-max-nr</title>
     <step>
      <para>
       要查看当前的 aio-max-nr 设置，请运行以下命令：
      </para>
<screen><prompt>&gt; </prompt>cat /proc/sys/fs/aio-max-nr
65536</screen>
     </step>
     <step>
      <para>
       您可以使用以下命令在运行时更改 aio-max-nr：
      </para>
<screen><prompt role="root"># </prompt> echo 131072 &gt; /proc/sys/fs/aio-max-nr</screen>
     </step>
    </procedure>
    <para>
     要永久设置 <option>aio-max-nr</option>，请在自定义 sysctl 文件中添加相应的项。例如，在 <filename>/etc/sysctl.d/aio-max-nr.conf</filename> 中添加以下内容：
    </para>
<screen>fs.aio-max-nr = 1048576</screen>
   </sect3>
   <sect3 xml:id="sec-vt-best-io-techniques">
    <title>I/O 虚拟化</title>
    <para>
     SUSE 产品支持各种 I/O 虚拟化技术。下表列出了每种技术的优缺点。有关虚拟化中的 I/O 的详细信息，请参见<xref linkend="sec-vt-io"/>。
    </para>
    <table>
     <title>I/O 虚拟化解决方案</title>
     <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="20%"/>
      <colspec colnum="2" colname="2" colwidth="40%"/>
      <colspec colnum="3" colname="3" colwidth="40%"/>
      <thead>
       <row>
        <entry>
         <para>
          技术
         </para>
        </entry>
        <entry>
         <para>
          优点
         </para>
        </entry>
        <entry>
         <para>
          缺点
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry morerows="3">
         <para>
          设备分配（直通）
         </para>
        </entry>
        <entry>
         <para>
          Guest 直接访问设备
         </para>
        </entry>
        <entry>
         <para>
          多个 Guest 之间不共享
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          性能较高
         </para>
        </entry>
        <entry>
         <para>
          实时迁移较为复杂
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para/>
        </entry>
        <entry>
         <para>
          每个 Guest 的 PCI 设备限制为 8
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para/>
        </entry>
        <entry>
         <para>
          服务器上的插槽数有限
         </para>
        </entry>
       </row>
       <row>
        <entry morerows="1">
         <para>
          全虚拟化（IDE、SATA、SCSI、e1000）
         </para>
        </entry>
        <entry>
         <para>
          VM Guest 兼容性
         </para>
        </entry>
        <entry>
         <para>
          性能较差
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          便于实时迁移
         </para>
        </entry>
        <entry>
         <para>
          模拟操作
         </para>
        </entry>
       </row>
       <row>
        <entry morerows="2">
         <para>
          半虚拟化（virtio-blk、virtio-net、virtio-scsi）
         </para>
        </entry>
        <entry>
         <para>
          性能较好
         </para>
        </entry>
        <entry>
         <para>
          经修改的 Guest（PV 驱动程序）
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          便于实时迁移
         </para>
        </entry>
        <entry>
         <para/>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          主机与 VM Guest 高效通信
         </para>
        </entry>
        <entry>
         <para/>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-fs">
   <title>储存和文件系统</title>
   <para>
    VM Guest 的储存空间可以是块设备（例如，物理磁盘上的分区），也可以是文件系统上的映像文件：
   </para>
   <table>
    <title>块设备与磁盘映像的对比</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="10%"/>
     <colspec colnum="2" colname="2" colwidth=""/>
     <colspec colnum="3" colname="3" colwidth=""/>
     <thead>
      <row>
       <entry>
        <para>
         技术
        </para>
       </entry>
       <entry>
        <para>
         优点
        </para>
       </entry>
       <entry>
        <para>
         缺点
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         块设备
        </para>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           性能更好
          </para>
         </listitem>
         <listitem>
          <para>
           使用标准工具进行管理/磁盘修改
          </para>
         </listitem>
         <listitem>
          <para>
           可从主机访问（利与弊）
          </para>
         </listitem>
        </itemizedlist>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           设备管理
          </para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         映像文件
        </para>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           更易于系统管理
          </para>
         </listitem>
         <listitem>
          <para>
           可轻松移动、克隆、扩展、备份域
          </para>
         </listitem>
         <listitem>
          <para>
           提供用于操作映像的综合工具包 (guestfs)
          </para>
         </listitem>
         <listitem>
          <para>
           通过稀疏文件减少开销
          </para>
         </listitem>
         <listitem>
          <para>
           通过完全分配来获得最佳性能
          </para>
         </listitem>
        </itemizedlist>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           性能比块设备低
          </para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>

   <para>
    有关映像格式和维护映像的详细信息，请参见<xref linkend="sec-vt-best-img"/>。
   </para>
   <para>
    如果您的映像储存在 NFS 共享中，您应检查一些服务器和客户端参数来改进对 VM Guest 映像的访问。
   </para>
   <sect3 xml:id="sec-vt-best-fs-nfs-rw">
    <title>NFS 读取/写入（客户端）</title>
    <para>
     <option>rsize</option> 和 <option>wsize</option> 选项指定客户端与服务器相互之间来回传递的数据区块的大小。您应确保 NFS 读取/写入大小足够大（尤其是对于较大的 I/O）。请将 <filename>/etc/fstab</filename> 中的 <option>rsize</option> 和 <option>wsize</option> 参数的值增至 16 KB。这将确保在有任何挂起实例时，所有操作都可以被冻结。
    </para>
<screen>nfs_server:/exported/vm_images<co xml:id="co-nfs-server"/> /mnt/images<co xml:id="co-nfs-mnt"/> nfs<co xml:id="co-nfs-nfs"/> rw<co xml:id="co-nfs-rw"/>,hard<co xml:id="co-nfs-hard"/>,sync<co xml:id="co-nfs-sync"/>, rsize=8192<co xml:id="co-nfs-rsize"/>,wsize=8192<co xml:id="co-nfs-wsize"/> 0 0</screen>
    <calloutlist>
     <callout arearefs="co-nfs-server">
      <para>
       NFS 服务器的主机名和导出路径名。
      </para>
     </callout>
     <callout arearefs="co-nfs-mnt">
      <para>
       将 NFS 导出的共享挂载到的位置。
      </para>
     </callout>
     <callout arearefs="co-nfs-nfs">
      <para>
       这是 <option>nfs</option> 挂载点。
      </para>
     </callout>
     <callout arearefs="co-nfs-rw">
      <para>
       此挂载点可以读取/写入模式访问。
      </para>
     </callout>
     <callout arearefs="co-nfs-hard">
      <para>
       确定 NFS 请求超时后 NFS 客户端的恢复行为。<option>hard</option> 是避免数据损坏的最佳选项。
      </para>
     </callout>
     <callout arearefs="co-nfs-sync">
      <para>
       任何将数据写入该挂载点上的文件的系统调用都会导致在系统调用将控制权交回给用户空间之前将数据刷新到服务器。
      </para>
     </callout>
     <callout arearefs="co-nfs-rsize">
      <para>
       NFS 客户端从 NFS 服务器上的文件读取数据时，在每个网络 READ 请求中可以接收的最大字节数。
      </para>
     </callout>
     <callout arearefs="co-nfs-wsize">
      <para>
       NFS 客户端向 NFS 服务器上的文件写入数据时，在每个网络 WRITE 请求中可以发送的最大字节数。
      </para>
     </callout>
    </calloutlist>
   </sect3>
   <sect3 xml:id="sec-vt-best-fs-nfs-threads">
    <title>NFS 线程（服务器）</title>
    <para>
     NFS 服务器应当有足够多的 NFS 线程来处理多线程工作负载。使用 <command>nfsstat</command> 工具可获取服务器上的一些 RPC 统计信息：
    </para>
<screen><prompt>&gt; </prompt><command>sudo</command> nfsstat -rc
Client rpc stats:
calls      retrans    authrefrsh
6401066    198          0          0</screen>
    <para>
     如果 <literal>retrans</literal> 等于 0，则表示一切正常。否则，客户端需要重新传输，此时需增大 <filename>/etc/sysconfig/nfs</filename> 中的 <envar>USE_KERNEL_NFSD_NUMBER</envar> 变量并进行相应调整，直到 <literal>retrans</literal> 等于 <literal>0</literal>。
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-perf-cpu">
   <title>CPU</title>
   <para>
    主机 CPU<quote>组件</quote>在指派时将被<quote>转换</quote>为 VM Guest 中的虚拟 CPU。这些组件可能是：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>CPU 处理器</emphasis>：它描述主 CPU 单元，通常有多个核心并且可能支持超线程。
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>CPU 核心</emphasis>：一个主 CPU 单元可以提供多个核心，核心的接近性能够加快计算过程并降低能源成本。
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>CPU 超线程</emphasis>：此实现用于改进计算的并行化，但其效率不如专用核心。
     </para>
    </listitem>
   </itemizedlist>
   <sect3 xml:id="sec-vt-best-perf-cpu-assign">
    <title>指派 CPU</title>
    <para>
     所有 VM Guest 的虚拟 CPU 的累积数量超过主机 CPU 的数量时，会发生 CPU 过量使用。当没有过量使用并且每个虚拟 CPU 均匹配 VM 主机服务器上的一个硬件处理器或核心时，可能会达到最佳性能。事实上，VM Guest 在一台过量使用的主机上运行会发生延迟增加，每个 VM Guest 的吞吐量也可能受到负面影响。因此，您应尽量避免过量使用 CPU。
    </para>
    <para>
     要决定是否允许过量使用 CPU，需要对整体工作负载有较好的先验了解。例如，如果您知道所有 VM Guest 虚拟 CPU 的负载都不会超过 50%，那么您就可以假设将主机过量使用 2 倍时（这意味着在一台具有 64 个 CPU 的主机上，总共有 128 个虚拟 CPU）可以正常工作。相反，如果您知道 VM Guest 的所有虚拟 CPU 在大多数情况下都会尝试以 100% 的负载运行，那么即使虚拟 CPU 比主机的 CPU 多一个也属于不适当的配置。
    </para>
    <para>
     过量使用达到一定的程度时，即虚拟 CPU 的累积数量超过 VM 主机服务器物理核心数量的 8 倍时，很可能会导致系统出现故障和不稳定，因此应当避免出现这一情况。
    </para>
    <para>
     除非您确切知道一个 VM Guest 需要多少个虚拟 CPU，否则应当从一个开始。比较好的经验法则是将 VM 中的 CPU 工作负载控制在大约 70%（有关监视工具的信息，请参见<xref linkend="sec-util-processes"/>）。如果您在 VM Guest 中分配的处理器超过所需，这将会对主机和 Guest 的性能产生负面影响。循环效率将会降低，因为未使用的 vCPU 仍然会导致计时器中断。如果您主要在 VM Guest 上运行单线程应用程序，那么单个虚拟 CPU 是最好的选择。
    </para>
    <para>
      要是一个 VM Guest 具有的虚拟 CPU 比 VM 主机服务器具有的 CPU 还多，则一律属于不适当的配置。
    </para>
   </sect3>
   <sect3 xml:id="sec-vt-best-perf-cpu-guests">
    <title>VM Guest CPU 配置</title>
    <para>
     本节介绍如何为 VM Guest 选择和配置 CPU 类型。您还将了解如何将虚拟 CPU 固定到主机系统上的物理 CPU。有关虚拟 CPU 配置和微调参数的详细信息，请参考 <link xlink:href="https://libvirt.org/formatdomain.html#elementsCPU"/> 上的 libvirt 文档。
    </para>
    <sect4 xml:id="sec-vt-best-perf-cpu-guests-model">
     <title>虚拟 CPU 型号和功能</title>
     <para>
      可以为每个 VM Guest 单独指定 CPU 型号和拓扑。配置选项包括选择特定的 CPU 模型和排除某些 CPU 功能等。<filename>/usr/share/libvirt/cpu_map/</filename> 目录中的文件列出了预定义的 CPU 型号。采用与主机类似的 CPU 型号和拓扑通常能提供最佳性能。可以通过运行 <command>virsh capabilities</command> 来显示主机系统 CPU 型号和拓扑。
     </para>
     <para>
      请注意，如果更改了默认虚拟 CPU 配置，在将 VM Guest 迁移到硬件不同的主机时，需要关闭 VM Guest。有关 VM Guest 迁移的详细信息，请参见<xref linkend="sec-libvirt-admin-migrate"/>。
     </para>
     <para>
      要为 VM Guest 指定某个特定的 CPU 型号，请向 VM Guest 配置文件添加相应的项。下面的示例配置了具有固定 TSC 功能的 Broadwell CPU：
     </para>
<screen>&lt;cpu mode='custom' match='exact'&gt;
  &lt;model&gt;Broadwell&lt;/model&gt;
  &lt;feature name='invtsc'/&gt;
  &lt;/cpu&gt;</screen>
     <para>
      对于与主机物理 CPU 最类似的虚拟 CPU，可以使用 <literal>&lt;cpu mode=&apos;host-passthrough&apos;&gt;</literal>。请注意，<literal>host-passthrough</literal> CPU 型号可能与主机物理 CPU 并不完全相似，因为 KVM 默认会屏蔽任何不可迁移的功能。例如，invtsc 就不包含在虚拟 CPU 功能集中。虽然 libvirt 允许任意直通 KVM 命令行参数，但它并不直接支持更改默认 KVM 行为。仍以 <literal>invtsc</literal> 为例，您可以在 VM Guest 配置文件中使用以下命令行直通来实现主机 CPU（包括 <literal>invtsc</literal>） 的直通：
     </para>
<screen>&lt;domain type='kvm' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'&gt;
     &lt;qemu:commandline&gt;
     &lt;qemu:arg value='-cpu'/&gt;
     &lt;qemu:arg value='host,migratable=off,+invtsc'/&gt;
     &lt;/qemu:commandline&gt;
     ...
     &lt;/domain&gt;
     </screen>
     <note>
      <title><literal>host-passthrough</literal> 模式</title>
      <para>
       由于 <literal>host-passthrough</literal> 会向虚拟 CPU 公开物理 CPU 细节，因此无法迁移到不同的硬件。有关更多信息，请参见<xref linkend="sec-vt-best-perf-cpu-guests-vcpumigration"/>。
      </para>
     </note>
    </sect4>
    <sect4 xml:id="sec-vt-best-perf-cpu-guests-vcpupin">
     <title>虚拟 CPU 固定</title>
     <para>
      虚拟 CPU 固定用于将虚拟 CPU 线程限制到一组物理 CPU 。<literal>vcpupin</literal> 元素指定虚拟 CPU 可以使用的物理主机 CPU。如果未设置此元素且未指定 <literal>vcpu</literal> 元素的 <literal>cpuset</literal> 属性，则虚拟 CPU 可以自由使用任何物理 CPU。
     </para>
     <para>
      虚拟 CPU 固定可提高物理 CPU 缓存命中率，对 CPU 密集型工作负载用处很大。要将虚拟 CPU 固定到特定的物理 CPU，请运行以下命令：
     </para>
<screen><prompt>&gt; </prompt>virsh vcpupin <replaceable>DOMAIN_ID</replaceable> --vcpu <replaceable>vCPU_NUMBER</replaceable>
VCPU: CPU Affinity
----------------------------------
0: 0-7
<prompt role="root"># </prompt>virsh vcpupin SLE15 --vcpu 0 0 --config</screen>
     <para>
      最后一个命令会在 XML 配置中生成下面一项：
     </para>
<screen>&lt;cputune&gt;
   &lt;vcpupin vcpu='0' cpuset='0'/&gt;
&lt;/cputune&gt;</screen>
     <note>
      <title>NUMA 节点上的虚拟 CPU 固定</title>
      <para>
       要将 VM Guest 的 CPU 和内存限制为一个 NUMA 节点，可以在 NUMA 系统上使用虚拟 CPU 固定和内存分配策略。有关 NUMA 微调的详细信息，请参见<xref linkend="sec-vt-best-perf-numa"/>。
      </para>
     </note>
     <warning>
      <title>虚拟 CPU 固定和实时迁移</title>
      <para>
       虽然 <literal>vcpupin</literal> 可以提高性能，但它可能会使实时迁移复杂化。有关虚拟 CPU 迁移注意事项的详细信息，请参见<xref linkend="sec-vt-best-perf-cpu-guests-vcpumigration"/>。
      </para>
     </warning>
    </sect4>
    <sect4 xml:id="sec-vt-best-perf-cpu-guests-vcpumigration">
     <title>虚拟 CPU 迁移注意事项</title>
     <para>
      选择包含所有最新功能的虚拟 CPU 型号可以提高 VM Guest 工作负载的性能，但通常会牺牲可迁移性。除非群集中的所有主机都包含最新的 CPU 功能，否则如果目标主机缺少新功能，迁移便可能会失败。如果比起最新的 CPU 功能，更为重要的是虚拟 CPU的可迁移性，那么应当使用标准化的 CPU 型号和功能集。<command>virsh cpu-baseline</command> 命令可帮助定义可以跨所有主机迁移的标准化虚拟 CPU。下面的命令（在迁移群集中的每台主机上运行时）会在 <literal>all-hosts-caps.xml</literal> 中说明所有主机功能的集合。
     </para>
<screen><prompt>&gt; </prompt><command>sudo</command> virsh capabilities &gt;&gt; all-hosts-cpu-caps.xml</screen>
     <para>
      您可以使用 <command>virsh cpu-baseline</command> 根据 all-hosts-caps.xml 中收集的每个主机的功能创建跨所有主机兼容的虚拟 CPU 定义。
     </para>
<screen><prompt>&gt; </prompt><command>sudo</command> virsh cpu-baseline all-hosts-caps.xml</screen>
     <para>
      生成的 CPU 定义可用作 VM Guest 配置文件中的 <literal>cpu</literal> 元素。
     </para>
     <para>
      在逻辑层面上而言，虚拟 CPU 固定是一种硬件直通形式。固定会将物理资源与虚拟资源进行配对，也可能会给迁移带来问题。例如，如果请求的物理资源在目标主机上不可用，或者源主机与目标主机的 NUMA 拓扑不同，迁移将会失败。有关实时迁移的更多建议，请参见<xref linkend="libvirt-admin-live-migration-requirements"/>。
     </para>
    </sect4>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-perf-numa">
   <title>NUMA 微调</title>
   <para>
    NUMA 是 Non Uniform Memory Access（非一致性内存访问）的首字母缩写。NUMA 系统具有多个物理 CPU，每个 CPU 都附有本地内存。每个 CPU 还可以访问其他 CPU 的内存（称为<quote>远程内存访问</quote>），但速度比访问本地内存要慢得多。如果调节不当，NUMA 系统会对 VM Guest 性能造成负面影响。本节介绍了在 NUMA 主机上部署 VM Guest 时应当考虑的控制，不过最终微调取决于工作负载。在配置和部署 VM 时，始终要考虑主机拓扑。
   </para>
   <para>
    <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise Server</phrase></phrase> 包含一个 NUMA 自动平衡器，该平衡器通过将内存放置在与处理它的 CPU 相同的 NUMA 节点上来尽力减少远程内存访问。此外，标准工具（例如 <command>cgset</command>）和虚拟化工具（例如 libvirt）提供了将 VM Guest 资源限制为物理资源的机制。
   </para>
   <para>
    <command>numactl</command> 用于检查主机 NUMA 功能：
   </para>
<screen><prompt>&gt; </prompt><command>sudo</command> numactl --hardware
available: 4 nodes (0-3)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 72 73 74 75 76 77 78
79 80 81 82 83 84 85 86 87 88 89
node 0 size: 31975 MB
node 0 free: 31120 MB
node 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 90 91 92 93
94 95 96 97 98 99 100 101 102 103 104 105 106 107
node 1 size: 32316 MB
node 1 free: 31673 MB
node 2 cpus: 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 108 109 110
111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
node 2 size: 32316 MB
node 2 free: 31726 MB
node 3 cpus: 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 126 127 128
129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
node 3 size: 32314 MB
node 3 free: 31387 MB
node distances:
node   0   1   2   3
0:  10  21  21  21
1:  21  10  21  21
2:  21  21  10  21
3:  21  21  21  10</screen>
   <para>
    该 <command>numactl</command> 输出显示这是一个具有 4 个节点或单元的 NUMA 系统，每个节点或单元包含 36 个 CPU 和大约 32G 内存。<command>virsh capabilities</command> 也可用于检查系统 NUMA 功能和 CPU 拓扑。
   </para>
   <sect3 xml:id="sec-vt-best-perf-numa-balancing">
    <title>NUMA 平衡</title>
    <para>
     在 NUMA 计算机上，如果 CPU 访问远程内存，将会造成性能损失。NUMA 自动平衡功能会扫描任务的地址空间并取消页映射。这样做可以检测页是否正确放置，或者是否要将数据迁移到运行任务的本地内存节点。在定义的时间间隔内（使用 <literal>numa_balancing_scan_delay_ms</literal> 配置），该任务会扫描其地址空间中的下次扫描大小页数（使用 <literal>numa_balancing_scan_size_mb</literal> 配置）。在到达地址空间的末尾时，扫描器会从头重新开始。
    </para>
    <para>
     较高的扫描率会导致较高的系统开销，因为必须捕获页错误，并需要迁移数据。但扫描率越高，当工作负载模式更改时，任务的内存迁移到本地节点的速度就越快。这样可最大限度减少远程内存访问对性能造成的影响。以下 <command>sysctl</command> 指令控制扫描延迟的阈值和扫描的页数：
    </para>
<screen><prompt>&gt; </prompt><command>sudo</command> sysctl -a | grep numa_balancing
kernel.numa_balancing = 1<co xml:id="co-numa-balancing"/>
kernel.numa_balancing_scan_delay_ms = 1000<co xml:id="co-numa-delay"/>
kernel.numa_balancing_scan_period_max_ms = 60000<co xml:id="co-numa-pmax"/>
kernel.numa_balancing_scan_period_min_ms = 1000<co xml:id="co-numa-pmin"/>
kernel.numa_balancing_scan_size_mb = 256<co xml:id="co-numa-size"/></screen>
    <calloutlist>
     <callout arearefs="co-numa-balancing">
      <para>
       启用/禁用基于页错误的 NUMA 自动平衡
      </para>
     </callout>
     <callout arearefs="co-numa-delay">
      <para>
       任务初始派生时用于任务的起始扫描延迟
      </para>
     </callout>
     <callout arearefs="co-numa-pmax">
      <para>
       扫描任务的虚拟内存的最大时间（以毫秒为单位）
      </para>
     </callout>
     <callout arearefs="co-numa-pmin">
      <para>
       扫描任务的虚拟内存的最小时间（以毫秒为单位）
      </para>
     </callout>
     <callout arearefs="co-numa-size">
      <para>
       某次给定的扫描需要扫描的页大小（以兆字节为单位）
      </para>
     </callout>
    </calloutlist>
    <para>
     有关更多信息，请参见<xref linkend="cha-tuning-numactl"/>。
    </para>
    <para>
     NUMA 自动平衡的主要目标是在相同节点的内存上重新安排任务（以便 CPU 跟随内存），或者将内存的页复制到相同的节点上（以便内存跟随 CPU）。
    </para>
    <warning>
     <title>任务布局</title>
     <para>
      由于不同的任务之间可能会共享内存，因此并没有可定义运行任务的最佳位置的通用规则。为了获得最佳性能，建议将共享同一节点上的内存的任务分成一组。使用 <command># cat /proc/vmstat | grep numa_</command> 可查看 NUMA 统计信息。
     </para>
    </warning>
   </sect3>
   <sect3 xml:id="sec-vt-best-perf-numa-cpuset">
    <title>使用 CPUset 控制器控制内存分配</title>
    <para>
     cgroup cpuset 控制器可用于将进程使用的内存限制在一个 NUMA 节点。可用的 cpuset 内存策略模式有以下三种：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>interleave</literal>：此内存放置策略也称为轮替。对于需要将线程本地数据放到相应节点上的作业，此策略可提供显著的改进。当交错目标不可用时，它将被转移到另一个节点。
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>bind</literal>：此策略仅将内存放到一个节点上，这意味着如果内存不足，分配将会失败。
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>preferred</literal>：此策略应用首选项来向节点分配内存。如果此节点上没有足够的内存空间，则会回退到另一个节点。
      </para>
     </listitem>
    </itemizedlist>
    <para>
     您可以使用来自 <package>libcgroup-tools</package> 软件包的 <command>cgset</command> 工具更改内存策略模式：
    </para>
<screen><prompt>&gt; </prompt><command>sudo</command> cgset -r cpuset.mems=<replaceable>NODE</replaceable> sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>/emulator</screen>
    <para>
     要将页迁移到某个节点，请使用 <command>migratepages</command> 工具：
    </para>
<screen><prompt>&gt; </prompt>migratepages <replaceable>PID</replaceable> <replaceable>FROM-NODE</replaceable> <replaceable>TO-NODE</replaceable></screen>
    <para>
     要检查是否一切正常，请使用 <command>cat /proc/<replaceable>PID</replaceable>/status | grep Cpus</command>。
    </para>
    <note>
     <title>内核 NUMA/cpuset 内存策略</title>
     <para>
      有关详细信息，请参见<link xlink:href="https://www.kernel.org/doc/Documentation/vm/numa_memory_policy.txt">内核 NUMA 内存策略</link>和 <link xlink:href="https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt">cpusets 内存策略</link>。另请查看 <link xlink:href="https://libvirt.org/formatdomain.html#elementsNUMATuning">Libvirt NUMA Tuning（Libvirt NUMA 微调）文档</link>。
     </para>
    </note>
   </sect3>
   <sect3 xml:id="sec-vt-best-perf-numa-vmguest">
    <title>VM Guest：NUMA 相关配置</title>
    <para>
     <systemitem class="library">libvirt</systemitem> 允许设置虚拟 NUMA 和内存访问策略。<command>virt-install</command> 或 <command>virt-manager</command> 不支持配置这些设置，需要使用 <command>virsh edit</command> 手动编辑 VM Guest 配置文件来完成该操作。
    </para>
    <sect4 xml:id="sec-vt-best-perf-numa-vmguest-topo">
     <title>VM Guest 虚拟 NUMA 拓扑</title>
     <para>
      创建类似于主机 NUMA 拓扑的 VM Guest 虚拟 NUMA (vNUMA) 通常可以提高传统的大型可扩展工作负载的性能。可以在 XML 配置中使用 <literal>numa</literal> 元素指定 VM Guest vNUMA 拓扑：
     </para>
<screen>&lt;cpu&gt;
...
  &lt;numa&gt;
    &lt;cell<co xml:id="co-numa-cell"/> id="0"<co xml:id="co-numa-id"/> cpus='0-1'<co xml:id="co-numa-cpus"/> memory='512000' unit='KiB'/&gt;
    &lt;cell id="1" cpus='2-3' memory='256000'<co xml:id="co-numa-mem"/>
    unit='KiB'<co xml:id="co-numa-unit"/> memAccess='shared'<co xml:id="co-numa-memaccess"/>/&gt;
  &lt;/numa&gt;
  ...
&lt;/cpu&gt;</screen>
     <calloutlist>
      <callout arearefs="co-numa-cell">
       <para>
        每个 <literal>cell</literal> 元素指定一个 vNUMA 单元或节点
       </para>
      </callout>
      <callout arearefs="co-numa-id">
       <para>
        所有单元都应具有 <literal>id</literal> 属性，用于在其他配置块中引用所需单元。如果没有，将按从 0 开始的升序为单元指派 ID。
       </para>
      </callout>
      <callout arearefs="co-numa-cpus">
       <para>
        属于节点一部分的 CPU 或 CPU 范围
       </para>
      </callout>
      <callout arearefs="co-numa-mem">
       <para>
        节点内存
       </para>
      </callout>
      <callout arearefs="co-numa-unit">
       <para>
        指定节点内存时所用的单位
       </para>
      </callout>
      <callout arearefs="co-numa-memaccess">
       <para>
        可选属性，可控制将内存映射为<option>共享</option>还是<option>私用</option>。仅对支持大页的内存有效。
       </para>
      </callout>
     </calloutlist>
     <para>
      要查找 VM Guest 将其页分配到的位置，请使用 <command>cat /proc/<replaceable>PID</replaceable>/numa_maps</command> 和 <command>cat /sys/fs/cgroup/memory/sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>/memory.numa_stat</command>.
     </para>
     <warning>
      <title>NUMA 规范</title>
      <para>
       <systemitem class="library">libvirt</systemitem> VM Guest NUMA 规范目前仅适用于 QEMU/KVM。
      </para>
     </warning>
    </sect4>
    <sect4 xml:id="sec-vt-best-perf-numa-vmguest-alloc-libvirt">
     <title>使用 <systemitem class="library">libvirt</systemitem> 控制内存分配</title>
     <para>
      如果 VM Guest 具有 vNUMA 拓扑（请参见<xref linkend="sec-vt-best-perf-numa-vmguest-topo"/>），便可使用 <literal>numatune</literal> 元素将内存固定到主机 NUMA 节点。此方法目前仅适用于 QEMU/KVM Guest。请参见<xref linkend="sec-vt-best-perf-numa-alloc-libvirt-non-vnuma"/>了解如何配置非 vNUMA VM Guest。
     </para>
<screen>&lt;numatune&gt;
    &lt;memory mode="strict"<co xml:id="co-numat-mode"/> nodeset="1-4,^3"<co xml:id="co-numat-nodeset"/>/&gt;
    &lt;memnode<co xml:id="co-numat-memnode"/> cellid="0"<co xml:id="co-numat-cellid"/> mode="strict" nodeset="1"/&gt;
    &lt;memnode cellid="2" placement="strict"<co xml:id="co-numat-placement"/> mode="preferred" nodeset="2"/&gt;
&lt;/numatune&gt;</screen>
     <calloutlist>
      <callout arearefs="co-numat-mode">
       <para>
        可用的策略包括 <literal>interleave</literal>（类似于轮替）、<literal>strict</literal>（默认）或 <literal>preferred</literal>。
       </para>
      </callout>
      <callout arearefs="co-numat-nodeset">
       <para>
        指定 NUMA 节点。
       </para>
      </callout>
      <callout arearefs="co-numat-memnode">
       <para>
        指定每个 Guest NUMA 节点的内存分配策略（如果未定义此元素，则会回退并使用 <literal>memory</literal> 元素）。
       </para>
      </callout>
      <callout arearefs="co-numat-cellid">
       <para>
        对要应用设置的 Guest NUMA 节点寻址。
       </para>
      </callout>
      <callout arearefs="co-numat-placement">
       <para>
        可以使用 placement 属性来指示域进程的内存放置模式，值可以是 <literal>auto</literal> 或 <literal>strict</literal>。
       </para>
      </callout>
     </calloutlist>
     <important xml:id="sec-vt-best-perf-numa-alloc-libvirt-non-vnuma">
      <title>非 vNUMA VM Guest</title>
      <para>
       在非 vNUMA VM Guest 上，将内存固定到主机 NUMA 节点的命令如下例所示：
      </para>
<screen>&lt;numatune&gt;
   &lt;memory mode="strict" nodeset="0-1"/&gt;
&lt;/numatune&gt;</screen>
      <para>
       本例中从主机节点 <literal>0</literal> 和 <literal>1</literal> 分配内存。如果无法满足这些内存要求，启动 VM Guest 的操作将会失败。<command>virt-install</command> 也通过 <option>--numatune</option> 选项支持此配置。
      </para>
     </important>
     <warning>
      <title>跨 NUMA 节点的内存和 CPU</title>
      <para>
       您应当避免跨 NUMA 节点分配 VM Guest 内存，并防止虚拟 CPU 跨 NUMA 节点浮动。
      </para>
     </warning>
    </sect4>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-vt-best-img">
  <title>VM Guest 映像</title>

  <para>
   映像是用于储存 VM Guest 的操作系统和数据的虚拟磁盘。可以使用 <command>qemu-img</command> 命令来创建、维护和查询映像。有关 <command>qemu-img</command> 工具和示例的详细信息，请参见<xref linkend="cha-qemu-guest-inst-qemu-img-create"/>。
  </para>

  <sect2 xml:id="sec-vt-best-img-imageformat">
   <title>VM Guest 映像格式</title>
   <para>
    QEMU 可识别源自其他虚拟化技术的某些储存格式。通过识别这些格式，QEMU 可以利用最初用于在这些其他虚拟化技术下运行的数据储存或整个 Guest。有些格式只支持只读模式。要以读取/写入模式使用它们，请使用 <command>qemu-img</command> 将它们转换为完全受支持的 QEMU 储存格式。否则，它们在 QEMU Guest 中将只能用作只读数据储存。
   </para>
   <para>
    使用 <command>qemu-img info <replaceable>VMGUEST.IMG</replaceable></command> 可获取有关现有映像的信息，例如格式、虚拟大小、物理大小、快照（如果有）。
   </para>
   <note>
    <title>性能</title>
    <para>
     建议将磁盘映像转换为 raw 或 qcow2 格式以获得较好的性能。
    </para>
   </note>
   <warning>
    <title>无法压缩加密映像</title>
    <para>
     创建映像时，无法在输出文件中使用压缩 (<option>-c</option>) 的同时指定加密选项 (<option>-e</option>)。
    </para>
   </warning>
   <sect3 xml:id="sec-vt-best-img-imageformat-raw">
    <title>Raw 格式</title>
    <para/>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       此格式十分简单，并且容易导出到所有其他模拟器/超级管理程序。
      </para>
     </listitem>
     <listitem>
      <para>
       它提供的性能最佳（I/O 开销最少）。
      </para>
     </listitem>
     <listitem>
      <para>
       它会占用文件系统上的所有已分配空间。
      </para>
     </listitem>
     <listitem>
      <para>
       Raw 格式允许将 VM Guest 映像复制到物理设备 (<command>dd if=<replaceable>VMGUEST.RAW</replaceable> of=<replaceable>/dev/sda</replaceable></command>)。
      </para>
     </listitem>
     <listitem>
      <para>
       这与 VM Guest 看到的字节对字节相同，因此会浪费大量空间。
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="sec-vt-best-img-imageformat-qcow2">
    <title>qcow2 格式</title>
    <para/>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       使用此格式可缩小映像大小（这在文件系统不支持空洞时很有用）。
      </para>
     </listitem>
     <listitem>
      <para>
       它具有可选的 AES 加密（现已弃用）。
      </para>
     </listitem>
     <listitem>
      <para>
       基于 Zlib 的压缩选项。
      </para>
     </listitem>
     <listitem>
      <para>
       支持多个 VM 快照（内部、外部）。
      </para>
     </listitem>
     <listitem>
      <para>
       可提升性能和稳定性。
      </para>
     </listitem>
     <listitem>
      <para>
       支持更改后备文件。
      </para>
     </listitem>
     <listitem>
      <para>
       支持一致性检查。
      </para>
     </listitem>
     <listitem>
      <para>
       性能低于 raw 格式。
      </para>
     </listitem>
    </itemizedlist>
    <variablelist>
     <varlistentry>
      <term>l2-cache-size</term>
      <listitem>
       <para>
        qcow2 可以为随机读取/写入访问提供与 raw 格式相同的性能，但需要合适的缓存大小。默认情况下，缓存大小设置为 1 MB。这将提供最大 8 GB 磁盘大小的良好性能。如果您需要更大的磁盘大小，则需要调整缓存大小。假设磁盘大小为 64 GB (64*1024 = 65536)，则缓存大小需为 65536 / 8192B = 8 MB（<option>-drive format=qcow2,l2-cache-size=8M</option>）。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>群集大小</term>
      <listitem>
       <para>
        qcow2 格式提供更改群集大小的功能。值必须介于 512 KB 到 2 MB 之间。较小的群集大小可以改善映像文件的大小，而较大的群集大小通常可提供更好的性能。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>预分配</term>
      <listitem>
       <para>
        具有预分配元数据的映像最初便比较大，但在其需要增长时可以提升性能。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>迟缓的引用计数</term>
      <listitem>
       <para>
        为了避免元数据 I/O 并提高性能，系统会延迟更新引用计数。这对于 <option>cache=writethrough</option> 的情况尤为有用。此选项不会批量进行元数据更新，但如果由于主机崩溃而必须重构建引用计数表，则在下次使用 <command>qemu-img check -r all</command> 打开时会自动完成重构建。请注意，此过程需要一段时间。
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="sec-vt-best-img-imageformat-qed">
    <title>qed 格式</title>
    <para>
     qed 是一种改进型 qcow（QEMU 写入时复制）格式。由于 qcow2 可提供 qed 的所有优点以及其他功能，因此 qed 现已弃用。
    </para>
   </sect3>
   <sect3 xml:id="sec-vt-best-img-imageformat-vmdk">
    <title>VMDK 格式</title>
    <para>
     VMware 3、4 或 6 映像格式，用于与该产品交换映像。
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-img-overlay">
   <title>覆盖磁盘映像</title>
   <para>
    qcow2 和 qed 格式提供了一种创建基本映像（也称为后备文件）和在基本映像之上覆盖映像的方法。后备文件对于还原到已知状态并丢弃覆盖非常有用。如果向映像写入数据，后备映像将保持不变，所有更改都将记录在覆盖映像文件中。除非使用 <option>commit</option> 监视命令（或 <command>qemu-img commit</command>），否则后备文件永远都不会修改。
   </para>
   <para>
    要创建覆盖映像，请运行以下命令：
   </para>
<screen><prompt role="root"># </prompt>qemu-img create -o<co xml:id="co-1-minoro"/>backing_file=vmguest.raw<co xml:id="co-1-backingfile"/>,backing_fmt=raw<co xml:id="co-1-backingfmt"/>\
     -f<co xml:id="co-1-minorf"/> qcow2 vmguest.cow<co xml:id="co-1-imagename"/></screen>
   <calloutlist>
    <callout arearefs="co-1-minoro">
     <para>
      使用 <option>-o ? </option> 可获取可用选项的概述。
     </para>
    </callout>
    <callout arearefs="co-1-backingfile">
     <para>
      后备文件名。
     </para>
    </callout>
    <callout arearefs="co-1-backingfmt">
     <para>
      指定后备文件的文件格式。
     </para>
    </callout>
    <callout arearefs="co-1-minorf">
     <para>
      指定 VM Guest 的映像格式。
     </para>
    </callout>
    <callout arearefs="co-1-imagename">
     <para>
      VM Guest 的映像名称，它只会记录与后备文件的差异。
     </para>
    </callout>
   </calloutlist>
   <warning>
    <title>后备映像路径</title>
    <para>
     您不应更改后备映像的路径，否则将需要对其进行调整。路径储存在覆盖映像文件中。要更新路径，应当创建一个从原始路径到新路径的符号链接，然后使用 <command>qemu-img</command>
     <option>rebase</option> 选项。
    </para>
<screen>
<prompt role="root"># </prompt>ln -sf /var/lib/images/<replaceable>OLD_PATH</replaceable>/vmguest.raw  \
 /var/lib/images/<replaceable>NEW_PATH</replaceable>/vmguest.raw
<prompt role="root"># </prompt>qemu-img rebase<co xml:id="co-2-rebase"/> -u<co xml:id="co-2-unsafe"/> -b<co xml:id="co-2-minorb"/> \
 /var/lib/images/<replaceable>OLD_PATH</replaceable>/vmguest.raw /var/lib/images/<replaceable>NEW_PATH</replaceable>/vmguest.cow
</screen>
    <calloutlist>
     <callout arearefs="co-2-rebase">
      <para>
       <command>rebase</command> 子命令告知 <command>qemu-img</command> 更改后备文件映像。
      </para>
     </callout>
     <callout arearefs="co-2-unsafe">
      <para>
       <option>-u</option> 选项会激活不安全模式（请参见下面的注释）。<option>rebase</option> 有两种不同的运行模式：
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <emphasis>安全</emphasis>：这是默认模式，执行真正的变基操作。安全模式是一种较为耗时的操作。
        </para>
       </listitem>
       <listitem>
        <para>
         <emphasis>不安全</emphasis>：不安全模式 (<option>-u</option>) 只更改后备文件名和文件名的格式，而不检查文件的内容。如要重命名或移动后备文件，则应使用此模式。
        </para>
       </listitem>
      </itemizedlist>
     </callout>
     <callout arearefs="co-2-minorb">
        <para>
         使用 <option>-b</option> 来指定要使用的后备映像，映像路径是命令的最后一个参数。
        </para>
     </callout>
    </calloutlist>
   </warning>
   <para>
    常见的用法是使用后备文件来启动新的 Guest。假设我们有一个可以使用的 <filename>sle15_base.img</filename> VM Guest（未经任何修改的新安装）。这将是我们的后备文件。现在需要在更新后的系统和具有不同内核的系统上测试一个新软件包。我们可以使用 <filename>sle15_base.img</filename>，通过创建指向此后备文件 (<filename>sle15_base.img</filename>) 的 qcow2 覆盖文件来实例化新的 SUSE Linux Enterprise VM Guest。
   </para>
   <para>
    在本例中，我们将为更新后的系统使用 <filename>sle15_updated.qcow2</filename>，为具有不同内核的系统使用 <filename>sle15_kernel.qcow2</filename>。
   </para>
   <para>
    要创建这两个精简预配的系统，需使用带 <option>-b</option> 选项的 <command>qemu-img</command> 命令行：
   </para>
<screen><prompt role="root"># </prompt>qemu-img create -b /var/lib/libvirt/sle15_base.img -f qcow2 \
/var/lib/libvirt/sle15_updated.qcow2
Formatting 'sle15_updated.qcow2', fmt=qcow2 size=17179869184
backing_file='sle15_base.img' encryption=off cluster_size=65536
lazy_refcounts=off nocow=off
<prompt role="root"># </prompt>qemu-img create -b /var/lib/libvirt/sle15_base.img -f qcow2 \
/var/lib/libvirt/sle15_kernel.qcow2
Formatting 'sle15_kernel.qcow2', fmt=qcow2 size=17179869184
backing_file='vmguest-sle15_base.img' encryption=off cluster_size=65536
lazy_refcounts=off nocow=off</screen>
   <para>
    映像现在便可供使用，您可以在不变动初始 <filename>sle15_base.img</filename> 后备文件的情况下执行测试。所有更改都将储存在新的覆盖映像中。此外，您还可以使用这些新映像作为后备文件来创建新的覆盖。
   </para>
<screen><prompt role="root"># </prompt>qemu-img create -b sle15_kernel.qcow2 -f qcow2 sle15_kernel_TEST.qcow2</screen>
   <para>
    将 <option>--backing-chain</option> 选项与 <command>qemu-img info</command> 结合使用时，该命令将会递归返回关于整个后备链的所有信息：
   </para>
<screen><prompt role="root"># </prompt>qemu-img info --backing-chain
/var/lib/libvirt/images/sle15_kernel_TEST.qcow2
image: sle15_kernel_TEST.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: sle15_kernel.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle15_kernel.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: SLE15.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle15_base.img
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 16G
cluster_size: 65536
Format specific information:
compat: 1.1
lazy refcounts: true</screen>
   <figure xml:id="fig-qemu-img-overlay">
    <title>理解映像覆盖</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>



  <sect2 xml:id="sec-vt-best-img-open-img">
   <title>打开 VM Guest 映像</title>
   <para>
    要访问映像的文件系统，请使用 <package>guestfs-tools</package>.如果系统上没有安装此工具，您可以使用其他 Linux 工具挂载映像。应避免访问不可信或未知的 VM Guest 的映像系统，因为这可能会导致安全问题（有关详细信息，请阅读 <link xlink:href="https://www.berrange.com/posts/2013/02/20/a-reminder-why-you-should-never-mount-guest-disk-images-on-the-host-os/">D. Berrangé 发布的帖子</link>）。
   </para>
   <sect3 xml:id="sec-vt-best-img-open-img-raw">
    <title>打开 Raw 映像</title>
    <procedure>
     <title>挂载 Raw 映像</title>
     <step>
      <para>
       要能够挂载映像，请找到一台空闲的循环设备。下面的命令会显示第一台未使用的循环设备（在本例中为 <filename>/dev/loop1</filename>）。
      </para>
<screen><prompt role="root"># </prompt>losetup -f
/dev/loop1</screen>
     </step>
     <step>
      <para>
       将映像（在本例中为 <filename>SLE15.raw</filename>）与循环设备相关联：
      </para>
<screen><prompt role="root"># </prompt>losetup /dev/loop1 SLE15.raw</screen>
     </step>
     <step>
      <para>
       通过获取有关循环设备的详细信息来检查映像是否已成功与循环设备关联：
      </para>
<screen><prompt role="root"># </prompt>losetup -l
NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop1         0      0         0  0    /var/lib/libvirt/images/SLE15.raw</screen>
     </step>
     <step>
      <para>
       使用 <command>kpartx</command> 查看映像的分区：
      </para>
<screen><prompt role="root"># </prompt>kpartx -a<co xml:id="co-kpartx-a"/> -v<co xml:id="co-kpartx-v"/> /dev/loop1
add map loop1p1 (254:1): 0 29358080 linear /dev/loop1 2048</screen>
      <calloutlist>
       <callout arearefs="co-kpartx-a">
        <para>
         添加分区设备映射。
        </para>
       </callout>
       <callout arearefs="co-kpartx-v">
        <para>
         详细模式。
        </para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       现在挂载映像分区（在下面的示例中，挂载到 <filename>/mnt/sle15mount</filename>）：
      </para>
<screen><prompt role="root"># </prompt>mkdir /mnt/sle15mount
<prompt role="root"># </prompt>mount /dev/mapper/loop1p1 /mnt/sle15mount</screen>
     </step>
    </procedure>
    <note>
     <title>包含 LVM 的 Raw 映像</title>
     <para>
      如果 raw 映像包含 LVM 卷组，您应当使用 LVM 工具挂载分区。有关详细信息，请参见 <xref linkend="sec-lvm-found"/>.
     </para>
    </note>
    <procedure>
     <title>卸载 Raw 映像</title>
     <step>
      <para>
       卸载映像的所有已挂载分区，例如：
      </para>
<screen><prompt role="root"># </prompt>umount /mnt/sle15mount</screen>
     </step>
     <step xml:id="st-umount-raw">
      <para>
       使用 <command>kpartx</command> 删除分区设备映射：
      </para>
<screen><prompt role="root"># </prompt>kpartx -d /dev/loop1</screen>
     </step>
     <step>
      <para>
       使用 <command>losetup</command> 解除设备关联：
      </para>
<screen><prompt role="root"># </prompt>losetup -d /dev/loop1</screen>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec-vt-best-img-open-img-qcow2">
    <title>打开 qcow2 映像</title>
    <procedure>
     <title>挂载 qcow2 映像</title>
     <step>
      <para>
       首先，您需要装载 <literal>nbd</literal>（网络块设备）模块。下面的示例装载了该模块并支持 16 个块设备 (<option>max_part=16</option>)。使用 <command>dmesg</command> 检查操作是否成功：
      </para>
<screen><prompt role="root"># </prompt>modprobe nbd max_part=16
<prompt role="root"># </prompt>dmesg | grep nbd
[89155.142425] nbd: registered device at major 43</screen>
     </step>
     <step>
      <para>
       使用 <command>qemu-nbd</command> 命令将 VM Guest 映像（例如 <filename>SLE15.qcow2</filename>）与 NBD 设备（在下面的示例中为 <filename>/debv/nbd0</filename>）相连。请务必使用空闲的 NBD 设备：
      </para>
<screen><prompt role="root"># </prompt>qemu-nbd -c<co xml:id="co-qemunbd-minusc"/> /dev/nbd0<co xml:id="co-qemunbd-device"/> SLE15.qcow2<co xml:id="co-qemunbd-image"/></screen>
      <calloutlist>
       <callout arearefs="co-qemunbd-minusc">
        <para>
         将 <filename>SLE15.qcow2</filename> 与本地 NBD 设备 <filename>/dev/nbd0</filename> 相连
        </para>
       </callout>
       <callout arearefs="co-qemunbd-device">
        <para>
         要使用的 NBD 设备
        </para>
       </callout>
       <callout arearefs="co-qemunbd-image">
        <para>
         要使用的 VM Guest 映像
        </para>
       </callout>
      </calloutlist>
      <tip>
       <title>检查是否有空闲的 NBD 设备</title>
       <para>
        要检查 NBD 设备是否空闲，请运行以下命令：
       </para>
<screen><prompt role="root"># </prompt>lsof /dev/nbd0
COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF  NODE NAME
qemu-nbd 15149 root   10u   BLK   43,0      0t0 47347 /dev/nbd0</screen>
       <para>
        如果命令生成如上所示的输出，则表示设备处于忙碌状态（非空闲）。还可以通过查看是否存在 <filename>/sys/devices/virtual/block/nbd0/pid</filename> 文件来确认设备是否空闲。
       </para>
      </tip>
     </step>
     <step>
      <para>
       使用 <command>partprobe</command> 通知操作系统有关分区表的更改：
      </para>
<screen><prompt role="root"># </prompt>partprobe /dev/nbd0 -s
/dev/nbd0: msdos partitions 1 2
<prompt role="root"># </prompt>dmesg | grep nbd0 | tail -1
[89699.082206]  nbd0: p1 p2</screen>
     </step>
     <step>
      <para>
       在上面的示例中，<filename>SLE15.qcow2</filename> 包含 <filename>/dev/nbd0p1</filename> 和 <filename>/dev/nbd0p2</filename> 这两个分区。在挂载这些分区之前，使用 <command>vgscan</command> 检查它们是否属于 LVM 卷：
      </para>
<screen><prompt role="root"># </prompt>vgscan -v
    Wiping cache of LVM-capable devices
    Wiping internal VG cache
    Reading all physical volumes. This may take a while...
    Using volume group(s) on command line.
    No volume groups found.</screen>
     </step>
     <step>
      <para>
       如果没有找到 LVM 卷，可以使用 <command>mount</command> 挂载分区：
      </para>
<screen><prompt role="root"># </prompt>mkdir /mnt/nbd0p2
# mount /dev/nbd0p1 /mnt/nbd0p2</screen>
      <para>
       有关如何处理 LVM 卷的信息，请参见<xref linkend="sec-lvm-found"/>。
      </para>
     </step>
    </procedure>
    <procedure>
     <title>卸载 qcow2 映像</title>
     <step>
      <para>
       卸载映像的所有已挂载分区，例如：
      </para>
<screen><prompt role="root"># </prompt>umount /mnt/nbd0p2</screen>
     </step>
     <step xml:id="st-umount-qcow2">
      <para>
       断开映像与 <filename>/dev/nbd0</filename> 设备的连接。
      </para>
<screen><prompt role="root"># </prompt>qemu-nbd -d /dev/nbd0</screen>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec-lvm-found">
    <title>打开包含 LVM 的映像</title>
    <warning>
     <para>
      如果 VM 主机服务器使用的 VG 名称为 <literal>system</literal>，而 Guest 映像使用的 VG 名称也是 <literal>system</literal>，LVM 将在其激活过程中报错。一种变通方法是暂时重命名 Guest VG，而正确的做法是为 Guest 使用与 VM 主机服务器不同的 VG 名称。
     </para>
    </warning>
    <procedure>
     <title>挂载包含 LVM 的映像</title>
     <step>
      <para>
       要检查映像是否包含 LVM 组，请使用 <command>vgscan -v</command>。如果映像包含 LVM 组，命令的输出将如下所示：
      </para>
<screen><prompt role="root"># </prompt>vgscan -v
Wiping cache of LVM-capable devices
Wiping internal VG cache
Reading all physical volumes.  This may take a while...
Finding all volume groups
Finding volume group "system"
Found volume group "system" using metadata type lvm2</screen>
     </step>
     <step>
      <para>
       在系统上找到了 <literal>system</literal> LVM 卷组。您可以使用 <command>vgdisplay <replaceable>VOLUMEGROUPNAME</replaceable></command>（在此处的示例中，<replaceable>VOLUMEGROUPNAME</replaceable> 为 <literal>system</literal>）获取有关此卷的详细信息。应当激活这个卷组以将 LVM 分区作为设备公开，这样系统便能挂载它们。使用 <command>vgchange</command>：
      </para>
<screen><prompt role="root"># </prompt>vgchange -ay -v
Finding all volume groups
Finding volume group "system"
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/home
Creating system-home
Loading system-home table (254:0)
Resuming system-home (254:0)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/root
Creating system-root
Loading system-root table (254:1)
Resuming system-root (254:1)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/swap
Creating system-swap
Loading system-swap table (254:2)
Resuming system-swap (254:2)
Activated 3 logical volumes in volume group system
    3 logical volume(s) in volume group "system" now active</screen>
     </step>
     <step>
      <para>
       卷组中的所有分区都列于 <filename>/dev/mapper</filename> 目录中。现在即可挂载它们。
      </para>
<screen><prompt role="root"># </prompt>ls /dev/mapper/system-*
/dev/mapper/system-home  /dev/mapper/system-root /dev/mapper/system-swap

<prompt role="root"># </prompt>mkdir /mnt/system-root
<prompt role="root"># </prompt>mount  /dev/mapper/system-root /mnt/system-root

<prompt role="root"># </prompt>ls /mnt/system-root/
bin   dev  home  lib64       mnt  proc        root  sbin     srv  tmp  var
boot  etc  lib   lost+found  opt  read-write  run   selinux  sys  usr</screen>
     </step>
    </procedure>
    <procedure>
     <title>卸载包含 LVM 的映像</title>
     <step>
      <para>
       使用 <command>umount</command> 卸载所有分区
      </para>
<screen><prompt role="root"># </prompt>umount /mnt/system-root</screen>
     </step>
     <step>
      <para>
       使用 <command>vgchange -an <replaceable>VOLUMEGROUPNAME</replaceable></command> 停用 LVM 卷组
      </para>
<screen><prompt role="root"># </prompt>vgchange -an -v system
Using volume group(s) on command line
Finding volume group "system"
Found volume group "system"
Removing system-home (254:0)
Found volume group "system"
Removing system-root (254:1)
Found volume group "system"
Removing system-swap (254:2)
Deactivated 3 logical volumes in volume group system
0 logical volume(s) in volume group "system" now active</screen>
     </step>
     <step>
      <para>
       现在有以下两种选择：
      </para>
      <itemizedlist>
       <listitem>
        <para>
         如果是 qcow2 映像，按<xref linkend="st-umount-qcow2"/> (<command>qemu-nbd -d /dev/nbd0</command>) 中所述继续。
        </para>
       </listitem>
       <listitem>
        <para>
         如果是 raw 映像，按<xref linkend="st-umount-raw"/>（<command>kpartx -d /dev/loop1</command>、<command>losetup -d /dev/loop1</command>）中所述继续。
        </para>
       </listitem>
      </itemizedlist>
      <important>
       <title>检查卸载是否成功</title>
       <para>
        您应使用 <command>losetup</command>、<command>qemu-nbd</command>、<command>mount</command> 或 <command>vgscan</command> 等系统命令确认卸载是否成功。如果没有成功卸载，您在使用 VM Guest 时可能会遇到问题，因为它的系统映像会在不同的地方使用。
       </para>
      </important>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-img-share">
   <title>文件系统共享</title>
   <para>
    您可以使用 <tag class="element">filesystem</tag> 元素访问 VM Guest 中的主机目录。在下面的示例中，我们将共享 <filename>/data/shared</filename> 目录并将其挂载 VM Guest 中。请注意，<tag class="attribute">accessmode</tag> 参数仅适用于 QEMU/KVM 驱动程序的 <tag class="attribute">type='mount'</tag>（<tag class="attribute">type</tag> 的大多数其他值仅可用于 LXC 驱动程序）。
   </para>
<screen>&lt;filesystem type='mount'<co xml:id="co-fs-mount"/> accessmode='mapped'<co xml:id="co-fs-mode"/>&gt;
   &lt;source dir='/data/shared'<co xml:id="co-fs-sourcedir"/>&gt;
   &lt;target dir='shared'<co xml:id="co-fs-targetdir"/>/&gt;
&lt;/filesystem&gt;</screen>
   <calloutlist>
    <callout arearefs="co-fs-mount">
     <para>
      要挂载 VM Guest 的主机目录。
     </para>
    </callout>
    <callout arearefs="co-fs-mode">
     <para>
      访问模式（安全模式）设置为 <literal>mapped</literal> 将为访问提供超级管理程序的权限。使用 <literal>passthrough</literal> 会以 VM Guest 中用户的权限访问此共享。
     </para>
    </callout>
    <callout arearefs="co-fs-sourcedir">
     <para>
      要与 VM Guest 共享的路径。
     </para>
    </callout>
    <callout arearefs="co-fs-targetdir">
     <para>
      挂载命令的路径的名称或标签。
     </para>
    </callout>
   </calloutlist>
   <para>
    要在 VM Guest 上挂载 <literal>shared</literal> 目录，请使用以下命令：在 VM Guest 下，现在需要挂载 <literal>target dir=&apos;shared&apos;</literal>：
   </para>
<screen><prompt role="root"># </prompt>mkdir /opt/mnt_shared
<prompt role="root"># </prompt>mount shared -t 9p /opt/mnt_shared -o trans=virtio</screen>
   <para>
    有关详细信息，请参见 <link xlink:href="https://libvirt.org/formatdomain.html#elementsFilesystems"><systemitem class="library">libvirt</systemitem> 文件系统</link>和 <link xlink:href="http://wiki.qemu.org/Documentation/9psetup">QEMU 9psetup</link>。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-vt-best-vmguests">
  <title>VM Guest 配置</title>

  <sect2 xml:id="sec-vt-best-vmguests-virtio">
   <title>Virtio 驱动程序</title>
   <para>
    为了提高 VM Guest 性能，建议在 VM Guest 中使用半虚拟化驱动程序。此类用于 KVM 的驱动程序的虚拟化标准是 <literal>virtio</literal> 驱动程序，它专为在虚拟环境中运行而设计。Xen 使用类似的半虚拟化设备驱动程序（例如 Windows* Guest 中的 <link xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link>）。
  </para>
   <sect3 xml:id="sec-vt-best-vmguests-virtio-virtio-blk">
    <title><literal>virtio blk</literal></title>
    <para>
     <literal>virtio_blk</literal> 是磁盘的 virtio 块设备。要为块设备使用 <literal>virtio blk</literal> 驱动程序，请在 <tag class="element">disk</tag> 定义中指定 <tag class="attribute">bus='virtio'</tag> 属性：
    </para>
<screen>&lt;disk type='....' device='disk'&gt;
    ....
    &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</screen>
    <important>
     <title>磁盘设备名称</title>
     <para>
      <literal>virtio</literal> 磁盘设备命名为 <literal>/dev/vd[a-z][1-9]</literal>。如果从非 virtio 磁盘迁移 Linux Guest，您需要调整 GRUB 配置中的 <literal>root=</literal> 参数，并重新生成 <filename>initrd</filename> 文件，否则系统将无法引导。在运行其他操作系统的 VM Guest 上，可能还需要相应地调整或重新安装引导加载程序。
     </para>
    </important>
    <important>
     <title>通过 <command>qemu-system-ARCH</command> 使用 <literal>virtio</literal> 磁盘</title>
     <para>
      运行 <command>qemu-system-ARCH</command> 时，请使用 <option>-drive</option> 选项来向 VM Guest 添加磁盘。请参见<xref linkend="cha-qemu-guest-inst-qemu-kvm"/>中的示例。<option>-hd[abcd]</option> 选项不适用于 virtio 磁盘。
     </para>
    </important>
   </sect3>
   <sect3 xml:id="sec-vt-best-vmguests-virtio-virtio-net">
    <title>virtio net</title>
    <para>
     <literal>virtio_net</literal> 是 virtio 网络设备。内核模块应在 Guest 引导时自动在其中装载。您需要启动该服务以使网络可用。
    </para>
<screen>&lt;interface type='network'&gt;
    ...
    &lt;model type='virtio' /&gt;
&lt;/interface&gt;</screen>
   </sect3>
   <sect3 xml:id="sec-vt-best-vmguests-virtio-balloon">
    <title>virtio balloon</title>
    <para>
     virtio balloon 用于对 Guest 实施主机内存过量使用。对于 Linux Guest，气球驱动程序在 Guest 内核中运行；而对于 Windows Guest，气球驱动程序包含在 VMDP 软件包中。<literal>virtio_balloon</literal> 是用于从 VM Guest 提供或获取内存的 PV 驱动程序。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>充气气球</emphasis>：将内存从 Guest 返回到主机内核（对于 KVM）或超级管理程序（对于 Xen）
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>放气气球</emphasis>：Guest 将有更多可用内存
      </para>
     </listitem>
    </itemizedlist>
    <para>
     它由 <literal>currentMemory</literal> 和 <literal>memory</literal> 选项控制。
    </para>
<screen>&lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;1048576&lt;/currentMemory&gt;
    [...]
    &lt;devices&gt;
        &lt;memballoon model='virtio'/&gt;
    &lt;/devices&gt;</screen>
    <para>
     您也可以使用 <command>virsh</command> 来更改它：
    </para>
<screen><prompt>&gt; </prompt>virsh setmem <replaceable>DOMAIN_ID</replaceable> <replaceable>MEMORY in KB</replaceable></screen>
   </sect3>
   <sect3 xml:id="sec-vt-best-vmguests-virtio-check">
    <title>检查 virtio 是否存在</title>
    <para>
     可以使用以下命令检查 virtio 块 PCI：
    </para>
<screen><prompt>&gt; </prompt>find /sys/devices/ -name virtio*
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2</screen>
    <para>
     要查找与 <filename>vdX</filename> 关联的块设备，请运行以下命令：
    </para>
<screen><prompt>&gt; </prompt>find /sys/devices/ -name virtio* -print  -exec ls {}/block 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2
vda</screen>
    <para>
     要获取有关 virtio 块的详细信息，请运行以下命令：
    </para>
<screen><prompt>&gt; </prompt>udevadm info -p /sys/devices/pci0000:00/0000:00:08.0/virtio2
P: /devices/pci0000:00/0000:00:08.0/virtio2
E: DEVPATH=/devices/pci0000:00/0000:00:08.0/virtio2
E: DRIVER=virtio_blk
E: MODALIAS=virtio:d00000002v00001AF4
E: SUBSYSTEM=virtio</screen>
    <para>
     要检查所有正在使用的 virtio 驱动程序，请运行以下命令：
    </para>
<screen><prompt>&gt; </prompt>find /sys/devices/ -name virtio* -print  -exec ls -l {}/driver 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
lrwxrwxrwx 1 root root 0 Jun 17 15:48 /sys/devices/pci0000:00/0000:00:06.0/virtio0/driver -&gt; ../../../../bus/virtio/drivers/virtio_console
/sys/devices/pci0000:00/0000:00:07.0/virtio1
lrwxrwxrwx 1 root root 0 Jun 17 15:47 /sys/devices/pci0000:00/0000:00:07.0/virtio1/driver -&gt; ../../../../bus/virtio/drivers/virtio_balloon
/sys/devices/pci0000:00/0000:00:08.0/virtio2
lrwxrwxrwx 1 root root 0 Jun 17 14:35 /sys/devices/pci0000:00/0000:00:08.0/virtio2/driver -&gt; ../../../../bus/virtio/drivers/virtio_blk</screen>
   </sect3>

   <sect3 xml:id="sec-vt-best-vmguests-virtio-drv-opt">
    <title>查找设备驱动程序选项</title>
    <para>
     Virtio 设备和其他驱动程序具有各种选项。要列出所有选项，请使用 <command>qemu-system-ARCH</command> 命令的 <option>help</option> 参数。
    </para>
<screen><prompt>&gt; </prompt>qemu-system-x86_64 -device virtio-net,help
virtio-net-pci.ioeventfd=on/off
virtio-net-pci.vectors=uint32
virtio-net-pci.indirect_desc=on/off
virtio-net-pci.event_idx=on/off
virtio-net-pci.any_layout=on/off
.....</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-perf-cirrus">
   <title>Cirrus 视频驱动程序</title>
   <para>
    为了获得 16 位色、高兼容性和更佳性能，建议使用 <literal>cirrus</literal> 视频驱动程序。
   </para>
   <note>
    <title><systemitem class="library">libvirt</systemitem></title>
    <para>
     <systemitem class="library">libvirt</systemitem> 会忽略 <literal>vram</literal> 值，因为视频大小已硬编码在 QEMU 中。
    </para>
   </note>
<screen>&lt;video&gt;
   &lt;model type='cirrus' vram='9216' heads='1'/&gt;
&lt;/video&gt;</screen>
  </sect2>

  <sect2 xml:id="sec-vt-best-entropy">
   <title>更好的熵</title>
   <para>
    Virtio RNG（随机数字生成器）是作为硬件 RNG 设备向 Guest 公开的半虚拟化设备。在主机端，如果没有硬件支持，可以将其连接到多个熵来源之一（包括一台真正的硬件 RNG 设备和主机的 <filename>/dev/random</filename>）。Linux 内核包含 2.6.26 和更高版本的设备的 Guest 驱动程序。
   </para>
   <para>
    系统熵收集自各种非确定性硬件事件，主要用于加密应用程序。虚拟随机数字生成器设备（半虚拟化设备）允许主机将熵传递到 VM Guest 操作系统。这将在 VM Guest 中产生更好的熵。
   </para>
   <para>
    要使用 Virtio RNG，请在 <command>virt-manager</command> 中或直接在 VM Guest 的 XML 配置中添加 <literal>RNG</literal> 设备：
   </para>
<screen>&lt;devices&gt;
   &lt;rng model='virtio'&gt;
       &lt;backend model='random'&gt;/dev/random&lt;/backend&gt;
   &lt;/rng&gt;
&lt;/devices&gt;</screen>
   <para>
    主机现在应该在使用 <filename>/dev/random</filename>：
   </para>
<screen><prompt>&gt; </prompt>lsof /dev/random
qemu-syst 4926 qemu    6r   CHR    1,8      0t0 8199 /dev/random</screen>
   <para>
    在 VM Guest 上，可使用以下命令检查熵的来源：
   </para>
<screen><prompt>&gt; </prompt>cat /sys/devices/virtual/misc/hw_random/rng_available</screen>
   <para>
    可使用以下命令检查当前用于熵的设备：
   </para>
<screen><prompt>&gt; </prompt>cat /sys/devices/virtual/misc/hw_random/rng_current
virtio_rng.0</screen>
   <para>
    您应在 VM Guest 上安装 <package>rng-tools</package> 软件包，启用该服务，然后将其启动。在 <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise Server</phrase></phrase> 15 下，执行以下命令：
   </para>
<screen><prompt role="root"># </prompt>zypper in rng-tools
<prompt role="root"># </prompt>systemctl enable rng-tools
<prompt role="root"># </prompt>systemctl start rng-tools</screen>
  </sect2>

  <sect2 xml:id="sec-vt-best-perf-disable">
   <title>禁用未使用的工具和设备</title>
   <para>
    对每台主机应仅使用一种虚拟化技术。例如，不要在同一台主机上使用 KVM 和 Xen。否则可能会出现可用资源减少、安全风险增加，以及软件更新队列变长的问题。即使仔细配置了分配给每种技术的资源量，主机也可能会出现整体可用性降低和性能下降的情况。
   </para>
   <para>
    应尽量减少主机上可用的软件和服务的数量。大多数默认安装的操作系统都没有针对 VM 的使用进行优化。因此请安装真正需要的组件，并去除 VM Guest 中的所有其他组件。
   </para>
   <para>
    Windows* Guest：
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      禁用屏幕保护程序
     </para>
    </listitem>
    <listitem>
     <para>
      去除所有图像效果
     </para>
    </listitem>
    <listitem>
     <para>
      禁用硬盘索引（如不需要）
     </para>
    </listitem>
    <listitem>
     <para>
      检查已启动的服务列表并禁用不需要的服务
     </para>
    </listitem>
    <listitem>
     <para>
      检查并去除所有不需要的设备
     </para>
    </listitem>
    <listitem>
     <para>
      禁用系统更新（如不需要），或对其进行配置以避免重引导或关闭主机时出现任何延迟
     </para>
    </listitem>
    <listitem>
     <para>
      检查防火墙规则
     </para>
    </listitem>
    <listitem>
     <para>
      适当地安排备份和防病毒更新
     </para>
    </listitem>
    <listitem>
     <para>
      安装 <link xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link> 半虚拟化驱动程序以获得最佳性能
     </para>
    </listitem>
    <listitem>
     <para>
      查看操作系统建议，例如<link xlink:href="http://windows.microsoft.com/en-us/windows/optimize-windows-better-performance#optimize-windows-better-performance=windows-7">优化 Windows 以提高性能</link>网页上的建议。
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Linux Guest：
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      去除或不要启动 X Window 系统（如不需要）
     </para>
    </listitem>
    <listitem>
     <para>
      检查已启动的服务列表并禁用不需要的服务
     </para>
    </listitem>
    <listitem>
     <para>
      查看有关可实现更佳性能的内核参数的操作系统建议
     </para>
    </listitem>
    <listitem>
     <para>
      仅安装真正需要的软件
     </para>
    </listitem>
    <listitem>
     <para>
      优化可预测任务（系统更新、硬盘检查等）的调度
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec-vt-best-perf-mtype">
   <title>更新 Guest 计算机类型</title>
   <para>
    QEMU 计算机类型定义与迁移和会话管理尤为相关的体系结构细节。随着对 QEMU 的更改或改进，新的计算机类型将会增加。出于兼容性原因，旧计算机类型仍然受支持，但是为了利用改进的功能，我们建议在升级时一律迁移到最新的计算机类型。
   </para>
   <para>
    为 Linux Guest 更改 Guest 的计算机类型基本上是透明的。对于 Windows* Guest，我们建议截取 Guest 的快照或备份 Guest，以防出现 Windows* 在其检测到的更改的计算机类型上运行时有问题，随后用户决定还原到创建 Guest 时使用的原始计算机类型的情况。
   </para>
   <note>
    <title>更改计算机类型</title>
    <para>
     相关文档，请参见<xref linkend="sec-libvirt-config-machinetype-virsh"/>。
    </para>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-vt-best-vm-setup-config">
  <title>特定于 VM Guest 的配置和设置</title>

  <note>
   <para>
    本节内容仅适用于 QEMU / KVM 超级管理程序。
   </para>
  </note>

  <sect2 xml:id="sec-vt-best-acpi">
   <title>ACPI 测试</title>
   <para>
    能否更改 VM Guest 的状态在很大程度上取决于操作系统。在生产环境中使用 VM Guest 之前，务必要测试此功能。例如，大多数 Linux 操作系统默认会禁用此功能，因此需要启用此操作（通常通过 PolKit）。
   </para>
   <para>
    必须在 Guest 中启用 ACPI 才能正常关机。要检查是否已启用 ACPI，请运行以下命令：
   </para>
<screen><prompt>&gt; </prompt>virsh dumpxml <replaceable>VMNAME</replaceable> | grep acpi</screen>
   <para>
    如果没有列显任何内容，则说明计算机上没有启用 ACPI。使用 <command>virsh edit</command> 在 XML 的 &lt;domain&gt; 下添加以下内容：
   </para>
<screen>&lt;features&gt;
   &lt;acpi/&gt;
&lt;/features&gt;</screen>
   <para>
    如果 ACPI 是在 Windows Server* Guest 安装期间启用的，那么仅在 VM Guest 配置中开启它并不够。有关详细信息，请参见 <link xlink:href="https://support.microsoft.com/en-us/kb/309283"/>。
    <remark>FIXME: URL is for XP and Server 2003. Is there something newer? -
    sknorr, 2017-06-07</remark>
   </para>


   <para>
    无论 VM Guest 的配置如何，始终都可以从 Guest 操作系统内部实现正常关机。
   </para>
  </sect2>

  <sect2 xml:id="sec-vt-best-guest-kbd">
   <title>键盘布局</title>
   <para>
    尽管您可以使用 <command>qemu-system-ARCH</command> 命令来指定键盘布局，但仍建议在 <systemitem class="library">libvirt</systemitem> XML 文件中配置键盘布局。要在使用 vnc 连接到远程 VM Guest 时更改键盘布局，应编辑 VM Guest XML 配置文件。例如，要添加 <literal>en-us</literal> 键映射，请在 <literal>&lt;devices&gt;</literal> 部分添加：
   </para>
<screen>&lt;graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/&gt;</screen>
   <para>
    检查 <literal>vncdisplay</literal> 配置并连接到 VM Guest：
   </para>
<screen><prompt>&gt; </prompt>virsh vncdisplay sles15 127.0.0.1:0</screen>
  </sect2>

  <sect2 xml:id="sec-vt-best-spice-default-url">
   <title>Spice 默认监听 URL</title>
   <para>
    如果主机上除了 <literal>lo</literal> 以外没有为其他网络接口指派 IPv4 地址，spice 服务器监听的默认地址将不起作用。将会出现如下错误：
   </para>
<screen><prompt>&gt; </prompt>virsh start sles15
error: Failed to start domain sles15
error: internal error: process exited while connecting to monitor: ((null):26929): Spice-Warning **: reds.c:2330:reds_init_socket: getaddrinfo(127.0.0.1,5900): Address family for hostname not supported
2015-08-12T11:21:14.221634Z qemu-system-x86_64: failed to initialize spice server</screen>
   <para>
    要解决此问题，可以将 <filename>/etc/libvirt/qemu.conf</filename> 中 <literal>spice_listen</literal> 的默认值更改为本地 IPv6 地址 <systemitem class="ipaddress">::1</systemitem>。您也可以为每个 VM Guest 更改 spice 服务器监听地址，使用 <command>virsh edit</command> 向 <literal>graphics type=&apos;spice&apos;</literal> 元素添加监听 XML 属性：
   </para>
<screen>&lt;graphics type='spice' listen='::1' autoport='yes'/&gt;&gt;</screen>
  </sect2>

  <sect2 xml:id="sec-vt-best-xml-to-qemu">
   <title>XML 到 QEMU 命令行</title>
   <para>
    有时，使用 QEMU 命令行从 XML 文件启动 VM Guest 可能很有用。
   </para>
<screen><prompt>&gt; </prompt>virsh domxml-to-native<co xml:id="co-domxml-native"/> qemu-argv<co xml:id="co-domxml-argv"/> SLE15.xml<co xml:id="co-domxml-file"/></screen>
   <calloutlist>
    <callout arearefs="co-domxml-native">
     <para>
      将域 XML 格式的 XML 文件转换为本机 Guest 配置
     </para>
    </callout>
    <callout arearefs="co-domxml-argv">
     <para>
      对于 QEMU/KVM 超级管理程序，格式参数需要设为 qemu-argv
     </para>
    </callout>
    <callout arearefs="co-domxml-file">
     <para>
      要使用的域 XML 文件
     </para>
    </callout>
   </calloutlist>
<screen><prompt>&gt; </prompt><command>sudo</command> virsh domxml-to-native qemu-argv /etc/libvirt/qemu/SLE15.xml
LC_ALL=C PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
   QEMU_AUDIO_DRV=none /usr/bin/qemu-system-x86_64 -name SLE15 -machine \
   pc-i440fx-2.3,accel=kvm,usb=off -cpu SandyBridge -m 4048 -realtime \
   mlock=off -smp 4,sockets=4,cores=1,threads=1 -uuid 8616d00f-5f05-4244-97cc-86aeaed8aea7 \
   -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/SLE15.monitor,server,nowait \
   -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew \
   -global kvm-pit.lost_tick_policy=discard -no-hpet \
   -no-shutdown -global PIIX4_PM.disable_s3=1 -global PIIX4_PM.disable_s4=1 \
   -boot strict=on -device ich9-usb-ehci1,id=usb,bus=pci.0,addr=0x4.0x7 \
   -device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,multifunction=on,addr=0x4 \
   -device ich9-usb-uhci2,masterbus=usb.0,firstport=2,bus=pci.0,addr=0x4.0x1 \
   -device ich9-usb-uhci3,masterbus=usb.0,firstport=4,bus=pci.0,addr=0x4.0x2 \
   -drive file=/var/lib/libvirt/images/SLE15.qcow2,if=none,id=drive-virtio-disk0,format=qcow2,cache=none \
   -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=2 \
   -drive if=none,id=drive-ide0-0-1,readonly=on,format=raw  \
   -device ide-cd,bus=ide.0,unit=1,drive=drive-ide0-0-1,id=ide0-0-1 -netdev tap,id=hostnet0  \
   -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:28:04:a9,bus=pci.0,addr=0x3,bootindex=1 \
   -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 \
   -vnc 127.0.0.1:0 -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 \
   -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on</screen>
  </sect2>

  <sect2 xml:id="sec-vt-best-kernel-parameter">
   <title>在引导时更改内核参数</title>
   <sect3 xml:id="sec-vt-best-kernel-parameter-sle11">
    <title>SUSE Linux Enterprise 11</title>
    <para>
     要在引导时更改 SLE 11 产品的值，需要修改 <filename>/boot/grub/menu.lst</filename> 文件，在其中添加 <option>OPTION=parameter</option>。然后重引导系统。
    </para>
   </sect3>
   <sect3 xml:id="sec-vt-best-kernel-parameter-sle12">
    <title>SUSE Linux Enterprise 12 和 15</title>
    <para>
     要在引导时更改 SLE 12 和 15 产品的值，需要修改 <filename>/etc/default/grub</filename> 文件。查找以 <option>GRUB_CMDLINE_LINUX_DEFAULT</option> 开头的变量，然后在末尾添加 <option>OPTION=parameter</option>（如已存在，则更改为使用正确的值）。
    </para>
    <para>
     现在需要重新生成 <literal>grub2</literal> 配置：
    </para>
<screen># grub2-mkconfig -o /boot/grub2/grub.cfg</screen>
    <para>
     然后重引导系统。
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-guest-device-to-xml">
   <title>在 XML 配置中添加设备</title>
   <para>
    要基于 XML 文件创建新的 VM Guest，可使用特殊标记 <literal>qemu:commandline</literal> 来指定 QEMU 命令行。例如，要添加 virtio-balloon-pci，请在 XML 配置文件的末尾（&lt;/domain&gt; 标记之前）添加此块：
   </para>
<screen>&lt;qemu:commandline&gt;
    &lt;qemu:arg value='-device'/&gt;
    &lt;qemu:arg value='virtio-balloon-pci,id=balloon0'/&gt;
&lt;/qemu:commandline&gt;</screen>
  </sect2>
  <sect2 xml:id="sec-vm-guest-vcpu">
   <title>添加和去除 CPU</title>
   <para>
    某些虚拟化环境允许在运行虚拟机的同时添加或去除 CPU。
   </para>
   <para>
    要安全去除 CPU，请先执行以下命令将这些 CPU 停用
   </para>
   <screen><prompt role="root"># </prompt><command>echo 0 &gt; /sys/devices/system/cpu/cpu<replaceable>X</replaceable>/online</command></screen>
   <para>
    请将 <replaceable>X</replaceable> 替换为 CPU 编号。要使 CPU 重新联机，请执行
   </para>
<screen><prompt role="root"># </prompt><command>echo 1 &gt; /sys/devices/system/cpu/cpu<replaceable>X</replaceable>/online</command></screen>
 </sect2>
 </sect1>






 <sect1 xml:id="sec-vt-best-refs">
  <title>更多信息</title>

  <para/>

  <itemizedlist>
   <listitem>
    <para>
     <link xlink:href="https://kernel.org/doc/ols/2009/ols2009-pages-19-28.pdf">使用 KSM 增加内存密度</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://www.linux-kvm.org/page/KSM">linux-kvm.org KSM</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://www.kernel.org/doc/Documentation/vm/ksm.txt">KSM 的内核文档</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/329123/">ksm - 适用于 linux v4 的动态页共享驱动程序</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://www.espenbraastad.no/post/memory-ballooning/">内存气球</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://wiki.libvirt.org/page/Virtio">libvirt virtio</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://www.kernel.org/doc/html/latest/block/bfq-iosched.html">BFQ（预算公平队列）</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://www.kernel.org/doc/Documentation/sysctl/kernel.txt">sysctl 的文档</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/525459/">LWN 随机数字</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://wiki.mikejung.biz/KVM_/_Xen">KVM / Xen 调整</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://events.linuxfoundation.org/sites/events/files/slides/CloudOpen2013_Khoa_Huynh_v3.pdf">Khoa Huynh 博士，IBM Linux 技术中心</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/plain/Documentation/admin-guide/kernel-parameters.txt">内核参数</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/374424/">大页管理 (Mel Gorman)</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt">内核 hugetlbpage</link>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</article>
