<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
                 type="text/xml"
                 title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<article xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="article-administration-slemicro" xml:lang="en">
 <title>&admin;</title>
 <info>
  <productnumber>&productnumber;</productnumber><productname>&productname;</productname><date>
<?dbtimestamp format="B d, Y"?></date>
  <abstract>
   <para>
    This guide describes the administration of &slem;.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <section xml:id="sec-snapshots">
  <title>Snapshots</title>
  <warning>
   <title>Snapshots are mandatory</title>
   <para>
    As snapshots are crucial for the correct functioning of &slema;, do not
    disable the feature, and ensure that the root partition is big enough to
    store the snapshots.
   </para>
  </warning>
  <para>
   When a snapshot is created, both the snapshot and the original point to the
   same blocks in the file system. So, initially a snapshot does not occupy
   additional disk space. If data in the original file system is modified,
   changed data blocks are copied while the old data blocks are kept for the
   snapshot.
  </para>
  <para>
   Snapshots always reside on the same partition or subvolume on which the
   snapshot has been taken. It is not possible to store snapshots on a
   different partition or subvolume. As a result, partitions containing
   snapshots need to be larger than partitions which do not contain snapshots.
   The exact amount depends strongly on the number of snapshots you keep and
   the amount of data modifications. As a rule of thumb, give partitions twice
   as much space as you normally would. To prevent disks from running out of
   space, old snapshots are automatically cleaned up.
  </para>
  <para>
   Snapshots that are known to be working properly are marked as
   <emphasis>important</emphasis>.
  </para>
  <section xml:id="sec-exclude-directories">
   <title>Directories excluded from snapshots</title>
   <para>
    As some directories store user-specific or volatile data, these directories
    are excluded from snapshots:
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>/home</filename></term>
     <listitem>
      <para>
       Contains users' data. Excluded so that the data will not be included in
       snapshots and thus potentially overwritten by a rollback operation.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/root</filename></term>
     <listitem>
      <para>
       Contains root's data. Excluded so that the data will not be included in
       snapshots and thus potentially overwritten by a rollback operation.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/opt</filename></term>
     <listitem>
      <para>
       Third-party products usually get installed to <filename>/opt</filename>.
       Excluded so that these applications are not uninstalled during
       rollbacks.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv</filename></term>
     <listitem>
      <para>
       Contains data for Web and FTP servers. Excluded in order to avoid data
       loss on rollbacks.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/usr/local</filename></term>
     <listitem>
      <para>
       This directory is used when manually installing software. It is excluded
       to avoid uninstalling these installations on rollbacks.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/var</filename></term>
     <listitem>
      <para>
       This directory contains many variable files, including logs, temporary
       caches, third-party products in <filename>/var/opt</filename>, and is
       the default location for virtual machine images and databases.
       Therefore, a separate subvolume is created with Copy-On-Write disabled,
       so as to exclude all of this variable data from snapshots.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/tmp</filename></term>
     <listitem>
      <para>
       The directory contains temporary data.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>the architecture-specific <filename>/boot/grub2</filename> directory</term>
     <listitem>
      <para>
       Rollback of the boot loader binaries is not supported.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </section>
  <section xml:id="sec-snapshot-size">
   <title>Showing exclusive disk space used by snapshots</title>
   <para>
    Snapshots share data, for efficient use of storage space, so using ordinary
    commands like <command>du</command> and <command>df</command> won't measure
    used disk space accurately. When you want to free up disk space on Btrfs
    with quotas enabled, you need to know how much exclusive disk space is used
    by each snapshot, rather than shared space. The <command>btrfs</command>
    command provides a view of space used by snapshots:
   </para>
<screen>
&prompt.root;btrfs qgroup show -p /
qgroupid         rfer         excl parent  
--------         ----         ---- ------  
0/5          16.00KiB     16.00KiB ---     
[...]    
0/272         3.09GiB     14.23MiB 1/0     
0/273         3.11GiB    144.00KiB 1/0     
0/274         3.11GiB    112.00KiB 1/0     
0/275         3.11GiB    128.00KiB 1/0     
0/276         3.11GiB     80.00KiB 1/0     
0/277         3.11GiB    256.00KiB 1/0     
0/278         3.11GiB    112.00KiB 1/0     
0/279         3.12GiB     64.00KiB 1/0     
0/280         3.12GiB     16.00KiB 1/0     
1/0           3.33GiB    222.95MiB --- 
</screen>
   <para>
    The <literal>qgroupid</literal> column displays the identification number
    for each subvolume, assigning a qgroup level/ID combination.
   </para>
   <para>
    The <literal>rfer</literal> column displays the total amount of data
    referred to in the subvolume.
   </para>
   <para>
    The <literal>excl</literal> column displays the exclusive data in each
    subvolume.
   </para>
   <para>
    The <literal>parent</literal> column shows the parent qgroup of the
    subvolumes.
   </para>
   <para>
    The final item, <literal>1/0</literal>, shows the totals for the parent
    qgroup. In the above example, 222.95 MiB will be freed if all subvolumes
    are removed. Run the following command to see which snapshots are
    associated with each subvolume:
   </para>
<screen>&prompt.root;btrfs subvolume list -st /</screen>
  </section>
 </section>
 <section xml:id="sec-transactional-udate">
  <title>Administration using transactional updates</title>
  <para>
   &slema; was designed to use a read-only root file system. This means that
   after the deployment is complete, you are not able to perform direct
   modifications to the root file system, e.g. by using
   <command>zypper</command>. Instead, &slem; introduces the concept of
   transactional updates which enables you to modify your system and keep it up
   to date.
  </para>
  <para>
   The key features of transactional updates are the following:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     They are atomic - the update is applied only if it completes successfully.
    </para>
   </listitem>
   <listitem>
    <para>
     Changes are applied in a separate snapshot and so do not influence the
     running system.
    </para>
   </listitem>
   <listitem>
    <para>
     Changes can easily be rolled back.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Each time you call the <command>transactional-update</command> command to
   change your system&mdash;either to install a package, perform an update or
   apply a patch&mdash;the following actions take place:
  </para>
  <procedure>
   <title>Modifying the root file system</title>
   <step>
    <para>
     A new read-write snapshot is created from your current root file system,
     or from a snapshot that you specified.
    </para>
   </step>
   <step>
    <para>
     All changes are applied (updates, patches or package installation).
    </para>
   </step>
   <step>
    <para>
     The snapshot is switched back to read-only mode.
    </para>
   </step>
   <step>
    <para>
     The new root file system snapshot is prepared, so that it will be active
     after you reboot.
    </para>
   </step>
   <step>
    <para>
     After rebooting, the new root file system is set as the default snapshot.
    </para>
    <note>
     <para>
      Bear in mind that without rebooting your system, the changes will not be
      applied.
     </para>
    </note>
   </step>
  </procedure>
  <warning>
   <para>
    In case you do not reboot your machine before performing further changes,
    the <command>transactional-update</command> command will create a new
    snapshot from the current root file system. This means that you will end up
    with several parallel snapshots, each including that particular change but
    not changes from the other invocations of the command. After reboot, the
    most recently created snapshot will be used as your new root file system,
    and it will not include changes done in the previous snapshots.
   </para>
  </warning>
  <section xml:id="sec-command-list">
   <title><command>transactional-update</command> usage</title>
   <para>
    The <command>transactional-update</command> command enables atomic
    installation or removal of updates; updates are applied only if all of them
    can be successfully installed. <command>transactional-update</command>
    creates a snapshot of your system and use it to update the system. Later
    you can restore this snapshot. All changes become active only after reboot.
   </para>
   <para>
    The <command>transactional-update</command> command syntax is as follows:
   </para>
<screen>
transactional-update <option>[option]</option> <replaceable>[general_command]</replaceable> <replaceable>[package_command]</replaceable> <replaceable>standalone_command</replaceable>
</screen>
   <note>
    <title>Running <command>transactional-update</command> without arguments.</title>
    <para>
     If you do not specify any command or option while running the
     <command>transactional-update</command> command, the system updates
     itself.
    </para>
   </note>
   <para>
    Possible command parameters are described further.
   </para>
   <variablelist>
    <title><command>transactional-update</command> options</title>
    <varlistentry>
     <term><literal>--interactive, -i</literal></term>
     <listitem>
      <para>
       Can be used along with a package command to turn on interactive mode.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--non-interactive, -n</literal></term>
     <listitem>
      <para>
       Can be used along with a package command to turn on non-interactive
       mode.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--continue [<replaceable>number</replaceable>], -c</literal></term>
     <listitem>
      <para>
       The <command>--continue</command> option is for making multiple changes
       to an existing snapshot without rebooting.
      </para>
      <para>
       The default <command>transactional-update</command> behavior is to
       create a new snapshot from the current root file system. If you forget
       something, such as installing a new package, you have to reboot to apply
       your previous changes, run <command>transactional-update</command> again
       to install the forgotten package, and reboot again. You cannot run the
       <command>transactional-update</command> command multiple times without
       rebooting to add more changes to the snapshot, because this will create
       separate independent snapshots that do not include changes from the
       previous snapshots.
      </para>
      <para>
       Use the <command>--continue</command> option to make as many changes as
       you want without rebooting. A separate snapshot is made each time, and
       each snapshot contains all the changes you made in the previous
       snapshots, plus your new changes. Repeat this process as many times as
       you want, and when the final snapshot includes everything you want,
       reboot the system, and your final snapshot becomes the new root file
       system.
      </para>
      <para>
       Another useful feature of the <command>--continue</command> option is
       you may select any existing snapshot as the base for your new snapshot.
       The following example demonstrates running
       <command>transactional-update</command> to install a new package in a
       snapshot based on snapshot 13, and then running it again to install
       another package:
      </para>
<screen>&prompt.root;<command>transactional-update pkg install <replaceable>package_1</replaceable></command></screen>
<screen>&prompt.root;<command>transactional-update --continue 13 pkg install <replaceable>package_2</replaceable></command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--no-selfupdate</literal></term>
     <listitem>
      <para>
       Disables self updating of <command>transactional-update</command>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--drop-if-no-change, -d</literal></term>
     <listitem>
      <para>
       Discards the snapshot created by <command>transactional-update</command>
       if there were no changes to the root file system. If there are some
       changes to the <filename>/etc</filename> directory, those changes merged
       back to the current file system.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--quiet</literal></term>
     <listitem>
      <para>
       The <command>transactional-update</command> command will not output to
       <literal>stdout</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--help, -h</literal></term>
     <listitem>
      <para>
       Prints help for the <command>transactional-update</command> command.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--version</literal></term>
     <listitem>
      <para>
       Displays the version of the <command>transactional-update</command>
       command.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The general commands are the following:
   </para>
   <variablelist>
    <title>General commands</title>
    <varlistentry>
     <term><literal>cleanup-snapshots</literal></term>
     <listitem>
      <para>
       The command marks all unused snapshots that are intended to be removed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>cleanup-overlays</literal></term>
     <listitem>
      <para>
       The command removes all unused overlay layers of
       <filename>/etc</filename>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>cleanup</literal></term>
     <listitem>
      <para>
       The command combines the <literal>cleanup-snapshots</literal> and
       <literal>cleanup-overlays</literal> commands. For more details refer to
       <xref linkend="sec-cleanup-algorithm"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>grub.cfg</literal></term>
     <listitem>
      <para>
       Use this command to rebuild the GRUB boot loader configuration file.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>bootloader</literal></term>
     <listitem>
      <para>
       The command reinstall the boot loader.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>initrd</literal></term>
     <listitem>
      <para>
       Use the command to rebuild <literal>initrd</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>kdump</literal></term>
     <listitem>
      <para>
       In case you perform changes to your hardware or storage, you may need to
       rebuild the kdump initrd.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>shell</literal></term>
     <listitem>
      <para>
       Opens a read-write shell in the new snapshot before exiting. The command
       is typically used for debugging purposes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>reboot</literal></term>
     <listitem>
      <para>
       The system reboots after the transactional-update is complete.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>run </literal><replaceable>&lt;command&gt;</replaceable></term>
     <listitem>
      <para>
       Runs the provided command in a new snapshot.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>setup-selinux</literal></term>
     <listitem>
      <para>
       Installs and enables targeted SELinux policy.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The package commands are the following:
   </para>
   <variablelist>
    <title>Package commands</title>
    <varlistentry>
     <term><literal>dup</literal></term>
     <listitem>
      <para>
       Performs upgrade of your system. The default option for this command is
       <literal>--non-interactive</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>migration</literal></term>
     <listitem>
      <para>
       The command migrates your system to a selected target. Typically it is
       used to upgrade your system if it has been registered via &scc;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>patch</literal></term>
     <listitem>
      <para>
       Checks for available patches and installs them. The default option for
       this command is <literal>--non-interactive</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>pkg install</literal></term>
     <listitem>
      <para>
       Installs individual packages from the available channels using the
       <command>zypper install</command> command. This command can also be used
       to install Program Temporary Fix (PTF) RPM files. The default option for
       this command is <literal>--interactive</literal>.
      </para>
<screen>&prompt.root;<command>transactional-update pkg install <replaceable>package_name</replaceable></command></screen>
      <para>
       or
      </para>
<screen>&prompt.root;<command>transactional-update pkg install <replaceable>rpm1 rpm2</replaceable></command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>pkg remove</literal></term>
     <listitem>
      <para>
       Removes individual packages from the active snapshot using the
       <command>zypper remove</command> command. This command can also be used
       to remove PTF RPM files. The default option for this command is
       <literal>--interactive</literal>.
      </para>
<screen>&prompt.root;<command>transactional-update pkg remove <replaceable>package_name</replaceable></command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>pkg update</literal></term>
     <listitem>
      <para>
       Updates individual packages from the active snapshot using the
       <command>zypper update</command> command. Only packages that are part of
       the snapshot of the base file system can be updated. The default option
       for this command is <literal>--interactive</literal>.
      </para>
<screen>&prompt.root;<command>transactional-update pkg update <replaceable>package_name</replaceable></command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>register</literal></term>
     <listitem>
      <para>
       The register command enables you to register/deregister your system. For
       a complete usage description, refer to
       <xref linkend="sec-register-command"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>up</literal></term>
     <listitem>
      <para>
       Updates installed packages to newer versions. The default option for
       this command is <literal>--non-interactive</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The standalone commands are the following:
   </para>
   <variablelist>
    <title>Standalone commands</title>
    <varlistentry>
     <term><literal>rollback</literal> <replaceable>&lt;snapshot number&gt;</replaceable></term>
     <listitem>
      <para>
       This sets the default subvolume. The current system is set as the new
       default root file system. If you specify a number, that snapshot is used
       as the default root file system. On a read-only file system, it does not
       create any additional snapshots.
      </para>
<screen>&prompt.root;<command>transactional-update rollback <replaceable>snapshot_number</replaceable></command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>rollback last</literal></term>
     <listitem>
      <para>
       This command sets the last known to be working snapshot as the default.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>status</literal></term>
     <listitem>
      <para>
       This prints a list of available snapshots. The currently booted one is
       marked with an asterisk, the default snapshot is marked with a plus
       sign.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <section xml:id="sec-register-command">
    <title>The <literal>register</literal> command</title>
    <para>
     The <literal>register</literal> command enables you to handle all tasks
     regarding registration and subscription management. You can supply the
     following options:
    </para>
    <variablelist>
     <varlistentry>
      <term><literal>--list-extensions</literal></term>
      <listitem>
       <para>
        With this option, the command will list available extensions for your
        system. You can use the output to find a product identifier for product
        activation.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>-p, --product</literal></term>
      <listitem>
       <para>
        Use this option to specify a product for activation. The product
        identifier has the following format:
        <emphasis>&lt;name&gt;/&lt;version&gt;/&lt;architecture&gt;</emphasis>,
        for example <literal>sle-module-live-patching/15.3/x86_64</literal>.
        The appropriate command will then be the following:
       </para>
<screen>&prompt.root;transactional-update register -p sle-module-live-patching/15.3/x86_64</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>-r, --regcode</literal></term>
      <listitem>
       <para>
        Register your system with the provided registration code. The command
        will register the subscription and enable software repositories.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>-d, --de-register</literal></term>
      <listitem>
       <para>
        The option deregisters the system, or when used along with the
        <literal>-p</literal> option, deregisters an extension.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>-e, --email</literal></term>
      <listitem>
       <para>
        Specify an email address that will be used in &scc; for registration.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>--url</literal></term>
      <listitem>
       <para>
        Specify the URL of your registration server. The URL is stored in the
        configuration and will be used in subsequent command invocations. For
        example:
       </para>
<screen>&prompt.root;transactional-update register --url https://scc.suse.com</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>-s, --status</literal></term>
      <listitem>
       <para>
        Displays the current registration status in JSON format.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>--write-config</literal></term>
      <listitem>
       <para>
        Writes the provided options value to the
        <filename>/etc/SUSEConnect</filename> configuration file.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>--cleanup</literal></term>
      <listitem>
       <para>
        Removes old system credentials.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>--version</literal></term>
      <listitem>
       <para>
        Prints the version.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>--help</literal></term>
      <listitem>
       <para>
        Displays usage of the command.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </section>
  </section>
  <section xml:id="sec-cleanup-algorithm">
   <title>Snapshots cleanup</title>
   <para>
    If you run the command <command>transactional-update cleanup</command>, all
    old snapshots without a cleanup algorithm will have one set. All important
    snapshots are also marked. The command also removes all unreferenced (and
    thus unused) <filename>/etc</filename> overlay directories in
    <filename>/var/lib/overlay</filename>.
   </para>
   <para>
    The snapshots with the set <literal>number</literal> cleanup algorithm will
    be deleted according to the rules configured in
    <filename>/etc/snapper/configs/root</filename> by the following parameters:
   </para>
   <variablelist>
    <varlistentry>
     <term>NUMBER_MIN_AGE</term>
     <listitem>
      <para>
       Defines the minimum age of a snapshot (in seconds) that can be
       automatically removed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>NUMBER_LIMIT/NUMBER_LIMIT_IMPORTANT</term>
     <listitem>
      <para>
       Defines the maximum count of stored snapshots. The cleaning algorithms
       delete snapshots above the specified maximum value, without taking the
       snapshot and file system space into account. The algorithms also delete
       snapshots above the minimum value until the limits for the snapshot and
       file system are reached.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The snapshot cleanup is also preformed regularly by &systemd;.
   </para>
  </section>
  <section xml:id="sec-rollback-procedure">
   <title>System rollback</title>
   <para>
    GRUB 2 enables booting from btrfs snapshots and thus allows you to use any
    older functional snapshot in case that the new snapshot does not work
    correctly.
   </para>
   <para>
    When booting a snapshot, the parts of the file system included in the
    snapshot are mounted read-only; all other file systems and parts that are
    excluded from snapshots are mounted read-write and can be modified.
   </para>
   <tip>
    <title>Rolling back to a specific installation state</title>
    <para>
     An initial bootable snapshot is created at the end of the initial system
     installation. You can go back to that state at any time by booting this
     snapshot. The snapshot can be identified by the description <literal>after
     installation</literal>.
    </para>
   </tip>
   <para>
    There are two methods how you can perform a system rollback.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      From a running system you can set the default snapshot, see more in
      <xref linkend="proc-running-snapshot"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Especially in cases where the current snapshot is broken, you can boot to
      the new snapshot and set it then default, for details refer to
      <xref linkend="proc-boot-snapshot"/>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    In case your current snapshot is functional, you can use the following
    procedure for system rollback.
   </para>
   <procedure xml:id="proc-running-snapshot">
    <title>Rollback from a running system</title>
    <step>
     <para>
      Choose the snapshot that should be set as default, run:
     </para>
<screen>&prompt.root;transactional-update status</screen>
     <para>
      to get a list of available snapshots. Note the number of the snapshot to
      be set as default.
     </para>
    </step>
    <step>
     <para>
      Set the snapshot as the default by running:
     </para>
<screen>&prompt.root;transactional-update rollback <replaceable>snapshot_number</replaceable></screen>
     <para>
      If you omit the <replaceable>snapshot number</replaceable>, the current
      snapshot will be set as default.
     </para>
    </step>
    <step>
     <para>
      Reboot your system to boot in to the new default snapshot.
     </para>
    </step>
   </procedure>
   <para>
    The following procedure is used in case the current snapshot is broken and
    you are not able to boot into it.
   </para>
   <procedure xml:id="proc-boot-snapshot">
    <title>Rollback to a working snapshot</title>
    <step>
     <para>
      Reboot your system and select <literal>Start bootloader from a read-only
      snapshot</literal>
     </para>
    </step>
    <step>
     <para>
      Choose a snapshot to boot. The snapshots are sorted according to the date
      of creation, with the latest one at the top.
     </para>
    </step>
    <step>
     <para>
      Log in to your system and check whether everything works as expected.
      Data written to directories excluded from the snapshots will stay
      untouched.
     </para>
    </step>
    <step>
     <para>
      If the snapshot you booted into is not suitable for rollback, reboot your
      system and choose another one.
     </para>
     <para>
      If the snapshot works as expected, you can perform rollback by running
      the following command:
     </para>
<screen>&prompt.root;<command>transactional-update rollback</command></screen>
     <para>
      And reboot afterwards.
     </para>
    </step>
   </procedure>
  </section>
  <section xml:id="sec-automatic-updates">
   <title>Managing automatic transactional updates</title>
   <para>
    Automatic updates are controlled by a <command>systemd.timer</command> that
    runs once per day. This applies all updates, and informs
    <command>rebootmgrd</command> that the machine should be rebooted. You may
    adjust the time when the update runs, see systemd.timer(5) documentation.
   </para>
   <para>
    You can disable automatic transactional updates with this command:
   </para>
<screen>&prompt.root;<command>systemctl --now disable transactional-update.timer</command></screen>
  </section>
 </section>
 <section xml:id="sec-health-checker">
  <title>Health checker</title>
  <para>
   Health checker is a program delivered with &slema; that checks whether
   services are running properly during booting of your system.
  </para>
  <para>
   During the boot process, <literal>systemd</literal> calls Health checker,
   which in turn calls its plugins. Each plugin checks a particular service or
   condition. If each check passes, a status file
   (<filename>/var/lib/misc/health-checker.state</filename>) is created. The
   status file marks the current root file system as correct.
  </para>
  <para>
   If any of the health checker plugins reports an error, the action taken
   depends on a particular condition, as described below:
  </para>
  <variablelist>
   <varlistentry>
    <term><emphasis>The snapshot is booted for the first time.</emphasis></term>
    <listitem>
     <para>
      If the current snapshot is different from the last one that worked
      properly, an automatic rollback to the last working snapshot is
      performed. This means that the last change performed to the file system
      broke the snapshot.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><emphasis>The snapshot has already booted correctly in the past.</emphasis></term>
    <listitem>
     <para>
      There could be just a temporary problem, and the system is rebooted
      automatically.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><emphasis>The reboot of a previously correctly booted snapshot has failed.</emphasis></term>
    <listitem>
     <para>
      If there was already a problem during boot and automatic reboot has been
      triggered, but the problem still persists, then the system is kept
      running to enable to the administrator to fix the problem. The services
      that are tested by the health checker plugins are stopped if possible.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <section xml:id="sec-health-plugins">
   <title>Adding custom plugins</title>
   <para>
    Health checker supports the addition of your own plugins to check services
    during the boot process. Each plugin is a bash script that must fulfill the
    following requirements:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Plugins are located within a specific
      directory&mdash;<filename>/usr/libexec/health-checker</filename>
     </para>
    </listitem>
    <listitem>
     <para>
      The service that will be checked by the particular plugin must be defined
      in the <literal>Unit</literal> section of the
      <filename>/usr/lib/systemd/system/health-checker.service</filename> file.
      For example, the <literal>etcd</literal> service is defined as follows:
     </para>
<screen>
[Unit]
...
After=etcd.service
...
</screen>
    </listitem>
    <listitem>
     <para>
      Each plugin must have functions called <literal>run.checks</literal> and
      <literal>stop_services</literal> defined. The
      <literal>run.checks</literal> function checks whether a particular
      service has started properly. Bear in mind that service that has not been
      enabled by systemd, should be ignored. The function
      <literal>stop_services</literal> is called to stop the particular service
      in case the service has not been started properly. You can use the plugin
      template for your reference.
     </para>
    </listitem>
   </itemizedlist>
  </section>
 </section>
 <!--
 <section xml:id="sec-admin-cockpit">
  <title>&slema; administration using Cockpit</title>
  <para>
   Cockpit is a web-based graphical interface that enables you to manage your
   &slema; deployments from one place. Cockpit is included in the delivered
   pre-built images, or can be installed if you are installing your own
   instances manually. For details regarding the manual installation, refer to
   <xref linkend="sec-settings-software"/>.
  </para>
  <para>
   Though Cockpit is present in the pre-built images by default, the plugin for
   administration of virtual machines needs to be installed manually. You can
   do so by installing the <literal>microos-cockpit</literal> pattern as
   described bellow. Use the command below as well in case Cockpit is not
   installed on your system.
  </para>
<screen>&prompt.root;transactional-update pkg install -t pattern microos-cockpit</screen>
  <para>
   Reboot your machine to switch to the latest snapshot.
  </para>
  <note>
   <title>Cockpit's plugins installed from the <literal>microos-cockpit</literal> pattern may differ according to technologies installed on your system</title>
   <para>
    The plugin <literal>Podman containers</literal> is installed only if the
    <emphasis>Containers Runtime for non-clustered systems</emphasis> patterns
    are installed on your system. Similarly, the <literal>Virtual
    Machines</literal> plugin is installed only if the <emphasis>KVM
    Virtualization Host</emphasis> pattern is installed on your system.
   </para>
  </note>
  <para>
   Before running Cockpit on you machine, you need to enable the cockpit socket
   in systemd by running:
  </para>
<screen>&prompt.root;systemctl enable &#8211;-now cockpit.socket</screen>
  <para>
   In case you have enabled the firewall, you also must open the firewall for
   Cockpit as follows:
  </para>
<screen>&prompt.root;firewall-cmd &#8211;-permanent &#8211;-zone=public &#8211;-add-service=cockpit</screen>
  <para>
   And then reload the firewall configuration by running:
  </para>
<screen>&prompt.root;firewall-cmd &#8211;-reload</screen>
  <para>
   Now you can access the Cockpit web interface by opening the following
   address in your web browser:
  </para>
<screen>https://<replaceable>IP_ADDRESS_OF_MACHINE</replaceable>:9090</screen>
  <para>
   A login screen opens. To login, use the same credentials as you use to login
   to your machine via console or SSH.
  </para>
  <figure>
   <title>Cockpit login screen</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata  fileref="cockpit_login_screen.png" width="100%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cockpit_login_screen.png" width="100%"/>
    </imageobject>
    <textobject role="description"><phrase>Cockpit login screen</phrase>
    </textobject>
   </mediaobject>
  </figure>
  <para>
   After successful login, the Cockpit web console opens. Here you can view and
   administer your system's performance, network interfaces, Podman containers,
   your virtual machines, services, accounts and logs. You can also access your
   machine using shell in a terminal emulator.
  </para>
  <figure>
   <title>Cockpit dashboard</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cockpit_dashboard.png" width="100%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cockpit_dashboard.png" width="100%"/>
    </imageobject>
    <textobject role="description"><phrase>Cockpit dashboard</phrase>
    </textobject>
   </mediaobject>
  </figure>
  <section xml:id="sec-users-cockpit">
   <title>Users administration</title>
   <note>
    <title>Users administration only for server administrators</title>
    <para>
     Only users with the <guimenu>Server administrator</guimenu> role can edit
     other users.
    </para>
   </note>
   <para>
    Cockpit enables you to manage users of your system. Click
    <guimenu>Accounts</guimenu> to open the user administration page. Here you
    can create a new account by clicking <guimenu>Create new account</guimenu>
    or manage already existing accounts by clicking on the particular account.
   </para>
   <figure>
    <title>The <guimenu>Accounts</guimenu> screen</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="cockpit_accounts_screen.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="cockpit_accounts_screen.png" width="100%"/>
     </imageobject>
     <textobject role="description"><phrase>Accounts screen</phrase>
     </textobject>
    </mediaobject>
   </figure>
   <section xml:id="sec-adding-users">
    <title>Creating new accounts</title>
    <para>
     Click <guimenu>Create new account</guimenu> to open the window that
     enables you to add a new user. Fill in the user's login and/or full name
     and password, then confirm the form by clicking <guimenu>Create</guimenu>.
    </para>
    <para>
     To add authorized SSH keys for the new user or set the <guimenu>Server
     administrator</guimenu> role, edit the already created account by clicking
     on it. For details, refer to <xref linkend="sec-modifying-users"/>.
    </para>
   </section>
   <section xml:id="sec-modifying-users">
    <title>Modifying accounts</title>
    <para>
     After clicking the user icon in the <guimenu>Accounts</guimenu> page, the
     user details view opens and you can edit the user.
    </para>
    <figure>
     <title>User details</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="cockpit_user_details.png" width="100%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="cockpit_user_details.png" width="100%"/>
      </imageobject>
      <textobject role="description"><phrase>User details</phrase>
      </textobject>
     </mediaobject>
    </figure>
    <para>
     In the user's details view, you perform the following actions.
    </para>
    <variablelist>
     <varlistentry>
      <term>Delete the user</term>
      <listitem>
       <para>
        Click <guimenu>Delete</guimenu> to remove the user from the system.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Terminate user's session</term>
      <listitem>
       <para>
        By clicking <guimenu>Terminate session</guimenu>, you can log out the
        particular user of the system.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Change user's role</term>
      <listitem>
       <para>
        By checking/unchecking the <guimenu>Server administrator</guimenu>
        check box, you can assign or remove the administrator role from the
        user.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Manage access to the account</term>
      <listitem>
       <para>
        You can lock the account or you can set a date when the account will
        expire.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Manage the user's password</term>
      <listitem>
       <para>
        Click <guimenu>Set password</guimenu> to set a new password for the
        account.
       </para>
       <para>
        By clicking <guimenu>Force change</guimenu>, the user will have to
        change the password on the next login.
       </para>
       <para>
        Click <guimenu>edit</guimenu> to set whether or when the password
        expires.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Add SSH key</term>
      <listitem>
       <para>
        You can add a SSH key for passwordless authentication via SSH. Click
        <guimenu>Add key</guimenu>, paste the contents of the public SSH key
        and confirm it by clicking <guimenu>Add</guimenu>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </section>
  </section>
  
  <section xml:id="sec-cockpit-updates">
   <title>Managing &slema; updates and snapshots</title>
   <note>
    <title>Snapshots and updates management only for system administrators</title>
    <para>
     Only users with the <guimenu>Server administrator</guimenu> role can
     update the system or perform a rollback to another snapshot.
    </para>
   </note>
   <para>
    Cockpit enables you to update your &slema; instance or perform a rollback
    from the <literal>Software Updates</literal> menu.
   </para>
   <figure>
    <title>Software Updates</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="cockpit_software_updates.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="cockpit_software_updates.png" width="100%"/>
     </imageobject>
     <textobject role="description"><phrase>Software Updates view</phrase>
     </textobject>
    </mediaobject>
   </figure>
   <section xml:id="sec-cockpit-updates-procedure">
    <title>Managing updates</title>
    <para>
     Click <guimenu>Check for Updates</guimenu> to get a list of new package
     updates and patches available for your system. You can view each package
     update or patch information by clicking the blue icon next to the package
     update/patch. It is recommended to install the patches marked as important
     as soon as possible.
    </para>
    <para>
     To apply the updates and patches, click <guimenu>Update and
     Reboot</guimenu>. The equivalent of the <command>transactional-update
     up</command> command is called and a new snapshot of your system is
     created. After the update completes successfully, your system will be
     rebooted and the new snapshot set as default.
    </para>
    <para>
     You can postpone the reboot of your system after the update process by
     selecting <guimenu>Update without Reboot</guimenu> from the three-dot
     menu. Bear in mind that you need to reboot the system to activate the
     snapshot with updates. If you perform further changes without rebooting
     the system beforehand, a new snapshot will be created from the same point
     as the snapshots with updates. Therefore, the new snapshot will not
     include the updates.
    </para>
    <para>
     You can also update &slema; using CLI. For details, refer to
     <xref linkend="sec-command-list"/>.
    </para>
   </section>
   <section xml:id="sec-cockpit-rollback">
    <title>Performing rollbacks</title>
    <para>
     In <literal>Snapshots &amp; Updates</literal> you can perform a rollback
     to one of older snapshots by clicking <guimenu>Rollback and
     Reboot</guimenu>, or <guimenu>Rollback without Reboot</guimenu> in
     three-dot menu.
    </para>
    <para>
     After rebooting the system, the snapshot you rolled back to will be set as
     active. Do not make any changes (install updates, packages, etc.) before
     rebooting your system, as the snapshot you rolled back to is not active.
     Any changes performed before you reboot your system will start from the
     currently active snapshot.
    </para>
    <para>
     For a system rollback procedure performed using CLI, refer to
     <xref linkend="sec-rollback-procedure"/>.
    </para>
   </section>
  </section>
 </section>

-->
 <section xml:id="sec-admin-toolbox">
  <title><literal>toolbox</literal> for &slema; debugging</title>
  <para>
   &slema; uses the <command>transactional-update</command> command to apply
   changes to the system, but the changes are applied only after reboot. That
   solution has several benefits, but it also has some disadvantages. If you
   need to debug your system and install a new tool, the tool will be available
   only after reboot. Therefore you are not able to debug the currently running
   system. For this reason a utility called <literal>toolbox</literal> has been
   developed.
  </para>
  <para>
   <literal>toolbox</literal> is a small script that pulls a container image
   and runs a privileged container based on that image. In the toolbox
   container you can install any tool you want with <command>zypper</command>
   and then use the tool without rebooting your system.
  </para>
  <para>
   To start the <literal>toolbox</literal> container, run the following:
  </para>
<screen>&prompt.root;/usr/bin/toolbox</screen>
  <para>
   If the script completes successfully, you will see the
   <literal>toolbox</literal> container prompt.
  </para>
  <note>
   <title>Obtaining the <literal>toolbox</literal> image</title>
   <para>
    You can also use Podman or Cockpit to pull the <literal>toolbox</literal>
    image and start a container based on that image.
   </para>
  </note>
 </section>
 <section xml:id="sec-performance-monitoring">
  <title>Monitoring performance</title>
  <para>
   For performance monitoring purposes, &slema; provides a container image that
   enables you to run the Performance Co-Pilot (PCP) analysis toolkit in a
   container. The toolkit comprises tools for gathering and processing
   performance information collected either in real time or from PCP archive logs.
  </para>
  <para>
   The performance data are collected by <emphasis>performance
   metrics domain agents</emphasis> and passed to the <emphasis>pmcd</emphasis>
   daemon. The daemon coordinates the gathering and exporting of performance
   statistics in response to requests from the PCP monitoring tools.
   <command>pmlogger</command> is then used to log the metrics. For details,
   refer to the
   <link xlink:href="https://pcp.readthedocs.io/en/latest/UAG/IntroductionToPcp.html#">PCP
   documentation</link>.
  </para>
  <section xml:id="sec-getting-pcp">
   <title>Getting the PCP container image</title>
   <para>
    The PCP container image is based on the <emphasis>BCI-Init</emphasis>
    container that utilizes &systemd; used to manage the PCP services.
   </para>
   <para>
    You can pull the container image using podman or from the Cockpit web
    management console. To pull the image by using podman, run the following
    command:
   </para>
<screen>&prompt.root;podman pull registry.suse.com/suse/pcp:latest</screen>
   <para>
    To get the container image using Cockpit, go to <guimenu>Podman
    containers</guimenu>, click <guimenu>Get new image</guimenu>, and search for
    <emphasis>pcp</emphasis>. Then select the image from the
    <literal>registry.suse.com</literal> for SLE 15 SP3 and download it.
   </para>
  </section>
  <section xml:id="sec-starting-pcp">
   <title>Running the PCP container</title>
   <para>
    The following command shows minimal options that you need to use to run a
    PCP container:
   </para>
<screen>&prompt.root;podman run -d  \
  --systemd always \
  -p <replaceable>HOST_IP:HOST_PORT:CONTAINER_PORT</replaceable> \
  -v <replaceable>HOST_DIR</replaceable>:/var/log/pcp/pmlogger \
  <replaceable>PCP_CONTAINER_IMAGE</replaceable></screen>
   <para>
    where the options have the following meaning:
   </para>
   <variablelist>
    <varlistentry>
     <term><option>-d</option></term>
     <listitem>
      <para>
       The container will run in a detached mode without tty.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--systemd always</option></term>
     <listitem>
      <para>
       Runs the container in the &systemd; mode. All services needed to run in
       the PCP container will be started automatically by &systemd; in the
       container.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--privileged</option></term>
     <listitem>
      <para>
       The container runs with extended privileges. Use this option if your
       system has SELinux enabled, otherwise the collected metrics will be
       incomplete.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>-v <replaceable>HOST_DIR</replaceable>:/var/log/pcp/pmlogger</option></term>
     <listitem>
      <para>
       Creates a bind mount so that <command>pmlogger</command> archives are written to the  <replaceable>HOST_DIR</replaceable> on the host. By
       default, <command>pmlogger</command> stores the collected metrics in
       <filename>/var/log/pcp/pmlogger</filename>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><replaceable>PCP_CONTAINER_IMAGE</replaceable></term>
     <listitem>
      <para>
       Is the downloaded PCP container image.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Other useful options of the <command>podman run</command> command follow:
   </para>
   <variablelist>
    <title>Other options</title>
    <varlistentry>
     <term><option>-p <replaceable>HOST_IP:HOST_PORT:CONTAINER_PORT</replaceable></option></term>
     <listitem>
      <para>
       Publishes ports of the container by mapping a container port onto a host
       port. If you do not specify <replaceable>HOST_IP</replaceable>, the ports
       will be mapped on the local host. If you omit the
       <replaceable>HOST_PORT</replaceable> value, a random port number will be
       used. By default, the <command>pmcd</command> daemon listens and exposes
       the PMAPI to receive metrics on the port <emphasis>44321</emphasis>, so
       it is recommended to map this port on the same port number on the host.
       The <command>pmproxy</command> daemon listens on and exposes the REST
       PMWEBAPI to access metrics on the <emphasis>44322</emphasis> port by
       default, so it is recommended to map this port on the same host port
       number.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--net host</option></term>
     <listitem>
      <para>
       The container uses the host's network. Use this option if you want to
       collect metrics from the host's network interfaces.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>-e</option></term>
     <listitem>
      <para>
       The option enables you to set the following environment variables:
      </para>
      <variablelist>
       <!-- <title>Environment variables</title> -->
       <varlistentry>
        <term>PCP_SERVICES</term>
        <listitem>
         <para>
          Is a comma-separated list of services to start by &systemd; in the
          container.
         </para>
         <para>
          Default services are: <literal>pmcd</literal>,
          <literal>pmie</literal>, <literal>pmlogger</literal>,
          <literal>pmproxy</literal>.
         </para>
         <para>
          You can use this variable, if you want to run a container with a list of
          services that is different from the default one, for example, only with
          <literal>pmlogger</literal>:
         </para>
<screen>&prompt.root;podman run -d \
  --name pmlogger \
  --systemd always \
  -e PCP_SERVICES=pmlogger  \
  -v pcp-archives:/var/log/pcp/pmlogger  \
  registry.suse.com/suse/pcp:latest</screen>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>HOST_MOUNT</term>
        <listitem>
         <para>
          Is a path inside the container to the bind mount of the host's root file system. The default value is not set.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>REDIS_SERVERS</term>
        <listitem>
         <para>
          Specifies a connection to a Redis server. In a non-clustered setup,
          provide a comma-separated list of hosts specs. In a clustered setup,
          provide any individual cluster host, other hosts in the cluster are
          discovered automatically. The default value is:
          <literal>localhost:6379</literal>.
         </para>
        </listitem>
       </varlistentry>
      </variablelist>
      <para>
       If you need to use different configuration then provided by the
       environment variables, proceed as described in
       <xref linkend="sec-pcp-services-configuration"/>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </section>
  <section xml:id="sec-pcp-services-configuration">
   <title>Configuring PCP services</title>
   <para>
    All services that run inside the PCP container have a default configuration
    that might not suit your needs. If you need a custom configuration that
    cannot be covered by the environment variables described above, create
    configuration files for the PCP services and pass them to the PCP using a
    bind mount as follows:
   </para>
<screen>&prompt.root;podman run -d \
  --name <replaceable>CONTAINER_NAME</replaceable> \
  --systemd always \
  -v $<replaceable>HOST_CONFIG</replaceable>:<replaceable>CONTAINER_CONFIG_PATH</replaceable>:z \
  -v <replaceable>HOST_LOGS_PATH</replaceable>:/var/log/pcp/pmlogger  \
  registry.suse.com/suse/pcp:latest</screen>
   <para>
    Where:
   </para>
   <variablelist>
    <varlistentry>
     <term><replaceable>CONTAINER_NAME</replaceable></term>
     <listitem>
      <para>
       Is an optional container name.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><replaceable>HOST_CONFIG</replaceable></term>
     <listitem>
      <para>
       Is an absolute path to the config you created on the host machine. You
       can choose any file name you want.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><replaceable>CONTAINER_CONFIG_PATH</replaceable></term>
     <listitem>
      <para>
       Is an absolute path to a particular configuration file inside the
       container. Each available configuration file is described in
       the corresponding sections further.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><replaceable>HOST_LOGS_PATH</replaceable></term>
     <listitem>
      <para>
       Is a directory that should be bind mount to the container logs.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For example, a container called <literal>pcp</literal>, with the
    configuration file <filename>pmcd</filename> on the host machine and the
    <filename>pcp-archives</filename> directory for logs on the host machine, is
    run by the following command:
   </para>
<screen>&prompt.root;podman run -d \
  --name pcp  \
  --systemd always \
  -v $(pwd)/pcp-archives:/var/log/pcp/pmlogger \
  -v $(pwd)/pmcd:/etc/sysconfig/pmcd \
registry.suse.com/suse/pcp:latest</screen>
   <section xml:id="sec-pmcd-daemon-configration">
    <title>Custom <command>pmcd</command> daemon configuration</title>
    <para>
     The <command>pmcd</command> daemon configuration is stored in the
     <filename>/etc/sysconfig/pmcd</filename> file. The file stores environment variables
       that modify the behavior of the <command>pmcd</command> daemon.
    </para>
    
    <section xml:id="sec-sysconfig-pmcd">
     <title>The <filename>/etc/sysconfig/pmcd</filename> file</title>
     <para>
      You can add the following variables to the file to configure the
      <command>pmcd</command> daemon:
     </para>
     <variablelist>
      <varlistentry>
       <term>PMCD_LOCAL</term>
       <listitem>
        <para>
         Defines whether the remote host can connect to the <command>pmcd</command>
         daemon. If set to <emphasis>0</emphasis>, remote connections to the
         daemon are allowed. If set to <emphasis>1</emphasis>, the daemon
         listens only on the local host. The default value is
         <emphasis>0</emphasis>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>PMCD_MAXPENDING</term>
       <listitem>
        <para>
         Defines the maximum count of pending connections to the agent. The
         default value is 5.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>PMCD_ROOT_AGENT</term>
       <listitem>
        <para>
         If the <literal>pmdaroot</literal> is enabled (the value is set to
         <emphasis>1</emphasis>), adding a new PDMA does not trigger restarting
         of other PMDAs. If <literal>pmdaroot</literal> is not enabled,
         <command>pmcd</command> will require to restart all PMDAs when a new
         PMDA is added. The default value is <emphasis>1</emphasis>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>PMCD_RESTART_AGENTS</term>
       <listitem>
        <para>
         If set to <emphasis>1</emphasis>, the <command>pmcd</command> daemon
         tries to restart any exited PMDA. Enable this option only if you have
         enabled <literal>pmdaroot</literal>, as <command>pmcd</command> itself
         does not have privileges to restart PMDA.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>PMCD_WAIT_TIMEOUT</term>
       <listitem>
        <para>
         Defines the maximum time in seconds, <command>pmcd</command> can wait
         to accept a connection. After this time, the connection is reported as
         failed. The default value is <emphasis>60</emphasis>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>PCP_NSS_INIT_MODE</term>
       <listitem>
        <para>
         Defines the mode in which <command>pmcd</command> initializes the NSS
         certificate database when secured connections are used. The default
         value is <literal>readonly</literal>. You can set the mode to
         <literal>readwrite</literal>, but if the initialization fails, the default value is used as a fallback.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      An example follows:
     </para>
<screen>
      PMCD_LOCAL=0
      PMCD_MAXPENDING=5
      PMCD_ROOT_AGENT=1
      PMCD_RESTART_AGENTS=1
      PMCD_WAIT_TIMEOUT=70
      PCP_NSS_INIT_MODE=readwrite
     </screen>
    </section>
    <!-- <section xml:id="sec-pmcd-options">
     <title>The <filename>/etc/pcp/pmcd/pmcd.options</filename> file</title>
     <para>
      The configuration file <filename>/etc/pcp/pmcd/pmcd.options</filename>
      stores possible command line options for the <command>pmcd</command>
      daemon. By default the options are commented out and not used when
      invoking the daemon. See the <command>pmcd</command> command options in
      <link xlink:href="https://man7.org/linux/man-pages/man1/pmcd.1.html">pmcd(1)</link>.
     </para>
     <para>
      Add the required options to your configuration file that will be mounted
      on the <filename>/etc/pcp/pmcd/pmcd.options</filename> file in the
      container. The options should be added to the configuration file in the
      following pattern:
     </para>
<screen>
-<replaceable>option</replaceable> <replaceable>OPTION_VALUE</replaceable>
</screen>
     <para>
      For example, the option that specifies an IP address on which the daemon
      listen for connections
     </para>
<screen>-i 192.168.0.100</screen>
    </section>
  -->
   </section>
   <section xml:id="sec-pmlogger-configuration">
    <title>Custom <command>pmlogger</command> configuration</title>
    <para>
     The custom configuration for the <command>pmlogger</command> is stored in
     the following configuration files:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <filename>/etc/sysconfig/pmlogger</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/etc/pcp/pmlogger/control.d/local</filename>
      </para>
     </listitem>
     <!-- <listitem>
      <para>
       <filename>/var/lib/pcp/config/pmlogger/config.default</filename>
      </para> -->
    </itemizedlist>
    <section xml:id="sec-sysconfig-pmlogger">
     <title>The <filename>/etc/sysconfig/pmlogger</filename> file</title>
     <para>
      You can use the following attributes to configure the
      <command>pmlogger</command>:
     </para>
     <variablelist>
      <varlistentry>
       <term>PMLOGGER_LOCAL</term>
       <listitem>
        <para>
         Defines whether <command>pmlogger</command> allows connections from
         remote hosts. If set to <emphasis>1</emphasis>,
         <command>pmlogger</command> allows connections from local host only.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>PMLOGGER_MAXPENDING</term>
       <listitem>
        <para>
         Defines the maximum count of pending connections. The default value is
         5.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>PMLOGGER_INTERVAL</term>
       <listitem>
        <para>
         Defines the default sampling interval <command>pmlogger</command>
         uses. The default value is <emphasis>60&nbsp;s</emphasis>. Keep in mind that
         this value can be overridden by the <command>pmlogger</command> command
         line.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>PMLOGGER_CHECK_SKIP_LOGCONF</term>
       <listitem>
        <para>
         Setting this option to <emphasis>yes</emphasis> disables the regeneration
         and checking of the <command>pmlogger</command> configuration if the
         configuration <command>pmlogger</command> comes from
         <command>pmlogconf</command>. The default behavior is to regenerate
         configuration files and check for changes every time
         <command>pmlogger</command> is started.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      An example follows:
     </para>
<screen>
PMLOGGER_LOCAL=1
PMLOGGER_MAXPENDING=5
PMLOGGER_INTERVAL=10
PMLOGGER_CHECK_SKIP_LOGCONF=yes
     </screen>
    </section>
    <section xml:id="sec-control-local">
     <title>The <filename>/etc/pcp/pmlogger/control.d/local</filename> file</title>
     <para>
      The file <filename>/etc/pcp/pmlogger/control.d/local</filename> stores
      specifications of the host, which metrics should be logged, the logging
      frequency (default is 24 hours), and <command>pmlogger</command> options.
      For example:
     </para>
<screen>
# === VARIABLE ASSIGNMENTS ===
#
# DO NOT REMOVE OR EDIT THE FOLLOWING LINE
$version=1.1

# Uncomment one of the lines below to enable/disable compression behaviour
# that is different to the pmlogger_daily default.
# Value is days before compressing archives, 0 is immediate compression,
# "never" or "forever" suppresses compression.
#
#$PCP_COMPRESSAFTER=0 
#$PCP_COMPRESSAFTER=3
#$PCP_COMPRESSAFTER=never
    
# === LOGGER CONTROL SPECIFICATIONS ===
#   
#Host           P?  S?  directory                       args

# local primary logger
LOCALHOSTNAME   y   n   PCP_ARCHIVE_DIR/LOCALHOSTNAME   -r -T24h10m -c config.default -v 100Mb
     </screen>
     <note>
      <title>Defaults point to local host</title>
      <para>
       If you run the <command>pmlogger</command> in a container on a different
       machine than the one that runs the <command>pmcd</command> (a client), change the
       following line to point to the client:
      </para>
<screen># local primary logger
<replaceable>CLIENT_HOSTNAME</replaceable>   y   n   PCP_ARCHIVE_DIR/<replaceable>CLIENT_HOSTNAME</replaceable>   -r -T24h10m -c config.default -v 100Mb</screen>
      <para>
       For example, for the <literal>slemicro_1</literal> host name, the line
       should look as follows:
      </para>
<screen># local primary logger
slemicro_1   y   n   PCP_ARCHIVE_DIR/slemicro_1   -r -T24h10m -c config.default -v 100Mb</screen>
     </note>
    </section>
    <!-- 
    <section xml:id="sec-config-default">
     <title>The <filename>/var/lib/pcp/config/pmlogger/config.default</filename> file</title>
     <para>
      The file <filename>/var/lib/pcp/config/pmlogger/config.default</filename>
      stores a set of default metrics that are logged on the system. Though you
      can add metrics to the file, do not modify the default metrics.
     </para>
    </section>
  -->
   </section>
  </section>
  <section xml:id="sec-autostart-onboot">
   <title>Starting the PCP container automatically on boot</title>
   <para>
    After you run the PCP container, you can configure &systemd; to start the
    container on boot. To do so, follow the procedure below:
   </para>
   <procedure>
    <step>
     <para>
      Create a unit file for the container by using the <command>podman
      generate systemd</command> command:
     </para>
<screen>&prompt.root;podman generate systemd --name <replaceable>CONTAINER_NAME</replaceable> > /etc/systemd/system/container-<replaceable>CONTAINER_NAME</replaceable>.service</screen>
     <para>
      where <replaceable>CONTAINER_NAME</replaceable> is the name of the PCP
      container you used when running the container from the container image.
     </para>
    </step>
    <step>
     <para>
      Enable the service in &systemd;:
     </para>
<screen>&prompt.root;systemctl enable container-<replaceable>CONTAINER_NAME</replaceable></screen>
    </step>
   </procedure>
  </section>
  <section xml:id="sec-collecting-metrics">
   <title>Metrics management</title>
   <section xml:id="sec-listing-metrics">
    <title>Listing available performance metrics</title>
    <para>
     From within the container, you can use the command <command>pminfo</command> to list metrics. For
     example, to list all available performance metrics, run:
    </para>
<screen>&prompt.root;pminfo</screen>
    <para>
     You can list a group of related metrics by specifying the metrics prefix:
    </para>
<screen>&prompt.root;pminfo <replaceable>METRIC_PREFIX</replaceable></screen>
    <para>
     For example, to list all metrics related to kernel, use:
    </para>
<screen>&prompt.root;pminfo disk

disk.dev.r_await
disk.dm.await
disk.dm.r_await
disk.md.await
disk.md.r_await
...
</screen>
    <para>
     You can also specify additional strings to narrow down the list of
     metrics, for example:
    </para>
<screen>&prompt.root;piminfo disk.dev

disk.dev.read
disk.dev.write
disk.dev.total
disk.dev.blkread
disk.dev.blkwrite
disk.dev.blktotal
...
</screen>
    <para>
     To get online help text of a particular metric, use the
     <option>-t</option> option followed by the metric, for example:
    </para>
<screen>&prompt.root;pminfo -t kernel.cpu.util.user

kernel.cpu.util.user [percentage of user time across all CPUs, including guest CPU time]
      </screen>
    <para>
     To display a description text of a particular metric, use the
     <option>-T</option> option followed by the metric, for example:
    </para>
<screen>&prompt.root;pminfo -T kernel.cpu.util.user

Help:
percentage of user time across all CPUs, including guest CPU time
      </screen>
   </section>
   <section xml:id="sec-local-metrics">
    <title>Checking local metrics</title>
    <para>
     After you start the PCP container, you can verify that metrics are being
     recorded properly by running the following command inside the container:
    </para>
<screen>&prompt.root;pcp

Performance Co-Pilot configuration on localhost:

 platform: Linux localhost 5.3.18-150300.59.68-default #1 SMP Wed May 4 11:29:09 UTC 2022 (ea30951) x86_64
 hardware: 1 cpu, 1 disk, 1 node, 1726MB RAM
 timezone: UTC
 services: pmcd pmproxy
     pmcd: Version 5.2.2-1, 9 agents, 4 clients
     pmda: root pmcd proc pmproxy xfs linux mmv kvm jbd2
 pmlogger: primary logger: /var/log/pcp/pmlogger/localhost/20220607.09.24
     pmie: primary engine: /var/log/pcp/pmie/localhost/pmie.log
      </screen>
    <para>
     Now check if the logs are written to a proper destination:
    </para>
<screen>&prompt.root;ls <replaceable>PATH_TO_PMLOGGER_LOGS</replaceable></screen>
    <para>
     where <replaceable>PATH_TO_PMLOGGER_LOGS</replaceable> should be
     <filename>/var/log/pcp/pmlogger/localhost/</filename> in this case.
    </para>
   </section>
   <section xml:id="sec-remote-metrics">
    <title>Recording metrics from remote systems</title>
    <para>
     You can deploy collector containers that collect metrics from different
     remote systems than the ones where the <command>pmlogger</command> containers are
     running. Each remote collector system needs the <command>pmcd</command>
     daemon and a set of <emphasis>pmda</emphasis>. To deploy several
     collectors with a centralized monitoring system, proceed as follows.
    </para>
    <procedure>
     <step>
      <para>
       On each system you want to collect metrics from (clients), run a
       container with the <command>pmcd</command> daemon:
      </para>
<screen>&prompt.root;podman run -d \
    --name pcp-pmcd \
    --privileged \
    --net host \
    --systemd always \
    -e PCP_SERVICES=pmcd \
    -e HOST_MOUNT=/host \
    -v /:/host:ro,rslave \
    registry.suse.com/suse/pcp:latest</screen>
     </step>
     <step xml:id="pmlogger-config">
      <para>
       On the monitoring system, create a <command>pmlogger</command>
       configuration file for each client
       <filename>control.<replaceable>CLIENT</replaceable></filename> with the
       following content:
      </para>
<screen>
$version=1.1
 
<replaceable>CLIENT_HOSTNAME</replaceable> n n PCP_ARCHIVE_DIR/<replaceable>CLIENT</replaceable> -N -r -T24h10m -c config.default -v 100Mb
          </screen>
      <para>
       Keep in mind that the <replaceable>CLIENT_HOSTNAME</replaceable> must be
       resolvable in DNS. You can use IP addresses or fully qualified domain
       names (FQDN) instead.
      </para>
     </step>
     <step>
      <para>
       On the monitoring system, create a directory for each client to store
       the recorded logs:
      </para>
<screen>&prompt.root;mkdir /root/pcp-archives/<replaceable>CLIENT</replaceable></screen>
      <para>
       For example, for <literal>slemicro_1</literal>:
      </para>
<screen>&prompt.root;mkdir /root/pcp-archives/slemicro_1</screen>
     </step>
     <step>
      <para>
       On the monitoring system, run a container with
       <command>pmlogger</command> for each client:
      </para>
<screen>&prompt.root;podman run -d \
    --name pcp-pmlogger-<replaceable>CLIENT</replaceable> \
    --systemd always \
    -e PCP_SERVICES=pmlogger \
    -v /root/pcp-archives/<replaceable>CLIENT</replaceable>:/var/log/pcp/pmlogger:z \
    -v $(pwd)/control.<replaceable>CLIENT</replaceable>:/etc/pcp/pmlogger/control.d/local:z \
    registry.suse.com/suse/pcp:latest
          </screen>
      <para>
       For example, for a client called <literal>slemicro_1</literal>:
      </para>
<screen>&prompt.root;podman run -d \
    --name pcp-pmlogger-slemicro_1 \
    --systemd always \
    -e PCP_SERVICES=pmlogger \
    -v /root/pcp-archives:/var/log/pcp/pmlogger:z \
    -v $(pwd)/control.slemicro_1:/etc/pcp/pmlogger/control.d/local:z \
    registry.suse.com/suse/pcp:latest
          </screen>
      <note>
       <para>
        The second bind mount points to the configuration file created in
        <xref linkend="pmlogger-config"/> and replaces the default
        <command>pmlogger</command> configuration. If you do not create this
        bind mount, <command>pmlogger</command> uses the default
        <filename>/etc/pcp/pmlogger/control.d/local</filename> file and logging
        from clients fails as the default configuration points to a local host.
        For details about the configuration file, refer to
        <xref linkend="sec-control-local"/>.
       </para>
      </note>
     </step>
     <step>
      <para>
       To check if the logs collection is working properly, run:
      </para>
<screen>&prompt.root;ls -l pcp-archives/<replaceable>CLIENT</replaceable>/<replaceable>CLIENT</replaceable></screen>
      <para>
       For example:
      </para>
<screen>&prompt.root;ls -l pcp-archives/slemicro_1/slemicro_1

total 1076
-rw-r--r--. 1 systemd-network systemd-network 876372 Jun  8 11:24 20220608.10.58.0
-rw-r--r--. 1 systemd-network systemd-network    312 Jun  8 11:22 20220608.10.58.index
-rw-r--r--. 1 systemd-network systemd-network 184486 Jun  8 10:58 20220608.10.58.meta
-rw-r--r--. 1 systemd-network systemd-network    246 Jun  8 10:58 Latest
-rw-r--r--. 1 systemd-network systemd-network  24595 Jun  8 10:58 pmlogger.log
          </screen>
     </step>
    </procedure>
   </section>
  </section>
 </section>
 <section xml:id="sec-user-mamagement">
  <title>User management</title>
  <para>
   You can define users during the deployment process of &slema;. Although you
   can define users as you want when deploying pre-built images, during the
   manual installation, you define only the &rootuser; user in the installation
   flow. Therefore, you might want to use other users than those provided
   during the installation process. There are two possibilities for adding
   users to an already installed system:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     using CLI - the command <command>useradd</command>. Run the following
     command for usage:
    </para>
<screen>&prompt.root;useradd --help</screen>
    <para>
     Bear in mind that a user that should have the server administrator role,
     must be included in the group <literal>wheel</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     using Cockpit; for details refer to <xref linkend="sec-users-cockpit"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
</article>
