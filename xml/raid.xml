<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE sect1
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<sect1 version="5.0" xml:id="sec.yast2.system.raid"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>Soft RAID Configuration with &yast;</title>
  <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
        </dm:bugtracker>
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
    </info>
    <indexterm>
 <primary>&yast;</primary>
 <secondary>RAID</secondary>
 </indexterm>

 <indexterm>
 <primary>RAID</primary>
 <secondary>&yast;</secondary>
 </indexterm>

 <indexterm>
 <primary>soft RAID</primary>
 <see>RAID</see>
 </indexterm>

 <para>
 This section describes actions required to create and configure various types
 of RAID. <phrase os="sles">In case you need background information about RAID, please
 refer to <xref linkend="sec.raid.intro"/></phrase>.
 </para>

<!-- TODO: don't know what todo with this doccomment:
 In section 2.3.1 RAID Levels to streamline the documentation you may
 want to remove the paragraphs on RAID 2 and RAID 3 and on RAID 4 and
 add these RAID levels to the list of other RAID levels under the
 heading below Other RAID Levels. Suggesting this because in the next
 section on configurating RAID only configuring RAID levels 0, 1, 5
 are mentioned. In addition, the paragraph on RAID 4 mentions a
 bottleneck for write access to the parity disk. This problem has
 typically been avoided in the "proprietary implementations created by
 hardware vendor" that you refer to under the Other RAID levels
 heading, for example, the Network Appliance implementation of RAID 4.

 You may also want to do the same with RAID 6.

 In section 2.3.2 Soft RAID Configuration with YaST when "Checking
 Persistent Superblock ensures" is mentioned it may be helpful to
 display in a figure where to check this option.
 -->
<!-- Jana: This section duplicates the content of the storage document.
 <para>
  The purpose of RAID (redundant array of independent disks) is to combine
  several hard disk partitions into one large <emphasis>virtual</emphasis>
  hard disk to optimize performance and/or data security. Most RAID
  controllers use the SCSI protocol because it can address a larger number
  of hard disks in a more effective way than the IDE protocol. It is also
  more suitable for the parallel command processing. There are some RAID
  controllers that support IDE or SATA hard disks. Soft RAID provides the
  advantages of RAID systems without the additional cost of hardware RAID
  controllers. However, this requires some CPU time and has memory
  requirements that make it unsuitable for high performance computers.
 </para>

 <para>
  With &productnamereg; , you can combine several hard disks into one
  soft RAID system. RAID implies several strategies for combining several
  hard disks in a RAID system, each with different goals, advantages, and
  characteristics. These variations are commonly known as <emphasis>RAID
  levels</emphasis>.
 </para>

 <para>
  Common RAID levels are:
 </para>

 <variablelist>
  <varlistentry>
   <term>RAID&nbsp;0</term>
   <listitem>
    <para>
     This level improves the performance of your data access by spreading
     out blocks of each file across multiple disk drives. Actually, this is
     not really a RAID, because it does not provide data backup, but the
     name <emphasis>RAID&nbsp;0</emphasis> for this type of system is
     commonly used. With RAID&nbsp;0, two or more hard disks are pooled
     together. Performance is enhanced, but the RAID system is destroyed and
     your data lost if even one hard disk fails.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>RAID&nbsp;1</term>
   <listitem>
    <para>
     This level provides adequate security for your data, because the data
     is copied to another hard disk 1:1. This is known as <emphasis>hard
     disk mirroring</emphasis>. If one disk is destroyed, a copy of its
     contents is available on the other one. All disks but one could be
     damaged without endangering your data. However, if the damage is not
     detected, the damaged data can be mirrored to the undamaged disk. This
     could result in the same loss of data. The writing performance suffers
     in the copying process compared to using single disk access (10 to 20 %
     slower), but read access is significantly faster in comparison to any
     one of the normal physical hard disks. The reason is that the duplicate
     data can be parallel-scanned. Generally it can be said that
     Level&nbsp;1 provides nearly twice the read transfer rate of single
     disks and almost the same write transfer rate as single disks.
    </para>
   </listitem>
  </varlistentry>
  -->
<!-- 2011-08-11 toba: these are not popular nor supported by yast
  <varlistentry>
   <term>RAID&nbsp;2 and RAID&nbsp;3</term>
   <listitem>
    <para>
     These are not typical RAID implementations. Level&nbsp;2 stripes data
     at the bit level rather than the block level. Level&nbsp;3 provides
     byte-level striping with a dedicated parity disk, and cannot service
     simultaneous multiple requests. These levels are rarely used.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>RAID&nbsp;4</term>
   <listitem>
    <para>
     Level&nbsp;4 provides block-level striping just like Level&nbsp;0
     combined with a dedicated parity disk. In the case of data disk
     failure, the parity data is used to create a replacement disk. However,
     the parallel disk may create a bottleneck for write access.
    </para>
   </listitem>
  </varlistentry>
  -->
  <!-- Jana: Removing this section as it duplicates the content of the storage guide.
  <varlistentry>
   <term>RAID&nbsp;5</term>
   <listitem>
    <para>
     RAID&nbsp;5 is an optimized compromise between Level&nbsp;0 and
     Level&nbsp;1, in terms of performance and redundancy. The hard disk
     space equals the number of disks used minus one. The data is
     distributed over the hard disks as with RAID&nbsp;0.
     <emphasis>Parity blocks</emphasis>, created on one of the partitions,
     exist for security reasons. They are linked to each other with XOR,
     enabling the contents to be reconstructed by the corresponding parity
     block in case of system failure. With RAID&nbsp;5, no more than one
     hard disk can fail at the same time. If one hard disk fails, it must be
     replaced when possible to avoid the risk of losing data.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>RAID&nbsp;6</term>
   <listitem>
    <para>
     To further increase the reliability of the RAID system, it is possible
     to use RAID&nbsp;6. In this level, even if two disks fail, the array
     still can be reconstructed. With RAID&nbsp;6, at least 4 hard disks
     are needed to run the array. Note that when running as software raid,
     this configuration needs a considerable amount of CPU time and memory.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>RAID&nbsp;10 (RAID&nbsp;1+0)</term>
   <listitem>
    <para>
     This RAID implementation combines features of RAID&nbsp;0 and
     RAID&nbsp;1: the data is first mirrored to separate disk arrays,
     which are inserted into a new RAID 0; type array. In each
     RAID&nbsp;1 sub-array, one disk can fail without any damage to the
     data. A minimum of four disks and an even number of disks is needed to
     run a RAID&nbsp;10. This type of RAID is used for database
     application where a huge load is expected.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>Other RAID Levels</term>
   <listitem>
    <para>
     Several other RAID levels have been developed (RAID&nbsp;2,
     RAID&nbsp;3, RAID&nbsp;4, RAIDn, RAID&nbsp;10,
     RAID&nbsp;0+1, RAID&nbsp;30, RAID&nbsp;50, etc.), some
     being proprietary implementations created by hardware vendors. These
     levels are not very common and therefore are not explained here.
    </para>
   </listitem>
  </varlistentry>
 </variablelist>
-->
 <sect2 xml:id="sec.yast2.system.raid.conf">
  <title>Soft RAID Configuration with &yast;</title>
  <para>
   The &yast; <guimenu>RAID</guimenu> configuration can be reached from
   the &yast; Expert Partitioner, described in
   <xref linkend="sec.yast2.i_y2_part_expert"/>. This partitioning tool
   enables you to edit and delete existing partitions and create new ones to
   be used with soft RAID:
  </para>
  <procedure>
   <step>
    <para>
     Select a hard disk from <guimenu>Hard Disks</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Change to the <guimenu>Partitions</guimenu> tab.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Add</guimenu> and enter the desired size of the raid
     partition on this disk.
    </para>
   </step>
   <step>
    <para>
     Use <guimenu>Do not Format the Partition</guimenu> and change the
     <guimenu>File System ID</guimenu> to <guimenu>0xFD Linux
     RAID</guimenu>. Do not mount this partition.
    </para>
   </step>
   <step>
    <para>
     Repeat this procedure until you have defined all the desired physical
     volumes on the available disks.
    </para>
   </step>
  </procedure>
  <para>
   For RAID&nbsp;0 and RAID&nbsp;1, at least two partitions are
   needed&mdash;for RAID&nbsp;1, usually exactly two and no more. If
   RAID&nbsp;5 is used, at least three partitions are required, RAID 6
   and RAID 10 require at least four partitions. It is recommended to use
   partitions of the same size only. The RAID partitions should be located
   on different hard disks to decrease the risk of losing data if one is
   defective (RAID&nbsp;1 and 5) and to optimize the performance of
   RAID&nbsp;0. After creating all the partitions to use with RAID, click
   <menuchoice><guimenu>RAID</guimenu><guimenu>Add
   RAID</guimenu></menuchoice> to start the RAID configuration.
  </para>
  <para>
   In the next dialog, choose between RAID levels 0, 1, 5, 6 and 10. Then,
   select all partitions with either the <quote>Linux RAID</quote> or
   <quote>Linux native</quote> type that should be used by the RAID system.
   No swap or DOS partitions are shown.
  </para>
  <tip>
   <title>Classify Disks</title>
   <para>
    For RAID types where the order of added disks matters, you can mark
    individual disks with one of the letters A to E. Click the
    <guimenu>Classify</guimenu> button, select the disk and click of the
    <guimenu>Class X</guimenu> buttons, where X is the letter you want to
    assign to the disk. Assign all available RAID disks this way, and
    confirm with <guimenu>OK</guimenu>. You can easily sort the classified
    disks with the <guimenu>Sorted</guimenu> or
    <guimenu>Interleaved</guimenu> buttons, or add a sort pattern from a
    text file with <guimenu>Pattern File</guimenu>.
   </para>
  </tip>
  <figure xml:id="fig.yast2.system.raid.conf">
   <title>RAID Partitions</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="yast2_raid4.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="yast2_raid4.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
   To add a previously unassigned partition to the selected RAID volume,
   first click the partition then <guimenu>Add</guimenu>. Assign all
   partitions reserved for RAID. Otherwise, the space on the partition
   remains unused. After assigning all partitions, click
   <guimenu>Next</guimenu> to select the available <guimenu>RAID
   Options</guimenu>.
  </para>
  <para>
   In this last step, set the file system to use, encryption and
   the mount point for the RAID volume. After completing the configuration
   with <guimenu>Finish</guimenu>, see the <filename>/dev/md0</filename>
   device and others indicated with <emphasis>RAID</emphasis> in the expert
   partitioner.
  </para>
 </sect2>

 <sect2 xml:id="sec.yast2.system.raid.trouble">
  <title>Troubleshooting</title>
  <para>
   Check the file <filename>/proc/mdstat</filename> to find out whether a
   RAID partition has been damaged. If Th system fails, shut
   down your Linux system and replace the defective hard disk with a new one
   partitioned the same way. Then restart your system and enter the command
   <command>mdadm /dev/mdX --add /dev/sdX</command>. Replace 'X' with your
   particular device identifiers. This integrates the hard disk
   automatically into the RAID system and fully reconstructs it.
  </para>
  <para>
   Note that although you can access all data during the rebuild, you may
   encounter some performance issues until the RAID has been fully rebuilt.
  </para>
 </sect2>

 <sect2 xml:id="sec.yast2.system.raid.information">
  <title>For More Information</title>
  <para>
   Configuration instructions and more details for soft RAID can be found in
   the HOWTOs at:
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <filename>/usr/share/doc/packages/mdadm/Software-RAID.HOWTO.html</filename>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://raid.wiki.kernel.org"/>
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Linux RAID mailing lists are available, such as
   <link xlink:href="http://marc.info/?l=linux-raid"/>.
  </para>
 </sect2>
</sect1>
