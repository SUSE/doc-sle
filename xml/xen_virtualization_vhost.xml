<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
  [
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- For supported environments, see
http://www.novell.com/rc/docrepository/public/37/basedocument.2008-06-10.9752681390/SUSE_Linux_Enterprise_10_SP2_Virtualization_Technology_Support_Technical_White_Paper_en.pdf -->
<chapter id="cha.xen.vhost">
 <title>Setting Up a Virtual Machine Host</title>
 <para>
  This section documents how to set up and use &productname; &productnumber;
  as a virtual machine host.
 </para>
 <para>
  In most cases, the hardware requirements for the &dom0; are the same as
  those for the &productname; operating system, but additional CPU, disk,
  memory, and network resources should be added to accommodate the resource
  demands of all planned &vmguest; systems.
 </para>
 <tip>
  <para>
   Remember that &vmguest; systems, just like physical machines, perform
   better when they run on faster processors and have access to more system
   memory.
  </para>
 </tip>
 <para>
  The following table lists the minimum hardware requirements for running a
  typical virtualized environment. Additional requirements have to be added
  for the number and type of the respective guest systems.
 </para>
 <table id="tab.xen.vhost" frame="topbot" rowsep="1" pgwide="0">
  <title>Hardware Requirements</title>
  <tgroup cols="3">
   <colspec colnum="1" colname="1" colwidth="24*"/>
   <colspec colnum="2" colname="2" colwidth="76*"/>
   <thead>
    <row>
     <entry>
      <para>
       System Component
      </para>
     </entry>
     <entry>
      <para>
       Minimum Requirements
      </para>
     </entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry>
      <para>
       Computer
      </para>
     </entry>
     <entry>
      <para>
       Computer with Pentium II or AMD K7 450 MHz processor
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Memory
      </para>
     </entry>
     <entry>
      <para>
       512 MB of RAM for the host
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Free Disk Space
      </para>
     </entry>
     <entry>
      <para>
       7 GB of available disk space for the host.
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Optical Drive
      </para>
     </entry>
     <entry colname="2">
      <para>
       DVD-ROM Drive
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Hard Drive
      </para>
     </entry>
     <entry>
      <para>
       20 GB
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Network Device
      </para>
     </entry>
     <entry>
      <para>
       Ethernet 100 Mbps
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       IP Address
      </para>
     </entry>
     <entry>
      <itemizedlist>
       <listitem>
        <para>
         One IP address on a subnet for the host.
        </para>
       </listitem>
       <listitem>
        <para>
         One IP address on a subnet for each &vmguest;.
        </para>
       </listitem>
      </itemizedlist>
      <para/>
     </entry>
    </row>
   </tbody>
  </tgroup>
 </table>
 <para>
  &xen; virtualization technology is available in &productname; products
  based on code path 10 and later. Code path 10 products include Open
  Enterprise Server 2 Linux, &productname; 10, &sled; 10, and &opensuse;
  10.x.
 </para>
 <para>
  The virtual machine host requires a number of software packages and their
  dependencies to be installed. To install all necessary packages, run
  &yast; <guimenu>Software Management</guimenu>, select
  <menuchoice><guimenu>View</guimenu>
  <guimenu>Patterns</guimenu></menuchoice> and choose <guimenu>&xen; Virtual
  Machine Host Server</guimenu> for installation. The installation can also
  be performed with &yast; using the module
  <menuchoice><guimenu>Virtualization</guimenu><guimenu>Install Hypervisor
  and Tools</guimenu></menuchoice>.
 </para>
 <para>
  After the &xen; software is installed, restart the computer.
 </para>
 <para>
  Updates are available through your update channel. To be sure to have the
  latest updates installed, run &yast; <guimenu>Online Update</guimenu>
  after the installation has finished.
 </para>
 <sect1 id="sec.xen.vhost.best">
  <title>Best Practices and Suggestions</title>

  <para>
   When installing and configuring the &sle; operating system on the host,
   be aware of the following best practices and suggestions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     If the host should always run as &xen; host, run &yast; <menuchoice>
     <guimenu>System</guimenu> <guimenu>Boot Loader</guimenu> </menuchoice>
     and activate the &xen; boot entry as default boot section.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       In &yast;, click <guimenu>System &gt; Boot Loader</guimenu>.
      </para>
     </listitem>
     <listitem>
      <para>
       Change the default boot to the <guimenu>&xen;</guimenu> label, then
       click <guimenu>Set as Default</guimenu>.
      </para>
     </listitem>
     <listitem>
      <para>
       Click <guimenu>Finish</guimenu>.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
<!--
   <listitem>
    <para>
     Disable powersave functionality on the host and all guest operating
     systems. On the host computer and other Linux computers, you can use
     the <command>chkconfig powersaved off</command> command if the service is
     installed.
    </para>
   </listitem> -->
   <listitem>
    <para>
     Close Virtual Machine Manager if you are not actively using it and
     restart it when needed. Closing Virtual Machine Manager does not affect
     the state of virtual machines.
    </para>
   </listitem>
   <listitem>
    <para>
     For best performance, only the applications and processes required for
     virtualization should be installed on the virtual machine host.
    </para>
   </listitem>
   <listitem>
<!-- TODO: this is outdated!
      - document Xen with openAIS, outline with old heartbeat is
         http://www.novell.com/coolsolutions/feature/19571.html
    -->
    <para>
     When using both, iSCSI and OCFS2 to host &xen; images, the latency
     required for OCFS2 default timeouts in SP2 may not be met. To
     reconfigure this timeout, run <command>/etc/init.d/o2cb
     configure</command> or edit <literal>O2CB_HEARTBEAT_THRESHOLD</literal>
     in the system configuration.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.xen.vhost.memory">
  <title>Managing &dom0; Memory</title>

  <para>
   When the host is set up, a percentage of system memory is reserved for
   the hypervisor, and all remaining memory is automatically allocated to
   &dom0;.
  </para>

  <para>
   A better solution is to set a default amount of memory for &dom0;, so the
   memory can be allocated appropriately to the hypervisor. An adequate
   amount would be 20 percent of the total system memory up to 2 GB. An
   appropriate minimum amount would be 512 MB.
  </para>

  <sect2 id="sec.xen.vhost.maxmem">
   <title>Setting a Maximum Amount of Memory</title>
   <procedure>
    <step>
     <para>
      Determine the amount of memory to set for &dom0;.
     </para>
    </step>
    <step>
     <para>
      At &dom0;, type <command>xm info</command> to view the amount of
      memory that is available on the machine. The memory that is currently
      allocated by &dom0; can be determined with the command <command>xm
      list</command>.
     </para>
    </step>
    <step>
     <para>
      Run <menuchoice> <guimenu>&yast;</guimenu> <guimenu>Boot
      Loader</guimenu> </menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Select the &xen; section.
     </para>
    </step>
    <step>
     <para>
      In <guimenu>Additional &xen; Hypervisor Parameters</guimenu>, add
      <command>dom0_mem=<replaceable>mem_amount</replaceable></command>
      where <replaceable>mem_amount</replaceable> is the maximum amount of
      memory to allocate to &dom0;. Add <command>K</command>,
      <command>M</command>, or <command>G</command>, to specify the size,
      for example, <command>dom0_mem=768M</command>.
     </para>
    </step>
    <step>
     <para>
      Restart the computer to apply the changes.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.xen.vhost.minmem">
   <title>Setting a Minimum Amount of Memory</title>
   <para>
    To set a minimum amount of memory for &dom0;, edit the
    <command>dom0-min-mem</command> parameter in the
    <filename>/etc/xen/xend-config.sxp</filename> file and restart Xend. For
    more information, see <xref linkend="sec.xen.manage.xend"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.xen.vhost.netcard">
  <title>Network Card in Fully Virtualized Guests</title>

  <para>
   In a fully virtualized guest, the default network card is an emulated
   Realtek network card. However, it also possible to use the split network
   driver to run the communication between &dom0; and a &vmguest;. By
   default, both interfaces are presented to the &vmguest;, because the
   drivers of some operating systems require both to be present.
  </para>

  <para>
   When using &sle;, only the paravirtualized network cards are available
   for the &vmguest; by default. The following network options are
   available:
  </para>

  <variablelist>
   <varlistentry>
    <term>emulated</term>
    <listitem>
     <para>
      To use a <quote>emulated</quote> network interface like an emulated
      Realtek card, specify <literal>(type ioemu)</literal> in the vif
      device section of the Xend configuration. An example configuration
      would look like:
     </para>
<screen>
(device
    (vif
        (bridge br0)
        (uuid e2b8f872-88c7-0a4a-b965-82f7d5bdd31e)
        (devid 0)
        (mac 00:16:3e:54:79:a6)
        (model rtl8139)
        (type ioemu)
    )
)
       </screen>
     <para>
      Find more details about editing the Xend configuration at
      <xref
         linkend="sec.xen.manage.vmachine"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>paravirtualized</term>
    <listitem>
     <para>
      When not specifying a model or type, Xend uses the paravirtualized
      network interface:
     </para>
<screen>
(device
    (vif
        (bridge br0)
        (mac 00:16:3e:50:66:a4)
        (script /etc/xen/scripts/vif-bridge)
        (uuid 0a94b603-8b90-3ba8-bd1a-ac940c326514)
        (backend 0)
    )
)
       </screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>emulated and paravirtualized</term>
    <listitem>
     <para>
      If the administrator should be offered both options, simply specify
      both, type and model. The Xend configuration would look like:
     </para>
<screen>
(device
    (vif
        (bridge br0)
        (uuid e2b8f872-88c7-0a4a-b965-82f7d5bdd31e)
        (devid 0)
        (mac 00:16:3e:54:79:a6)
        (model rtl8139)
        (type netfront)
    )
)
       </screen>
     <para>
      In this case, one of the network interfaces should be disabled on the
      &vmguest;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.xen.vhost.start">
  <title>Starting the Virtual Machine Host</title>

  <para>
   If virtualization software is correctly installed, the computer boots to
   display the &grub; boot loader with a <citetitle>Xen</citetitle> option
   on the menu. Select this option to start the virtual machine host.
  </para>

  <note>
   <title>Xen and Kdump</title>
   <para>
    In &xen;, the hypervisor manages the memory resource. If you need to
    reserve system memory for a recovery kernel in &dom0;, this memory has
    to be reserved by the hypervisor. Thus, it is necessary to add the
    parameter <systemitem>crashkernel=size@offset</systemitem> to the
    <literal>kernel</literal> line instead of using the line with the other
    boot options.
   </para>
  </note>

  <para>
   If the <citetitle>&xen;</citetitle> option is not on the &grub; menu,
   review the steps for installation and verify that the &grub; boot loader
   has been updated. If the installation has been done without selecting the
   &xen; pattern, run the &yast; <guimenu>Software Management</guimenu>,
   select the filter <guimenu>Patterns</guimenu> and choose <guimenu>&xen;
   Virtual Machine Host Server</guimenu> for installation.
  </para>

  <para>
   After booting the hypervisor, the &dom0; virtual machine starts and
   displays its graphical desktop environment. If you did not install a
   graphical desktop, the command line environment appears.
  </para>

  <tip>
   <title>Graphics Problems</title>
   <para>
    Sometimes it may happen that the graphics system does not work properly.
    In this case, add <literal>vga=ask</literal> to the boot parameters. To
    activate permanent settings, use <literal>vga=mode-0x???</literal> where
    <literal>???</literal> is calculated as <literal>0x100</literal> + VESA
    mode from
    <ulink
     url="http://en.wikipedia.org/wiki/VESA_BIOS_Extensions"/>,
    e.g. <literal>vga=mode-0x361</literal>.
   </para>
  </tip>

  <para>
   Before starting to install virtual guests, make sure that the system time
   is correct. To do this, configure NTP (Network Time Protocol) on the
   controlling domain:
  </para>

  <procedure>
   <step>
    <para>
     In &yast; select <menuchoice> <guimenu>Network Services</guimenu>
     <guimenu>NTP Configuration</guimenu> </menuchoice>.
    </para>
   </step>
   <step>
    <para>
     Select the option to automatically start the NTP daemon during boot.
     Provide the IP address of an existing NTP time server, then click
     <guimenu>Finish</guimenu>.
    </para>
   </step>
  </procedure>

  <note>
   <title>Time Services on Virtual Guests</title>
   <para>
    Hardware clocks commonly are not very precise. All modern operating
    systems try to correct the system time compared to the hardware time by
    means of an additional time source. To get the correct time on all
    &vmguest; systems, also activate the network time services on each
    respective guest or make sure that the guest uses the system time of the
    host. For more about <literal>Independent Wallclocks</literal> in
    &productname; see <xref linkend="sec.xen.guests.suse.time"/>.
   </para>
  </note>

  <para>
   For more information about managing virtual machines, see
   <xref linkend="cha.xen.manage"/>.
  </para>
 </sect1>
 <sect1 id="sec.xen.vhost.pciback">
  <title>&pciback;</title>

  <para>
   To take full advantage of &vmguest; systems, it is sometimes necessary to
   assign specific PCI devices to a dedicated domain. When using fully
   virtualized guests, this functionality is only available if the chipset
   of the system supports this feature, and if it is activated from the
   BIOS.
  </para>

  <para>
   This feature is available from both, AMD* and Intel*. For AMD machines,
   the feature is called IOMMU, in Intel speak, this is VT-d. Note that
   Intel-VT technology is not sufficient to use this feature for fully
   virtualized guests. To make sure that your computer supports this
   feature, ask your supplier specifically to deliver a system that supports
   &pciback;.
  </para>

  <itemizedlist>
   <title>Limitations</title>
   <listitem>
    <para>
     Some graphics drivers use highly optimized ways to access DMA. This is
     not always supported, and thus using graphics cards may be difficult.
    </para>
   </listitem>
   <listitem>
    <para>
     When accessing PCI devices behind a PCIe bridge, all of the PCI devices
     must be assigned to a single guest. This limitations does not apply to
     PCIe devices.
    </para>
   </listitem>
   <listitem>
    <para>
     Guests with dedicated PCI devices cannot be live migrated to a
     different host.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The configuration of &pciback; is twofold. First, the hypervisor must be
   informed at boot time that a PCI device should be available for
   reassigning. Second, the PCI device must be assigned to the &vmguest;.
  </para>

  <sect2 id="config.hypervisor.pciback">
   <title>Configuring the Hypervisor for &pciback;</title>
   <procedure>
    <step>
     <para>
      Select a device to reassign to a &vmguest;. To do this run
      <command>lspci</command> and read the device number. For example, if
      <command>lspci</command> contains the following line:
     </para>
<screen>06:01.0 Ethernet controller: Digital Equipment Corporation DECchip 21142/43 (rev 41)</screen>
     <para>
      In this case, the PCI number is <literal>(06:01.0)</literal>.
     </para>
    </step>
    <step>
     <para>
      Run <menuchoice><guimenu>&yast;</guimenu><guimenu>System</guimenu>
      <guimenu>Boot Loader</guimenu></menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Select the <literal>&xen;</literal> section and press
      <guimenu>Edit</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Add the PCI number to the <guimenu>Optional Kernel Command Line
      Parameter</guimenu> line:
     </para>
<screen>pciback.hide=(06:01.0)</screen>
    </step>
    <step>
     <para>
      Press <guimenu>OK</guimenu> and finish &yast;.
     </para>
    </step>
    <step>
     <para>
      Reboot the system.
     </para>
    </step>
    <step>
     <para>
      Check if the device is in the list of assignable devices with the
      command
     </para>
<screen>xm pci-list-assignable-devices</screen>
    </step>
   </procedure>
   <sect3>
    <title>Solution without Host System Restart</title>
    <para>
     If you want to avoid restarting the host system, there is an
     alternative procedure to prepare the host system for &pciback; via the
     <filename>/sys/bus/pci</filename> file system:
    </para>
    <procedure>
     <step>
      <para>
       Identify the PCI device and store it to a variable for easier
       handling.
      </para>
<screen># export PCI_DOMAIN_BUS_SLOT_FUNC=06:01.0</screen>
     </step>
     <step>
      <para>
       Check which driver is currently bound to the device and save its name
       to a variable.
      </para>
<screen># readlink /sys/bus/pci/devices/0000\:06:01.0/driver
../../../../bus/pci/drivers/igb
# export DRIVER_NAME=igb</screen>
     </step>
     <step>
      <para>
       Detach the driver from the device, and load the
       <systemitem>pciback</systemitem> module.
      </para>
<screen># echo -n $PCI_DOMAIN_BUS_SLOT_FUNC > \
/sys/bus/pci/drivers/$DRIVER_NAME/unbind
# modprobe pciback</screen>
     </step>
     <step>
      <para>
       Add a new slot to the <systemitem>pciback</systemitem>'s list.
      </para>
<screen># echo -n $PCI_DOMAIN_BUS_SLOT_FUNC > \
/sys/bus/pci/drivers/pciback/new_slot</screen>
     </step>
     <step>
      <para>
       Bind the PCI device to <systemitem>pciback</systemitem>.
      </para>
<screen># echo -n $PCI_DOMAIN_BUS_SLOT_FUNC > \
/sys/bus/pci/drivers/pciback/bind</screen>
     </step>
    </procedure>
    <para>
     The device is now ready to be used in &vmguest; by specifying
     <literal>'pci=[$PCI_DOMAIN_BUS_SLOT_FUNC]'</literal> in the guest
     config file.
    </para>
   </sect3>
  </sect2>

  <sect2>
   <title>Assigning PCI Devices to &vmguest; Systems</title>
   <para>
    There are several possibilities to dedicate a PCI device to a &vmguest;:
   </para>
   <variablelist>
    <varlistentry>
     <term>Adding the device while installing:</term>
     <listitem>
      <para>
       During installation, add the <literal>pci</literal> line to the
       configuration file:
      </para>
<screen>pci=['06:01.0']</screen>
      <tip id="managed.pci.devices">
       <para>
        If you want the &xen; tools to manage preparing and assigning a PCI
        device to a &vmguest; when it is activated, add
        <literal>managed=1</literal> to the PCI setting in the guest
        configuration file, denoting that it is a 'managed' PCI device:
       </para>
<screen>pci=['06:01.0,managed=1']</screen>
       <para>
        When the &vmguest; is activated, the &xen; tools will unbind the PCI
        device from its existing driver, bind it to pciback, and attach the
        device to the VM. When the VM is shut down, the tools will rebind the
        device to its original driver. When using the
        <emphasis>managed</emphasis> mode, there is no need to configure the
        hypervisor for PCI Pass-Through as described in
        <xref linkend="config.hypervisor.pciback"/>.
       </para>
      </tip>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Hot adding PCI devices to &vmguest; systems</term>
     <listitem>
      <para>
       The command <literal>xm</literal> may be used to add or remove PCI
       devices on the fly. To Add the device with number
       <literal>06:01.0</literal> to a guest with name
       <literal>sles11</literal> use:
      </para>
<screen>xm pci-attach sles11 06:01.0</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Adding the PCI device to Xend</term>
     <listitem>
      <para>
       To add the device to the Xend database, add the following section to
       the Xend database:
      </para>
<screen>
(device
    (pci
        (dev
            (slot 0x01)
            (domain 0x0)
            (bus 0x06)
            (vslt 0x0)
            (func 0x0)
        )
    )
)
      </screen>
      <para>
       For more information about modifying the Xend database, see
       <xref
        linkend="sec.xen.manage.vmachine"/>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    After assigning the PCI device to the &vmguest;, the guest system must
    care for the configuration and device drivers for this device.
   </para>
  </sect2>

  <sect2>
   <title>&vgaback;</title>
   <para>
    &xen; 4.0 and newer supports VGA graphics adapter pass-through on fully
    virtualized &vmguest;s. The guest can take full control of the graphics
    adapter with high performance full 3D and video acceleration.
   </para>
   <itemizedlist>
    <title>Limitations</title>
    <listitem>
     <para>
      &vgaback; functionality is similar to &pciback; and as such also
      requires IOMMU (or Intel VT-d) support from the motherboard chipset
      and BIOS.
     </para>
    </listitem>
    <listitem>
     <para>
      Only the primary graphics adapter (the one that is used when you power
      on the computer) can be used with &vgaback;.
     </para>
    </listitem>
    <listitem>
     <para>
      &vgaback; is supported only for fully virtualized guests. Paravirtual
      guests (PV) are not supported.
     </para>
    </listitem>
    <listitem>
     <para>
      The graphics card cannot be shared between multiple &vmguest;s using
      &vgaback; &mdash; you can dedicate it to one guest only.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    To enable &vgaback;, add the following settings to your fully
    virtualized guest configuration file
   </para>
<screen>gfx_passthru=1 
pci=['yy:zz.n']</screen>
   <para>
    where <literal>yy:zz.n</literal> is the PCI controller ID of the VGA
    graphics adapter as found with <command>lspci -v</command> on &dom0;.
   </para>
  </sect2>

  <sect2>
   <title>For More Information</title>
   <para>
    There are several resources that provide interesting information about
    &pciback; in the net:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <ulink url="http://wiki.xensource.com/xenwiki/VTdHowTo"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="http://software.intel.com/en-us/articles/intel-virtualization-technology-for-directed-io-vt-d-enhancing-intel-platforms-for-efficient-virtualization-of-io-devices/"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/34434.pdf"/>
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
</chapter>
