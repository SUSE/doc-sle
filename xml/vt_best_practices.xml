<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
                 type="text/xml"
                 title="Profiling step"?>
<!DOCTYPE article         [
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<article xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="article-vt-best-practices" xml:lang="en">
<?suse-quickstart color="suse"?>
 <title>Virtualization Best Practices</title>
 <subtitle>&productname; &productnumber;</subtitle>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
  </dm:docmanager><productname>&productname;</productname>
  <productnumber>&productnumber;</productnumber><date>
<?dbtimestamp format="B d, Y"?></date>
 </info>
<!--
     we can write it based on scenario?
     Virtualization Capabilities:
     Consolidation (hardware1+hardware2 -> hardware)
     Isolation
     Migration (hardware1 -> hardware2)
     Disaster recovery
     Dynamic load balancing
 -->
 <sect1 xml:id="sec-vt-best-scenario">
  <title>Virtualization scenarios</title>

  <para>
   Virtualization offers a lot of capabilities to your environment. It can be
   used in multiple scenarios. To get more details about it, refer to the
   <xref linkend="book-virt"/> and in particular, to the following sections:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <xref linkend="sec-virtualization-scenarios-capabilities"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec-virtualization-introduction-benefits"/>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   This best practice guide will provide advice for making the right choice in
   your environment. It will recommend or discourage the usage of options
   depending on your workload. Fixing configuration issues and performing
   tuning tasks will increase the performance of &vmguest;'s near to bare
   metal.
  </para>

  <remark>
   Bruce 20150806: There is no mention of caching considerations, of migration
   inhibitors, or basic strategies for how to do your storage or networking
   infrastructure, and only a little bit about how to map your virtualization
   requirements to host capabilities.  I see that the doc doesn't address the
   issue of when to use &xen; PV vs. &xen; FV vs. &kvm; at all.
  </remark>

<!-- Todo
       <Table Rowsep="1">
       <title>Scenario</title>
       <tgroup cols="3">
       <colspec colnum="1" colname="1" colwidth=""/>
       <colspec colnum="2" colname="2" colwidth=""/>
       <thead>
       <row>
       <entry>
       <para>Scenarios</para>
       </entry>
       <entry>
       <para>Option Recommended for</para>
       </entry>
       <entry>
       <para>Option Not recommended for</para>
       </entry>
       </row>
       </thead>
       <tbody>
       <row>
       <entry>
       <para>Consolidation</para>
       </entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Isolation</para>
       </entry>
       <entry></entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Migration</para>
       </entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Disaster Recovery</para>
       </entry>
       <entry>
       <mediaobject>
       <imageobject>
       <imagedata fileref="thumbup_green.png" width="25%" format="PNG"/>
       </imageobject>
       </mediaobject>
       </entry>
       </row>
       <row>
       <entry>
       <para>Dynamic Load Balancing</para>
       </entry>
       <entry></entry>
       <entry>
       <mediaobject>
       <imageobject>
       <imagedata fileref="thumbdown_red.png" width="25%" format="PNG"/>
       </imageobject>
       </mediaobject>
       </entry>
       </row>
       </tbody>
       </tgroup>
       </table>
  -->
 </sect1>
 <sect1 xml:id="sec-vt-best-intro">
  <title>Before you apply modifications</title>

  <sect2 xml:id="sec-vt-best-intro-backup">
   <title>Back up first</title>
   <para>
    Changing the configuration of the &vmguest; or the &vmhost; can lead to
    data loss or an unstable state. It is really important that you do backups
    of files, data, images, etc. before making any changes. Without backups you
    cannot restore the original state after a data loss or a misconfiguration.
    Do not perform tests or experiments on production systems.
   </para>
  </sect2>

  <sect2 xml:id="sec-vt-best-intro-testing">
   <title>Test your workloads</title>
   <para>
    The efficiency of a virtualization environment depends on many factors.
    This guide provides a reference for helping to make good choices when
    configuring virtualization in a production environment. Nothing is
    <emphasis>carved in stone</emphasis>. Hardware, workloads, resource
    capacity, etc. should all be considered when planning, testing, and
    deploying your virtualization infra-structure. Testing your virtualized
    workloads is vital to a successful virtualization implementation.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-vt-best-reco">
  <title>Recommendations</title>

  <sect2 xml:id="sec-vt-best-intro-libvirt">
   <title>Prefer the &libvirt; framework</title>
   <remark>add some explanation about how to enable/disable kvm with
   qemu-system-arch</remark>
   <para>
    &suse; strongly recommends using the &libvirt; framework to configure,
    manage, and operate &vmhost;s, containers and &vmguest;. It offers a single
    interface (GUI and shell) for all supported virtualization technologies and
    therefore is easier to use than the hypervisor-specific tools.
   </para>
   <para>
    We do not recommend using libvirt and hypervisor-specific tools at the same
    time, because changes done with the hypervisor-specific tools may not be
    recognized by the libvirt tool set. See
    <xref linkend="cha-libvirt-overview"/> for more information on libvirt.
   </para>
  </sect2>

  <sect2 xml:id="sec-vt-best-intro-qemu">
   <title>qemu-system-i386 compared to qemu-system-x86_64</title>
   <remark>Lin 20150808: Should we add some explanation about how to
   enable/disable kvm with qemu-system-arch</remark>
   <para>
    Similar to real 64-bit PC hardware, <command>qemu-system-x86_64</command>
    supports &vmguest;s running a 32-bit or a 64-bit operating system. Because
    <command>qemu-system-x86_64</command> usually also provides better
    performance for 32-bit guests, &suse; generally recommends using
    <command>qemu-system-x86_64</command> for both 32-bit and 64-bit &vmguest;s
    on KVM. Scenarios where <command>qemu-system-i386</command> is known to
    perform better are not supported by &suse;.
   </para>
   <para>
    Xen also uses binaries from the qemu package but prefers
    <command>qemu-system-i386</command>, which can be used for both 32-bit and
    64-bit Xen &vmguest;s. To maintain compatibility with the upstream Xen
    Community, SUSE encourages using <command>qemu-system-i386</command> for
    Xen &vmguest;s.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-vt-best-hostlevel">
  <title>&vmhost; configuration and resource allocation</title>

  <para>
   Allocation of resources for &vmguest;s is a crucial point when
   administrating virtual machines. When assigning resources to &vmguest;s, be
   aware that overcommitting resources may affect the performance of the
   &vmhost; and the &vmguest;s. If all &vmguest;s request all their resources
   simultaneously, the host needs to be able to provide all of them. If not,
   the host's performance will be negatively affected and this will in turn
   also have negative effects on the &vmguest;'s performance.
  </para>

  <sect2 xml:id="sec-vt-best-mem">
   <title>Memory</title>
   <para>
    Linux manages memory in units called pages. On most systems the default
    page size is 4 KB. Linux and the CPU need to know which pages belong to
    which process. That information is stored in a page table. If a lot of
    processes are running, it takes more time to find where the memory is
    mapped, because of the time required to search the page table. To speed up
    the search, the TLB (Translation Lookaside Buffer) was invented. But on a
    system with a lot of memory, the TLB is not enough. To avoid any fallback
    to normal page table (resulting in a cache miss, which is time consuming),
    huge pages can be used. Using huge pages will reduce TLB overhead and TLB
    misses (pagewalk). A host with 32 GB (32*1014*1024 = 33,554,432 KB) of
    memory and a 4 KB page size has a TLB with <emphasis>33,554,432/4 =
    8,388,608</emphasis> entries. Using a 2 MB (2048 KB) page size, the TLB
    only has <emphasis>33554432/2048 = 16384</emphasis> entries, considerably
    reducing TLB misses.
   </para>
   <sect3 xml:id="sec-vt-best-mem-huge-pages">
    <title>Configuring the &vmhost; and the &vmguest; to use huge pages</title>
    <para>
     Current CPU architectures support larger pages than 4 KB: huge pages. To
     determine the size of huge pages available on your system (could be 2 MB
     or 1 GB), check the <literal>flags</literal> line in the output of
     <filename>/proc/cpuinfo</filename> for occurrences of
     <literal>pse</literal> and/or <literal>pdpe1gb</literal>.
    </para>
    <table>
     <title>Determine the available huge pages size</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="30%"/>
      <colspec colnum="2" colname="2" colwidth="70%"/>
      <thead>
       <row>
        <entry>
         <para>
          CPU flag
         </para>
        </entry>
        <entry>
         <para>
          Huge pages size available
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <para>
          Empty string
         </para>
        </entry>
        <entry>
         <para>
          No huge pages available
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          pse
         </para>
        </entry>
        <entry>
         <para>
          2 MB
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          pdpe1gb
         </para>
        </entry>
        <entry>
         <para>
          1 GB
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para>
     Using huge pages improves performance of &vmguest;s and reduces host
     memory consumption.
    </para>
    <para>
     By default the system uses THP. To make huge pages available on your
     system, activate it at boot time with <option>hugepages=1</option>,
     and&mdash;optionally&mdash;add the huge pages size with, for example,
     <option>hugepagesz=2MB</option>.
    </para>
    <note>
     <title>1 GB huge pages</title>
     <para>
      1 GB pages can only be allocated at boot time and cannot be freed
      afterward.
     </para>
    </note>
    <para>
     To allocate and use the huge page table (HugeTlbPage) you need to mount
     <filename>hugetlbfs</filename> with correct permissions.
    </para>
    <note>
     <title>Restrictions of huge pages</title>
     <para>
      Even if huge pages provide the best performance, they do come with some
      drawbacks. You lose features such as Memory ballooning (see
      <xref linkend="sec-vt-best-vmguests-virtio-balloon"/>), KSM (see
      <xref linkend="sec-vt-best-perf-ksm"/>), and huge pages cannot be
      swapped.
     </para>
    </note>
    <procedure>
     <title>Configuring the use of huge pages</title>
     <step>
      <para>
       Mount <literal>hugetlbfs</literal> to
       <filename>/dev/hugepages</filename>:
      </para>
<screen>&prompt.user;&sudo; mount -t hugetlbfs hugetlbfs /dev/hugepages</screen>
     </step>
     <step>
      <para>
       To reserve memory for huge pages use the <command>sysctl</command>
       command. If your system has a huge page size of 2 MB (2048 KB), and you
       want to reserve 1 GB (1,048,576 KB) for your &vmguest;, you need
       <emphasis>1,048,576/2048=512</emphasis> pages in the pool:
      </para>
<screen>&prompt.user;&sudo; sysctl vm.nr_hugepages=<replaceable>512</replaceable></screen>
      <para>
       The value is written to <filename>/proc/sys/vm/nr_hugepages</filename>
       and represents the current number of <emphasis>persistent</emphasis>
       huge pages in the kernel's huge page pool.
       <emphasis>Persistent</emphasis> huge pages will be returned to the huge
       page pool when freed by a task.
      </para>
     </step>
     <step>
      <para>
       Add the <literal>memoryBacking</literal> element in the &vmguest;
       configuration file (by running <command>virsh edit
       <replaceable>CONFIGURATION</replaceable></command>).
      </para>
<screen>&lt;memoryBacking&gt;
  &lt;hugepages/&gt;
&lt;/memoryBacking&gt;</screen>
     </step>
     <step>
      <para>
       Start your &vmguest; and check on the host whether it uses hugepages:
      </para>
<screen>&prompt.user;cat /proc/meminfo | grep HugePages_
HugePages_Total:<co xml:id="co-hp-total"/>     512
HugePages_Free:<co xml:id="co-hp-free"/>       92
HugePages_Rsvd:<co xml:id="co-hp-rsvd"/>        0
HugePages_Surp:<co xml:id="co-hp-surp"/>        0</screen>
      <calloutlist>
       <callout arearefs="co-hp-total">
        <para>
         Size of the pool of huge pages
        </para>
       </callout>
       <callout arearefs="co-hp-free">
        <para>
         Number of huge pages in the pool that are not yet allocated
        </para>
       </callout>
       <callout arearefs="co-hp-rsvd">
        <para>
         Number of huge pages for which a commitment to allocate from the pool
         has been made, but no allocation has yet been made
        </para>
       </callout>
       <callout arearefs="co-hp-surp">
        <para>
         Number of huge pages in the pool above the value in
         <filename>/proc/sys/vm/nr_hugepages</filename>. The maximum number of
         surplus huge pages is controlled by
         <filename>/proc/sys/vm/nr_overcommit_hugepages</filename>
        </para>
       </callout>
      </calloutlist>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec-vt-best-mem-thp">
    <title>Transparent huge pages</title>
    <para>
     Transparent huge pages (THP) provide a way to dynamically allocate huge
     pages with the <command>khugepaged</command> kernel thread, rather than
     manually managing their allocation and use. Workloads with contiguous
     memory access patterns can benefit greatly from THP. A 1000 fold decrease
     in page faults can be observed when running synthetic workloads with
     contiguous memory access patterns. Conversely, workloads with sparse
     memory access patterns (like databases) may perform poorly with THP. In
     such cases it may be preferable to disable THP by adding the kernel
     parameter <option>transparent_hugepage=never</option>, rebuild your grub2
     configuration, and reboot. Verify if THP is disabled with:
    </para>
<screen>&prompt.user;cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]</screen>
    <para>
     If disabled, the value <literal>never</literal> is shown in square
     brackets like in the example above.
    </para>
    <note>
     <title>&xen;</title>
     <para>
      THP is not available under &xen;.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="sec-vt-best-mem-xen">
    <title>&xen;-specific memory notes</title>
    <sect4 xml:id="sec-vt-best-mem-xen-dom-0">
     <title>Managing domain-0 memory</title>
     <para>
      In previous versions of &sls;, the default memory allocation scheme of a
      &xen; host was to allocate all host physical memory to &dom0; and enable
      auto-ballooning. Memory was automatically ballooned from &dom0; when
      additional domains were started. This behavior has always been error
      prone and disabling it was strongly encouraged. Starting in &sls; 15 SP1,
      auto-ballooning has been disabled by default and &dom0; is given 10% of
      host physical memory + 1GB. For example, on a host with 32GB of physical
      memory, 4.2GB of memory is allocated for &dom0;.
     </para>
     <para>
      The use of <option>dom0_mem</option> &xen; command line option in
      <filename>/etc/default/grub</filename> is still supported and encouraged
      (see <xref linkend="sec-vt-best-kernel-parameter"/> for more
      information). You can restore the old behavior by setting
      <option>dom0_mem</option> to the host physical memory size and enabling
      the <option>autoballoon</option> setting in
      <filename>/etc/xen/xl.conf</filename>.
     </para>
    </sect4>
    <sect4 xml:id="sec-vt-best-mem-xen-tmpfs">
     <title>xenstore in <systemitem>tmpfs</systemitem></title>
     <para>
      When using &xen;, we recommend to place the xenstore database on
      <systemitem>tmpfs</systemitem>. xenstore is used as a control plane by
      the xm/xend and xl/libxl toolstacks and the front-end and back-end
      drivers servicing domain I/O devices. The load on xenstore increases
      linearly as the number of running domains increase. If you anticipate
      hosting many &vmguest; on a &xen; host, move the xenstore database onto
      tmpfs to improve overall performance of the control plane. Mount the
      <filename>/var/lib/xenstored</filename> directory on tmpfs:
     </para>
<screen>&prompt.user;&sudo; mount -t tmpfs tmpfs /var/lib/xenstored/</screen>
    </sect4>
   </sect3>
   <sect3 xml:id="sec-vt-best-perf-ksm">
    <title>KSM and page sharing</title>
    <para>
     Kernel Samepage Merging is a kernel feature that allows for lesser memory
     consumption on the &vmhost; by sharing data &vmguest;s have in common. The
     KSM daemon <systemitem class="daemon">ksmd</systemitem> periodically scans
     user memory looking for pages of identical content which can be replaced
     by a single write-protected page. To enable KSM, run:
    </para>
<screen>&prompt.user;&sudo; echo 1 &gt; /sys/kernel/mm/ksm/run</screen>
    <para>
     One advantage of using KSM from a &vmguest;'s perspective is that all
     guest memory is backed by host anonymous memory. You can share
     <emphasis>pagecache</emphasis>, <emphasis>tmpfs</emphasis> or any kind of
     memory allocated in the guest.
    </para>
    <para>
     KSM is controlled by <systemitem>sysfs</systemitem>. You can check KSM's
     values in <filename>/sys/kernel/mm/ksm/</filename>:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>pages_shared</literal>: The number of shared pages that are
       being used (read-only).
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pages_sharing</literal>: The number of sites sharing the pages
       (read-only).
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pages_unshared</literal>: The number of pages that are unique
       and repeatedly checked for merging (read-only).
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pages_volatile</literal>: The number of pages that are changing
       too fast to be considered for merging (read-only).
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>full_scans</literal>: The number of times all mergeable areas
       have been scanned (read-only).
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>sleep_millisecs</literal>: The number of milliseconds
       <systemitem class="daemon">ksmd</systemitem> should sleep before the
       next scan. A low value will overuse the CPU, consuming CPU time that
       could be used for other tasks. We recommend a value greater than
       <literal>1000</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pages_to_scan</literal>: The number of present pages to scan
       before ksmd goes to sleep. A high value will overuse the CPU. We
       recommend to start with a value of <literal>1000</literal>, and then
       adjust as necessary based on the KSM results observed while testing your
       deployment.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>merge_across_nodes</literal>: By default the system merges
       pages across NUMA nodes. Set this option to <literal>0</literal> to
       disable this behavior.
      </para>
     </listitem>
    </itemizedlist>
    <note>
     <title>Use cases</title>
     <para>
      KSM is a good technique to over-commit host memory when running multiple
      instances of the same application or &vmguest;. When applications and
      &vmguest; are heterogeneous and do not share any common data, it is
      preferable to disable KSM. In a mixed heterogeneous and homogeneous
      environment, KSM can be enabled on the host but disabled on a per
      &vmguest; basis. Use <command>virsh edit</command> to disable page
      sharing of a &vmguest; by adding the following to the guest's XML
      configuration:
     </para>
<screen>&lt;memoryBacking&gt;
   &lt;nosharepages/&gt;
&lt;/memoryBacking&gt;</screen>
    </note>
    <warning>
     <title>Avoid out-of-memory conditions</title>
     <para>
      KSM can free up some memory on the host system, but the administrator
      should reserve enough swap to avoid out-of-memory conditions if that
      shareable memory decreases. If the amount of shareable memory decreases,
      the use of physical memory is increased.
     </para>
    </warning>
    <warning>
     <title>Memory access latencies</title>
     <para>
      By default, KSM will merge common pages across NUMA nodes. If the merged,
      common page is now located on a distant NUMA node (relative to the node
      running the &vmguest; vCPUs), this may degrade &vmguest; performance. If
      increased memory access latencies are noticed in the &vmguest;, disable
      cross-node merging with the <literal>merge_across_nodes</literal> sysfs
      control:
     </para>
<screen>&prompt.user;&sudo; echo 0 &gt; /sys/kernel/mm/ksm/merge_across_nodes</screen>
    </warning>
   </sect3>
   <sect3 xml:id="sec-vt-best-mem-hot">
    <title>&vmguest;: memory hotplug</title>
    <para>
     To optimize the usage of your host memory, it may be useful to hotplug
     more memory for a running &vmguest; when required. To support memory
     hotplugging, you must first configure the
     <literal>&lt;maxMemory&gt;</literal> tag in the &vmguest;'s configuration
     file:
    </para>
<screen>&lt;maxMemory<co xml:id="co-mem-hot-max"/> slots='16'<co xml:id="co-mem-hot-slots"/> unit='KiB'&gt;20971520<co xml:id="co-mem-hot-size"/>&lt;/maxMemory&gt;
  &lt;memory<co xml:id="co-mem-hot-mem"/> unit='KiB'&gt;1048576&lt;/memory&gt;
&lt;currentMemory<co xml:id="co-mem-hot-curr"/> unit='KiB'&gt;1048576&lt;/currentMemory&gt;</screen>
    <calloutlist>
     <callout arearefs="co-mem-hot-max">
      <para>
       Runtime maximum memory allocation of the guest.
      </para>
     </callout>
     <callout arearefs="co-mem-hot-slots">
      <para>
       Number of slots available for adding memory to the guest
      </para>
     </callout>
     <callout arearefs="co-mem-hot-size">
      <para>
       Valid units are:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         "KB" for kilobytes (1,000 bytes)
        </para>
       </listitem>
       <listitem>
        <para>
         "k" or "KiB" for kibibytes (1,024 bytes)
        </para>
       </listitem>
       <listitem>
        <para>
         "MB" for megabytes (1,000,000 bytes)
        </para>
       </listitem>
       <listitem>
        <para>
         "M" or "MiB" for mebibytes (1,048,576 bytes)
        </para>
       </listitem>
       <listitem>
        <para>
         "GB" for gigabytes (1,000,000,000 bytes)
        </para>
       </listitem>
       <listitem>
        <para>
         "G" or "GiB" for gibibytes (1,073,741,824 bytes)
        </para>
       </listitem>
       <listitem>
        <para>
         "TB" for terabytes (1,000,000,000,000 bytes)
        </para>
       </listitem>
       <listitem>
        <para>
         "T" or "TiB" for tebibytes (1,099,511,627,776 bytes)
        </para>
       </listitem>
      </itemizedlist>
     </callout>
     <callout arearefs="co-mem-hot-mem">
      <para>
       Maximum allocation of memory for the guest at boot time
      </para>
     </callout>
     <callout arearefs="co-mem-hot-curr">
      <para>
       Actual allocation of memory for the guest
      </para>
     </callout>
    </calloutlist>
    <para>
     To hotplug memory devices into the slots, create a file
     <filename>mem-dev.xml</filename> like the following:
    </para>
<screen>&lt;memory model='dimm'&gt;
  &lt;target&gt;
  &lt;size unit='KiB'>524287&lt;/size&gt;
  &lt;node>0&lt;/node&gt;
  &lt;/target&gt;
&lt;/memory&gt;</screen>
    <para>
     And attach it with the following command:
    </para>
<screen>&prompt.user;virsh attach-device vm-name mem-dev.xml</screen>
    <para>
     For memory device hotplug, the guest must have at least 1 NUMA cell
     defined (see <xref linkend="sec-vt-best-perf-numa-vmguest-topo"/>).
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-perf-swap">
   <title>Swap</title>
   <para>
    <emphasis>Swap</emphasis> is usually used by the system to store underused
    physical memory (low usage, or not accessed for a long time). To prevent
    the system running out of memory, setting up a minimum swap is highly
    recommended.
   </para>
   <sect3 xml:id="sec-vt-best-perf-swap-swappiness">
    <title><literal>swappiness</literal></title>
    <para>
     The <literal>swappiness</literal> setting controls your system's swap
     behavior. It defines how memory pages are swapped to disk. A high value of
     <emphasis>swappiness</emphasis> results in a system that swaps more often.
     Available values range from <literal>0</literal> to
     <literal>100</literal>. A value of <literal>100</literal> tells the system
     to find inactive pages and put them in swap. A value of <option>0</option>
     disables swapping.
<!-- reduces the systems
     tendency to swap user space processes but does not disable swap
     completely (this is now the case with kernel =&gt; 3.5). -->
    </para>
    <para>
     To do some testing on a live system, change the value of
     <filename>/proc/sys/vm/swappiness</filename> on the fly and check the
     memory usage afterward:
    </para>
<screen>&prompt.user;&sudo; echo 35 &gt; /proc/sys/vm/swappiness</screen>
<screen>&prompt.user;free -h
total       used       free     shared    buffers     cached
Mem:      24616680    4991492   19625188     167056     144340    2152408
-/+ buffers/cache:    2694744   21921936
Swap:      6171644          0    6171644</screen>
    <para>
     To permanently set a swappiness value, add a line in
     <filename>/etc/systcl.conf</filename>, for example:
    </para>
<screen>vm.swappiness = 35</screen>
    <para>
     You can also control the swap by using the
     <literal>swap_hard_limit</literal> element in the XML configuration of
     your &vmguest;. Before setting this parameter and using it in a production
     environment, do some testing because the host can terminate the domain if
     the value is too low.
    </para>
<screen>&lt;memtune&gt;<co xml:id="co-mem-1"/>
  &lt;hard_limit unit='G'&gt;1&lt;/hard_limit&gt;<co xml:id="co-mem-hard"/>
  &lt;soft_limit unit='M'&gt;128&lt;/soft_limit&gt;<co xml:id="co-mem-soft"/>
  &lt;swap_hard_limit unit='G'&gt;2&lt;/swap_hard_limit&gt;<co xml:id="co-mem-swap"/>
&lt;/memtune&gt;</screen>
    <calloutlist>
     <callout arearefs="co-mem-1">
      <para>
       This element provides memory tunable parameters for the domain. If this
       is omitted, it defaults to the defaults provided b the operating system.
      </para>
     </callout>
     <callout arearefs="co-mem-hard">
      <para>
       Maximum memory the guest can use. To avoid any problems on the &vmguest;
       it is strongly recommended not to use this parameter.
      </para>
     </callout>
     <callout arearefs="co-mem-soft">
      <para>
       The memory limit to enforce during memory contention.
      </para>
     </callout>
     <callout arearefs="co-mem-swap">
      <para>
       The maximum memory plus swap the &vmguest; can use.
      </para>
     </callout>
    </calloutlist>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-io">
   <title>I/O</title>
   <para/>
   <sect3 xml:id="sec-vt-best-perf-io">
    <title>I/O Scheduler</title>
    <para>
     The I/O scheduler for &sle; 15 SP2 and up is Budget Fair Queueing (BFQ). The main aim
     of the BFQ scheduler is to provide a fair allocation of the disk I/O
     bandwidth for all processes that request an I/O operation. You can have
     different I/O schedulers for different devices.
    </para>
    <para>
     To get better performance in host and &vmguest;, use
     <literal>none</literal> in the &vmguest; (disable the I/O scheduler) and
     the <literal>mq-deadline</literal> scheduler for a virtualization host.
    </para>
    <procedure>
     <title>Checking and changing the I/O scheduler at runtime</title>
     <step>
      <para>
       To check your current I/O scheduler for your disk (replace
       <replaceable>sdX</replaceable> by the disk you want to check), run:
      </para>
<screen>&prompt.user;cat /sys/block/<replaceable>sdX</replaceable>/queue/scheduler
mq-deadline kyber [bfq] none</screen>
      <para>
       The value in square brackets is the one currently selected
       (<literal>bfq</literal> in the example above).
      </para>
     </step>
     <step>
      <para>
       You can change the scheduler at runtime with the following command:
      </para>
<screen>&prompt.user;&sudo; echo mq-deadline &gt; /sys/block/<replaceable>sdX</replaceable>/queue/scheduler</screen>
     </step>
    </procedure>
    <para>
     If you need to specify different I/O schedulers for each disk, create the
     file <filename>/usr/lib/tmpfiles.d/IO_ioscheduler.conf</filename> with
     content similar to the following example. It defines the
     <literal>mq-deadline</literal> scheduler for <filename>/dev/sda</filename>
     and the <literal>none</literal> scheduler for
     <filename>/dev/sdb</filename>. This feature is available on &slea; 12 and
     up.
    </para>
<screen>w /sys/block/sda/queue/scheduler - - - - mq-deadline
w /sys/block/sdb/queue/scheduler - - - - none</screen>
   </sect3>
   <sect3 xml:id="sec-vt-best-io-async">
    <title>Asynchronous I/O</title>
    <para>
     Many of the virtual disk back-ends use Linux Asynchronous I/O (aio) in
     their implementation. By default, the maximum number of aio contexts is
     set to 65536, which can be exceeded when running hundreds of &vmguest;s
     using virtual disks serviced by Linux Asynchronous I/O. When running large
     numbers of &vmguest;s on a &vmhost;, consider increasing
     /proc/sys/fs/aio-max-nr.
    </para>
    <procedure>
     <title>Checking and changing aio-max-nr at runtime</title>
     <step>
      <para>
       To check your current aio-max-nr setting run:
      </para>
<screen>&prompt.user;cat /proc/sys/fs/aio-max-nr
65536</screen>
     </step>
     <step>
      <para>
       You can change aio-max-nr at runtime with the following command:
      </para>
<screen>&prompt.user;&sudo; echo 131072 &gt; /proc/sys/fs/aio-max-nr</screen>
     </step>
    </procedure>
    <para>
     To permanently set aio-max-nr, add an entry to a local sysctl file. For
     example, append the following to
     <filename>/etc/sysctl.d/99-sysctl.conf</filename>:
    </para>
<screen>fs.aio-max-nr = 1048576</screen>
   </sect3>
   <sect3 xml:id="sec-vt-best-io-techniques">
    <title>I/O Virtualization</title>
    <para>
     &suse; products support various I/O virtualization technologies. The
     following table lists advantages and disadvantages of each technology. For
     more information about I/O in virtualization refer to the
     <xref linkend="sec-vt-io"/> chapter in the <citetitle>&productname;
     &productnumber; Virtualization Guide</citetitle>.
    </para>
    <table>
     <title>I/O Virtualization solutions</title>
     <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="20%"/>
      <colspec colnum="2" colname="2" colwidth="40%"/>
      <colspec colnum="3" colname="3" colwidth="40%"/>
      <thead>
       <row>
        <entry>
         <para>
          Technology
         </para>
        </entry>
        <entry>
         <para>
          Advantage
         </para>
        </entry>
        <entry>
         <para>
          Disadvantage
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry morerows="3">
         <para>
          Device Assignment (pass-through)
         </para>
        </entry>
        <entry>
         <para>
          Device accessed directly by the guest
         </para>
        </entry>
        <entry>
         <para>
          No sharing among multiple guests
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          High performance
         </para>
        </entry>
        <entry>
         <para>
          Live migration is complex
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para/>
        </entry>
        <entry>
         <para>
          PCI device limit is 8 per guest
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para/>
        </entry>
        <entry>
         <para>
          Limited number of slots on a server
         </para>
        </entry>
       </row>
       <row>
        <entry morerows="1">
         <para>
          Full virtualization (IDE, SATA, SCSI, e1000)
         </para>
        </entry>
        <entry>
         <para>
          &vmguest; compatibility
         </para>
        </entry>
        <entry>
         <para>
          Bad performance
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Easy for live migration
         </para>
        </entry>
        <entry>
         <para>
          Emulated operation
         </para>
        </entry>
       </row>
       <row>
        <entry morerows="2">
         <para>
          Para-virtualization (virtio-blk, virtio-net, virtio-scsi)
         </para>
        </entry>
        <entry>
         <para>
          Good performance
         </para>
        </entry>
        <entry>
         <para>
          Modified guest (PV drivers)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Easy for live migration
         </para>
        </entry>
        <entry>
         <para/>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Efficient host communication with &vmguest;
         </para>
        </entry>
        <entry>
         <para/>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-fs">
   <title>Storage and file system</title>
   <para>
    Storage space for &vmguest;s can either be a block device (for example, a
    partition on a physical disk), or an image file on the file system:
   </para>
   <table>
    <title>Block devices compared to disk images</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="10%"/>
     <colspec colnum="2" colname="2" colwidth=""/>
     <colspec colnum="3" colname="3" colwidth=""/>
     <thead>
      <row>
       <entry>
        <para>
         Technology
        </para>
       </entry>
       <entry>
        <para>
         Advantages
        </para>
       </entry>
       <entry>
        <para>
         Disadvantages
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         Block devices
        </para>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           Better performance
          </para>
         </listitem>
         <listitem>
          <para>
           Use standard tools for administration/disk modification
          </para>
         </listitem>
         <listitem>
          <para>
           Accessible from host (pro and con)
          </para>
         </listitem>
        </itemizedlist>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           Device management
          </para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Image files
        </para>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           Easier system management
          </para>
         </listitem>
         <listitem>
          <para>
           Easily move, clone, expand, back up domains
          </para>
         </listitem>
         <listitem>
          <para>
           Comprehensive toolkit (guestfs) for image manipulation
          </para>
         </listitem>
         <listitem>
          <para>
           Reduce overhead through sparse files
          </para>
         </listitem>
         <listitem>
          <para>
           Fully allocate for best performance
          </para>
         </listitem>
        </itemizedlist>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           Lower performance than block devices
          </para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
<!--
       <sect3 id="vt-best-fs-general">
       <title>General</title>
       <para>
       Access CD/DVD -> storage pool
       </para>
       <para>
       deleting pool
       </para>
       <para>
       Brtfs and guest image
       </para>
       <para>
       &qemu; direct access to host drives (-drive file=)
       </para>
       </sect2>
   -->
   <para>
    For detailed information about image formats and maintaining images refer
    to <xref linkend="sec-vt-best-img"/>.
   </para>
   <para>
    If your image is stored on an NFS share, you should check some server and
    client parameters to improve access to the &vmguest; image.
   </para>
   <sect3 xml:id="sec-vt-best-fs-nfs-rw">
    <title>NFS read/write (client)</title>
    <para>
     Options <option>rsize</option> and <option>wsize</option> specify the size
     of the chunks of data that the client and server pass back and forth to
     each other. You should ensure NFS read/write sizes are sufficiently large,
     especially for large I/O. Change the <option>rsize</option> and
     <option>wsize</option> parameter in your <filename>/etc/fstab</filename>
     by increasing the value to 16 KB. This will ensure that all operations can
     be frozen if there is any instance of hanging.
    </para>
<screen>nfs_server:/exported/vm_images<co xml:id="co-nfs-server"/> /mnt/images<co xml:id="co-nfs-mnt"/> nfs<co xml:id="co-nfs-nfs"/> rw<co xml:id="co-nfs-rw"/>,hard<co xml:id="co-nfs-hard"/>,sync<co xml:id="co-nfs-sync"/>, rsize=8192<co xml:id="co-nfs-rsize"/>,wsize=8192<co xml:id="co-nfs-wsize"/> 0 0</screen>
    <calloutlist>
     <callout arearefs="co-nfs-server">
      <para>
       NFS server's host name and export path name.
      </para>
     </callout>
     <callout arearefs="co-nfs-mnt">
      <para>
       Where to mount the NFS exported share.
      </para>
     </callout>
     <callout arearefs="co-nfs-nfs">
      <para>
       This is an <option>nfs</option> mount point.
      </para>
     </callout>
     <callout arearefs="co-nfs-rw">
      <para>
       This mount point will be accessible in read/write.
      </para>
     </callout>
     <callout arearefs="co-nfs-hard">
      <para>
       Determines the recovery behavior of the NFS client after an NFS request
       times out. <option>hard</option> is the best option to avoid data
       corruption.
      </para>
     </callout>
     <callout arearefs="co-nfs-sync">
      <para>
       Any system call that writes data to files on that mount point causes
       that data to be flushed to the server before the system call returns
       control to user space.
      </para>
     </callout>
     <callout arearefs="co-nfs-rsize">
      <para>
       Maximum number of bytes in each network READ request that the NFS client
       can receive when reading data from a file on an NFS server.
      </para>
     </callout>
     <callout arearefs="co-nfs-wsize">
      <para>
       Maximum number of bytes per network WRITE request that the NFS client
       can send when writing data to a file on an NFS server.
      </para>
     </callout>
    </calloutlist>
   </sect3>
   <sect3 xml:id="sec-vt-best-fs-nfs-threads">
    <title>NFS threads (server)</title>
    <para>
     Your NFS server should have enough NFS threads to handle multi-threaded
     workloads. Use the <command>nfsstat</command> tool to get some RPC
     statistics on your server:
    </para>
<screen>&prompt.user;&sudo; nfsstat -rc
Client rpc stats:
calls      retrans    authrefrsh
6401066    198          0          0</screen>
    <para>
     If the <literal>retrans</literal> is equal to 0, everything is fine.
     Otherwise, the client needs to retransmit, so increase the
     <envar>USE_KERNEL_NFSD_NUMBER</envar> variable in
     <filename>/etc/sysconfig/nfs</filename>, and adjust accordingly until
     <literal>retrans</literal> is equal to <literal>0</literal>.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-perf-cpu">
   <title>CPUs</title>
   <para>
    Host CPU <quote>components</quote> will be <quote>translated</quote> to
    virtual CPUs in a &vmguest; when being assigned. These components can
    either be:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>CPU processor</emphasis>: this describes the main CPU unit,
      which usually has multiple cores and may support Hyper-Threading.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>CPU core</emphasis>: a main CPU unit can provide more than one
      core, and the proximity of cores speeds up the computation process and
      reduces energy costs.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>CPU Hyper-Threading</emphasis>: this implementation is used to
      improve parallelization of computations, but this is not as efficient as
      a dedicated core.
     </para>
    </listitem>
   </itemizedlist>
   <sect3 xml:id="sec-vt-best-perf-cpu-assign">
    <title>Assigning CPUs</title>
    <para>
     CPU overcommit occurs when the cumulative number of virtual CPUs of
     all &vmguest;s becomes higher than the number of host CPUs.
     Best performance is likely to be achieved when there is no overcommit
     and each virtual CPU matches one hardware processor or core on the
     &vmhost;.
     In fact, &vmguest;s running on an overcommitted host will experience
     increased latency and a negative effect on per-&vmguest; throughput
     is also likely to be observed. Therefore, you should try to avoid
     overcommitting CPUs.
    </para>
    <para>
     Deciding whether to allow CPU overcommit or not requires good a-priori
     knowledge of workload as a whole.
     For instance, if you know that all the &vmguest;s virtual CPUs will not
     be loaded more than 50% then you can assume that overcommitting the host
     by a factor of 2 (which means having 128 virtual CPUs in total, on a host
     with 64 CPUs) will work well.
     On the other hand, if you know that all the virtual CPUs of the &vmguest;s
     will try to run at 100% for most of the time then even having one virtual
     CPU more than than the host has CPUs is already a misconfiguration.
    </para>
    <para>
     Overcommitting to a point where the cumulative number of virtual CPUs is
     higher than 8 times the number of physical cores of the &vmhost; will most
     likely lead to a malfunctioning and unstable system and should hence be
     avoided.
    </para>
    <para>
     Unless you know exactly how many virtual CPUs are required for a &vmguest;,
     you should start with one. A good rule of thumb is to target a CPU workload
     of approximately 70% inside your VM (see
     <xref linkend="sec-util-processes"/> for information on monitoring tools).
     If you allocate more processors than needed in the &vmguest;, this will
     negatively affect the performance of host and guest. Cycle efficiency will
     be degraded, as the unused vCPU will still cause timer interrupts.
     In case you primarily run single threaded applications on a &vmguest;, a
     single virtual CPU is the best choice.
    </para>
    <para>
      A single &vmguest; with more virtual CPUs than the &vmhost; has CPUs
      is always a misconfiguration.
    </para>
   </sect3>
   <sect3 xml:id="sec-vt-best-perf-cpu-guests">
    <title>&vmguest; CPU configuration</title>
    <para>
     This section describes how to choose and configure a CPU type for a
     &vmguest;. You will also learn how to pin virtual CPUs to physical CPUs on
     the host system. For more information about virtual CPU configuration and
     tuning parameters refer to the libvirt documentation at
     <link
     xlink:href="https://libvirt.org/formatdomain.html#elementsCPU"/>.
    </para>
    <sect4 xml:id="sec-vt-best-perf-cpu-guests-model">
     <title>Virtual CPU models and features</title>
     <para>
      The CPU model and topology can be specified individually for each
      &vmguest;. Configuration options range from selecting specific CPU models
      to excluding certain CPU features. Predefined CPU models are listed in
      the <filename>/usr/share/libvirt/cpu_map.xml</filename>. A CPU model and
      topology that is similar to the host generally provides the best
      performance. The host system CPU model and topology can be displayed by
      running <command>virsh capabilities</command>.
     </para>
     <para>
      Note that changing the default virtual CPU configuration will require a
      &vmguest; shutdown when migrating it to a host with different hardware.
      More information on &vmguest; migration is available at
      <xref
      linkend="sec-libvirt-admin-migrate"/>.
     </para>
     <para>
      To specify a particular CPU model for a &vmguest;, add a respective entry
      to the &vmguest; configuration file. The following example configures a
      Broadwell CPU with the invariant TSC feature:
     </para>
<screen>&lt;cpu mode='custom' match='exact'&gt;
  &lt;model&gt;Broadwell&lt;/model&gt;
  &lt;feature name='invtsc'/&gt;
  &lt;/cpu&gt;</screen>
     <para>
      For a virtual CPU that most closely resembles the host physical CPU,
      <literal>&lt;cpu mode='host-passthrough'&gt;</literal> can be used. Note
      that a <literal>host-passthrough</literal> CPU model may not exactly
      resemble the host physical CPU, since by default KVM will mask any
      non-migratable features. For example invtsc is not included in the
      virtual CPU feature set. Changing the default KVM behavior is not
      directly supported through libvirt, although it does allow arbitrary
      passthrough of KVM command line arguments. Continuing with the
      <literal>invtsc</literal> example, you can achieve passthrough of the
      host CPU (including <literal>invtsc</literal>) with the following command
      line passthrough in the &vmguest; configuration file:
     </para>
<screen>&lt;domain type='kvm' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'&gt;
     &lt;qemu:commandline&gt;
     &lt;qemu:arg value='-cpu'/&gt;
     &lt;qemu:arg value='host,migratable=off,+invtsc'/&gt;
     &lt;/qemu:commandline&gt;
     ...
     &lt;/domain&gt;
     </screen>
     <note>
      <title>The <literal>host-passthrough</literal> mode</title>
      <para>
       Since <literal>host-passthrough</literal> exposes the physical CPU
       details to the virtual CPU, migration to dissimilar hardware is not
       possible. See
       <xref linkend="sec-vt-best-perf-cpu-guests-vcpumigration"/> for more
       information.
      </para>
     </note>
    </sect4>
    <sect4 xml:id="sec-vt-best-perf-cpu-guests-vcpupin">
     <title>Virtual CPU pinning</title>
     <para>
      Virtual CPU pinning is used to constrain virtual CPU threads to a set of
      physical CPUs. The <literal>vcpupin</literal> element specifies the
      physical host CPUs that a virtual CPU can use. If this element is not set
      and the attribute <literal>cpuset</literal> of the
      <literal>vcpu</literal> element is not specified, the virtual CPU is free
      to use any of the physical CPUs.
     </para>
     <para>
      CPU intensive workloads can benefit from virtual CPU pinning by
      increasing the physical CPU cache hit ratio. To pin a virtual CPU to a
      specific physical CPU, run the following commands:
     </para>
<screen>&prompt.user;virsh vcpupin <replaceable>DOMAIN_ID</replaceable> --vcpu <replaceable>vCPU_NUMBER</replaceable>
VCPU: CPU Affinity
----------------------------------
0: 0-7
&prompt.root;virsh vcpupin SLE15 --vcpu 0 0 --config</screen>
     <para>
      The last command generates the following entry in the XML configuration:
     </para>
<screen>&lt;cputune&gt;
   &lt;vcpupin vcpu='0' cpuset='0'/&gt;
&lt;/cputune&gt;</screen>
     <note>
      <title>Virtual CPU pinning on NUMA nodes</title>
      <para>
       To confine a &vmguest;'s CPUs and its memory to a NUMA node, you can use
       virtual CPU pinning and memory allocation policies on a NUMA system. See
       <xref linkend="sec-vt-best-perf-numa"/> for more information related to
       NUMA tuning.
      </para>
     </note>
     <warning>
      <title>Virtual CPU pinning and live migration</title>
      <para>
       Even though <literal>vcpupin</literal> can improve performance, it can
       complicate live migration. See
       <xref linkend="sec-vt-best-perf-cpu-guests-vcpumigration"/> for more
       information on virtual CPU migration considerations.
      </para>
     </warning>
    </sect4>
    <sect4 xml:id="sec-vt-best-perf-cpu-guests-vcpumigration">
     <title>Virtual CPU migration considerations</title>
     <para>
      Selecting a virtual CPU model containing all the latest features may
      improve performance of a &vmguest; workload, but often at the expense of
      migratability. Unless all hosts in the cluster contain the latest CPU
      features, migration can fail when a destination host lacks the new
      features. If migratability of a virtual CPU is preferred over the latest
      CPU features, a normalized CPU model and feature set should be used. The
      <command>virsh cpu-baseline</command> command can help define a
      normalized virtual CPU that can be migrated across all hosts. The
      following command, when run on each host in the migration cluster,
      illustrates collection of all hosts' capabilities in
      <literal>all-hosts-caps.xml</literal>.
     </para>
<screen>&prompt.user;&sudo; virsh capabilities >> all-hosts-cpu-caps.xml</screen>
     <para>
      With the capabilities from each host collected in all-hosts-caps.xml, use
      <command>virsh cpu-baseline</command> to create a virtual CPU definition
      that will be compatible across all hosts.
     </para>
<screen>&prompt.user;&sudo; virsh cpu-baseline all-hosts-caps.xml</screen>
     <para>
      The resulting virtual CPU definition can be used as the
      <literal>cpu</literal> element in &vmguest; configuration file.
     </para>
     <para>
      At a logical level, virtual CPU pinning is a form of hardware
      passthrough. Pinning couples physical resources to virtual resources, and
      can also be problematic for migration. For example, the migration will
      fail if the requested physical resources are not available on the
      destination host, or if the source and destination hosts have different
      NUMA topologies. For more recommendations about Live Migration see
      <xref linkend="libvirt-admin-live-migration-requirements"/>.
     </para>
    </sect4>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-perf-numa">
   <title>NUMA tuning</title>
   <para>
    NUMA is an acronym for Non Uniform Memory Access. A NUMA system has
    multiple physical CPUs, each with local memory attached. Each CPU can also
    access other CPUs' memory, known as <quote>remote memory access</quote>,
    but it is much slower than accessing local memory. NUMA systems can
    negatively impact &vmguest; performance if not tuned properly. Although
    ultimately tuning is workload dependent, this section describes controls
    that should be considered when deploying &vmguest;s on NUMA hosts. Always
    consider your host topology when configuring and deploying VMs.
   </para>
   <para>
    &productname; contains a NUMA auto-balancer that strives to reduce remote
    memory access by placing memory on the same NUMA node as the CPU processing
    it. In addition, standard tools such as <command>cgset</command> and
    virtualization tools such as libvirt provide mechanisms to constrain
    &vmguest; resources to physical resources.
   </para>
   <para>
    <command>numactl</command> is used to check for host NUMA capabilities:
   </para>
<screen>&prompt.user;&sudo; numactl --hardware
available: 4 nodes (0-3)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 72 73 74 75 76 77 78
79 80 81 82 83 84 85 86 87 88 89
node 0 size: 31975 MB
node 0 free: 31120 MB
node 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 90 91 92 93
94 95 96 97 98 99 100 101 102 103 104 105 106 107
node 1 size: 32316 MB
node 1 free: 31673 MB
node 2 cpus: 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 108 109 110
111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
node 2 size: 32316 MB
node 2 free: 31726 MB
node 3 cpus: 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 126 127 128
129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
node 3 size: 32314 MB
node 3 free: 31387 MB
node distances:
node   0   1   2   3
0:  10  21  21  21
1:  21  10  21  21
2:  21  21  10  21
3:  21  21  21  10</screen>
   <para>
    The <command>numactl</command> output shows this is a NUMA system with 4
    nodes or cells, each containing 36 CPUs and approximately 32G memory.
    <command>virsh capabilities</command> can also be used to examine the
    systems NUMA capabilities and CPU topology.
   </para>
   <sect3 xml:id="sec-vt-best-perf-numa-balancing">
    <title>NUMA balancing</title>
    <para>
     On NUMA machines, there is a performance penalty if remote memory is
     accessed by a CPU. Automatic NUMA balancing scans a task's address space
     and unmaps pages. By doing so, it detects whether pages are properly
     placed or whether to migrate the data to a memory node local to where the
     task is running. In defined intervals (configured with
     <literal>numa_balancing_scan_delay_ms</literal>), the task scans the next
     scan size number of pages (configured with
     <literal>numa_balancing_scan_size_mb</literal>) in its address space. When
     the end of the address space is reached the scanner restarts from the
     beginning.
    </para>
    <para>
     Higher scan rates cause higher system overhead as page faults must be
     trapped and data needs to be migrated. However, the higher the scan rate,
     the more quickly a task's memory is migrated to a local node when the
     workload pattern changes. This minimizes the performance impact caused by
     remote memory accesses. These <command>sysctl</command> directives control
     the thresholds for scan delays and the number of pages scanned:
    </para>
<screen>&prompt.user;&sudo; sysctl -a | grep numa_balancing
kernel.numa_balancing = 1<co xml:id="co-numa-balancing"/>
kernel.numa_balancing_scan_delay_ms = 1000<co xml:id="co-numa-delay"/>
kernel.numa_balancing_scan_period_max_ms = 60000<co xml:id="co-numa-pmax"/>
kernel.numa_balancing_scan_period_min_ms = 1000<co xml:id="co-numa-pmin"/>
kernel.numa_balancing_scan_size_mb = 256<co xml:id="co-numa-size"/></screen>
    <calloutlist>
     <callout arearefs="co-numa-balancing">
      <para>
       Enables/disables automatic page fault-based NUMA balancing
      </para>
     </callout>
     <callout arearefs="co-numa-delay">
      <para>
       Starting scan delay used for a task when it initially forks
      </para>
     </callout>
     <callout arearefs="co-numa-pmax">
      <para>
       Maximum time in milliseconds to scan a task's virtual memory
      </para>
     </callout>
     <callout arearefs="co-numa-pmin">
      <para>
       Minimum time in milliseconds to scan a task's virtual memory
      </para>
     </callout>
     <callout arearefs="co-numa-size">
      <para>
       Size in megabytes' worth of pages to be scanned for a given scan
      </para>
     </callout>
    </calloutlist>
    <para>
     For more information see <xref linkend="cha-tuning-numactl"/>.
    </para>
    <para>
     The main goal of automatic NUMA balancing is either to reschedule tasks on
     the same node's memory (so the CPU follows the memory), or to copy the
     memory's pages to the same node (so the memory follows the CPU).
    </para>
    <warning>
     <title>Task placement</title>
     <para>
      There are no rules to define the best place to run a task, because tasks
      could share memory with other tasks. For best performance, it is
      recommended to group tasks sharing memory on the same node. Check NUMA
      statistics with <command># cat /proc/vmstat | grep numa_</command>.
     </para>
    </warning>
   </sect3>
   <sect3 xml:id="sec-vt-best-perf-numa-cpuset">
    <title>Memory allocation control with the CPUset controller</title>
    <para>
     The cgroups cpuset controller can be used confine memory used by a process
     to a NUMA node. There are three cpuset memory policy modes available:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>interleave</literal>: This is a memory placement policy which
       is also known as round-robin. This policy can provide substantial
       improvements for jobs that need to place thread local data on the
       corresponding node. When the interleave destination is not available, it
       will be moved to another node.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>bind</literal>: This will place memory only on one node, which
       means in case of insufficient memory, the allocation will fail.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>preferred</literal>: This policy will apply a preference to
       allocate memory to a node. If there is not enough space for memory on
       this node, it will fall back to another node.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     You can change the memory policy mode with the <command>cgset</command>
     tool from the <package>libcgroup-tools</package> package:
    </para>
<screen>&prompt.user;&sudo; cgset -r cpuset.mems=<replaceable>NODE</replaceable> sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>/emulator</screen>
    <para>
     To migrate pages to a node, use the <command>migratepages</command> tool:
    </para>
<screen>&prompt.user;migratepages <replaceable>PID</replaceable> <replaceable>FROM-NODE</replaceable> <replaceable>TO-NODE</replaceable></screen>
    <para>
     To check everything is fine. use: <command>cat
     /proc/<replaceable>PID</replaceable>/status | grep Cpus</command>.
    </para>
    <note>
     <title>Kernel NUMA/cpuset memory policy</title>
     <para>
      For more information see
      <link xlink:href="https://www.kernel.org/doc/Documentation/vm/numa_memory_policy.txt">Kernel
      NUMA memory policy</link> and
      <link xlink:href="https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt">cpusets
      memory policy</link>. Check also the
      <link xlink:href="https://libvirt.org/formatdomain.html#elementsNUMATuning">Libvirt
      NUMA Tuning documentation</link>.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="sec-vt-best-perf-numa-vmguest">
    <title>&vmguest;: NUMA related configuration</title>
    <para>
     &libvirt; allows to set up virtual NUMA and memory access policies.
     Configuring these settings is not supported by
     <command>virt-install</command> or <command>virt-manager</command> and
     needs to be done manually by editing the &vmguest; configuration file with
     <command>virsh edit</command>.
    </para>
    <sect4 xml:id="sec-vt-best-perf-numa-vmguest-topo">
     <title>&vmguest; virtual NUMA topology</title>
     <para>
      Creating a &vmguest; virtual NUMA (vNUMA) policy that resembles the host
      NUMA topology can often increase performance of traditional large,
      scale-up workloads. &vmguest; vNUMA topology can be specified using the
      <literal>numa</literal> element in the XML configuration:
     </para>
<screen>&lt;cpu&gt;
...
  &lt;numa&gt;
    &lt;cell<co xml:id="co-numa-cell"/> id="0"<co xml:id="co-numa-id"/> cpus='0-1'<co xml:id="co-numa-cpus"/> memory='512000' unit='KiB'/&gt;
    &lt;cell id="1" cpus='2-3' memory='256000'<co xml:id="co-numa-mem"/>
    unit='KiB'<co xml:id="co-numa-unit"/> memAccess='shared'<co
    xml:id="co-numa-memaccess"/>/&gt;
  &lt;/numa&gt;
  ...
&lt;/cpu&gt;</screen>
     <calloutlist>
      <callout arearefs="co-numa-cell">
       <para>
        Each <literal>cell</literal> element specifies a vNUMA cell or node
       </para>
      </callout>
      <callout arearefs="co-numa-id">
       <para>
        All cells should have an <literal>id</literal> attribute, allowing to
        reference the cell in other configuration blocks. Otherwise cells are
        assigned ids in ascending order starting from 0.
       </para>
      </callout>
      <callout arearefs="co-numa-cpus">
       <para>
        The CPU or range of CPUs that are part of the node
       </para>
      </callout>
      <callout arearefs="co-numa-mem">
       <para>
        The node memory
       </para>
      </callout>
      <callout arearefs="co-numa-unit">
       <para>
        Units in which node memory is specified
       </para>
      </callout>
      <callout arearefs="co-numa-memaccess">
       <para>
        Optional attribute which can control whether the memory is to be mapped
        as <option>shared</option> or <option>private</option>. This is valid
        only for hugepages-backed memory.
       </para>
      </callout>
     </calloutlist>
     <para>
      To find where the &vmguest; has allocated its pages. use: <command>cat
      /proc/<replaceable>PID</replaceable>/numa_maps</command> and <command>cat
      /sys/fs/cgroup/memory/sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>/memory.numa_stat</command>.
     </para>
     <warning>
      <title>NUMA specification</title>
      <para>
       The &libvirt; &vmguest; NUMA specification is currently only available
       for &qemu;/&kvm;.
      </para>
     </warning>
    </sect4>
    <sect4 xml:id="sec-vt-best-perf-numa-vmguest-alloc-libvirt">
     <title>Memory allocation control with &libvirt;</title>
     <para>
      If the &vmguest; has a vNUMA topology (see
      <xref
      linkend="sec-vt-best-perf-numa-vmguest-topo"/>), memory can
      be pinned to host NUMA nodes using the <literal>numatune</literal>
      element. This method is currently only available for &qemu;/&kvm; guests.
      See <xref
      linkend="sec-vt-best-perf-numa-alloc-libvirt-non-vnuma"/>
      for how to configure non-vNUMA &vmguest;s.
     </para>
<screen>&lt;numatune&gt;
    &lt;memory mode="strict"<co xml:id="co-numat-mode"/> nodeset="1-4,^3"<co xml:id="co-numat-nodeset"/>/&gt;
    &lt;memnode<co xml:id="co-numat-memnode"/> cellid="0"<co xml:id="co-numat-cellid"/> mode="strict" nodeset="1"/&gt;
    &lt;memnode cellid="2" placement="strict"<co xml:id="co-numat-placement"/> mode="preferred" nodeset="2"/&gt;
&lt;/numatune&gt;</screen>
     <calloutlist>
      <callout arearefs="co-numat-mode">
       <para>
        Policies available are: <literal>interleave</literal> (round-robin
        like), <literal>strict</literal> (default) or
        <literal>preferred</literal>.
       </para>
      </callout>
      <callout arearefs="co-numat-nodeset">
       <para>
        Specify the NUMA nodes.
       </para>
      </callout>
      <callout arearefs="co-numat-memnode">
       <para>
        Specify memory allocation policies for each guest NUMA node (if this
        element is not defined then this will fall back and use the
        <literal>memory</literal> element).
       </para>
      </callout>
      <callout arearefs="co-numat-cellid">
       <para>
        Addresses the guest NUMA node for which the settings are applied.
       </para>
      </callout>
      <callout arearefs="co-numat-placement">
       <para>
        The placement attribute can be used to indicate the memory placement
        mode for a domain process, the value can be <literal>auto</literal> or
        <literal>strict</literal>.
       </para>
      </callout>
     </calloutlist>
     <important xml:id="sec-vt-best-perf-numa-alloc-libvirt-non-vnuma">
      <title>Non-vNUMA &vmguest;</title>
      <para>
       On a non-vNUMA &vmguest;, pinning memory to host NUMA nodes is done like
       in the following example:
      </para>
<screen>&lt;numatune&gt;
   &lt;memory mode="strict" nodeset="0-1"/&gt;
&lt;/numatune&gt;</screen>
      <para>
       In this example, memory is allocated from the host nodes
       <literal>0</literal> and <literal>1</literal>. In case these memory
       requirements cannot be fulfilled, starting the &vmguest; will fail.
       <command>virt-install</command> also supports this configuration with
       the <option>--numatune</option> option.
      </para>
     </important>
     <warning>
      <title>Memory and CPU across NUMA nodes</title>
      <para>
       You should avoid allocating &vmguest; memory across NUMA nodes, and
       prevent virtual CPUs from floating across NUMA nodes.
      </para>
     </warning>
    </sect4>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-vt-best-img">
  <title>&vmguest; images</title>

  <para>
   Images are virtual disks used to store the operating system and data of
   &vmguest;s. They can be created, maintained and queried with the
   <command>qemu-img</command> command. Refer to
   <xref
   linkend="cha-qemu-guest-inst-qemu-img-create"/> for more
   information on the <command>qemu-img</command> tool and examples.
  </para>

  <sect2 xml:id="sec-vt-best-img-imageformat">
   <title>&vmguest; image formats</title>
   <para>
    Certain storage formats which &qemu; recognizes have their origins in other
    virtualization technologies. By recognizing these formats, &qemu; can
    leverage either data stores or entire guests that were originally targeted
    to run under these other virtualization technologies. Some formats are
    supported only in read-only mode. To use them in read/write mode, convert
    them to a fully supported &qemu; storage format (using
    <command>qemu-img</command>). Otherwise they can only be used as read-only
    data store in a QEMU guest.
   </para>
   <para>
    Use <command>qemu-img info <replaceable>VMGUEST.IMG</replaceable></command>
    to get information about an existing image, such as: the format, the
    virtual size, the physical size, snapshots if available.
   </para>
   <note>
    <title>Performance</title>
    <para>
     It is recommended to convert the disk images to either raw or qcow2 to
     achieve good performance.
    </para>
   </note>
   <warning>
    <title>Encrypted images cannot be compressed</title>
    <para>
     When you create an image, you cannot use compression (<option>-c</option>)
     in the output file together with the encryption option
     (<option>-e</option>).
    </para>
   </warning>
   <sect3 xml:id="sec-vt-best-img-imageformat-raw">
    <title>Raw format</title>
    <para/>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       This format is simple and easily exportable to all other
       emulators/hypervisors.
      </para>
     </listitem>
     <listitem>
      <para>
       It provides best performance (least I/O overhead).
      </para>
     </listitem>
     <listitem>
      <para>
       If your file system supports holes (for example in Ext2 or Ext3 on Linux
       or NTFS on Windows*), then only the written sectors will reserve space.
      </para>
     </listitem>
     <listitem>
      <para>
       The raw format allows to copy a &vmguest; image to a physical device
       (<command>dd if=<replaceable>VMGUEST.RAW</replaceable>
       of=<replaceable>/dev/sda</replaceable></command>).
      </para>
     </listitem>
     <listitem>
      <para>
       It is byte-for-byte the same as what the &vmguest; sees, so this wastes
       a lot of space.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="sec-vt-best-img-imageformat-qcow2">
    <title>qcow2 format</title>
    <para/>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       Use this to have smaller images (useful if your file system does not
       supports holes).
      </para>
     </listitem>
     <listitem>
      <para>
       It has optional AES encryption (now deprecated).
      </para>
     </listitem>
     <listitem>
      <para>
       Zlib-based compression option.
      </para>
     </listitem>
     <listitem>
      <para>
       Support of multiple VM snapshots (internal, external).
      </para>
     </listitem>
     <listitem>
      <para>
       Improved performance and stability.
      </para>
     </listitem>
     <listitem>
      <para>
       Supports changing the backing file.
      </para>
     </listitem>
     <listitem>
      <para>
       Supports consistency checks.
      </para>
     </listitem>
     <listitem>
      <para>
       Less performance than raw format.
      </para>
     </listitem>
    </itemizedlist>
    <variablelist>
     <varlistentry>
      <term>l2-cache-size</term>
      <listitem>
       <para>
        qcow2 can provide the same performance for random read/write access as
        raw format, but it needs a well-sized cache size. By default cache size
        is set to 1 MB. This will give good performance up to a disk size of 8
        GB. If you need a bigger disk size, you need to adjust the cache size.
        For a disk size of 64 GB (64*1024 = 65536), you need 65536 / 8192B = 8
        MB of cache (<option>-drive format=qcow2,l2-cache-size=8M</option>).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Cluster size</term>
      <listitem>
       <para>
        The qcow2 format offers the capability to change the cluster size. The
        value must be between 512&nbsp;KB and 2&nbsp;MB. Smaller cluster sizes
        can improve the image file size whereas larger cluster sizes generally
        provide better performance.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Preallocation</term>
      <listitem>
       <para>
        An image with preallocated metadata is initially larger but can improve
        performance when the image needs to grow.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Lazy refcounts</term>
      <listitem>
       <para>
        Reference count updates are postponed with the goal of avoiding
        metadata I/O and improving performance. This is particularly beneficial
        with <option>cache=writethrough</option>. This option does not batch
        metadata updates, but if in case of host crash, the reference count
        tables must be rebuilt, this is done automatically at the next open
        with <command>qemu-img check -r all</command>. Note that this takes
        some time.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="sec-vt-best-img-imageformat-qed">
    <title>qed format</title>
    <para>
     qed is a follow-on qcow (&qemu; Copy On Write) format. Because qcow2
     provides all the benefits of qed and more, qed is now deprecated.
    </para>
   </sect3>
   <sect3 xml:id="sec-vt-best-img-imageformat-vmdk">
    <title>VMDK format</title>
    <para>
     VMware 3, 4, or 6 image format, for exchanging images with that product.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-img-overlay">
   <title>Overlay disk images</title>
   <para>
    The qcow2 and qed formats provide a way to create a base image (also called
    backing file) and overlay images on top of the base image. A backing file
    is useful to be able to revert to a known state and discard the overlay. If
    you write to the image, the backing image will be untouched and all changes
    will be recorded in the overlay image file. The backing file will never be
    modified unless you use the <option>commit</option> monitor command (or
    <command>qemu-img commit</command>).
   </para>
   <para>
    To create an overlay image:
   </para>
<screen>&prompt.root;qemu-img create -o<co xml:id="co-1-minoro"/>backing_file=vmguest.raw<co xml:id="co-1-backingfile"/>,backing_fmt=raw<co xml:id="co-1-backingfmt"/>\
     -f<co xml:id="co-1-minorf"/> qcow2 vmguest.cow<co xml:id="co-1-imagename"/></screen>
   <calloutlist>
    <callout arearefs="co-1-minoro">
     <para>
      Use <option>-o ?</option> for an overview of available options.
     </para>
    </callout>
    <callout arearefs="co-1-backingfile">
     <para>
      The backing file name.
     </para>
    </callout>
    <callout arearefs="co-1-backingfmt">
     <para>
      Specify the file format for the backing file.
     </para>
    </callout>
    <callout arearefs="co-1-minorf">
     <para>
      Specify the image format for the &vmguest;.
     </para>
    </callout>
    <callout arearefs="co-1-imagename">
     <para>
      Image name of the &vmguest;, it will only record the differences from the
      backing file.
     </para>
    </callout>
   </calloutlist>
   <warning>
    <title>Backing image path</title>
    <para>
     You should not change the path to the backing image, otherwise you will
     need to adjust it. The path is stored in the overlay image file. To update
     the path, you should make a symbolic link from the original path to the
     new path and then use the <command>qemu-img</command>
     <option>rebase</option> option.
    </para>
<screen>&prompt.root;ln -sf /var/lib/images/vmguest.raw  /var/lib/images/SLE15/vmguest.raw
&prompt.root;qemu-img rebase<co xml:id="co-2-rebase"/>-u<co xml:id="co-2-unsafe"/> -b<co xml:id="co-2-minorb"/> /var/lib/images/vmguest.raw /var/lib/images/SLE15/vmguest.cow<co xml:id="co-2-image"/></screen>
    <para>
     The <command>rebase</command> subcommand tells <command>qemu-img</command>
     to change the backing file image. The <option>-u</option> option activates
     the unsafe mode (see note below). The backing image to be used is
     specified with <option>-b</option> and the image path is the last argument
     of the command.
    </para>
    <para>
     There are two different modes in which <option>rebase</option> can
     operate:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>Safe</emphasis>: This is the default mode and performs a real
       rebase operation. The safe mode is a time-consuming operation.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Unsafe</emphasis>: The unsafe mode (<option>-u</option>) only
       changes the backing files name and the format of the file name without
       making any checks on the files contents. You should use this mode to
       rename or moving a backing file.
      </para>
     </listitem>
    </itemizedlist>
   </warning>
   <para>
    A common use is to initiate a new guest with the backing file. Let's assume
    we have a <filename>sle15_base.img</filename> &vmguest; ready to be used
    (fresh installation without any modification). This will be our backing
    file. Now you need to test a new package, on an updated system and on a
    system with a different kernel. We can use
    <filename>sle15_base.img</filename> to instantiate the new &sle; &vmguest;
    by creating a qcow2 overlay file pointing to this backing file
    (<filename>sle15_base.img</filename>).
   </para>
   <para>
    In our example we will use <filename>sle15_updated.qcow2</filename> for the
    updated system, and <filename>sle15_kernel.qcow2</filename> for the system
    with a different kernel.
   </para>
   <para>
    To create the two thin provisioned systems use the
    <command>qemu-img</command> command line with the <option>-b</option>
    option:
   </para>
<screen>&prompt.root;qemu-img create -b /var/lib/libvirt/sle15_base.img -f qcow2 \
/var/lib/libvirt/sle15_updated.qcow2
Formatting 'sle15_updated.qcow2', fmt=qcow2 size=17179869184
backing_file='sle15_base.img' encryption=off cluster_size=65536
lazy_refcounts=off nocow=off
&prompt.root;qemu-img create -b /var/lib/libvirt/sle15_base.img -f qcow2 \
/var/lib/libvirt/sle15_kernel.qcow2
Formatting 'sle15_kernel.qcow2', fmt=qcow2 size=17179869184
backing_file='vmguest-sle15_base.img' encryption=off cluster_size=65536
lazy_refcounts=off nocow=off</screen>
   <para>
    The images are now usable, and you can do your test without touching the
    initial <filename>sle15_base.img</filename> backing file. All changes will
    be stored in the new overlay images. Additionally, you can also use these
    new images as a backing file, and create a new overlay.
   </para>
<screen>&prompt.root;qemu-img create -b sle15_kernel.qcow2 -f qcow2 sle15_kernel_TEST.qcow2</screen>
   <para>
    When using <command>qemu-img info</command> with the option
    <option>--backing-chain</option>, it will return all information about the
    entire backing chain recursively:
   </para>
<screen>&prompt.root;qemu-img info --backing-chain
/var/lib/libvirt/images/sle15_kernel_TEST.qcow2
image: sle15_kernel_TEST.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: sle15_kernel.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle15_kernel.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: SLE15.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle15_base.img
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 16G
cluster_size: 65536
Format specific information:
compat: 1.1
lazy refcounts: true</screen>
   <figure xml:id="fig-qemu-img-overlay">
    <title>Understanding image overlay</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

<!--
       <sect2 id="vt-best-img-diskio">
       <title>Disk IO modes</title>
       <table>
       <title>Notation conventions</title>
       <tgroup cols="2">
       <colspec colnum="1" colname="1"/>
       <colspec colnum="2" colname="2"/>
       <thead>
       <row>
       <entry>
       <para>Mode</para>
       </entry>
       <entry>
       <para>Description</para>
       </entry>
       </row>
       </thead>
       <tbody>
       <row>
       <entry>
       <para>Disk IO Modes
       </para>
       </entry>
       <entry>
       <para>Native</para>
       </entry>
       </row>
       <row>
       <entry>
       <para>kernel asynchronous IO</para>
       </entry>
       <entry>
       <para>threads</para>
       </entry>
       </row>
       <row>
       <entry>
       <para>host user-mode based threads</para>
       </entry>
       <entry>
       <para>default, 'threads' mode in SLES</para>
       </entry>
       </row>
       </tbody>
       </tgroup>
       </table>
       </sect2>
   -->

  <sect2 xml:id="sec-vt-best-img-open-img">
   <title>Opening a &vmguest; image</title>
   <para>
    To access the file system of an image, use the
    <package>guestfs-tools</package>. If you do not have this tool installed on
    your system you can mount an image with other Linux tools. Avoid accessing
    an untrusted or unknown &vmguest;'s image system because this can lead to
    security issues (for more information, read
    <link
  xlink:href="https://www.berrange.com/posts/2013/02/20/a-reminder-why-you-should-never-mount-guest-disk-images-on-the-host-os/">D.
    Berrangé's post</link>).
   </para>
   <sect3 xml:id="sec-vt-best-img-open-img-raw">
    <title>Opening a raw image</title>
    <procedure>
     <title>Mounting a raw image</title>
     <step>
      <para>
       To be able to mount the image, find a free loop device. The following
       command displays the first unused loop device,
       <filename>/dev/loop1</filename> in this example.
      </para>
<screen>&prompt.root;losetup -f
/dev/loop1</screen>
     </step>
     <step>
      <para>
       Associate an image (<filename>SLE15.raw</filename> in this example) with
       the loop device:
      </para>
<screen>&prompt.root;losetup /dev/loop1 SLE15.raw</screen>
     </step>
     <step>
      <para>
       Check whether the image has successfully been associated with the loop
       device by getting detailed information about the loop device:
      </para>
<screen>&prompt.root;losetup -l
NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop1         0      0         0  0    /var/lib/libvirt/images/SLE15.raw</screen>
     </step>
     <step>
      <para>
       Check the image's partitions with <command>kpartx</command>:
      </para>
<screen>&prompt.root;kpartx -a<co xml:id="co-kpartx-a"/> -v<co xml:id="co-kpartx-v"/> /dev/loop1
add map loop1p1 (254:1): 0 29358080 linear /dev/loop1 2048</screen>
      <calloutlist>
       <callout arearefs="co-kpartx-a">
        <para>
         Add partition device mappings.
        </para>
       </callout>
       <callout arearefs="co-kpartx-v">
        <para>
         Verbose mode.
        </para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       Now mount the image partition(s) (to
       <filename>/mnt/sle15mount</filename> in the following example):
      </para>
<screen>&prompt.root;mkdir /mnt/sle15mount
&prompt.root;mount /dev/mapper/loop1p1 /mnt/sle15mount</screen>
     </step>
    </procedure>
    <note>
     <title>Raw image with LVM</title>
     <para>
      If your raw image contains an LVM volume group you should use LVM tools
      to mount the partition. Refer to <xref linkend="sec-lvm-found"/>.
     </para>
    </note>
    <procedure>
     <title>Unmounting a raw image</title>
     <step>
      <para>
       Unmount all mounted partitions of the image, for example:
      </para>
<screen>&prompt.root;umount /mnt/sle15mount</screen>
     </step>
     <step xml:id="st-umount-raw">
      <para>
       Delete partition device mappings with <command>kpartx</command>:
      </para>
<screen>&prompt.root;kpartx -d /dev/loop1</screen>
     </step>
     <step>
      <para>
       Detach the devices with <command>losetup</command>
      </para>
<screen>&prompt.root;losetup -d /dev/loop1</screen>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec-vt-best-img-open-img-qcow2">
    <title>Opening a qcow2 image</title>
    <procedure>
     <title>Mounting a qcow2 image</title>
     <step>
      <para>
       First you need to load the <literal>nbd</literal> (network block
       devices) module. The following example loads it with support for 16
       block devices (<option>max_part=16</option>). Check with
       <command>dmesg</command> whether the operation was successful:
      </para>
<screen>&prompt.root;modprobe nbd max_part=16
&prompt.root;dmesg | grep nbd
[89155.142425] nbd: registered device at major 43</screen>
     </step>
     <step>
      <para>
       Connect the &vmguest; image (for example
       <filename>SLE15.qcow2</filename>) to an NBD device
       (<filename>/debv/nbd0</filename> in the following example) with the
       <command>qemu-nbd</command> command. Make sure to use a free NBD device:
      </para>
<screen>&prompt.root;qemu-nbd -c<co xml:id="co-qemunbd-minusc"/> /dev/nbd0<co xml:id="co-qemunbd-device"/> SLE15.qcow2<co xml:id="co-qemunbd-image"/></screen>
      <calloutlist>
       <callout arearefs="co-qemunbd-minusc">
        <para>
         Connect <filename>SLE15.qcow2</filename> to the local NBD device
         <filename>/dev/nbd0</filename>
        </para>
       </callout>
       <callout arearefs="co-qemunbd-device">
        <para>
         NBD device to use
        </para>
       </callout>
       <callout arearefs="co-qemunbd-image">
        <para>
         &vmguest; image to use
        </para>
       </callout>
      </calloutlist>
      <tip>
       <title>Checking for a free NBD device</title>
       <para>
        To check whether an NBD device is free, run the following command:
       </para>
<screen>&prompt.root;lsof /dev/nbd0
COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF  NODE NAME
qemu-nbd 15149 root   10u   BLK   43,0      0t0 47347 /dev/nbd0</screen>
       <para>
        If the command produces an output like in the example above, the device
        is busy (not free). This can also be confirmed by the presence of the
        <filename>/sys/devices/virtual/block/nbd0/pid</filename> file.
       </para>
      </tip>
     </step>
     <step>
      <para>
       Inform the operating system about partition table changes with
       <command>partprobe</command>:
      </para>
<screen>&prompt.root;partprobe /dev/nbd0 -s
/dev/nbd0: msdos partitions 1 2
&prompt.root;dmesg | grep nbd0 | tail -1
[89699.082206]  nbd0: p1 p2</screen>
     </step>
     <step>
      <para>
       In the example above, the <filename>SLE15.qcow2</filename> contains two
       partitions: <filename>/dev/nbd0p1</filename> and
       <filename>/dev/nbd0p2</filename>. Before mounting these partitions, use
       <command>vgscan</command> to check whether they belong to an LVM volume:
      </para>
<screen>&prompt.root;vgscan -v
    Wiping cache of LVM-capable devices
    Wiping internal VG cache
    Reading all physical volumes. This may take a while...
    Using volume group(s) on command line.
    No volume groups found.</screen>
     </step>
     <step>
      <para>
       If no LVM volume has been found, you can mount the partition with
       <command>mount</command>:
      </para>
<screen>&prompt.root;mkdir /mnt/nbd0p2
# mount /dev/nbd0p1 /mnt/nbd0p2</screen>
      <para>
       Refer to <xref linkend="sec-lvm-found"/> for information on how to
       handle LVM volumes.
      </para>
     </step>
    </procedure>
    <procedure>
     <title>Unmounting a qcow2 image</title>
     <step>
      <para>
       Unmount all mounted partitions of the image, for example:
      </para>
<screen>&prompt.root;umount /mnt/nbd0p2</screen>
     </step>
     <step xml:id="st-umount-qcow2">
      <para>
       Disconnect the image from the <filename>/dev/nbd0</filename> device.
      </para>
<screen>&prompt.root;qemu-nbd -d /dev/nbd0</screen>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec-lvm-found">
    <title>Opening images containing LVM</title>
    <procedure>
     <title>Mounting images containing LVM</title>
     <step>
      <para>
       To check images for LVM groups, use <command>vgscan -v</command>. If an
       image contains LVM groups, the output of the command looks like the
       following:
      </para>
<screen>&prompt.root;vgscan -v
Wiping cache of LVM-capable devices
Wiping internal VG cache
Reading all physical volumes.  This may take a while...
Finding all volume groups
Finding volume group "system"
Found volume group "system" using metadata type lvm2</screen>
     </step>
     <step>
      <para>
       The <literal>system</literal> LVM volume group has been found on the
       system. You can get more information about this volume with
       <command>vgdisplay <replaceable>VOLUMEGROUPNAME</replaceable></command>
       (in our case <replaceable>VOLUMEGROUPNAME</replaceable> is
       <literal>system</literal>). You should activate this volume group to
       expose LVM partitions as devices so the system can mount them. Use
       <command>vgchange</command>:
      </para>
<screen>&prompt.root;vgchange -ay -v
Finding all volume groups
Finding volume group "system"
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/home
Creating system-home
Loading system-home table (254:0)
Resuming system-home (254:0)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/root
Creating system-root
Loading system-root table (254:1)
Resuming system-root (254:1)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/swap
Creating system-swap
Loading system-swap table (254:2)
Resuming system-swap (254:2)
Activated 3 logical volumes in volume group system
    3 logical volume(s) in volume group "system" now active</screen>
     </step>
     <step>
      <para>
       All partitions in the volume group will be listed in the
       <filename>/dev/mapper</filename> directory. You can simply mount them
       now.
      </para>
<screen>&prompt.root;ls /dev/mapper/system-*
/dev/mapper/system-home  /dev/mapper/system-root /dev/mapper/system-swap

&prompt.root;mkdir /mnt/system-root
&prompt.root;mount  /dev/mapper/system-root /mnt/system-root

&prompt.root;ls /mnt/system-root/
bin   dev  home  lib64       mnt  proc        root  sbin     srv  tmp  var
boot  etc  lib   lost+found  opt  read-write  run   selinux  sys  usr</screen>
     </step>
    </procedure>
    <procedure>
     <title>Unmounting images containing LVM</title>
     <step>
      <para>
       Unmount all partitions (with <command>umount</command>)
      </para>
<screen>&prompt.root;umount /mnt/system-root</screen>
     </step>
     <step>
      <para>
       Deactivate the LVM volume group (with <command>vgchange -an
       <replaceable>VOLUMEGROUPNAME</replaceable></command>)
      </para>
<screen>&prompt.root;vgchange -an -v system
Using volume group(s) on command line
Finding volume group "system"
Found volume group "system"
Removing system-home (254:0)
Found volume group "system"
Removing system-root (254:1)
Found volume group "system"
Removing system-swap (254:2)
Deactivated 3 logical volumes in volume group system
0 logical volume(s) in volume group "system" now active</screen>
     </step>
     <step>
      <para>
       Now you have two choices:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         In case of a qcow2 image, proceed as described in
         <xref linkend="st-umount-qcow2"/> (<command>qemu-nbd -d
         /dev/nbd0</command>).
        </para>
       </listitem>
       <listitem>
        <para>
         In case of a raw image, proceeds as described in
         <xref linkend="st-umount-raw"/> (<command>kpartx -d
         /dev/loop1</command>; <command>losetup -d /dev/loop1</command>).
        </para>
       </listitem>
      </itemizedlist>
      <important>
       <title>Check for a successful unmount</title>
       <para>
        You should double-check that unmounting succeeded by using a system
        command like <command>losetup</command>, <command>qemu-nbd</command>,
        <command>mount</command> or <command>vgscan</command>. If this is not
        the case you may have trouble using the &vmguest; because its system
        image is used in different places.
       </para>
      </important>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-img-share">
   <title>File system sharing</title>
   <para>
    You can access a host directory in the &vmguest; using the <tag
    class="element">filesystem</tag> element. In the following example we will
    share the <filename>/data/shared</filename> directory and mount it in the
    &vmguest;. Note that the <tag class="attribute">accessmode</tag> parameter
    only works with <tag class="attribute">type='mount'</tag> for the
    &qemu;/&kvm; driver (most other values for <tag
    class="attribute">type</tag> are exclusively used for the LXC driver).
   </para>
<screen>&lt;filesystem type='mount'<co xml:id="co-fs-mount"/> accessmode='mapped'<co xml:id="co-fs-mode"/>&gt;
   &lt;source dir='/data/shared'<co xml:id="co-fs-sourcedir"/>&gt;
   &lt;target dir='shared'<co xml:id="co-fs-targetdir"/>/&gt;
&lt;/filesystem&gt;</screen>
   <calloutlist>
    <callout arearefs="co-fs-mount">
     <para>
      A host directory to mount &vmguest;.
     </para>
    </callout>
    <callout arearefs="co-fs-mode">
     <para>
      Access mode (the security mode) set to <literal>mapped</literal> will
      give access with the permissions of the hypervisor. Use
      <literal>passthrough</literal> to access this share with the permissions
      of the user inside the &vmguest;.
     </para>
    </callout>
    <callout arearefs="co-fs-sourcedir">
     <para>
      Path to share with the &vmguest;.
     </para>
    </callout>
    <callout arearefs="co-fs-targetdir">
     <para>
      Name or label of the path for the mount command.
     </para>
    </callout>
   </calloutlist>
   <para>
    To mount the <literal>shared</literal> directory on the &vmguest;, use the
    following commands: Under the &vmguest; now you need to mount the
    <literal>target dir='shared'</literal>:
   </para>
<screen>&prompt.root;mkdir /opt/mnt_shared
&prompt.root;mount shared -t 9p /opt/mnt_shared -o trans=virtio</screen>
   <para>
    See
    <link xlink:href="https://libvirt.org/formatdomain.html#elementsFilesystems">&libvirt;
    File System </link> and
    <link xlink:href="http://wiki.qemu.org/Documentation/9psetup">&qemu;
    9psetup</link> for more information.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-vt-best-vmguests">
  <title>&vmguest; configuration</title>

  <sect2 xml:id="sec-vt-best-vmguests-virtio">
   <title>Virtio driver</title>
   <para>
    To increase &vmguest; performance it is recommended to use paravirtualized
    drivers within the &vmguest;s. The virtualization standard for such drivers
    for &kvm; are the <literal>virtio</literal> drivers, which are designed for
    running in a virtual environment. &xen; uses similar paravirtualized device
    drivers (like
    <link
    xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link>
    in a Windows* guest). For a better understanding of this topic, refer to
    <xref linkend="sec-vt-io"/>.
   </para>
   <sect3 xml:id="sec-vt-best-vmguests-virtio-virtio-blk">
    <title><literal>virtio blk</literal></title>
    <para>
     <literal>virtio_blk</literal> is the virtio block device for disk. To use
     the <literal>virtio blk</literal> driver for a block device, specify the
     <tag class="attribute">bus='virtio'</tag> attribute in the
     <tag
     class="element">disk</tag> definition:
    </para>
<screen>&lt;disk type='....' device='disk'&gt;
    ....
    &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</screen>
    <important>
     <title>Disk device names</title>
     <para>
      <literal>virtio</literal> disk devices are named
      <literal>/dev/vd[a-z][1-9]</literal>. If you migrate a Linux guest from a
      non-virtio disk you need to adjust the <literal>root=</literal> parameter
      in the GRUB configuration, and regenerate the <filename>initrd</filename>
      file. Otherwise the system cannot boot. On &vmguest;s with other
      operating systems, the boot loader may need to be adjusted or reinstalled
      accordingly, too.
     </para>
    </important>
    <important>
     <title>Using <literal>virtio</literal> disks with <command>qemu-system-ARCH</command></title>
     <para>
      When running <command>qemu-system-ARCH</command>, use the
      <option>-drive</option> option to add a disk to the &vmguest;. See
      <xref linkend="cha-qemu-guest-inst-qemu-kvm"/> for an example. The
      <option>-hd[abcd]</option> option will not work for virtio disks.
     </para>
    </important>
   </sect3>
   <sect3 xml:id="sec-vt-best-vmguests-virtio-virtio-net">
    <title>virtio net</title>
    <para>
     <literal>virtio_net</literal> is the virtio network device. The kernel
     modules should be loaded automatically in the guest at boot time. You need
     to start the service to make the network available.
    </para>
<screen>&lt;interface type='network'&gt;
    ...
    &lt;model type='virtio' /&gt;
&lt;/interface&gt;</screen>
   </sect3>
   <sect3 xml:id="sec-vt-best-vmguests-virtio-balloon">
    <title>virtio balloon</title>
    <para>
     The virtio balloon is used for host memory over-commits for guests. For
     Linux guests, the balloon driver runs in the guest kernel, whereas for
     Windows guests, the balloon driver is in the VMDP package.
     <literal>virtio_balloon</literal> is a PV driver to give or take memory
     from a &vmguest;.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>Inflate balloon</emphasis>: Return memory from guest to host
       kernel (for &kvm;) or to hypervisor (for &xen;)
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Deflate balloon</emphasis>: Guest will have more available
       memory
      </para>
     </listitem>
    </itemizedlist>
    <para>
     It is controlled by the <literal>currentMemory</literal> and
     <literal>memory</literal> options.
    </para>
<screen>&lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;1048576&lt;/currentMemory&gt;
    [...]
    &lt;devices&gt;
        &lt;memballoon model='virtio'/&gt;
    &lt;/devices&gt;</screen>
    <para>
     You can also use <command>virsh</command> to change it:
    </para>
<screen>&prompt.user;virsh setmem <replaceable>DOMAIN_ID</replaceable> <replaceable>MEMORY in KB</replaceable></screen>
   </sect3>
   <sect3 xml:id="sec-vt-best-vmguests-virtio-check">
    <title>Checking virtio presence</title>
    <para>
     You can check the virtio block PCI with:
    </para>
<screen>&prompt.user;find /sys/devices/ -name virtio*
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2</screen>
    <para>
     To find the block device associated with <filename>vdX</filename>:
    </para>
<screen>&prompt.user;find /sys/devices/ -name virtio* -print  -exec ls {}/block 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2
vda</screen>
    <para>
     To get more information on the virtio block:
    </para>
<screen>&prompt.user;udevadm info -p /sys/devices/pci0000:00/0000:00:08.0/virtio2
P: /devices/pci0000:00/0000:00:08.0/virtio2
E: DEVPATH=/devices/pci0000:00/0000:00:08.0/virtio2
E: DRIVER=virtio_blk
E: MODALIAS=virtio:d00000002v00001AF4
E: SUBSYSTEM=virtio</screen>
    <para>
     To check all virtio drivers being used:
    </para>
<screen>&prompt.user;find /sys/devices/ -name virtio* -print  -exec ls -l {}/driver 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
lrwxrwxrwx 1 root root 0 Jun 17 15:48 /sys/devices/pci0000:00/0000:00:06.0/virtio0/driver -&gt; ../../../../bus/virtio/drivers/virtio_console
/sys/devices/pci0000:00/0000:00:07.0/virtio1
lrwxrwxrwx 1 root root 0 Jun 17 15:47 /sys/devices/pci0000:00/0000:00:07.0/virtio1/driver -&gt; ../../../../bus/virtio/drivers/virtio_balloon
/sys/devices/pci0000:00/0000:00:08.0/virtio2
lrwxrwxrwx 1 root root 0 Jun 17 14:35 /sys/devices/pci0000:00/0000:00:08.0/virtio2/driver -&gt; ../../../../bus/virtio/drivers/virtio_blk</screen>
   </sect3>
<!--
       <para>
       Currently performance is much better when using a host kernel configured with <literal>CONFIG_HIGH_RES_TIMERS</literal>. Another option is use HPET/RTC and <option>-clock=</option> &qemu; option.
       </para>
   -->
   <sect3 xml:id="sec-vt-best-vmguests-virtio-drv-opt">
    <title>Find device driver options</title>
    <para>
     Virtio devices and other drivers have various options. To list all of
     them, use the <option>help</option> parameter of
     the<command>qemu-system-ARCH</command> command.
    </para>
<screen>&prompt.user;qemu-system-x86_64 -device virtio-net,help
virtio-net-pci.ioeventfd=on/off
virtio-net-pci.vectors=uint32
virtio-net-pci.indirect_desc=on/off
virtio-net-pci.event_idx=on/off
virtio-net-pci.any_layout=on/off
.....</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-perf-cirrus">
   <title>Cirrus video driver</title>
   <para>
    To get 16-bit color, high compatibility and better performance it is
    recommended to use the <literal>cirrus</literal> video driver.
   </para>
   <note>
    <title>&libvirt;</title>
    <para>
     &libvirt; ignores the <literal>vram</literal> value because video size has
     been hardcoded in &qemu;.
    </para>
   </note>
<screen>&lt;video&gt;
   &lt;model type='cirrus' vram='9216' heads='1'/&gt;
&lt;/video&gt;</screen>
  </sect2>

  <sect2 xml:id="sec-vt-best-entropy">
   <title>Better entropy</title>
   <para>
    Virtio RNG (random number generator) is a paravirtualized device that is
    exposed as a hardware RNG device to the guest. On the host side, it can be
    wired up to one of several sources of entropy (including a real hardware
    RNG device and the host's <filename>/dev/random</filename>) if hardware
    support does not exist. The Linux kernel contains the guest driver for the
    device from version 2.6.26 and higher.
   </para>
   <para>
    The system entropy is collected from various non-deterministic hardware
    events and is mainly used by cryptographic applications. The virtual random
    number generator device (paravirtualized device) allows the host to pass
    through entropy to &vmguest; operating systems. This results in a better
    entropy in the &vmguest;.
   </para>
   <para>
    To use Virtio RNG, add an <literal>RNG</literal> device in
    <command>virt-manager</command> or directly in the &vmguest;'s XML
    configuration:
   </para>
<screen>&lt;devices&gt;
   &lt;rng model='virtio'&gt;
       &lt;backend model='random'&gt;/dev/random&lt;/backend&gt;
   &lt;/rng&gt;
&lt;/devices&gt;</screen>
   <para>
    The host now should used <filename>/dev/random</filename>:
   </para>
<screen>&prompt.user;lsof /dev/random
qemu-syst 4926 qemu    6r   CHR    1,8      0t0 8199 /dev/random</screen>
   <para>
    On the &vmguest;, the source of entropy can be checked with:
   </para>
<screen>&prompt.user;cat /sys/devices/virtual/misc/hw_random/rng_available</screen>
   <para>
    The current device used for entropy can be checked with:
   </para>
<screen>&prompt.user;cat /sys/devices/virtual/misc/hw_random/rng_current
virtio_rng.0</screen>
   <para>
    You should install the <package>rng-tools</package> package on the
    &vmguest;, enable the service, and start it. Under &productname; 15, do the
    following:
   </para>
<screen>&prompt.root;zypper in rng-tools
&prompt.root;systemctl enable rng-tools
&prompt.root;systemctl start rng-tools</screen>
  </sect2>

  <sect2 xml:id="sec-vt-best-perf-disable">
   <title>Disable unused tools and devices</title>
   <para>
    Per host, use one virtualization technology only. For example, do not use
    &kvm; and Containers on the same computer. Otherwise, you may find yourself
    with a reduced amount of available resources, increased security risk and a
    longer software update queue. Even when the amount of resources allocated
    to each of the technologies is configured carefully, the host may suffer
    from reduced overall availability and degraded performance.
   </para>
   <para>
    Minimize the amount of software and services available on hosts. Most
    default installations of operating systems are not optimized for VM usage.
    Install what you really need and remove all other components in the
    &vmguest;.
   </para>
   <para>
    Windows* Guest:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Disable the screen saver
     </para>
    </listitem>
    <listitem>
     <para>
      Remove all graphical effects
     </para>
    </listitem>
    <listitem>
     <para>
      Disable indexing of hard disks if not necessary
     </para>
    </listitem>
    <listitem>
     <para>
      Check the list of started services and disable the ones you do not need
     </para>
    </listitem>
    <listitem>
     <para>
      Check and remove all unneeded devices
     </para>
    </listitem>
    <listitem>
     <para>
      Disable system update if not needed, or configure it to avoid any delay
      while rebooting or shutting down the host
     </para>
    </listitem>
    <listitem>
     <para>
      Check the Firewall rules
     </para>
    </listitem>
    <listitem>
     <para>
      Schedule backups and anti-virus updates appropriately
     </para>
    </listitem>
    <listitem>
     <para>
      Install the
      <link xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link>
      paravirtualized driver for best performance
     </para>
    </listitem>
    <listitem>
     <para>
      Check the operating system recommendations, such as on the
      <link xlink:href="http://windows.microsoft.com/en-us/windows/optimize-windows-better-performance#optimize-windows-better-performance=windows-7">Microsoft
      Windows* 7 better performance</link> Web page.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Linux Guest:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Remove or do not start the X Window System if not necessary
     </para>
    </listitem>
    <listitem>
     <para>
      Check the list of started services and disable the ones you do not need
     </para>
    </listitem>
    <listitem>
     <para>
      Check the OS recommendations for kernel parameters that enable better
      performance
     </para>
    </listitem>
    <listitem>
     <para>
      Only install software that you really need
     </para>
    </listitem>
    <listitem>
     <para>
      Optimize the scheduling of predictable tasks (system updates, hard disk
      checks, etc.)
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec-vt-best-perf-mtype">
   <title>Updating the guest machine type</title>
   <para>
    &qemu; machine types define details of the architecture that are
    particularly relevant for migration and session management. As changes or
    improvements to &qemu; are made, new machine types are added. Old machine
    types are still supported for compatibility reasons, but to take advantage
    of improvements, we recommend to always migrate to the latest machine type
    when upgrading.
   </para>
   <para>
    Changing the guest's machine type for a Linux guest will mostly be
    transparent. For Windows* guests, we recommend to take a snapshot or backup
    of the guest&mdash;in case Windows* has issues with the changes it detects
    and subsequently the user decides to revert to the original machine type
    the guest was created with.
   </para>
   <note>
    <title>Changing the machine type</title>
    <para>
     Refer to <xref linkend="sec-libvirt-config-machinetype-virsh"/> for
     documentation.
    </para>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-vt-best-vm-setup-config">
  <title>&vmguest;-specific configurations and settings</title>

  <sect2 xml:id="sec-vt-best-acpi">
   <title>ACPI testing</title>
   <para>
    The ability to change a &vmguest;'s state heavily depends on the operating
    system. It is very important to test this feature before any use of your
    &vmguest;s in production. For example, most Linux operating systems disable
    this capability by default, so this requires you to enable this operation
    (mostly through &pk;).
   </para>
   <para>
    ACPI must be enabled in the guest for a graceful shutdown to work. To check
    if ACPI is enabled, run:
   </para>
<screen>&prompt.user;virsh dumpxml <replaceable>VMNAME</replaceable> | grep acpi</screen>
   <para>
    If nothing is printed, ACPI is not enabled for your machine. Use
    <command>virsh edit</command> to add the following XML under
    &lt;domain&gt;:
   </para>
<screen>&lt;features&gt;
   &lt;acpi/&gt;
&lt;/features&gt;</screen>
   <para>
    If ACPI was enabled during a Windows Server&thrdmrk; guest installation, it
    is not sufficient to turn it on in the &vmguest; configuration only. For
    more information, see
    <link xlink:href="https://support.microsoft.com/en-us/kb/309283"/>.
    <remark>FIXME: URL is for XP and Server 2003. Is there something newer? -
    sknorr, 2017-06-07</remark>
   </para>
<!-- URL is currently(?) broken - sknorr, 2017-06-07-->
<!-- <link xlink:href="https://support.microsoft.com/en-us/kb/314088"/> -->
   <para>
    Regardless of the &vmguest;'s configuration, a graceful shutdown is always
    possible from within the guest operating system.
   </para>
  </sect2>

  <sect2 xml:id="sec-vt-best-guest-kbd">
   <title>Keyboard layout</title>
   <para>
    Though it is possible to specify the keyboard layout from a
    <command>qemu-system-ARCH</command> command, it is recommended to configure
    it in the &libvirt; XML file. To change the keyboard layout while
    connecting to a remote &vmguest; using vnc, you should edit the &vmguest;
    XML configuration file. For example, to add an <literal>en-us</literal>
    keymap, add in the <literal>&lt;devices&gt;</literal> section:
   </para>
<screen>&lt;graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/&gt;</screen>
   <para>
    Check the <literal>vncdisplay</literal> configuration and connect to your
    &vmguest;:
   </para>
<screen>&prompt.user;virsh vncdisplay sles15 127.0.0.1:0</screen>
  </sect2>

  <sect2 xml:id="sec-vt-best-spice-default-url">
   <title>Spice default listen URL</title>
   <para>
    If no network interface other than <literal>lo</literal> is assigned an
    IPv4 address on the host, the default address on which the spice server
    listens will not work. An error like the following one will occur:
   </para>
<screen>&prompt.user;virsh start sles15
error: Failed to start domain sles15
error: internal error: process exited while connecting to monitor: ((null):26929): Spice-Warning **: reds.c:2330:reds_init_socket: getaddrinfo(127.0.0.1,5900): Address family for hostname not supported
2015-08-12T11:21:14.221634Z qemu-system-x86_64: failed to initialize spice server</screen>
   <para>
    To fix this, you can change the default <literal>spice_listen</literal>
    value in <filename>/etc/libvirt/qemu.conf</filename> using the local IPv6
    address <systemitem class="ipaddress">::1</systemitem>. The spice server
    listening address can also be changed on a per &vmguest; basis, use
    <command>virsh edit</command> to add the listen XML attribute to the
    <literal>graphics type='spice'</literal> element:
   </para>
<screen>&lt;graphics type='spice' listen='::1' autoport='yes'/&gt;></screen>
  </sect2>

  <sect2 xml:id="sec-vt-best-xml-to-qemu">
   <title>XML to &qemu; command line</title>
   <para>
    Sometimes it could be useful to get the &qemu; command line to launch the
    &vmguest; from the XML file.
   </para>
<screen>&prompt.user;virsh domxml-to-native<co xml:id="co-domxml-native"/> qemu-argv<co xml:id="co-domxml-argv"/> SLE15.xml<co xml:id="co-domxml-file"/></screen>
   <calloutlist>
    <callout arearefs="co-domxml-native">
     <para>
      Convert the XML file in domain XML format to the native guest
      configuration
     </para>
    </callout>
    <callout arearefs="co-domxml-argv">
     <para>
      For the &qemu;/&kvm; hypervisor, the format argument needs be qemu-argv
     </para>
    </callout>
    <callout arearefs="co-domxml-file">
     <para>
      Domain XML file to use
     </para>
    </callout>
   </calloutlist>
<screen>&prompt.user;&sudo; virsh domxml-to-native qemu-argv /etc/libvirt/qemu/SLE15.xml
LC_ALL=C PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
   QEMU_AUDIO_DRV=none /usr/bin/qemu-system-x86_64 -name SLE15 -machine \
   pc-i440fx-2.3,accel=kvm,usb=off -cpu SandyBridge -m 4048 -realtime \
   mlock=off -smp 4,sockets=4,cores=1,threads=1 -uuid 8616d00f-5f05-4244-97cc-86aeaed8aea7 \
   -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/SLE15.monitor,server,nowait \
   -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew \
   -global kvm-pit.lost_tick_policy=discard -no-hpet \
   -no-shutdown -global PIIX4_PM.disable_s3=1 -global PIIX4_PM.disable_s4=1 \
   -boot strict=on -device ich9-usb-ehci1,id=usb,bus=pci.0,addr=0x4.0x7 \
   -device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,multifunction=on,addr=0x4 \
   -device ich9-usb-uhci2,masterbus=usb.0,firstport=2,bus=pci.0,addr=0x4.0x1 \
   -device ich9-usb-uhci3,masterbus=usb.0,firstport=4,bus=pci.0,addr=0x4.0x2 \
   -drive file=/var/lib/libvirt/images/SLE15.qcow2,if=none,id=drive-virtio-disk0,format=qcow2,cache=none \
   -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=2 \
   -drive if=none,id=drive-ide0-0-1,readonly=on,format=raw  \
   -device ide-cd,bus=ide.0,unit=1,drive=drive-ide0-0-1,id=ide0-0-1 -netdev tap,id=hostnet0  \
   -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:28:04:a9,bus=pci.0,addr=0x3,bootindex=1 \
   -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 \
   -vnc 127.0.0.1:0 -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 \
   -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on</screen>
  </sect2>

  <sect2 xml:id="sec-vt-best-kernel-parameter">
   <title>Change kernel parameters at boot time</title>
   <sect3 xml:id="sec-vt-best-kernel-parameter-sle11">
    <title>&sle; 11</title>
    <para>
     To change the value for &slea; 11 products at boot time, you need to
     modify your <filename>/boot/grub/menu.lst</filename> file by adding the
     <option>OPTION=parameter</option>. Then reboot your system.
    </para>
   </sect3>
   <sect3 xml:id="sec-vt-best-kernel-parameter-sle12">
    <title>&sle; 12 and 15</title>
    <para>
     To change the value for &slea; 12 and 15 products at boot time, you need
     to modify your <filename>/etc/default/grub</filename> file. Find the
     variable starting with <option>GRUB_CMDLINE_LINUX_DEFAULT</option> and add
     at the end <option>OPTION=parameter</option> (or change it with the
     correct value if it is already available).
    </para>
    <para>
     Now you need to regenerate your <literal>grub2</literal> configuration:
    </para>
<screen># grub2-mkconfig -o /boot/grub2/grub.cfg</screen>
    <para>
     Then reboot your system.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-vt-best-guest-device-to-xml">
   <title>Add a device to an XML configuration</title>
   <para>
    To create a new &vmguest; based on an XML file, you can specify the &qemu;
    command line using the special tag <literal>qemu:commandline</literal>. For
    example, to add a virtio-balloon-pci, add this block at the end of the XML
    configuration file (before the &lt;/domain&gt; tag):
   </para>
<screen>&lt;qemu:commandline&gt;
    &lt;qemu:arg value='-device'/&gt;
    &lt;qemu:arg value='virtio-balloon-pci,id=balloon0'/&gt;
&lt;/qemu:commandline&gt;</screen>
  </sect2>
 </sect1>
<!-- TODO
      <sect2 id="vt-best-guest-clock">
      <title>Clock setting</title>
      <para>

      clock setting (common source)
      kvm clock and ntp
      </para>
      Use Time Stamp Counter (TSC) as
      clocksource (if host CPU supports TSC)

kvm-clock
hpet
acpi_pm
jiffies
tsc
</sect2>

<sect2 id="vt-guest-guest-bio">
<title>Bio-based</title>
<para>
bio-based driver on slow devices
</para>
</sect2>
 -->
<!--
     <sect1 id="vt-best-snapsav">
     <title>Saving, migrating and snapshoting</title>
     <para>
     Migration requirements/Recomendations
     save VM and start/boot (memory invalid)
     snapshot naming importance
     avoid qemu-img snapshot
     cache mode in live migration
     guestfs and live system
     </para>
     </sect1>


<sect1 id="vt-best-security">
<title>Security consideration</title>
<para>
Connection to guest: security policy
Authentication for libvirtd and VNC need to be configured separately
&qemu; Guest Agent
The VNC TLS (set at start)
</para>
</sect1>

<sect1 id="vt-best-pcipass">
<title>pcpipass</title>
<para>
Pci device (not online!, managed/unmanaged)
howto check SR-IOV capabilities
</para>
</sect1>
 -->
<!--
     <sect1 xml:id="sec-vt-best-net">
     <title>Network tips</title>
     <para>
     </para>

<sect2 xml:id="sec-vt-best-net-vnic">


<title>Virtual NICs</title>
<para>
virtio-net (&kvm;) : multi-queue option
vhost-net (&kvm;) : Default vNIC, best performance
netbk (&xen;) : kernel threads vs. tasklets
</para>
</sect2>

<sect2 xml:id="sec-vt-best-net-enic">
<title>Emulated NICs</title>
<para>
e1000: Default and preferred emulated NIC
rtl8139
</para>
</sect2>

<sect2 xml:id="sec-vt-best-net-sharednic">
<title>Shared physical NICs</title>
<para>
SR-IOV: macvtap
Physicial NICs : PCI pass-through
</para>
</sect2>

<sect2 xml:id="sec-vt-best-general">
<title>Network general</title>
<para>
use multi-network to avoid congestion
admin, storage, migration ...
use arp-filter to prevent arp flux

same MTU in all devices to avoid fragmentation
yast to configure bridge
Network MAC address
bridge configuration in bridge.conf file
PCI pass-through Vfio to improve performance
</para>
</sect2>
</sect1>
 -->
<!--
     <sect1 xml:id="sec-vt-best-debug">
     <title>Troubleshooting/debugging</title>
     <para>
     </para>

<sect2 xml:id="sec-vt-best-debug-xen">
<title>&xen;</title>
<para>
</para>
<sect3 xml:id="sec-vt-best-debug-xen-log">
<title>&xen; log files</title>
<para>
libxl logs:
<filename>/var/log/xen/*</filename>
qemu-dm-domain.log, xl-domain.log
bootloader.log, vm-install, xen-hotplug
Process specific logs, often requiring debug log levels to be useful
Some logs require 'set -x' to be added to /etc/xen/scripts/*

libvirt logs:
<filename>/var/log/libvirt/libxl</filename>
libxl-driver.log
domain.log
</para>
</sect3>
<sect3 xml:id="sec-vt-best-debug-xen-hypervisor">
<title>Daemon and hypervisor logs</title>
<para>
View systemd journal for specific units/daemons: <command>journalctl
<command>journalctl [\-\-follow] –unit xencommons.service</command>
<command>journalctl /usr/sbin/xenwatchdogd</command>
xl dmesg
&xen; hypervisor logs
</para>
</sect3>

<sect3 xml:id="sec-vt-best-debug-xen-loglevel">
<title>Increasing logging levels</title>
<para>
Log levels are increased through xen parameters:
</para>
<screen>loglvl=all</screen>
<para>
Increased logging for &xen; hypervisor
</para>
<screen>guest_loglvl=all</screen>
<para>
Increased logging for guest domain actions Grub2 config:

Edit <filename>/etc/default/grub</filename>, then recreate <filename>grub.cfg</filename>:
GRUB_CMDLINE_XEN_DEFAULT=”loglvl=all guest_loglvl=all”
<command>grub2-mkconfig -o /boot/grub/grub.cfg</command>
</para>
</sect3>
</sect2>

<sect2 xml:id="sec-vt-best-debug-support">
<title>Support</title>
<para></para>
<sect3 xml:id="sec-vt-best-debug-support-config">
<title>Supportconfig and virtualization</title>
<para>
Core files:
basic-environment.txt
Reports detected virtualization hypervisor
Under some hypervisors (xen), subsequent general checks might be incomplete

Hypervisor specific files:
kvm.txt, xen.txt
Both logs contain general information:
RPM version/verification of important packages
Kernel, hardware, network details
</para>
</sect3>

<sect3 xml:id="sec-vt-best-debug-support-kvm">
<title>kvm.txt</title>
<para>
libvirt details
General libvirt details
Libvirt daemon status
&kvm; statistics
virsh version, capabilities, nodeinfo, etc...

Domain list and configurations
Conf and log files
<filename>/etc/libvirt/libvirtd.conf</filename>
Last 500 lines from <filename>/var/log/libvirt/qemu/domain.log</filename>
</para>
</sect3>

<sect3 xml:id="sec-vt-best-debug-support-xen">
<title>xen.txt</title>
<para>
Daemon status
xencommons, xendomains and xen-watchdog daemons
grub/grub2 configuration (for xen.gz parameters)

libvirt details
Domain list and configurations

xl details
Domain list and configurations
Conf and Log files
<filename>/etc/xen/xl.conf</filename>, <filename>/etc/libvirt/libvirtd.conf</filename>
Last 500 lines from <filename>/var/log/xen/*</filename>,
<filename>/var/log/libvirt/libxl/*</filename>
Output of <command>xl dmesg</command> and <command>xl info</command>
</para>
</sect3>
</sect2>

<sect2 xml:id="sec-vt-best-debug-advanced">
<title>Advanced debugging options</title>
<para>
Serial console
</para>
<screen>GRUB_CMDLINE_XEN_DEFAULT=“loglvl=all guest_loglvl=all console=com1 com1=115200,8n1”</screen>
<screen>GRUB_CMDLINE_LINUX_DEFAULT=“console=ttyS0,115200”</screen>
<para>
Debug keys
<command>xl debug keys h; xl dmesg</command>
<command>xl debug keys q; xl dmesg</command>
Additional &xen; debug tools:
<command>xenstore-{ls,read,rm,watch,write}, xentrace, xentop</command>

Capturing Guest Logs
Capturing guest logs during triggered problem:
Connect to domain:
<command>virsh console domname</command>
Execute problem command
Capturing domain boot messages
</para>
<screen>xl create -c VM config file</screen>
<screen>virsh create VM config file \-\-console</screen>
</sect2>
<sect2 xml:id="sec-vt-best-trouble">
<title>Troubleshooting installations</title>
<para>
virt-manager and virt-install logs:
Found in <filename>~/.cache/virt-manager</filename>

Debugging virt-manager:
<command>virt-manager \-\-no-fork</command>
Sends messages directly to screen and log file
</para>
<screen>LIBVIRT_DEBUG=1 virt-manager \-\-no-fork</screen>
<para>
See libvirt messages in <filename>/var/log/messages</filename>

Use <command>xl</command> to rule out libvirt layer
</para>
</sect2>
<sect2 xml:id="sec-vt-best-trouble-libvirt">
<title>Troubleshooting libvirt</title>
<para>
Client side troubleshooting
</para>
<screen>LIBVIRT_DEBUG=1
1: debug, 2: info, 3: warning, 4: error</screen>
<para>
Server side troubleshooting
<filename>/etc/libvirt/libvirtd.conf</filename> (restart libvirtd after changes)
log_level = 1
log_output = “1:file:/var/log/libvirtd.log”
log_filters = “1:qemu 3:remote”
</para>
</sect2>

<sect2 xml:id="sec-vt-best-trouble-kernel">
<title>Kernel cores</title>
<para>
Host cores -vs- guest domain cores
Host cores are enabled through Kdump YaST module
For &xen; dom0 cores, 'crashkernel=size@offset' should be added as a &xen; hypervisor parameter

Guest cores require:
on_crash[action]on_crash tag
Possible coredump actions are:
coredump-restart     Dump core, then restart the VM
coredump-destroy    Dup core, then terminate the VM
Crashes are written to:
<filename>/var/lib/libvirt/{libxl,qemu}/dump</filename>
<filename>/var/lib/xen/dump</filename>  (if using xl).
</para>
</sect2>


<sect2 xml:id="sec-vt-best-debug-other">
<title>Other</title>
<para>
VGA trouble debug
</para>
</sect2>
</sect1>
 -->
 <sect1 xml:id="sec-vt-best-hypervisors-containers">
  <title>Hypervisors compared to containers</title>

  <para/>

  <table>
   <title>Hypervisors compared to containers</title>
   <tgroup cols="3">
    <colspec colnum="1" colwidth="20%"/>
    <colspec colnum="2" colwidth="30%"/>
    <colspec colnum="3" colwidth="30%"/>
    <thead>
     <row>
      <entry>
       <para>
        Features
       </para>
      </entry>
      <entry>
       <para>
        Hypervisors
       </para>
      </entry>
      <entry>
       <para>
        Containers
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        Technologies
       </para>
      </entry>
      <entry>
       <para>
        Emulation of a physical computing environment
       </para>
      </entry>
      <entry>
       <para>
        Use kernel host
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        System layer level
       </para>
      </entry>
      <entry>
       <para>
        Managed by a virtualization layer (Hypervisor)
       </para>
      </entry>
      <entry>
       <para>
        Rely on kernel namespaces and cgroups
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Level (layer)
       </para>
      </entry>
      <entry>
       <para>
        Hardware level
       </para>
      </entry>
      <entry>
       <para>
        Software level
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Virtualization mode available
       </para>
      </entry>
      <entry>
       <para>
        FV or PV
       </para>
      </entry>
      <entry>
       <para>
        None, only user space
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Security
       </para>
      </entry>
      <entry>
       <para>
        Strong
       </para>
      </entry>
      <entry>
       <para>
        Warning: Security is very low
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Confinement
       </para>
      </entry>
      <entry>
       <para>
        Full isolation
       </para>
      </entry>
      <entry>
       <para>
        Warning: Host kernel (OS must be compatible with kernel version)
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Operating system
       </para>
      </entry>
      <entry>
       <para>
        Any operating system
       </para>
      </entry>
      <entry>
       <para>
        Only Linux (must be "kernel" compatible)
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Type of system
       </para>
      </entry>
      <entry>
       <para>
        Full OS needed
       </para>
      </entry>
      <entry>
       <para>
        Scope is an instance of Linux
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Boot time
       </para>
      </entry>
      <entry>
       <para>
        Slow to start (OS delay)
       </para>
      </entry>
      <entry>
       <para>
        Really quick start
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Overhead
       </para>
      </entry>
      <entry>
       <para>
        High
       </para>
      </entry>
      <entry>
       <para>
        Very low
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Efficiency
       </para>
      </entry>
      <entry>
       <para>
        Depends on OS
       </para>
      </entry>
      <entry>
       <para>
        Very efficient
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Sharing with host
       </para>
      </entry>
      <entry>
       <para>
        Warning: Complex because of isolation
       </para>
      </entry>
      <entry>
       <para>
        Sharing is easy (host sees everything; container sees its own objects)
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Migration
       </para>
      </entry>
      <entry>
       <para>
        Supports migration (live mode)
       </para>
      </entry>
      <entry>
       <para>
        Warning: Not possible
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <sect2 xml:id="sec-vt-best-hypervisors-containers-both">
   <title>Getting the best of both worlds</title>
   <para>
    Even if the above table seems to indicate that running a single application
    in a highly secure way is not possible, <command>virt-sandbox</command>
    will allow running a single application in a &kvm; guest, starting with
    &sls; 12 SP1. <command>virt-sandbox</command> bootstraps any command within
    a Linux kernel with a minimal root file system.
   </para>
   <para>
    The guest root file system can either be the root file system mounted
    read-only or a disk image. The following steps will show how to set up a
    sandbox with qcow2 disk image as root file system.
   </para>
   <procedure>
    <step>
     <para>
      Create the disk image using <command>qemu-img</command>:
     </para>
<screen>&prompt.root;qemu-img create -f qcow2 rootfs.qcow2 6G</screen>
    </step>
    <step>
     <para>
      Format the disk image:
     </para>
<screen>&prompt.root;modprobe nbd<co xml:id="co-vsmkfs-modprobe"/>
&prompt.root;/usr/bin/qemu-nbd --format qcow2 -n -c /dev/nbd0 $PWD/test-base.qcow2<co xml:id="co-vsmkfs-qemu-nbd"/>
&prompt.root;mkfs.ext3 /dev/nbd0<co xml:id="co-vsmkfs-do"/></screen>
     <calloutlist>
      <callout arearefs="co-vsmkfs-modprobe">
       <para>
        Make sure the nbd module is loaded: it is not loaded by default and
        will only be used to format the qcow image.
       </para>
      </callout>
      <callout arearefs="co-vsmkfs-qemu-nbd">
       <para>
        Create an NBD device for the qcow2 image. This device will then behave
        like any other block device. The example uses
        <replaceable>/dev/nbd0</replaceable> but any other free NBD device will
        work.
       </para>
      </callout>
      <callout arearefs="co-vsmkfs-do">
       <para>
        Format the disk image directly. Note that no partition table has been
        created: <command>virt-sandbox</command> considers the image to be a
        partition, not a disk.
       </para>
       <para>
        The partition formats that can be used are limited: the Linux kernel
        bootstrapping the sandbox needs to have the corresponding features
        built in. The Ext4 module is also available at the sandbox start-up
        time.
       </para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>
      Now populate the newly formatted image:
     </para>
<screen>&prompt.root;guestmount -a base.qcow2 -m /dev/sda:/ /mnt<co
     xml:id="co-vsfs-mount"/>

&prompt.root;zypper --root /mnt ar cd:///?devices=/dev/dvd SLES15_DVD
&prompt.root;zypper --root /mnt in -t pattern Minimal<co xml:id="co-vsfs-install"/>

&prompt.root;guestunmount /mnt<co xml:id="co-vsfs-unmount"/></screen>
     <calloutlist>
      <callout arearefs="co-vsfs-mount">
       <para>
        Mount the qcow2 image using the <command>guestfs</command> tools.
       </para>
      </callout>
      <callout arearefs="co-vsfs-install">
       <para>
        Use Zypper with the <literal>--root</literal> parameter to add a &sls;
        repository and install the <literal>Minimal</literal> pattern in the
        disk image. Any additional package or configuration change should be
        performed in this step.
       </para>
       <note>
        <title>Using backing chains</title>
        <para>
         To share the root file system between several sandboxes, create qcow2
         images with a common disk image as backing chain as described in
         <xref linkend="sec-vt-best-img-overlay"/>.
        </para>
       </note>
      </callout>
      <callout arearefs="co-vsfs-unmount">
       <para>
        Unmount the qcow2 image.
       </para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>
      Run the sandbox, using <command>virt-sandbox</command>. This command has
      many interesting options, read its man page to discover them all. The
      command can be run as &rootuser; or as an unprivileged user.
     </para>
<screen>&prompt.root;virt-sandbox -n <replaceable>NAME</replaceable> \
     -m host-image:/=$PWD/rootfs.qcow2 \ <co xml:id="co-vs-rootfs"/>
     -m host-bind:/srv/www=/guests/www \ <co xml:id="co-vs-bind"/>
     -m ram:/tmp=100MiB \
     -m ram:/run=100MiB \ <co xml:id="co-vs-tmpfs"/>
     -N source=default,address=192.168.122.12/24 \ <co xml:id="co-vs-net"/>
     -- \
     <replaceable>/bin/sh</replaceable></screen>
     <calloutlist>
      <callout arearefs="co-vs-rootfs">
       <para>
        Mount the created disk image as the root file system. Note that without
        any image being mounted as <filename>/</filename>, the host root file
        system is read-only mounted as the guest one.
       </para>
       <para>
        The host-image mount is not reserved for the root file system, it can
        be used to mount any disk image anywhere in the guest.
       </para>
      </callout>
      <callout arearefs="co-vs-bind">
       <para>
        The host-bind mount is pretty convenient for sharing files and
        directories between the host and the guest. In this example the host
        directory <filename>/guests/www</filename> is mounted as
        <filename>/srv/www</filename> in the sandbox.
       </para>
      </callout>
      <callout arearefs="co-vs-tmpfs">
       <para>
        The RAM mounts are defining <literal>tmpfs</literal> mounts in the
        sandbox.
       </para>
      </callout>
      <callout arearefs="co-vs-net">
       <para>
        The network uses a network defined in libvirt. When running as an
        unprivileged user, the source can be omitted, and the &kvm; user
        networking feature will be used. Using this option requires the
        <package>dhcp-client</package> and <package>iproute2</package>
        packages, which are part of the &sls; <literal>Minimal</literal>
        pattern.
       </para>
      </callout>
     </calloutlist>
    </step>
   </procedure>
  </sect2>
 </sect1>
<!--
SLE11: /etc/X11/xorg.conf driver to cirrus
 before mkinitrd: by changing /etc/fstab later, or by creating a symlink /dev/hda2 -> /dev/xvda2...
 -->
 <sect1 xml:id="sec-vt-best-xen-pv-fv">
  <title>&xen;: converting a paravirtual (PV) guest to a fully virtual (FV/HVM) guest</title>

  <para>
   This chapter explains how to convert a &xen; paravirtual machine into a
   &xen; fully virtualized machine.
  </para>

  <procedure>
   <title>Guest side</title>
   <para>
    In order to start the guest in FV mode, you have to run the following steps
    inside the guest.
   </para>
   <step>
    <para>
     Prior to converting the guest, apply all pending patches and reboot the
     guest.
    </para>
   </step>
   <step>
    <para>
     FV machines use the <literal>-default</literal> kernel. If this kernel is
     not already installed, install the <literal>kernel-default</literal>
     package (while running in PV mode).
    </para>
   </step>
   <step>
    <para>
     PV machines typically use disk names such as <literal>vda*</literal>.
     These names must be changed to the FV <literal>hd*</literal> syntax. This
     change must be done in the following files:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <filename>/etc/fstab</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/boot/grub/menu.lst</filename> (&slsa; 11 only)
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/boot/grub*/device.map</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/etc/sysconfig/bootloader</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/etc/default/grub</filename> (&slsa; 12 and 15 only)
      </para>
     </listitem>
    </itemizedlist>
    <note>
     <title>Prefer UUIDs</title>
     <para>
      You should use UUIDs or logical volumes within your
      <filename>/etc/fstab</filename>. Using UUID simplifies using attached
      network storage, multipathing, and virtualization. To find the UUID of
      your disk use the command <command>blkid</command>.
     </para>
    </note>
   </step>
   <step>
    <para>
     To avoid any error regenerating the initrd with the required modules you
     can create a symlink from <filename>/dev/hda2</filename> to
     <filename>/dev/xvda2</filename> etc. by using the <command>ln</command>:
    </para>
<screen>ln -sf /dev/xvda2 /dev/hda2
ln -sf /dev/xvda1 /dev/hda1
.....</screen>
   </step>
   <step>
    <para>
     PV and FV machines use different disk and network driver modules. These FV
     modules must be added to the initrd manually. The expected modules are
     <literal>xen-vbd</literal> (for disk) and <literal>xen-vnif</literal> (for
     network). These are the only PV drivers for a fully virtualized &vmguest;.
     All other modules, such as <literal>ata_piix</literal>,
     <literal>ata_generic</literal> and <literal>libata</literal>, should be
     added automatically.
    </para>
    <tip>
     <title>Adding modules to the initrd</title>
     <itemizedlist>
      <listitem>
       <para>
        On &slsa; 11, you can add modules to the
        <literal>INITRD_MODULES</literal> line in the
        <filename>/etc/sysconfig/kernel</filename> file. For example:
       </para>
<screen>INITRD_MODULES="xen-vbd xen-vnif"</screen>
       <para>
        Run <command>mkinitrd</command> to build a new initrd containing the
        modules.
       </para>
      </listitem>
      <listitem>
       <para>
        On &slsa; 12 and 15, open or create
        <filename>/etc/dracut.conf.d/10-virt.conf</filename> and add the
        modules with <literal>force_drivers</literal> by adding a line as in
        the example below (mind the leading whitespace):
       </para>
<screen>force_drivers+=" xen-vbd xen-vnif"</screen>
       <para>
        Run <command>dracut -f --kver
        <replaceable>KERNEL_VERSION</replaceable>-default</command> to build a
        new initrd (for the default version of the kernel) that contains the
        required modules.
       </para>
       <note>
        <title>Find your kernel version</title>
        <para>
         Use the <command>uname -r</command> command to get the current version
         used on your system.
        </para>
       </note>
      </listitem>
     </itemizedlist>
    </tip>
   </step>
   <step>
    <para>
     Before shutting down the guest, set the default boot parameter to the
     <literal>-default</literal> kernel using <command>yast
     bootloader</command>.
    </para>
   </step>
   <step>
    <para>
     Under &productname; 11, if you have an X server running on your guest, you
     need to adjust the <filename>/etc/X11/xorg.conf</filename> file in order
     to adjust the X driver. Search for <literal>fbdev</literal> and change to
     <literal>cirrus</literal>.
    </para>
<screen>Section "Device"
          Driver       "cirrus"
          ......
          EndSection</screen>
    <note>
     <title>&productname; 12/15 and Xorg</title>
     <para>
      Under &productname; 12/15, Xorg will automatically adjust the driver
      needed to be able to get a working X server.
     </para>
    </note>
   </step>
   <step>
    <para>
     Shut down the guest.
    </para>
   </step>
  </procedure>

  <procedure>
   <title>Host side</title>
   <para>
    The following steps explain the action you have to perform on the host.
   </para>
   <step>
    <para>
     To start the guest in FV mode, the configuration of the VM must be
     modified to match an FV configuration. Editing the configuration of the VM
     can easily be done using <command>virsh edit [DOMAIN]</command>. The
     following changes are recommended:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Make sure the machine, the type and the <literal>loader</literal>
       entries in the OS section are changed from <literal>xenpv</literal> to
       <literal>xenfv</literal>. The updated OS section should look similar to:
      </para>
<screen>&lt;os&gt;
          &lt;type arch='x86_64' machine='xenfv'&gt;hvm&lt;/type&gt;
          &lt;loader&gt;/usr/lib/xen/boot/hvmloader&lt;/loader&gt;
          &lt;boot dev='hd'/&gt;
&lt;/os&gt;</screen>
     </listitem>
     <listitem>
      <para>
       In the OS section remove anything that is specific to PV guest:
      </para>
      <itemizedlist>
       <listitem>
<screen>&lt;bootloader&gt;pygrub&lt;/bootloader&gt;</screen>
       </listitem>
       <listitem>
<screen>&lt;kernel&gt;/usr/lib/grub2/x86_64-xen/grub.xen&lt;/kernel&gt;</screen>
       </listitem>
       <listitem>
<screen>&lt;cmdline&gt;xen-fbfront.video=4,1024,768&lt;/cmdline&gt;</screen>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       In the devices section, add the qemu emulator as:
      </para>
<screen>&lt;emulator&gt;/usr/lib/xen/bin/qemu-system-i386&lt;/emulator&gt;</screen>
     </listitem>
     <listitem>
      <para>
       Update the disk configuration so the target device and bus use the FV
       syntax. This requires replacing the <literal>xen</literal> disk bus with
       <literal>ide</literal>, and the <literal>vda</literal> target device
       with <literal>hda</literal>. The changes should look similar to:
      </para>
<screen>&lt;target dev='hda' bus='ide'/&gt;</screen>
     </listitem>
     <listitem>
      <para>
       Change the bus for the mouse and keyboard from <literal>xen</literal> to
       <literal>ps2</literal>. Also add a new USB tablet device:
      </para>
<screen>&lt;input type='mouse' bus='ps2'/&gt;
          &lt;input type='keyboard' bus='ps2'/&gt;
&lt;input type='tablet' bus='usb'/&gt;</screen>
     </listitem>
     <listitem>
      <para>
       Change the console target type from <literal>xen</literal> to
       <literal>serial</literal>:
      </para>
<screen>&lt;console type='pty'&gt;
          &lt;target type='serial' port='0'/&gt;
&lt;/console&gt;</screen>
     </listitem>
     <listitem>
      <para>
       Change the video configuration from <literal>xen</literal> to
       <literal>cirrus</literal>, with 8&nbsp;MB of VRAM:
      </para>
<screen>&lt;video&gt;
          &lt;model type='cirrus' vram='8192' heads='1' primary='yes'/&gt;
&lt;/video&gt;</screen>
     </listitem>
     <listitem>
      <para>
       If desired, add <literal>acpi</literal> and <literal>apic</literal> to
       the features of the VM:
      </para>
<screen>&lt;features&gt;
          &lt;acpi/&gt;
          &lt;apic/&gt;
&lt;/features&gt;</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Start the guest (using <command>virsh</command> or
     <command>virt-manager</command>). If the guest is running kernel-default
     (as verified through <command>uname -a</command>), the machine is running
     in Fully Virtual mode.
    </para>
   </step>
  </procedure>

  <note>
   <title>guestfs-tools</title>
   <para>
    To script this process, or work on disk images directly, you can use the
    guestfs-tools suite (see <xref linkend="sec-guestfs-tools"/> for more
    information). Numerous tools exist there to help modify disk images.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec-vt-best-refs">
  <title>External references</title>

  <para></para>

  <itemizedlist>
   <listitem>
    <para>
     <link xlink:href="https://kernel.org/doc/ols/2009/ols2009-pages-19-28.pdf">Increasing
     memory density using KSM</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://www.linux-kvm.org/page/KSM">linux-kvm.org
     KSM</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://www.kernel.org/doc/Documentation/vm/ksm.txt">KSM's
     kernel documentation</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/329123/">ksm - dynamic page
     sharing driver for linux v4</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://www.espenbraastad.no/post/memory-ballooning/">Memory
     Ballooning</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://wiki.libvirt.org/page/Virtio">libvirt
     virtio</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://www.kernel.org/doc/html/latest/block/bfq-iosched.html">BFQ (Budget Fair Queueing)</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="https://www.kernel.org/doc/Documentation/sysctl/kernel.txt">Documentation
     for sysctl</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/525459/">LWN Random
     Number</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://wiki.mikejung.biz/KVM_/_Xen">&kvm; / &xen;
     tweaks</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="http://events.linuxfoundation.org/sites/events/files/slides/CloudOpen2013_Khoa_Huynh_v3.pdf">Dr.
     Khoa Huynh, IBM Linux Technology Center</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
      xlink:href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/plain/Documentation/admin-guide/kernel-parameters.txt">Kernel
     Parameters</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://lwn.net/Articles/374424/">Huge pages
     Administration (Mel Gorman)</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt">kernel
     hugetlbpage</link>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</article>
