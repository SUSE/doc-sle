<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.raid" xml:lang="en">
 <title>Software RAID Configuration</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  The purpose of RAID (redundant array of independent disks) is to combine
  several hard disk partitions into one large virtual hard disk to optimize
  performance, data security, or both. Most RAID controllers use the SCSI
  protocol because it can address a larger number of hard disks in a more
  effective way than the IDE protocol and is more suitable for parallel
  processing of commands. There are some RAID controllers that support IDE or
  SATA hard disks. Software RAID provides the advantages of RAID systems
  without the additional cost of hardware RAID controllers. However, this
  requires some CPU time and has memory requirements that make it unsuitable
  for real high performance computers.
 </para>
 <important>
  <title>RAID on Cluster File Systems</title>
  <para>
   Software RAID underneath clustered file systems needs to be set up using a
   cluster multi-device (Cluster MD). Refer to the &hasi; documentation at
   <link
     xlink:href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/cha_ha_cluster-md.html"/>.
  </para>
 </important>
 <para>
  &sle; offers the option of combining several hard disks into one soft RAID
  system. RAID implies several strategies for combining several hard disks in a
  RAID system, each with different goals, advantages, and characteristics.
  These variations are commonly known as <emphasis>RAID levels</emphasis>.
 </para>
 <sect1 xml:id="sec.raid.intro">
  <title>Understanding RAID Levels</title>

  <para>
   This section describes common RAID levels 0, 1, 2, 3, 4, 5, and nested RAID
   levels.
  </para>

  <sect2 xml:id="sec.raid.intro.raid0">
   <title>RAID&nbsp;0</title>
   <para>
    This level improves the performance of your data access by spreading out
    blocks of each file across multiple disks. Actually, this is not really a
    RAID, because it does not provide data backup, but the name
    <emphasis>RAID&nbsp;0</emphasis> for this type of system has become the
    norm. With RAID&nbsp;0, two or more hard disks are pooled together. The
    performance is very good, but the RAID system is destroyed and your data
    lost if even one hard disk fails.
   </para>
  </sect2>

  <sect2 xml:id="sec.raid.intro.raid1">
   <title>RAID&nbsp;1</title>
   <para>
    This level provides adequate security for your data, because the data is
    copied to another hard disk 1:1. This is known as <emphasis>hard disk
    mirroring</emphasis>. If a disk is destroyed, a copy of its contents is
    available on another mirrored disk. All disks except one could be damaged
    without endangering your data. However, if damage is not detected, damaged
    data might be mirrored to the correct disk and the data is corrupted that
    way. The writing performance suffers a little in the copying process
    compared to when using single disk access (10 to 20 percent slower), but
    read access is significantly faster in comparison to any one of the normal
    physical hard disks, because the data is duplicated so can be scanned in
    parallel. RAID 1 generally provides nearly twice the read transaction rate
    of single disks and almost the same write transaction rate as single disks.
   </para>
  </sect2>

  <sect2 xml:id="sec.raid.intro.raid23">
   <title>RAID&nbsp;2 and RAID&nbsp;3</title>
   <para>
    These are not typical RAID implementations. Level&nbsp;2 stripes data at
    the bit level rather than the block level. Level&nbsp;3 provides byte-level
    striping with a dedicated parity disk and cannot service simultaneous
    multiple requests. Both levels are rarely used.
   </para>
  </sect2>

  <sect2 xml:id="sec.raid.intro.raid4">
   <title>RAID&nbsp;4</title>
   <para>
    Level&nbsp;4 provides block-level striping like Level&nbsp;0 combined with
    a dedicated parity disk. If a data disk fails, the parity data is used to
    create a replacement disk. However, the parity disk might create a
    bottleneck for write access. Nevertheless, Level&nbsp;4 is sometimes used.
   </para>
  </sect2>

  <sect2 xml:id="sec.raid.intro.raid5">
   <title>RAID&nbsp;5</title>
   <para>
    RAID&nbsp;5 is an optimized compromise between Level&nbsp;0 and
    Level&nbsp;1 in terms of performance and redundancy. The hard disk space
    equals the number of disks used minus one. The data is distributed over the
    hard disks as with RAID&nbsp;0. <emphasis>Parity blocks</emphasis>, created
    on one of the partitions, are there for security reasons. They are linked
    to each other with XOR, enabling the contents to be reconstructed by the
    corresponding parity block in case of system failure. With RAID&nbsp;5, no
    more than one hard disk can fail at the same time. If one hard disk fails,
    it must be replaced when possible to avoid the risk of losing data.
   </para>
  </sect2>

  <sect2 xml:id="sec.raid.intro.raid6">
   <title>RAID&nbsp;6</title>
   <para>
    RAID&nbsp;6 is essentially an extension of RAID&nbsp;5 that allows for
    additional fault tolerance by using a second independent distributed parity
    scheme (dual parity). Even if two of the hard disks fail during the data
    recovery process, the system continues to be operational, with no data
    loss.
   </para>
   <para>
    RAID&nbsp;6 provides for extremely high data fault tolerance by sustaining
    multiple simultaneous drive failures. It handles the loss of any two
    devices without data loss. Accordingly, it requires N+2 drives to store N
    drives worth of data. It requires a minimum of four devices.
   </para>
   <para>
    The performance for RAID&nbsp;6 is slightly lower but comparable to
    RAID&nbsp;5 in normal mode and single disk failure mode. It is very slow in
    dual disk failure mode. A RAID&nbsp;6 configuration needs a considerable
    amount of CPU time and memory for write operations.
   </para>
   <table>
    <title>Comparison of RAID 5 and RAID 6</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="3334*"/>
     <colspec colnum="2" colname="2" colwidth="3334*"/>
     <colspec colnum="3" colname="3" colwidth="3334*"/>
     <thead>
      <row>
       <entry>
        <para>
         Feature
        </para>
       </entry>
       <entry>
        <para>
         RAID 5
        </para>
       </entry>
       <entry>
        <para>
         RAID 6
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         Number of devices
        </para>
       </entry>
       <entry>
        <para>
         N+1, minimum of 3
        </para>
       </entry>
       <entry>
        <para>
         N+2, minimum of 4
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Parity
        </para>
       </entry>
       <entry>
        <para>
         Distributed, single
        </para>
       </entry>
       <entry>
        <para>
         Distributed, dual
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Performance
        </para>
       </entry>
       <entry>
        <para>
         Medium impact on write and rebuild
        </para>
       </entry>
       <entry>
        <para>
         More impact on sequential write than RAID 5
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Fault-tolerance
        </para>
       </entry>
       <entry>
        <para>
         Failure of one component device
        </para>
       </entry>
       <entry>
        <para>
         Failure of two component devices
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </sect2>

  <sect2 xml:id="sec.raid.intro.raid_nested">
   <title>Nested and Complex RAID Levels</title>
   <para>
    Several other RAID levels have been developed, such as RAIDn, RAID&nbsp;10,
    RAID&nbsp;0+1, RAID&nbsp;30, and RAID&nbsp;50. Some are proprietary
    implementations created by hardware vendors. Examples for creating
    RAID&nbsp;10 configurations can be found in <xref linkend="cha.raid10"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.raid.yast">
  <title>Soft RAID Configuration with &yast;</title>

  <para>
   The &yast; soft RAID configuration can be reached from the &yast; Expert
   Partitioner. This partitioning tool also enables you to edit and delete
   existing partitions and create new ones that should be used with soft RAID.
   These instructions apply on setting up RAID levels 0, 1, 5, and 6. Setting
   up RAID&nbsp;10 configurations is explained in <xref linkend="cha.raid10"/>.
  </para>

  <procedure>
   <step>
    <para>
     Launch &yast; and open the <guimenu>Partitioner</guimenu>.
    </para>
   </step>
   <step>
    <para>
     If necessary, create partitions that should be used with your RAID
     configuration. Do not format them and set the partition type to
     <guimenu>0xFD Linux RAID</guimenu>. When using existing partitions it is
     not necessary to change their partition type&mdash;&yast; will
     automatically do so. Refer to <xref linkend="sec.yast2.i_y2_part_expert"/>
     for details.
    </para>
    <para>
     It is strongly recommended to use partitions stored on different hard
     disks to decrease the risk of losing data if one is defective (RAID&nbsp;1
     and 5) and to optimize the performance of RAID&nbsp;0.
    </para>
    <para>
     For RAID&nbsp;0 at least two partitions are needed. RAID 1 requires
     exactly two partitions, while at least three partitions are required for
     RAID&nbsp;5. A RAID&nbsp;6 setup requires at least four partitions. It is
     recommended to use only partitions of the same size because each segment
     can contribute only the same amount of space as the smallest sized
     partition.
    </para>
   </step>
   <step>
    <para>
     In the left panel, select <guimenu>RAID</guimenu>.
    </para>
    <para>
     A list of existing RAID configurations opens in the right panel.
    </para>
   </step>
   <step>
    <para>
     At the lower left of the RAID page, click <guimenu>Add RAID</guimenu>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_raid2_a.png" width="80%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_raid2_a.png" width="100%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Select a <guimenu>RAID Type</guimenu> and <guimenu>Add</guimenu> an
     appropriate number of partitions from the <guimenu>Available
     Devices</guimenu> dialog.
    </para>
    <para>
     You can optionally assign a <guimenu>RAID Name</guimenu> to your RAID. It
     will make it available as
     <filename>/dev/md/<replaceable>name</replaceable></filename>. See
     <xref linkend="sec.raid.yast.names"/> for more information.
    </para>
    <figure xml:id="fig.yast2.raid3">
     <title>Example RAID 5 Configuration</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_raid3_a.png" width="80%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_raid3_a.png" width="100%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
     Proceed with <guimenu>Next</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Select the <guimenu>Chunk Size</guimenu> and, if applicable, the
     <guimenu>Parity Algorithm</guimenu>. The optimal chunk size depends on the
     type of data and the type of RAID. See
     <link xlink:href="https://raid.wiki.kernel.org/index.php/RAID_setup#Chunk_sizes"/>
     for more information. More information on parity algorithms can be found
     with <command>man 8 mdadm</command> when searching for the
     <option>--layout</option> option. If unsure, stick with the defaults.
    </para>
   </step>
   <step>
    <para>
     Choose a <guimenu>Role</guimenu> for the volume. Your choice here only
     affects the default values for the upcoming dialog. They can be changed in
     the next step. If in doubt, choose <guimenu>Raw Volume
     (Unformatted)</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Under <guimenu>Formatting Options</guimenu>, select <guimenu>Format
     Partition</guimenu>, then select the <guimenu>File system</guimenu>. The
     content of the <guimenu>Options</guimenu> menu depends on the file system.
     Usually there is no need to change the defaults.
    </para>
    <para>
     Under <guimenu>Mounting Options</guimenu>, select <guimenu>Mount
     partition</guimenu>, then select the mount point. Click <guimenu>Fstab
     Options</guimenu> to add special mounting options for the volume.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Finish</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Next</guimenu>, verify that the changes are listed, then
     click <guimenu>Finish</guimenu>.
    </para>
   </step>
  </procedure>

  <sect2 xml:id="sec.raid.yast.names">
   <title>RAID Names</title>
   <para>
    By default, software RAID devices have numeric names following the pattern
    <literal>mdN</literal>, where <literal>N</literal> is a number. As such
    they can be accessed as, for example, <filename>/dev/md127</filename> and
    are listed as <literal>md127</literal> in <filename>/proc/mdstat</filename>
    and <filename>/proc/partitions</filename>. Working with these names can be
    clumsy. &productname; offers two ways to work around this problem:
   </para>
   <variablelist>
    <varlistentry>
     <term>Providing a Named Link to the Device</term>
     <listitem>
      <para>
       You can optionally specify a name for the RAID device when creating it
       with &yast; or on the command line with <command>mdadm --create
       '/dev/md/</command> <replaceable>name</replaceable>'. The device name
       will still be <literal>mdN</literal>, but a link
       <filename>/dev/md/<replaceable>name</replaceable></filename> will be
       created:
      </para>
<screen>&prompt.user;ls -og /dev/md
total 0
lrwxrwxrwx 1 8 Dec  9 15:11 myRAID -&gt; ../md127</screen>
      <para>
       The device will still be listed as <literal>md127</literal> under
       <filename>/proc</filename>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Providing a Named Device</term>
     <listitem>
      <para>
       In case a named link to the device is not sufficient for your setup, add
       the line CREATE names=yes to <filename>/etc/mdadm.conf</filename> by
       running the following command:
      </para>
<screen>sudo echo "CREATE names=yes" &gt;&gt; /etc/mdadm.conf</screen>
      <para>
       It will cause names like <literal>myRAID</literal> to be used as a
       <quote>real</quote> device name. The device will not only be accessible
       at <filename>/dev/myRAID</filename>, but also be listed as
       <literal>myRAID</literal> under <filename>/proc</filename>. Note that
       this will only apply to RAIDs configured after the change to the
       configuration file. Active RAIDS will continue to use the
       <literal>mdN</literal> names until they get stopped and re-assembled.
      </para>
      <warning>
       <title>Incompatible Tools</title>
       <para>
        Not all tools may support named RAID devices. In case a tool expects a
        RAID device to be named <literal>mdN</literal>, it will fail to
        identify the devices.
       </para>
      </warning>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.raid.trouble">
  <title>Troubleshooting Software RAIDs</title>

  <para>
   Check the <filename>/proc/mdstat</filename> file to find out whether a RAID
   partition has been damaged. If a disk fails, shut down your Linux system and
   replace the defective hard disk with a new one partitioned the same way.
   Then restart your system and enter the command <command>mdadm /dev/mdX --add
   /dev/sdX</command>. Replace <literal>X</literal> with your particular device
   identifiers. This integrates the hard disk automatically into the RAID
   system and fully reconstructs it (for all RAID levels except for
   RAID&nbsp;0).
  </para>

  <para>
   Although you can access all data during the rebuild, you might encounter
   some performance issues until the RAID has been fully rebuilt.
  </para>

  <sect2 xml:id="sec.raid.trouble.autorecovery">
   <title>Recovery after Failing Disk is Back Again</title>
   <para>
    There are several reasons a disk included in a RAID array may fail. Here is
    a list of the most common ones:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Problems with the disk media.
     </para>
    </listitem>
    <listitem>
     <para>
      Disk drive controller failure.
     </para>
    </listitem>
    <listitem>
     <para>
      Broken connection to the disk.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    In the case of the disk media or controller failure, the device needs to be
    replaced or repaired. If a hot-spare was not configured within the RAID,
    then manual intervention is required.
   </para>
   <para>
    In the last case, the failed device can be automatically re-added by the
    <command>mdadm</command> command after the connection is repaired (which
    might be automatic).
   </para>
   <para>
    Because <command>md</command>/<command>mdadm</command> cannot reliably
    determine what caused the disk failure, it assumes a serious disk error and
    treats any failed device as faulty until it is explicitly told that the
    device is reliable.
   </para>
   <para>
    Under some circumstances&mdash;such as storage devices with the internal
    RAID array&mdash; the connection problems are very often the cause of the
    device failure. In such case, you can tell <command>mdadm</command> that it
    is safe to automatically <option>--re-add</option> the device after it
    appears. You can do this by adding the following line to
    <filename>/etc/mdadm.conf</filename>:
   </para>
<screen>POLICY action=re-add</screen>
   <para>
    Note that the device will be automatically re-added after re-appearing only
    if the <systemitem>udev</systemitem> rules cause <command>mdadm -I
    <replaceable>disk_device_name</replaceable></command> to be run on any
    device that spontaneously appears (default behavior), and if write-intent
    bitmaps are configured (they are by default).
   </para>
   <para>
    If you want this policy to only apply to some devices and not to the
    others, then the <literal>path=</literal> option can be added to the
    <literal>POLICY</literal> line in <filename>/etc/mdadm.conf</filename> to
    restrict the non-default action to only selected devices. Wild cards can be
    used to identify groups of devices. See <command>man 5 mdadm.conf</command>
    for more information.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.raid.more">
  <title>For More Information</title>

  <para>
   Configuration instructions and more details for soft RAID can be found in
   the HOWTOs at:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <citetitle>The Linux RAID wiki</citetitle>:
     <link xlink:href="https://raid.wiki.kernel.org/index.php/Linux_Raid"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <citetitle>The Software RAID HOWTO</citetitle> in the
     <filename>/usr/share/doc/packages/mdadm/Software-RAID.HOWTO.html</filename>
     file
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Linux RAID mailing lists are also available, such as
   <citetitle>linux-raid</citetitle> at
   <link xlink:href="http://marc.info/?l=linux-raid"/>.
  </para>
 </sect1>
</chapter>
