<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="tuning_cgroups.xml" version="5.0" xml:id="cha-tuning-cgroups">
 <title>内核控制组</title>
 <info>
      <abstract>
        <para>
    内核控制组 (<quote>cgroup</quote>) 是一种内核功能，可用于指派和限制进程的硬件与系统资源。还能以层次树状结构组织进程。
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
   </dm:bugtracker>
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec-tuning-cgroups-overview">
  <title>概述</title>
  <para>
   为每个进程刚好指派一个管理 cgroup。cgroup 在层次树状结构中按序列出。您可针对单个进程或针对层次树状结构中的整条分支设置资源限制，例如 CPU、内存、磁盘 I/O 或网络带宽用量。
  </para>
  <para>
   在 <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise Server</phrase></phrase> 上，<systemitem class="daemon">systemd</systemitem> 使用 cgroup 以分组（<systemitem class="daemon">systemd</systemitem> 称之为切片）形式组织所有进程。<systemitem class="daemon">systemd</systemitem> 还提供一个界面用于设置 cgroup 属性。
  </para>
  <para>
   命令 <command>systemd-cgls</command> 显示层次树状结构。
  </para>
  <para>
   本章仅提供概述。有关更多细节，请参见所列出的参考资料。
  </para>
 </sect1>

 <sect1 xml:id="sec-tuning-cgroups-accounting">
  <title>资源统计</title>
  <para>
  可以通过将各进程放置在不同的 cgroup 中来获取每个 cgroup 的特定资源消耗信息。
  </para>
  <para>
  统计进程相对较小，但开销并非为零，它所造成的影响取决于工作负载。请注意，对一个单元启用统计也会对直接包含在同一切片中的所有单元、该切片的所有父切片，以及直接包含在这些父切片中的单元启用统计。因此，统计成本并不独属于一个单元。
  </para>
  <para>
  可以使用诸如 <literal>MemoryAccounting=</literal> 的指令为每个单元设置统计，或者在 <filename>/etc/systemd/system.conf</filename> 中使用 <literal>DefaultMemoryAccounting=</literal> 指令为所有单元全局设置统计。有关可用指令的详尽列表，请参见 <literal>systemd.resource-control (5)</literal>。
  </para>
 </sect1>

 <sect1 xml:id="sec-tuning-cgroups-usage">
  <title>设置资源限制</title>
  <note>
    <title>隐式资源消耗</title>
    <para>
      请注意，资源消耗隐式取决于工作负载的执行环境（例如，库/内核中数据结构的大小、实用程序的派生行为、计算效率）。因此，如果环境发生变化，建议您（重新）校准您的限制。
    </para>
  </note>
  <para>
   可以使用 <command>systemctl set-property</command> 命令设置 <literal>cgroup</literal> 的限制。语法是：
  </para>
  <screen><prompt role="root">root # </prompt><command>systemctl set-property [--runtime] <replaceable>NAME</replaceable> <replaceable>PROPERTY1</replaceable>=<replaceable>VALUE</replaceable> [<replaceable>PROPERTY2</replaceable>=<replaceable>VALUE</replaceable>]</command></screen>
  <para>
   （可选）使用 <option>--runtime</option> 选项。如果使用此选项，下一次重引导后已设置的限制不会保留。
  </para>
  <para>
   请将 <replaceable>NAME</replaceable> 替换为 <systemitem class="daemon">systemd</systemitem> 服务片、范围、套接字、挂载或交换名称。请将属性替换为以下一项或多项：
  </para>
  <variablelist>
   <varlistentry>
    <term><literal>CPUQuota=</literal><replaceable>PERCENTAGE</replaceable></term>
    <listitem>
     <para>
      将 CPU 时间指派给进程。其值是后跟 <literal>%</literal> 作为后缀的百分比。此属性暗含 <literal>CPUAccounting=yes</literal>。
     </para>
     <para>
      示例：
     </para>
     <screen><prompt role="root">root # </prompt><command>systemctl set-property user.slice CPUQuota=50%</command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>MemoryLow=</literal><replaceable>BYTES</replaceable></term>
    <listitem>
     <para>
      进程中低于此限制的未用内存不会回收以作其他用途。为 <replaceable>BYTES</replaceable> 使用后缀 K、M、G 或 T。此属性暗含 <literal>MemoryAccounting=yes</literal>。
     </para>
     <para>
      示例：
     </para>
     <screen><prompt role="root">root # </prompt><command>systemctl set-property nginx.service MemoryLow=512M</command></screen>
     <note>
      <title>统一控制组层次结构</title>
      <para>
       仅当使用了统一控制组层次结构时此设置才可用，并会禁用 <option>MemoryLimit=</option>。要启用统一控制组层次结构，请将 <option>systemd.unified_cgroup_hierarchy=1</option> 作为内核命令行参数追加到 GRUB 2 引导加载程序。有关配置 GRUB 2 的更多细节，请参见<xref linkend="cha-grub2"/>。
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>MemoryHigh=</literal><replaceable>BYTES</replaceable></term>
    <listitem>
     <para>
      如果使用超出此限制的更多内存，将会主动抢占进程的内存。为 <replaceable>BYTES</replaceable> 使用后缀 K、M、G 或 T。此属性暗含 <literal>MemoryAccounting=yes</literal>。例如：
     </para>
     <screen><prompt role="root">root # </prompt><command>systemctl set-property nginx.service MemoryHigh=2G</command></screen>
     <note>
      <title>统一控制组层次结构</title>
      <para>
       仅当使用了统一控制组层次结构时此设置才可用，并会禁用 <option>MemoryLimit=</option>。要启用统一控制组层次结构，请将 <option>systemd.unified_cgroup_hierarchy=1</option> 作为内核命令行参数追加到 GRUB 2 引导加载程序。有关配置 GRUB 2 的更多细节，请参见<xref linkend="cha-grub2"/>。
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>MemoryMax=</literal><replaceable>BYTES</replaceable></term>
    <listitem>
     <para>
      设置已用内存的最大限制。如果使用的内存超出允许值，将会终止进程。为 <replaceable>BYTES</replaceable> 使用后缀 K、M、G 或 T。此属性暗含 <literal>MemoryAccounting=yes</literal>。
     </para>
     <para>
      示例：
     </para>
     <screen><prompt role="root">root # </prompt><command>systemctl set-property nginx.service MemoryMax=4G</command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>DeviceAllow=</literal></term>
    <listitem>
     <para>
      允许 read (<literal>r</literal>)、write (<literal>w</literal>) 和 mknod (<literal>m</literal>) 访问权限。命令接受设备节点说明符，以及以空格分隔的 <literal>r</literal>、<literal>w</literal> 或 <literal>m</literal> 列表。
     </para>
     <para>
      示例：
     </para>
     <screen><prompt role="root">root # </prompt><command>systemctl set-property system.slice DeviceAllow="/dev/sdb1 r"</command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>DevicePolicy=</literal><option>[auto|closed|strict]</option></term>
    <listitem>
     <para>
      设置为 <literal>strict</literal> 时，仅允许访问 <literal>DeviceAllow</literal> 中所列的设备。<literal>closed</literal> 额外允许访问标准伪设备，包括 <filename>/dev/null</filename>、<filename>/dev/zero</filename>、<filename>/dev/full</filename>、<filename>/dev/random</filename> 和 <filename>/dev/urandom</filename>。如果 <literal>DeviceAllow</literal> 中未定义特定的规则，<literal>auto</literal> 允许访问所有设备。<literal>auto</literal> 是默认设置。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   有关更多细节和完整属性列表，请参见 <command>man systemd.resource-control</command>。
  </para>
 </sect1>
 
 <sect1 xml:id="sec-tuning-cgroups-tasksmax">
  <title>使用 <literal>TasksMax</literal> 防止派生炸弹</title>
   <para>
    <systemitem class="daemon">systemd</systemitem> 228 附带值为 512 的 <literal>DefaultTasksMax</literal> 限制。即任何系统单元可一次性创建的进程数上限为 512 个。旧版没有默认限制。此限制的目的是通过防止失控的进程创建过多的派生项，或者滋生大量的线程而导致系统资源耗尽，以此提高安全性。
   </para> 
   <para>
    但您很快会发现，单一默认值并不能适用于所有用例。尤其是当 CPU 和 RAM 等其他资源不受限制时，512 限制值还是不够小，无法防止失控的进程导致系统崩溃；而对于创建大量线程的进程（例如数据库）而言，该值又不够大。在 <systemitem class="daemon">systemd</systemitem> 234 中，默认值已更改为 15%，即限制为 4915 个任务（值为 32768 的内核限制的 15%；请参见 <command>cat /proc/sys/kernel/pid_max</command>）。此默认值已经过编译，可在配置文件中更改。编译的默认值记录在 <filename>/etc/systemd/system.conf</filename> 中。您可以编辑此文件以覆盖默认值，我们还将在下面的章节中介绍其他方法。
   </para>

   <sect2 xml:id="sec-tasksmax-defaults">
    <title>查找当前的默认 <literal>TasksMax</literal> 值</title>
    <para>
     <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise Server</phrase></phrase> 随附了两个自定义配置用于覆盖系统单元和用户片的上游默认值，并将两者都设置为 <literal>infinity</literal>。<filename>/usr/lib/systemd/system.conf.d/20-suse-defaults.conf</filename> 包含下面几行：
    </para>
<screen>
[Manager]
DefaultTasksMax=infinity
</screen>
    <para>
     <filename>/usr/lib/systemd/system/user-.slice.d/20-suse-defaults.conf</filename> 包含下面几行：
    </para>
<screen>[Slice]
TasksMax=infinity
</screen>
    <para>
     <literal>infinity</literal> 表示没有限制。并不要求一定要更改默认值，但设置一些限制可能有助于防止失控进程导致系统崩溃。
    </para>
  </sect2>

  <sect2 xml:id="sec-edit-taskmax-default">
   <title>覆盖 <literal>DefaultTasksMax</literal> 值</title>
   <para>
    通过创建新的覆盖文件 <filename>/etc/systemd/system.conf.d/90-system-tasksmax.conf</filename> 更改全局 <literal>DefaultTasksMax</literal> 值，并写入下面几行以便为每个系统单元设置新的默认任务数限制（256 个）：
  </para>
<screen>
[Manager]
DefaultTasksMax=256
</screen>
  <para>
   装载新设置，然后校验设置是否已更改：
  </para>
<screen><prompt>tux &gt; </prompt><command>sudo</command> systemctl daemon-reload
<prompt>tux &gt; </prompt>systemctl show --property DefaultTasksMax
DefaultTasksMax=256
</screen>
  <para>
   根据您的需要调整此默认值。可根据需要在单个服务上设置更高的限制。本示例适用于 MariaDB。首先检查当前活动值：
  </para>
<screen>
<prompt>tux &gt; </prompt>systemctl status mariadb.service
  ● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset&gt;
   Active: active (running) since Tue 2020-05-26 14:15:03 PDT; 27min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
 Main PID: 11845 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 256)
   CGroup: /system.slice/mariadb.service
           └─11845 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
</screen>
  <para>
   Tasks 行显示 MariaDB 中当前有 30 个任务正在运行，而默认上限为 256，这对于数据库而言并不足够。以下示例演示如何将 MariaDB 的限制提高至 8192。使用 <command>systemctl edit</command> 创建新的覆盖文件，然后输入新值：
   </para>
<screen><prompt>tux &gt; </prompt><command>sudo</command> systemctl edit mariadb.service

[Service]
TasksMax=8192

<prompt>tux &gt; </prompt>systemctl status mariadb.service 
● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disab&gt;
  Drop-In: /etc/systemd/system/mariadb.service.d
           └─override.conf
   Active: active (running) since Tue 2020-06-02 17:57:48 PDT; 7min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
  Process: 3446 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper upgrade (code=exited, sta&gt;
  Process: 3440 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper install (code=exited, sta&gt;
 Main PID: 3452 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 8192)
   CGroup: /system.slice/mariadb.service
           └─3452 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
</screen>
   <para>
    <command>systemctl edit</command> 创建覆盖文件 <filename>/etc/systemd/system/mariadb.service.d/override.conf</filename>，其中仅包含您要应用到现有单元文件的更改。值不一定必须是 8192，但应该是适合您工作负载的限制。
   </para>
  </sect2>

  <sect2>
   <title>针对用户的默认 <literal>TasksMax</literal> 限制</title>
   <para>
    针对用户的默认限制应相当高，因为用户会话需要更多的资源。通过创建新文件（例如 <filename>/etc/systemd/system/user-.slice.d/user-taskmask.conf</filename>），针对用户设置您自己的默认值。以下示例设置的默认值为 16284：
   </para>
<screen>
[Slice]
TasksMax=16284
</screen>
   <para>
    然后重新装载 systemd 以装载新值，并校验更改：
   </para>
<screen><prompt>tux &gt; </prompt><command>sudo</command> systemctl daemon-reload

<prompt>tux &gt; </prompt>systemctl show --property TasksMax user-.slice
TasksMax=16284

<prompt>tux &gt; </prompt>systemctl show --property TasksMax user-1000.slice
TasksMax=16284
</screen>
   <para>
    如何知道要使用哪些值？根据您的工作负载、系统资源和其他资源配置而定。如果 <literal>TasksMax</literal> 值太小，您会看到诸如<emphasis>无法派生（资源暂时不可用）</emphasis>、<emphasis>无法创建用于处理新连接的线程</emphasis>，以及<emphasis>错误：函数调用“fork”失败，错误代码 11“资源暂时不可用”</emphasis>等错误消息。
   </para>
   <para>
    有关在 systemd 中配置系统资源的详细信息，请参见 <literal>systemd.resource-control (5)</literal>。
   </para>
  </sect2>
 </sect1>

 <sect1>
  <title>使用比例权重策略控制 I/O</title>
  <para>
      本节介绍如何使用 Linux 内核的块 I/O 控制器来指定 I/O 操作的优先级。cgroup blkio 子系统可以控制和监视对块设备上 I/O 的访问。包含 cgroup 子系统参数的状态对象以 cgroup 虚拟文件系统（也称为伪文件系统）内部的伪文件表示。
  </para>
  <para>      
      本节中的示例说明，将值写入其中一些伪文件如何能够限制访问或带宽，以及从其中一些伪文件读取值如何能够提供有关 I/O 操作的信息。示例是针对 cgroup-v1 和 cgroup-v2 提供的。
  </para>
  <para>
      您需要一个测试目录，包含两个文件分别用于测试性能和更改后的设置。使用 <command>yes</command> 命令可以快速创建所填充内容均为文本的测试文件。以下示例命令创建了一个测试目录，然后在其中填充了两个 537 MB 大小的文本文件：
      </para> 
  <screen><prompt role="root">host1:~ # </prompt>mkdir /io-cgroup
<prompt role="root">host1:~ # </prompt>cd /io-cgroup
<prompt role="root">host1:~ # </prompt>yes this is a test file | head -c 537MB &gt; file1.txt
<prompt role="root">host1:~ # </prompt>yes this is a test file | head -c 537MB &gt; file2.txt
</screen>
  <para>
    要运行示例，请打开三个命令外壳。两个外壳用于读取器进程，一个外壳用于运行控制 I/O 的步骤。在示例中已标记每个命令提示，以指示它是代表一个读取器进程还是 I/O。
  </para>

  <sect2>
   <title>使用 cgroup-v1</title>
   <para>
    使用以下比例权重策略文件可为某个读取器进程授予比访问同一磁盘的其他读取器进程更高的 I/O 操作优先级。 
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <filename>blkio.weight</filename>（仅在 4.20 版本及以下且具有旧式块层的内核中可用，使用 CFQ I/O 调度程序时可以使用）
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>blkio.bfq.weight</filename>（在 5.0 及更高版本且具有 blk-mq 的内核中可用，使用 BFQ I/O 调度程序时可以使用）
     </para>
    </listitem>
   </itemizedlist>
   <para>
    要进行测试，请使用 <filename>file2.txt</filename> 运行单个读取器进程（示例中会从某个 SSD 读取），但不控制其 I/O：
   </para>
<screen> 
<prompt role="root">[io-controller] host1:/io-cgroup # </prompt>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<prompt role="root">[io-controller] host1:/io-cgroup # </prompt>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 1.33049 s, 404 MB/s
</screen>
   <para>
    现在，运行一个从同一磁盘读取的后台进程：
   </para>
<screen>
<prompt role="root">[reader1] host1:/io-cgroup # </prompt>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<prompt role="root">[reader1] host1:/io-cgroup # </prompt>echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
<prompt role="root">[reader2] host1:/io-cgroup # </prompt>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 2.61592 s, 205 MB/s
</screen>
   <para>
    每个进程获得一半的吞吐量来执行 I/O 操作。接下来，设置两个控制组（每个进程一个），校验是否使用了 BFQ，并为 reader2 设置不同的权重：
   </para>
<screen>
<prompt role="root">[io-controller] host1:/io-cgroup # </prompt>cd /sys/fs/cgroup/blkio/
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>mkdir reader1
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>mkdir reader2
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>echo 5220 &gt; reader1/cgroup.procs
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>echo 5251 &gt; reader2/cgroup.procs
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>cat /sys/block/sda/queue/scheduler
mq-deadline kyber [bfq] none
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>cat reader1/blkio.bfq.weight
100
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>echo 200 &gt; reader2/blkio.bfq.weight
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>cat reader2/blkio.bfq.weight
200
</screen>
   <para>
    使用这些设置且 reader1 在后台运行的情况下，reader2 应获得比先前更高的吞吐量：
   </para>
<screen>
<prompt role="root">[reader1] host1:/io-cgroup # </prompt>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<prompt role="root">[reader1] host1:/io-cgroup # </prompt>echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
<prompt role="root">[reader2] host1:/io-cgroup # </prompt>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 2.06604 s, 260 MB/s
</screen>
   <para>
    比例权重的提高使得 reader2 的吞吐量增加。现在，再次将其权重提高一倍：
   </para>
<screen>
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>cat reader1/blkio.bfq.weight
100
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>echo 400 &gt; reader2/blkio.bfq.weight
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>cat reader2/blkio.bfq.weight
400
</screen>
   <para>
    这导致 reader2 的吞吐量再一次增加：
   </para>
<screen>
<prompt role="root">[reader1] host1:/io-cgroup # </prompt>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<prompt role="root">[reader1] host1:/io-cgroup # </prompt>echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
<prompt role="root">[reader2] host1:/io-cgroup # </prompt>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 1.69026 s, 318 MB/s
</screen>
  </sect2>
  <sect2>
   <title>使用 cgroup-v2</title>
   <para>
    首先，请按本章开头所述设置您的测试环境。
   </para>
   <para>
    然后，像使用 cgroup-v1 时那样，确保块 IO 控制器处于非活动状态。为此，请使用内核参数 <option>cgroup_no_v1=blkio</option> 进行引导。校验是否使用了此参数并且 IO 控制器 (cgroup-v2) 可用：
   </para>
<screen>
<prompt role="root">[io-controller] host1:/io-cgroup # </prompt>cat /proc/cmdline
BOOT_IMAGE=... cgroup_no_v1=blkio ...
<prompt role="root">[io-controller] host1:/io-cgroup # </prompt>cat /sys/fs/cgroup/unified/cgroup.controllers
io
</screen>
 <para>
   接下来启用 IO 控制器：
   </para>
<screen>
<prompt role="root">[io-controller] host1:/io-cgroup # </prompt>cd /sys/fs/cgroup/unified/
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/unified # </prompt>echo '+io' &gt; cgroup.subtree_control
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/unified # </prompt>cat cgroup.subtree_control
io
</screen>
<para>
    现在运行所有测试步骤，与 cgroup-v1 的步骤类似。请注意部分目录存在差异。使用 file2.txt 运行单个读取器进程（示例中会从某个 SSD 读取），但不控制其 I/O： 
</para>
<screen>
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/unified # </prompt>cd -
<prompt role="root">[io-controller] host1:/io-cgroup # </prompt>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<prompt role="root">[io-controller] host1:/io-cgroup # </prompt>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5633
[...]
</screen>
   <para>
    运行一个从同一磁盘读取的后台进程，并注意吞吐量值：
   </para>
<screen>
<prompt role="root">[reader1] host1:/io-cgroup # </prompt>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<prompt role="root">[reader1] host1:/io-cgroup # </prompt>echo $$; dd if=file1.txt of=/dev/null bs=4k
5633
[...]
<prompt role="root">[reader2] host1:/io-cgroup # </prompt>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5703
[...]
</screen>
<para>
    每个进程获得一半的吞吐量来执行 I/O 操作。设置两个控制组（每个进程一个），校验 BFQ 是否为活动的调度程序，并为 reader2 设置不同的权重： 
</para>
<screen>
<prompt role="root">[io-controller] host1:/io-cgroup # </prompt>cd -
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/unified # </prompt>mkdir reader1
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/unified # </prompt>mkdir reader2
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/unified # </prompt>echo 5633 &gt; reader1/cgroup.procs
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/unified # </prompt>echo 5703 &gt; reader2/cgroup.procs
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/unified # </prompt>cat /sys/block/sda/queue/scheduler
mq-deadline kyber [bfq] none
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/unified # </prompt>cat reader1/io.bfq.weight
default 100
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/unified # </prompt>echo 200 &gt; reader2/io.bfq.weight
<prompt role="root">[io-controller] host1:/sys/fs/cgroup/unified # </prompt>cat reader2/io.bfq.weight
default 200
</screen>
   <para>
    使用新设置测试吞吐量。reader2 应显示吞吐量增加。
   </para>
<screen>
<prompt role="root">[reader1] host1:/io-cgroup # </prompt>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<prompt role="root">[reader1] host1:/io-cgroup # </prompt>echo $$; dd if=file1 of=/dev/null bs=4k
5633
[...]
<prompt role="root">[reader2] host1:/io-cgroup # </prompt>echo $$; dd if=file2 of=/dev/null bs=4k count=131072
5703
[...]
</screen>
<para>
    尝试再次将 reader2 的权重提高一倍，然后测试新设置：
</para>
<screen>
<prompt role="root">[reader2] host1:/io-cgroup # </prompt>echo 400 &gt; reader1/blkio.bfq.weight
<prompt role="root">[reader2] host1:/io-cgroup # </prompt>cat reader2/blkio.bfq.weight
400
<prompt role="root">[reader1] host1:/io-cgroup # </prompt>sync; echo 3 &gt; /proc/sys/vm/drop_caches
<prompt role="root">[reader1] host1:/io-cgroup # </prompt>echo $$; dd if=file1.txt of=/dev/null bs=4k
[...]
<prompt role="root">[reader2] host1:/io-cgroup # </prompt>echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
[...]
</screen>
  </sect2>
  </sect1>
  
 <sect1>
  <title>更多信息</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     内核文档（软件包 <systemitem>kernel-source</systemitem>）：<filename>/usr/src/linux/Documentation/admin-guide/cgroup-v1</filename> 中的文件，以及文件 <filename>/usr/src/linux/Documentation/admin-guide/cgroup-v2.rst</filename>。
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/604609/"/> — Brown, Neil：Control Groups Series（控制组系列，发布于 2014 年，包含 7 个部分）。
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/243795/"/> — Corbet, Jonathan：Controlling memory use in containers（控制容器中的内存使用，发布于 2007 年）。
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/236038/"/> — Corbet, Jonathan：Process containers（进程容器，发布于 2007 年）。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
