<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
                 type="text/xml"
                 title="Profiling step"?>
<!DOCTYPE article         [
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<article xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="article.vt.best.practices" xml:lang="en">
<?suse-quickstart color="suse"?>
 <title>Virtualization Best Practices</title>
 <subtitle>&productname; &productnumber;</subtitle>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker>
   </dm:bugtracker>
  </dm:docmanager>
  <productname>&productname;</productname>
  <productnumber>&productnumber;</productnumber>
  <date><?dbtimestamp format="B d, Y"?></date>
 </info>
 <!--
     we can write it based on scenario?
     Virtualization Capabilities:
     Consolidation (hardware1+hardware2 -> hardware)
     Isolation
     Migration (hardware1 -> hardware2)
     Disaster recovery
     Dynamic load balancing
 -->
 <sect1 xml:id="sec.vt.best.scenario">
  <title>Virtualization Scenarios</title>

  <para>
   Virtualization offers a lot of capabilities to your environment. It can be
   used in multiple scenarios. To get more details about <link
   xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_capabilities.html">Virtualization
   Capabilities</link> and <link
   xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_benefits.html">Virtualization
   Benefits</link>, refer to the <link
   xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/book_virt.html">Virtualization
   Guide</link>.
  </para>

  <para>
   This best practice guide will provide advice for making the right choice in
   your environment. It will recommend or discourage the usage of options
   depending on your workload. Fixing configuration issues and performing tuning
   tasks will increase the performance of &vmguest;'s near to bare metal.
  </para>

  <remark>
   Bruce 20150806: There is no mention of caching considerations, of migration
   inhibitors, or basic strategies for how to do your storage or networking
   infrastructure, and only a little bit about how to map your virtualization
   requirements to host capabilities.  I see that the doc doesn't address the
   issue of when to use &xen; PV vs. &xen; FV vs. &kvm; at all.
  </remark>

<!-- Todo
       <Table Rowsep="1">
       <title>Scenario</title>
       <tgroup cols="3">
       <colspec colnum="1" colname="1" colwidth=""/>
       <colspec colnum="2" colname="2" colwidth=""/>
       <thead>
       <row>
       <entry>
       <para>Scenarios</para>
       </entry>
       <entry>
       <para>Option Recommended for</para>
       </entry>
       <entry>
       <para>Option Not recommended for</para>
       </entry>
       </row>
       </thead>
       <tbody>
       <row>
       <entry>
       <para>Consolidation</para>
       </entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Isolation</para>
       </entry>
       <entry></entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Migration</para>
       </entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Disaster Recovery</para>
       </entry>
       <entry>
       <mediaobject>
       <imageobject>
       <imagedata fileref="thumbup_green.png" width="25%" format="PNG"/>
       </imageobject>
       </mediaobject>
       </entry>
       </row>
       <row>
       <entry>
       <para>Dynamic Load Balancing</para>
       </entry>
       <entry></entry>
       <entry>
       <mediaobject>
       <imageobject>
       <imagedata fileref="thumbdown_red.png" width="25%" format="PNG"/>
       </imageobject>
       </mediaobject>
       </entry>
       </row>
       </tbody>
       </tgroup>
       </table>
  -->
 </sect1>
 <sect1 xml:id="sec.vt.best.intro">
  <title>Before You Apply Modifications</title>

  <sect2 xml:id="sec.vt.best.intro.backup">
   <title>Back Up First</title>
   <para>
    Changing the configuration of the &vmguest; or the &vmhost; can lead to
    data loss or an unstable state. It is really important that you do backups
    of files, data, images etc. before making any changes. Without backups you
    cannot restore the original state after a data loss or a misconfiguration.
    Do not perform tests or experiments on production systems.
    </para>
  </sect2>

  <sect2 xml:id="sec.vt.best.intro.testing">
   <title>Test Your Workloads</title>
   <para>
    The efficiency of a virtualization environment depends on many factors.
    This guide provides a reference for helping to make good choices when
    configuring virtualization in a production environment. Nothing is
    <emphasis>carved in stone</emphasis>. Hardware, workloads, resource
    capacity, etc. should all be considered when planning, testing, and
    deploying your virtualization infra-structure. Testing your virtualized
    workloads is vital to a successful virtualization implementation.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.vt.best.reco">
  <title>Recommendations</title>
  <sect2 xml:id="sec.vt.best.intro.libvirt">
   <title>Prefer the &libvirt; Framework</title>
   <remark>add some explanation about how to enable/disable kvm with
   qemu-system-arch</remark>
   <para>
    &suse; strongly recommends using the &libvirt; framework to configure,
    manage, and operate &vmhost;s, containers and &vmguest;. It offers a single
    interface (GUI and shell) for all supported virtualization technologies
    and therefore is easier to use than the hypervisor-specific tools.
   </para>
   <para>
    Using libvirt and hypervisor-specific tools at the same time is not
    recommended, because changes done with the hypervisor-specific tools may
    not be recognized by the libvirt tool set. See
     <link
       xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/cha_libvirt_overview.html"/>
     for more information on libvirt.
   </para>
  </sect2>

  <sect2 xml:id="sec.vt.best.intro.qemu">
   <title>qemu-system-i386 Compared to qemu-system-x86_64</title>
   <remark>Lin 20150808: Should we add some explanation about how to
   enable/disable kvm with qemu-system-arch</remark>
   <para>
    Similar to real 64-bit PC hardware, <command>qemu-system-x86_64</command>
    supports &vmguest;s running a 32-bit or a 64-bit operating system. Because
    <command>qemu-system-x86_64</command> usually also provides better
    performance for 32-bit guests, &suse; generally recommends using
    <command>qemu-system-x86_64</command>. Scenarios where
    <command>qemu-system-i386</command> is known to perform better are not
    supported by &suse;.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.vt.best.hostlevel">
  <title>&vmhost; Configuration and Resource Allocation</title>

  <para>
   Allocation of resources for &vmguest;s is a crucial point when
   administrating virtual machines. When assigning resources to &vmguest;s, be
   aware that overcommiting resources may affect the performance of the
   &vmhost; and the &vmguest;s. If all &vmguest;s request all their resources
   simultaneously, the host needs to be able to provide all of them. If not,
   the host's performance will be negatively affected and this will in turn
   also have negative effects of the &vmguest;'s performance.
  </para>

  <sect2 xml:id="sec.vt.best.mem">
   <title>Memory</title>
   <para>
    Linux manages memory in units called pages. On most systems the default
    page size is 4 KB. Linux and the CPU need to know which pages belong to
    which process. That information is stored in a page table. If a lot of
    processes are running, it takes more time to find where the memory is
    mapped, due to the time required to search the page table. To speed up the
    search, the TLB (Translation Lookaside Buffer) was invented. But on a
    system with a lot of memory, the TLB is not enough. To avoid any fallback
    to normal page table (resulting in a cache miss, which is time consuming),
    huge pages can be used. Using huge pages will reduce TLB overhead and TLB
    misses (pagewalk). A host with 32 GB (32*1014*1024 = 33,554,432 KB) of
    memory and a 4 KB page size has a TLB with: <emphasis>33,554,432/4 =
    8,388,608</emphasis> entries. Using a 2 MB (2048 KB) page size, the TLB
    only has <emphasis>33554432/2048 = 16384</emphasis> entries, considerably
    reducing TLB misses.
   </para>
   <sect3 xml:id="sec.vt.best.mem.huge_pages">
    <title>Configuring the &vmhost; and the &vmguest; to use Huge Pages</title>
    <para>
     Current CPU architectures support larger pages than 4 KB: huge pages.
     To determine the size of huge pages available on your system (could be
     2 MB or 1 GB), check the <literal>flags</literal> line in the output of
     <filename>/proc/cpuinfo</filename> for occurrences of
     <literal>pse</literal> and/or <literal>pdpe1gb</literal>.
    </para>
    <table>
     <title>Determine the Available Huge Pages Size</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="30%"/>
      <colspec colnum="2" colname="2" colwidth="70%"/>
      <thead>
       <row>
        <entry>
         <para>
          CPU flag
         </para>
        </entry>
        <entry>
         <para>
          Huge pages size available
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <para>
          Empty string
         </para>
        </entry>
        <entry>
         <para>
          No huge pages available
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          pse
         </para>
        </entry>
        <entry>
         <para>
          2 MB
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          pdpe1gb
         </para>
        </entry>
        <entry>
         <para>
          1 GB
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para>
     Using huge pages improves performance of &vmguest;s and reduces host
     memory consumption.
    </para>
    <para>
     By default the system uses THP. To make huge pages available on your
     system, activate it at boot time with <option>hugepages=1</option>,
     and–optionally–add the huge pages size with, for example,
     <option>hugepagesz=2MB</option>.
    </para>
    <note>
     <title>1 GB huge pages</title>
     <para>
      1 GB pages can only be allocated at boot time and cannot be freed
      afterward.
     </para>
    </note>
    <para>
     To allocate and use the huge page table (HugeTlbPage) you need to mount
     <filename>hugetlbfs</filename> with correct permissions.
    </para>
    <note>
     <title>Restrictions of Huge Pages</title>
     <para>
      Even if huge pages provide the best performance, they do come with
      some drawbacks. You lose features such as Memory ballooning (see
      <xref linkend="sec.vt.best.vmguests.virtio.balloon"/>), KSM (see
      <xref linkend="sec.vt.best.perf.ksm"/>, and huge pages cannot be swapped.
     </para>
    </note>
    <procedure>
     <title>Configuring the use of huge pages</title>
     <step>
      <para>
       Mount <literal>hugetlbfs</literal> to
       <filename>/dev/hugepages</filename>:
      </para>
      <screen>&prompt.user;&sudo; mount -t hugetlbfs hugetlbfs /dev/hugepages</screen>
     </step>
     <step>
      <para>
       To reserve memory for huge pages use the <command>sysctl</command>
       command. If your system has a huge page size of 2 MB (2048 KB), and you
       want to reserve 1 GB (1,048,576 KB) for your &vmguest;, you need
       <emphasis>1,048,576/2048=512</emphasis> pages in the pool:
      </para>
      <screen>&prompt.user;&sudo; sysctl vm.nr_hugepages=<replaceable>512</replaceable></screen>
      <para>
       The value is written to <filename>/proc/sys/vm/nr_hugepages</filename>
       and represents the current number of <emphasis>persistent</emphasis>
       huge pages in the kernel's huge page pool.
       <emphasis>Persistent</emphasis> huge pages will be returned to the huge
       page pool when freed by a task.
      </para>
     </step>
     <step>
      <para>
       Add the <literal>memoryBacking</literal> element in the &vmguest;
       configuration file (by running <command>virsh edit
       <replaceable>CONFIGURATION</replaceable></command>).
      </para>
      <screen>&lt;memoryBacking&gt;
  &lt;hugepages/&gt;
&lt;/memoryBacking&gt;</screen>
     </step>
     <step>
      <para>
       Start your &vmguest; and check on the host whether it uses hugepages:
      </para>
      <screen>&prompt.user;cat /proc/meminfo | grep HugePages_
HugePages_Total:<co xml:id="co.hp.total"/>     512
HugePages_Free:<co xml:id="co.hp.free"/>       92
HugePages_Rsvd:<co xml:id="co.hp.rsvd"/>        0
HugePages_Surp:<co xml:id="co.hp.surp"/>        0</screen>
      <calloutlist>
       <callout arearefs="co.hp.total">
        <para>
         Size of the pool of huge pages
        </para>
       </callout>
       <callout arearefs="co.hp.free">
        <para>
         Number of huge pages in the pool that are not yet allocated
        </para>
       </callout>
       <callout arearefs="co.hp.rsvd">
        <para>
         Number of huge pages for which a commitment to allocate from the
         pool has been made, but no allocation has yet been made
        </para>
       </callout>
       <callout arearefs="co.hp.surp">
        <para>
         Number of huge pages in the pool above the value in
         <filename>/proc/sys/vm/nr_hugepages</filename>. The maximum number
         of surplus huge pages is controlled by
         <filename>/proc/sys/vm/nr_overcommit_hugepages</filename>
        </para>
       </callout>
      </calloutlist>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec.vt.best.mem.thp">
    <title>Transparent Huge Pages</title>
    <para>
     Transparent huge pages (THP) provide a way to dynamically allocate huge
     pages with the <command>khugepaged</command> kernel thread, rather than
     manually managing their allocation and use. Workloads with contiguous
     memory access patterns can benefit greatly from THP. A 1000 fold
     decrease in page faults can be observed when running synthetic
     workloads with contiguous memory access patterns. Conversely, workloads
     with sparse memory access patterns (like databases) may perform poorly
     with THP. In such cases it may be preferable to disable THP by adding
     the kernel parameter <option>transparent_hugepage=never</option>,
     rebuild your grub2 configuration, and reboot. Verify if THP is disabled
     with:
    </para>
    <screen>&prompt.user;cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]</screen>
     <para>
      If disabled, the value <literal>never</literal> is shown in square
      brackets like in the example above.
     </para>
     <note>
      <title>&xen;</title>
      <para>
       THP is not available under &xen;
      </para>
     </note>
   </sect3>
      <sect3 xml:id="sec.vt.best.mem.xen">
    <title>&xen;-specific Memory Notes</title>
       <sect4 xml:id="sec.vt.best.mem.xen.dom-0">
     <title>Managing Domain-0 Memory</title>
     <para>
      When using the &xen; hypervisor, by default a small percentage of
      system memory is reserved for the hypervisor, with all remaining
      memory automatically allocated to Domain-0. When virtual machines are
      created, memory is ballooned out of Domain-0 to provide memory for the
      virtual machine. This process is called "autoballooning".
     </para>
     <para>
      Autoballooning has several limitations:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Reduced performance while dom0 is ballooning down to free memory for
        the new domain.
       </para>
      </listitem>
      <listitem>
       <para>
        Memory freed by ballooning is not confined to a specific NUMA node.
        This can result in performance problems in the new domain because of
        using a non-optimal NUMA configuration.
       </para>
      </listitem>
      <listitem>
       <para>
        Failure to start large domains due to delays while ballooning large
        amounts of memory from dom0.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      For these reasons, it is strongly recommended to disable
      autoballooning and give Domain-0 a predefined amount of memory.
     </para>
     <para>
      Autoballooning is controlled by the toolstack used to manage your &xen;
      installation. For the xl/libxl toolstack, autoballooning is controlled by
      the <option>autoballoon</option> setting in
      <filename>/etc/xen/xl.conf</filename>. For the libvirt+libxl toolstack,
      autoballooning is controlled by the <option>autoballoon</option> setting
      in <filename>/etc/libvirt/libxl.conf</filename>.
     </para>
     <para>
      The amount of memory initially allocated to Domain-0 is controlled by
      the &xen; hypervisor dom0_mem parameter. For example, to set the
      memory of Domain-0 to 8GB, add <option>dom0_mem=8G</option> to the
      &xen; hypervisor parameters.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        To set dom0_mem on &slea; 11 products, modify
        <filename>/boot/grub/menu.lst</filename>, adding
        <option>dom0_mem=XX</option> to the &xen; hypervisor (xen.gz)
        parameters. The change will be applied at next reboot.
       </para>
      </listitem>
      <listitem>
       <para>
        To set dom0_mem on &slea; 12 products, modify
        <filename>/etc/default/grub</filename>, adding
        <option>dom0_mem=XX</option> to
        <option>GRUB_CMDLINE_XEN_DEFAULT</option>. See <xref
        linkend="sec.vt.best.kernel.parameter"/> for more information.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Autoballooning is enabled by default since it is extremely difficult
      to determine a predefined amount of memory required by Domain-0.
      Memory needed by Domain-0 is heavily dependent on the number of hosted
      virtual machines and their configuration. Users must ensure Domain-0
      has sufficient memory resources to accommodate virtual machine
      workloads.
     </para>
    </sect4>
       <sect4 xml:id="sec.vt.best.mem.xen.tmpfs">
     <title>xenstore in <systemitem>tmpfs</systemitem></title>
     <para>
      When using &xen;, it is recommended to place the xenstore database on
      <systemitem>tmpfs</systemitem>. xenstore is used as a control plane by
      the xm/xend and xl/libxl toolstacks and the front-end and back-end drivers
      servicing domain I/O devices. The load on xenstore increases linearly as
      the number of running domains increase. If you anticipate hosting many
      &vmguest; on a &xen; host, move the xenstore database onto tmpfs to
      improve overall performance of the control plane. Mount the
      <filename>/var/lib/xenstored</filename> directory on tmpfs:
     </para>
     <screen>&prompt.user;&sudo; mount -t tmpfs tmpfs /var/lib/xenstored/</screen>
    </sect4>
   </sect3>
   <sect3 xml:id="sec.vt.best.perf.ksm">
    <title>KSM and Page Sharing</title>
    <para>
     Kernel Samepage Merging is a kernel feature that allows for lesser memory
     consumption on the &vmhost; by sharing data &vmguest;s have in common. The
     KSM daemon <systemitem class="daemon">ksmd</systemitem> periodically scans
     user memory looking for pages of identical content which can be replaced
     by a single write-protected page. To enable KSM run:
    </para>
    <screen>&prompt.user;&sudo; echo 1 &gt; /sys/kernel/mm/ksm/run</screen>
    <para>
     One advantage of using KSM from a &vmguest;'s perspective is that all
     guest memory is backed by host anonymous memory, so you can share
     <emphasis>pagecache</emphasis>, <emphasis>tmpfs</emphasis> or any kind of
     memory allocated in the guest.
    </para>
    <para>
     KSM is controlled by <systemitem>sysfs</systemitem>. You can check KSM's
     values in <filename>/sys/kernel/mm/ksm/</filename>:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>pages_shared</literal>: The number of shared pages that are
       being used (read-only).
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pages_sharing</literal>: The number of sites sharing the
       pages (read-only).
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pages_unshared</literal>: The number of pages that are unique
       and repeatedly checked for merging (read-only).
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pages_volatile</literal>: The number of pages that are changing
       too fast to be considered for merging (read-only).
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>full_scans</literal>: The number of times all mergeable areas
       have been scanned (read-only).
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>sleep_millisecs</literal>: The number of milliseconds
       <systemitem class="daemon">ksmd</systemitem> should sleep before the
       next scan. A low value will overuse the CPU, consuming CPU time that
       could be used for other tasks. A value greater than
       <literal>1000</literal> is recommended.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pages_to_scan</literal>: The number of present pages to scan
       before ksmd goes to sleep. A high value will overuse the CPU. It is
       recommended to start with a value of <literal>1000</literal>, and then
       adjust as necessary based on the KSM results observed while testing
       your deployment.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>merge_across_nodes</literal>: by default the system merges
       pages across NUMA nodes. Set this option to <literal>0</literal> to
       disable this behavior.
      </para>
     </listitem>
    </itemizedlist>
    <note>
     <title>Use Cases</title>
     <para>
      KSM is a good technique to over-commit host memory when running
      multiple instances of the same application or &vmguest;. When
      applications and &vmguest; are heterogeneous and do not share any
      common data, it is preferable to disable KSM. In a mixed heterogeneous
      and homogeneous environment, KSM can be enabled on the host but
      disabled on a per &vmguest; basis. Use <command>virsh edit</command>
      to disable page sharing of a &vmguest; by adding the following to the
      guest's XML configuration:
     </para>
<screen>&lt;memoryBacking&gt;
   &lt;nosharepages/&gt;
&lt;/memoryBacking&gt;</screen>
    </note>
    <warning>
     <title>Avoid Out-Of-Memory Conditions</title>
     <para>
      KSM can free up some memory on the host system, but the administrator
      should also reserve enough swap to avoid out-of-memory conditions in the
      event shareable memory decreases. A decrease in the amount of shareable
      memory results in an increase in the use of physical memory.
     </para>
    </warning>
    <warning>
     <title>Memory Access Latencies</title>
     <para>
      By default, KSM will merge common pages across NUMA nodes. This may
      degrade &vmguest; performance if the merged, common page is now located
      on a distant NUMA node, relative to the node running the &vmguest;
      vCPUs. If increased memory access latencies are noticed in the &vmguest;,
      disable cross-node merging with the
      <literal>merge_across_nodes</literal> sysfs control:
     </para>
     <screen>&prompt.user;&sudo; echo 0 &gt; /sys/kernel/mm/ksm/merge_across_nodes</screen>
    </warning>
   </sect3>
   <sect3 xml:id="sec.vt.best.mem.hot">
    <title>&vmguest;: Memory Hotplug</title>
    <para>
     To optimize the usage of your host memory, it may be useful to hotplug
     more memory for a running &vmguest; when required. To support memory
     hotplugging, you must first configure the
     <literal>&lt;maxMemory&gt;</literal> tag in the &vmguest;'s
     configuration file:
    </para>
<screen>&lt;maxMemory<co xml:id="co.mem.hot.max"/> slots='16'<co xml:id="co.mem.hot.slots"/> unit='KiB'&gt;20971520<co xml:id="co.mem.hot.size"/>&lt;/maxMemory&gt;
  &lt;memory<co xml:id="co.mem.hot.mem"/> unit='KiB'&gt;1048576&lt;/memory&gt;
&lt;currentMemory<co xml:id="co.mem.hot.curr"/> unit='KiB'&gt;1048576&lt;/currentMemory&gt;</screen>
    <calloutlist>
     <callout arearefs="co.mem.hot.max">
      <para>
       Runtime maximum memory allocation of the guest.
      </para>
     </callout>
     <callout arearefs="co.mem.hot.slots">
      <para>
       Number of slots available for adding memory to the guest
      </para>
     </callout>
     <callout arearefs="co.mem.hot.size">
      <para>
       Valid units are:
      </para>
      <itemizedlist>
       <listitem>
<para>
"KB" for kilobytes (1,000 bytes)
</para>
       </listitem>
       <listitem>
<para>
"k" or "KiB" for kibibytes (1,024 bytes)
</para>
       </listitem>
       <listitem>
<para>
"MB" for megabytes (1,000,000 bytes)
</para>
       </listitem>
       <listitem>
<para>
"M" or "MiB" for mebibytes (1,048,576 bytes)
</para>
       </listitem>
       <listitem>
<para>
"GB" for gigabytes (1,000,000,000 bytes)
</para>
       </listitem>
       <listitem>
<para>
"G" or "GiB" for gibibytes (1,073,741,824 bytes)
</para>
       </listitem>
       <listitem>
<para>
"TB" for terabytes (1,000,000,000,000 bytes)
</para>
       </listitem>
       <listitem>
<para>
"T" or "TiB" for tebibytes (1,099,511,627,776 bytes)
</para>
       </listitem>
      </itemizedlist>
     </callout>
     <callout arearefs="co.mem.hot.mem">
      <para>
       Maximum allocation of memory for the guest at boot time
      </para>
     </callout>
     <callout arearefs="co.mem.hot.curr">
      <para>
       Actual allocation of memory for the guest
      </para>
     </callout>
    </calloutlist>
    <para>
     To hotplug memory devices into the slots, create a file
     <filename>mem-dev.xml</filename> like the following:
    </para>
    <screen>&lt;memory model='dimm'&gt;
  &lt;target&gt;
  &lt;size unit='KiB'>524287&lt;/size&gt;
  &lt;node>0&lt;/node&gt;
  &lt;/target&gt;
&lt;/memory&gt;</screen>
    <para>
     And attach it with the following command:
    </para>
    <screen>&prompt.user;virsh attach-device vm-name mem-dev.xml</screen>
    <para>
     For memory device hotplug, the guest must have at least 1 NUMA cell
     defined (see <xref linkend="sec.vt.best.perf.numa.vmguest.topo"/>).
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.vt.best.perf.swap">
   <title>Swap</title>
   <para>
    <emphasis>Swap</emphasis> is usually used by the system to store
    underused physical memory (low usage, or not accessed for a long time).
    To prevent the system running out of memory, setting up a minimum swap
    is highly recommended.
   </para>
   <sect3 xml:id="sec.vt.best.perf.swap.swapiness">
    <title><literal>swappiness</literal></title>
    <para>
     The <literal>swappiness</literal> setting controls your system's swap
     behavior. It defines how memory pages are swapped to disk. A high value of
     <emphasis>swappiness</emphasis> results in a system that swaps more often.
     Available values range from <literal>0</literal> to
     <literal>100</literal>. A value of <literal>100</literal>> tells the
     system to find inactive pages and put them in swap. A value of
     <option>0</option> disables swapping.
     <!-- reduces the systems
     tendency to swap user space processes but does not disable swap
     completely (this is now the case with kernel =&gt; 3.5). -->
    </para>
    <para>
     To change the value and do some testing on a live system, change the value
     of <filename>/proc/sys/vm/swappiness</filename> on the fly and check the
     memory usage afterward:
    </para>
    <screen>&prompt.user;&sudo; echo 35 &gt; /proc/sys/vm/swappiness</screen>
    <screen>&prompt.user;free -h
total       used       free     shared    buffers     cached
Mem:      24616680    4991492   19625188     167056     144340    2152408
-/+ buffers/cache:    2694744   21921936
Swap:      6171644          0    6171644</screen>
    <para>
     To permanently set a swapiness value, add a line in
     <filename>/etc/systcl.conf</filename>, for example:
    </para>
<screen>vm.swappiness = 35</screen>
    <para>
     You can also control the swap by using the
     <literal>swap_hard_limit</literal> element in the XML configuration of
     your &vmguest;. It is highly recommended to do some testing before setting
     this parameter and using it in a production environment, because the host
     can terminate the domain if the value is too low.
    </para>
<screen>&lt;memtune&gt;<co xml:id="co.mem.1"/>
  &lt;hard_limit unit='G'&gt;1&lt;/hard_limit&gt;<co xml:id="co.mem.hard"/>
  &lt;soft_limit unit='M'&gt;128&lt;/soft_limit&gt;<co xml:id="co.mem.soft"/>
  &lt;swap_hard_limit unit='G'&gt;2&lt;/swap_hard_limit&gt;<co xml:id="co.mem.swap"/>
&lt;/memtune&gt;</screen>
    <calloutlist>
     <callout arearefs="co.mem.1">
      <para>
       This element provides memory tunable parameters for the domain. If
       this is omitted, it defaults to the defaults provided b the operating
       system.
      </para>
     </callout>
     <callout arearefs="co.mem.hard">
      <para>
       Maximum memory the guest can use. To avoid any problems on the
       &vmguest; it is strongly recommended to do not use this parameter.
      </para>
     </callout>
     <callout arearefs="co.mem.soft">
      <para>
       The memory limit to enforce during memory contention.
      </para>
     </callout>
     <callout arearefs="co.mem.swap">
      <para>
       The maximum memory plus swap the &vmguest; can use.
      </para>
     </callout>
    </calloutlist>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.vt.best.io">
   <title>I/O</title>
   <para/>
   <sect3 xml:id="sec.vt.best.perf.io">
   <title>I/O Scheduler</title>
   <para>
    The default I/O scheduler is Completely Fair Queuing (CFQ). The main aim
    of the CFQ scheduler is to provide a fair allocation of the disk I/O
    bandwidth for all processes that request an I/O operation. You can have
    different I/O schedulers for different devices.
   </para>
   <para>
    To get better performance in host and &vmguest; it is recommended to use
    <literal>noop</literal> in the &vmguest; (disable the I/O scheduler) and
    the <literal>deadline</literal> scheduler for a virtualization host.
   </para>
   <procedure>
    <title>Checking and Changing the I/O Scheduler at Runtime</title>
    <step>
     <para>
      To check your current I/O scheduler for your disk (replace
      <replaceable>sdX</replaceable> by the disk you want to check), run:
     </para>
     <screen>&prompt.user;cat /sys/block/<replaceable>sdX</replaceable>/queue/scheduler
noop deadline [cfq]</screen>
     <para>
      The value in square brackets is the one currently selected
      (<literal>cfq</literal> in the example above).
     </para>
    </step>
    <step>
     <para>
      You can change the scheduler at runtime with the following command:
     </para>
     <screen>&prompt.user;&sudo; echo deadline &gt; /sys/block/<replaceable>sdX</replaceable>/queue/scheduler</screen>
    </step>
   </procedure>

   <para>
    To permanently set an I/O scheduler for all disks of a system, use the
    Kernel parameter <literal>elevator</literal>. the respective values are
    <option>elevator=deadline</option> for the &vmhost;
    and <option>elevator=noop</option> for &vmguest;s. See <xref
    linkend="sec.vt.best.kernel.parameter"/> for further instructions.
   </para>
   <para>
    If you need to specify different I/O schedulers for each disk create the
    file <filename>/usr/lib/tmpfiles.d/IO_ioscheduler.conf</filename> with a
    content similar to the following example, which defines the
    <literal>deadline</literal> scheduler for <filename>/dev/sda</filename> and
    the <literal>noop</literal> scheduler for
    <filename>/dev/sdb</filename>. This feature is available on &slea; 12
    only.
   </para>
<screen>w /sys/block/sda/queue/scheduler - - - - deadline
w /sys/block/sdb/queue/scheduler - - - - noop</screen>
   </sect3>
   <sect3 xml:id="sec.vt.best.io.techniques">
    <title>I/O Virtualization</title>
    <para>
     SUSE products support various I/O Virtualization technologies. The
     following table lists advantages and disadvantages of each technology. For
     more information about I/O in virtualization refer to the <link
     xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_vt_io.html">I/O
     in Virtualization</link> chapter in the <citetitle>&productname;
     &productnumber; Virtualization Guide</citetitle>.
    </para>

    <table>
     <title>I/O Virtualization Solutions</title>
     <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="20%"/>
      <colspec colnum="2" colname="2" colwidth="40%"/>
      <colspec colnum="1" colname="1" colwidth="40%"/>
      <thead>
       <row>
        <entry>
         <para>
          Technology
         </para>
        </entry>
        <entry>
         <para>
          Advantage
         </para>
       </entry>
       <entry>
        <para>
         Disadvantage
        </para>
       </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry morerows="3">
         <para>
          Device Assignment
          (pass-through)
         </para>
        </entry>
        <entry>
         <para>
          Device accessed directly by the guest
         </para>
        </entry>
       <entry>
        <para>
         No sharing among multiple guests
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         High performance
        </para>
       </entry>
       <entry>
        <para>
         Live migration is complex
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para/>
       </entry>
       <entry>
        <para>
         PCI device limit is 8 per guest
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para/>
       </entry>
       <entry>
        <para>
         Limited number of slots on a server
        </para>
       </entry>
      </row>
      <row>
       <entry morerows="1">
        <para>
         Full virtualization
         (IDE, SATA, SCSI, e1000)
        </para>
       </entry>
       <entry>
        <para>
         &vmguest; compatibility
        </para>
       </entry>
       <entry>
        <para>
         Bad performance
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Easy for live migration
        </para>
       </entry>
       <entry>
        <para>
         Emulated operation
        </para>
       </entry>
      </row>
      <row>
       <entry morerows="2">
        <para>
         Para-virtualization
         (virtio-blk, virtio-net, virtio-scsi)
        </para>
       </entry>
       <entry>
        <para>
         Good performance
        </para>
       </entry>
       <entry>
        <para>
         Modified guest (PV drivers)
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Easy for live migration
        </para>
       </entry>
       <entry>
        <para/>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Efficient host communication with &vmguest;
        </para>
       </entry>
       <entry>
        <para/>
       </entry>
      </row>
     </tbody>
     </tgroup>
    </table>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.vt.best.fs">
   <title>Storage and File System</title>
   <para>
    Storage space for &vmguest;s can either be a block device (for example a
    partition on a physical disk), or an image file on the file system:
   </para>
   <table>
    <title>Block Devices Compared to Disk Images</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="10%"/>
     <colspec colnum="2" colname="2" colwidth=""/>
     <colspec colnum="1" colname="1" colwidth=""/>
     <thead>
      <row>
       <entry>
        <para>
         Technology
        </para>
       </entry>
       <entry>
        <para>
         Advantages
        </para>
       </entry>
       <entry>
        <para>
         Disadvantages
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         Block devices
        </para>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           Better performance
          </para>
         </listitem>
         <listitem>
          <para>
           Use standard tools for administration/disk modification
          </para>
         </listitem>
         <listitem>
           <para>
            Accessible from host (pro and con)
           </para>
         </listitem>
        </itemizedlist>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           Live migration is complex
           </para>
         </listitem>
         <listitem>
          <para>
           Impossible to increase capacity
          </para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Image files
        </para>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           Easier system management
          </para>
         </listitem>
         <listitem>
          <para>
           Easily move, clone, expand, back up domains
          </para>
         </listitem>
         <listitem>
          <para>
           Comprehensive toolkit (guestfs) for image manipulation
          </para>
         </listitem>
         <listitem>
          <para>
           Reduce overhead through sparse files
          </para>
         </listitem>
         <listitem>
          <para>
           Fully allocate for best performance
          </para>
         </listitem>
        </itemizedlist>
       </entry>
       <entry>
        <itemizedlist>
         <listitem>
          <para>
           Lower performance than block devices
          </para>
         </listitem>
        </itemizedlist>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>

   <!--
       <sect3 id="vt.best.fs.general">
       <title>General</title>
       <para>
       Access CD/DVD -> storage pool
       </para>
       <para>
       deleting pool
       </para>
       <para>
       Brtfs and guest image
       </para>
       <para>
       &qemu; direct access to host drives (-drive file=)
       </para>
       </sect2>
   -->
   <para>
    For detailed information about image formats and maintaining images refer
    to <xref linkend="sec.vt.best.img"/>.
   </para>

   <para>
    If your image is stored on an NFS share, you should check some server
    and client parameters to improve access to the &vmguest; image.
   </para>
   <sect3 xml:id="sec.vt.best.fs.nfs_rw">
    <title>NFS Read/Write (Client)</title>
    <para>
     Options <option>rsize</option> and <option>wsize</option> specify the size
     of the chunks of data that the client and server pass back and forth to
     each other. You should ensure NFS read/write sizes are sufficiently large,
     especially for large I/O. Change the <option>rsize</option> and
     <option>wsize</option> parameter in your <filename>/etc/fstab</filename>
     by increasing the value to 16 KB, which will ensure that all operations
     can be frozen if there is any instance of hanging.
     </para>
     <screen>nfs_server:/exported/vm_images<co xml:id="co.nfs.server"/> /mnt/images<co xml:id="co.nfs.mnt"/> nfs<co xml:id="co.nfs.nfs"/> rw<co xml:id="co.nfs.rw"/>,hard<co xml:id="co.nfs.hard"/>,sync<co xml:id="co.nfs.sync"/>, rsize=8192<co xml:id="co.nfs.rsize"/>,wsize=8192<co xml:id="co.nfs.wsize"/> 0 0</screen>
     <calloutlist>
      <callout arearefs="co.nfs.server">
       <para>
        NFS server's host name and export path name
       </para>
      </callout>
      <callout arearefs="co.nfs.mnt">
       <para>
        Where to mount the NFS exported share
       </para>
      </callout>
      <callout arearefs="co.nfs.nfs">
       <para>
        This is an <option>nfs</option> mount point
       </para>
      </callout>
      <callout arearefs="co.nfs.rw">
       <para>
        This mount point will be accessible in read/write
       </para>
      </callout>
      <callout arearefs="co.nfs.hard">
       <para>
        Determines the recovery behavior of the NFS client after an NFS
        request times out. <option>hard</option> is the best option to avoid
        data corruption
       </para>
      </callout>
      <callout arearefs="co.nfs.sync">
       <para>
        Any system call that writes data to files on that mount point causes
        that data to be flushed to the server before the system call returns
        control to user space
       </para>
      </callout>
      <callout arearefs="co.nfs.rsize">
       <para>
        Maximum number of bytes in each network READ request that the NFS
        client can receive when reading data from a file on an NFS server
       </para>
      </callout>
      <callout arearefs="co.nfs.wsize">
       <para>
        Maximum number of bytes per network WRITE request that the NFS
        client can send when writing data to a file on an NFS server.
       </para>
      </callout>
     </calloutlist>
    </sect3>
    <sect3 xml:id="sec.vt.best.fs.nfs_threads">
     <title>NFS Threads (Server)</title>
     <para>
      Your NFS server should have enough NFS threads to handle
      multi-threaded workloads. Use the <command>nfsstat</command> tool to
      get some RPC statistics on your server:
     </para>
     <screen>&prompt.user;&sudo; nfsstat -rc
Client rpc stats:
calls      retrans    authrefrsh
6401066    198          0          0</screen>
     <para>
      If the <literal>retrans</literal> is equal to 0, everything is fine.
      Otherwise, the client needs to retransmit, so increase the
      <envar>USE_KERNEL_NFSD_NUMBER</envar> variable in
      <filename>/etc/sysconfig/nfs</filename>, and adjust accordingly until
      <literal>retrans</literal> is equal to <literal>0</literal>.
     </para>
    </sect3>
   </sect2>

  <sect2 xml:id="sec.vt.best.perf.cpu">
   <title>CPUs</title>
   <para>
    Host CPU <quote>components</quote> will be <quote>translated</quote> to
    virtual CPUs in a &vmguest; when being assigned. These components can
    either be:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>CPU processor</emphasis>: this describes the main CPU unit,
      which usually has multiple cores and may support Hyper-Threading.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>CPU core</emphasis>: a main CPU unit can provide more than
      one core, and the proximity of cores speeds up the computation process
      and reduces energy costs.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>CPU Hyper-Threading</emphasis>: this implementation is used
      to improve parallelization of computations, but this is not as efficient
      as a dedicated core.
     </para>
    </listitem>
   </itemizedlist>

   <sect3 xml:id="sec.vt.best.perf.cpu.assign">
    <title>Assigning CPUs</title>
    <para>
     You should avoid overcommitting CPUs. Unless you know exactly how many
     virtual CPUs are required for a &vmguest;, you should start with a single
     virtual CPU per &vmguest;. Each virtual CPU should match one hardware
     processor or core on the &vmhost;.
    </para>
    <para>
     You should target a CPU workload of approximately 70% inside your VM (see
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_tuning/data/sec_util_processes.html"/> for information on monitoring
     tools). If you allocate more processors than needed in the &vmguest;, this
     will negatively affect the performance of host and guest: cycle efficiency
     will be degraded, the unused vCPU will consume timer interrupts and will
     idle loop. In case you primarily run single threaded applications on a
     &vmguest;, a single virtual CPU is the best choice.
    </para>
   </sect3>
   <sect3 xml:id="sec.vt.best.perf.cpu.guests">
    <title>&vmguest; CPU Configuration</title>
    <para>
     This section describes how to choose and configure a CPU type for a
     &vmguest;. You will also learn how to pin virtual CPUs to physical CPUs on
     the host system. For more information about virtual CPU configuration and
     tuning parameters refer to the libvirt documentation at <link
     xlink:href="https://libvirt.org/formatdomain.html#elementsCPU"/>.
    </para>
    <sect4 xml:id="sec.vt.best.perf.cpu.guests.model">
     <title>Virtual CPU Models and Features</title>
     <para>
      The CPU model and topology can be specified individually for each
      &vmguest;. Configuration options range from selecting specific CPU models
      to excluding certain CPU features. Predefined CPU models are listed in
      the <filename>/usr/share/libvirt/cpu_map.xml</filename>. Multiple sockets
      with a single core and a single thread generally provide the best
      performance. The list of available CPUs and topologies depends on the
      host system and can be displayed by running <command>virsh
      capabilities</command>.
     </para>
     <para>
      Note that changing the default virtual CPU configuration will require a
      &vmguest; shutdown when migrating it to a host with a different hardware.
      More information on &vmguest; migration is available at <link
      xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_libvirt_admin_migrate.html"/>.
     </para>
     <para>
      To specify a particular CPU model for a &vmguest;, add a respective entry
      to the &vmguest; configuration file. The following example configures a
      Broadwell CPU with the invariant TSC feature:
     </para>
     <screen>&lt;cpu mode='custom' match='exact'&gt;
  &lt;model&gt;Broadwell&lt;/model&gt;
  &lt;feature name='invtsc'/&gt;
&lt;/cpu&gt;</screen>
     <note>
      <title>The <literal>host-passthrough</literal> Mode</title>
      <para>
       With <literal>&lt;cpu mode='host-passthrough'&gt;</literal> the virtual
       CPU will be identical to the host CPU. Using this method makes it
       impossible to reproduced the &vmguest; on different hardware.
      </para>
     </note>
    </sect4>
    <sect4 xml:id="sec.vt.best.perf.cpu.guests.vcpupin">
     <title>Virtual CPU Pinning</title>
     <para>
      Virtual CPU pinning is used to constrain vCPU threads to a set of
      physical CPUs. The <literal>vcpupin</literal> element specifies to which
      physical host CPU a virtual CPU will be pinned to. If this element is not
      set and the attribute <literal>cpuset</literal> of the element
      <literal>vcpu</literal> is not specified, the virtual CPU is pinned to
      all the physical CPUs by default.
     </para>
     <para>
      CPU intensive workloads can benefit from virtual CPU pinning by
      increasing the physical CPU cache hit ratio. To pin a vCPU to a specific
      CPU run the following commands:
     </para>
     <screen>&prompt.user;virsh vcpupin <replaceable>DOMAIN_ID</replaceable> --vcpu <replaceable>vCPU_NUMBER</replaceable>
VCPU: CPU Affinity
----------------------------------
0: 0-7
&prompt.root;virsh vcpupin SLE12 --vcpu 0 0 --config</screen>
     <para>
      The last command generates the following entry in the XML configuration:
     </para>
     <screen>&lt;cputune&gt;
   &lt;vcpupin vcpu='0' cpuset='0'/&gt;
&lt;/cputune&gt;</screen>
     <note>
      <title>Virtual CPU pinning on NUMA Nodes</title>
      <para>
       On a NUMA system, virtual CPU pinning and memory allocation policies can
       be used to ensure a &vmguest;'s CPUs and its memory are confined to a
       NUMA node. See <xref linkend="sec.vt.best.perf.numa"/> for more information
       related to NUMA tuning.
      </para>
     </note>
     <warning>
      <title>Virtual CPU pinning and Live Migration</title>
      <para>
       Even though <literal>vcpupin</literal> can improve performance, keep in
       mind that live migration of a pinned &vmguest; can be problematic. If,
       for example, the requested physical resources are not be available on
       the destination host, or the source and destination hosts have different
       NUMA topologies, the migration will fail. For more recommendations about
       Live Migration see <link
       xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_libvirt_admin_migrate.html#libvirt_admin_live_migration_requirements">Virtualization
       Live Migration Requirements</link>.
      </para>
     </warning>
    </sect4>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.vt.best.perf.numa">
   <title>NUMA Tuning</title>
   <para>
    NUMA is an acronym for Non Uniform Memory Access. A NUMA system has
    multiple physical CPUs, each with local memory attached. Each CPU can also
    access other CPUs' memory, known as <quote>remote memory access</quote>,
    but it is much slower than accessing local memory. NUMA systems can
    negatively impact &vmguest; performance if not tuned properly. Although
    ultimately tuning is workload dependent, this section describes controls
    that should be considered when deploying &vmguest;s on NUMA hosts. Always
    consider your host topology when configuring and deploying VMs.
   </para>
   <para>
    &productname; contains a NUMA auto-balancer that strives to reduce remote
    memory access by placing memory on the same NUMA node as the CPU processing
    it. In addition, standard tools such as <command>cgset</command> and
    virtualization tools such as libvirt provide mechanisms to constrain
    &vmguest; resources to physical resources.
   </para>
   <para>
    <command>numactl</command> is used to check for host NUMA capabilities:
   </para>
   <screen>&prompt.user;&sudo; numactl --hardware
available: 4 nodes (0-3)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 72 73 74 75 76 77 78
79 80 81 82 83 84 85 86 87 88 89
node 0 size: 31975 MB
node 0 free: 31120 MB
node 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 90 91 92 93
94 95 96 97 98 99 100 101 102 103 104 105 106 107
node 1 size: 32316 MB
node 1 free: 31673 MB
node 2 cpus: 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 108 109 110
111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
node 2 size: 32316 MB
node 2 free: 31726 MB
node 3 cpus: 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 126 127 128
129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
node 3 size: 32314 MB
node 3 free: 31387 MB
node distances:
node   0   1   2   3
0:  10  21  21  21
1:  21  10  21  21
2:  21  21  10  21
3:  21  21  21  10</screen>
   <para>
    The <command>numactl</command> output shows this is a NUMA system with 4
    nodes or cells, each containing 36 CPUs and approximately 32G memory.
    <command>virsh capabilities</command> can also be used to examine the
    systems NUMA capabilities and CPU topology.
   </para>
   <sect3 xml:id="sec.vt.best.perf.numa.balancing">
    <title>NUMA Balancing</title>
    <para>
     On NUMA machines, there is a performance penalty if remote memory is
     accessed by a CPU. Automatic NUMA balancing scans a task's address space
     and unmaps pages to detect whether pages are properly placed or whether
     the data should be migrated to a memory node local to where the task is
     running. In defined intervals (configured with
     <literal>numa_balancing_scan_delay_ms</literal>), the task scans the next
     scan size number of pages (configured with
     <literal>numa_balancing_scan_size_mb</literal>) in its address
     space. When the end of the address space is reached the scanner restarts
     from the beginning.
    </para>
    <para>
     Higher scan rates cause higher system overhead as page faults must be
     trapped and data needs to be migrated. However, the higher the scan rate,
     the more quickly a task's memory is migrated to a local node when the
     workload pattern changes. This minimizes the performance impact due to
     remote memory accesses. These <command>sysctl</command> directives control
     the thresholds for scan delays and the number of pages scanned:
    </para>
    <screen>&prompt.user;&sudo; sysctl -a | grep numa_balancing
kernel.numa_balancing = 1<co xml:id="co.numa.balancing"/>
kernel.numa_balancing_scan_delay_ms = 1000<co xml:id="co.numa.delay"/>
kernel.numa_balancing_scan_period_max_ms = 60000<co xml:id="co.numa.pmax"/>
kernel.numa_balancing_scan_period_min_ms = 1000<co xml:id="co.numa.pmin"/>
kernel.numa_balancing_scan_size_mb = 256<co xml:id="co.numa.size"/></screen>
    <calloutlist>
     <callout arearefs="co.numa.balancing">
      <para>
       Enables/disables automatic page fault-based NUMA balancing
      </para>
     </callout>
     <callout arearefs="co.numa.delay">
      <para>
       Starting scan delay used for a task when it initially forks
      </para>
     </callout>
     <callout arearefs="co.numa.pmax">
      <para>
       Maximum time in milliseconds to scan a task's virtual memory
      </para>
     </callout>
     <callout arearefs="co.numa.pmin">
      <para>
       Minimum time in milliseconds to scan a task's virtual memory
      </para>
     </callout>
     <callout arearefs="co.numa.size">
      <para>
       Size in megabytes worth of pages to be scanned for a given scan
      </para>
     </callout>
    </calloutlist>
    <para>
     For more information see <link
     xlink:href="https://www.suse.com/documentation/sles-12/book_sle_tuning/data/cha_tuning_numactl.html"/>.
    </para>
    <para>
     The main goal of automatic NUMA balancing is either to reschedule tasks on
     the same node's memory, so the CPU follows the memory, or to copy the
     memory's pages to the same node, so the memory follows the CPU.
    </para>
    <warning>
     <title>Task Placement</title>
     <para>
      There are no rules to define the best place to run a task, because
      tasks could share memory with other tasks. For best performance, it is
      recommended to group tasks sharing memory on the same node. Check NUMA
      statistics with <command># cat /proc/vmstat | grep numa_</command>.
     </para>
    </warning>
   </sect3>
   <sect3 xml:id="sec.vt.best.perf.numa.cpuset">
    <title>Memory Allocation control with the CPUset Controller</title>
    <para>
     The cgroups cpuset controller can be used confine memory used by a
     process to a NUMA node. There are three cpuset memory policy modes
     available:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>interleave</literal>: This is a memory placement policy
       which is also known as round-robin. This policy can provide
       substantial improvements for jobs that need to place thread local
       data on the corresponding node. When the interleave destination is
       not available, it will be moved to another node.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>bind</literal>: This will place memory only on one node,
       which means in case of insufficient memory, the allocation will fail.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>preferred</literal>: This policy will apply a preference
       to allocate memory to a node, but if there is not enough space for
       memory on this node, it will fall back to another node.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     You can change the memory policy mode with the <command>cgset</command>
     tool from the <package>libcgroup-tools</package> package:
    </para>
    <screen>&prompt.user;&sudo; cgset -r cpuset.mems=<replaceable>NODE</replaceable> sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>/emulator</screen>
    <para>
     To migrate pages to a node, use the <command>migratepages</command>
     tool:
    </para>
    <screen>&prompt.user;migratepages <replaceable>PID</replaceable> <replaceable>FROM-NODE</replaceable> <replaceable>TO-NODE</replaceable></screen>
    <para>
     To check everything is fine. use: <command>cat
     /proc/<replaceable>PID</replaceable>/status | grep Cpus</command>.
    </para>
    <note>
     <title>Kernel NUMA/cpuset memory policy</title>
     <para>
      For more information see
      <link xlink:href="https://www.kernel.org/doc/Documentation/vm/numa_memory_policy.txt">Kernel
      NUMA memory policy</link> and
      <link xlink:href="https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt">cpusets
      memory policy</link>. Check also the
      <link xlink:href="https://libvirt.org/formatdomain.html#elementsNUMATuning">Libvirt
      NUMA Tuning documentation</link>.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="sec.vt.best.perf.numa.vmguest">
    <title>&vmguest;: NUMA Related Configuration</title>
    <para>
     &libvirt; allows to set up virtual NUMA and memory access policies.
     Configuring theses settings is not supported by
     <command>virt-install</command> or <command>virt-manager</command> and
     needs to be done manually by editing the &vmguest; configuration file with
     <command>virsh edit</command>.
    </para>
    <sect4 xml:id="sec.vt.best.perf.numa.vmguest.topo">
     <title>&vmguest; Virtual NUMA Topology</title>
     <para>
      Creating a &vmguest; virtual NUMA (vNUMA) policy that resembles the host
      NUMA topology can often increase performance of traditional large,
      scale-up workloads. &vmguest; vNUMA topology can be specified using the
      <literal>numa</literal> element in the XML configuration:
    </para>
    <screen>&lt;cpu&gt;
...
  &lt;numa&gt;
    &lt;cell<co xml:id="co.numa.cell"/> id='0'<co xml:id="co.numa.id"/> cpus='0-1'<co xml:id="co.numa.cpus"/> memory='512000' unit='KiB'/&gt;
    &lt;cell id='1' cpus='2-3' memory='256000'<co xml:id="co.numa.mem"/>
    unit='KiB'<co xml:id="co.numa.unit"/> memAccess='shared'<co
    xml:id="co.numa.memaccess"/>/&gt;
  &lt;/numa&gt;
  ...
&lt;/cpu&gt;</screen>
     <calloutlist>
      <callout arearefs="co.numa.cell">
       <para>
        Each <literal>cell</literal> element specifies a vNUMA cell or node
       </para>
      </callout>
      <callout arearefs="co.numa.id">
       <para>
        All cells should have an <literal>id</literal> attribute, allowing
        to reference the cell in other configuration blocks. Otherwise cells
        are assigned ids in ascending order starting from 0.
       </para>
      </callout>
      <callout arearefs="co.numa.cpus">
       <para>
        The CPU or range of CPUs that are part of the node
       </para>
      </callout>
      <callout arearefs="co.numa.mem">
       <para>
        The node memory
       </para>
      </callout>
      <callout arearefs="co.numa.unit">
       <para>
        Units in which node memory is specified
       </para>
      </callout>
      <callout arearefs="co.numa.memaccess">
       <para>
        Optional attribute which can control whether the memory is to be
        mapped as <option>shared</option> or <option>private</option>. This
        is valid only for hugepages-backed memory.
       </para>
      </callout>
     </calloutlist>
     <para>
      To find where the &vmguest; has allocated its pages. use: <command>cat
      /proc/<replaceable>PID</replaceable>/numa_maps</command> and
      <command>cat
      /sys/fs/cgroup/memory/sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>/memory.numa_stat</command>.
     </para>
     <warning>
      <title>NUMA specification</title>
      <para>
       The &libvirt; &vmguest; NUMA specification is currently only available
       for &qemu;/&kvm;.
      </para>
     </warning>
    </sect4>
    <sect4 xml:id="sec.vt.best.perf.numa.vmguest.alloc_libvirt">
     <title>Memory Allocation Control with &libvirt;</title>
     <para>
      If the &vmguest; has a vNUMA topology (see <xref
      linkend="sec.vt.best.perf.numa.vmguest.topo"/>), memory can be pinned to host
      NUMA nodes using the <literal>numatune</literal> element. This method
      is currently only available for &qemu;/&kvm; guests. See <xref
      linkend="sec.vt.best.perf.numa.alloc_libvirt.non-vnuma"/> for how to
      configure non-vNUMA &vmguest;s.
    </para>
<screen>&lt;numatune&gt;
    &lt;memory mode="strict"<co xml:id="co.numat.mode"/> nodeset="1-4,^3"<co xml:id="co.numat.nodeset"/>/&gt;
    &lt;memnode<co xml:id="co.numat.memnode"/> cellid="0"<co xml:id="co.numat.cellid"/> mode="strict" nodeset="1"/&gt;
    &lt;memnode cellid="2" placement="strict"<co xml:id="co.numat.placement"/> mode="preferred" nodeset="2"/&gt;
&lt;/numatune&gt;</screen>
     <calloutlist>
      <callout arearefs="co.numat.mode">
       <para>
        Policies available are: <literal>interleave</literal> (round-robin
        like), <literal>strict</literal> (default) or
        <literal>preferred</literal>.
       </para>
      </callout>
      <callout arearefs="co.numat.nodeset">
       <para>
        Specify the NUMA nodes.
       </para>
      </callout>
      <callout arearefs="co.numat.memnode">
       <para>
        Specify memory allocation policies for each guest NUMA node (if this
        element is not defined then this will fall back and use the
        <literal>memory</literal> element).
       </para>
      </callout>
      <callout arearefs="co.numat.cellid">
       <para>
        Addresses the guest NUMA node for which the settings are applied.
       </para>
      </callout>
      <callout arearefs="co.numat.placement">
       <para>
        The placement attribute can be used to indicate the memory placement
        mode for a domain process, the value can be <literal>auto</literal>
        or <literal>strict</literal>.
       </para>
      </callout>
     </calloutlist>
     <important xml:id="sec.vt.best.perf.numa.alloc_libvirt.non-vnuma">
      <title>Non-vNUMA &vmguest;</title>
      <para>
       On a non-vNUMA &vmguest;, pinning memory to host NUMA nodes is done
       like in the following example:
      </para>
      <screen>&lt;numatune&gt;
   &lt;memory mode="strict" nodeset="0-1"/&gt;
&lt;/numatune&gt;</screen>
      <para>
       In this example memory is allocated from the host nodes
       <literal>0</literal> and <literal>1</literal>. In case these memory
       requirements cannot be fulfilled, staring the &vmguest; will fail.
       <command>virt-install</command> also supports this configuration with
       the <option>--numatune</option> option.
      </para>
     </important>

     <warning>
      <title>Memory and CPU across NUMA nodes</title>
      <para>
       You should avoid allocating &vmguest; memory across NUMA nodes, and
       prevent virtual CPUs from floating across NUMA nodes.
      </para>
     </warning>
    </sect4>
   </sect3>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.vt.best.img">
  <title>&vmguest; Images</title>
  <para>
   Images are virtual disks used to store the operating system and data of
   &vmguest;s. They can be created, maintained and queried with the
   <command>qemu-img</command> command. Refer to <link
   xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html#cha.qemu.guest_inst.qemu-img.create">SLE12
   qemu-img documentation</link> for more information on the
   <command>qemu-img</command> tool and examples.
  </para>
  <sect2 xml:id="sec.vt.best.img.imageformat">
   <title>&vmguest; Image Formats</title>
   <para>
    Certain storage formats which &qemu; recognizes have their origins in
    other virtualization technologies. By recognizing these formats, &qemu;
    can leverage either data stores or entire guests that were originally
    targeted to run under these other virtualization technologies. Some
    formats are supported only in read-only mode, enabling either direct use
    of that read-only data store in a &qemu; guest or conversion to a fully
    supported &qemu; storage format (using <command>qemu-img</command>)
    which could then be used in read/write mode. See &sle;
    <link xlink:href="https://www.suse.com/releasenotes/x86_64/SUSE-SLES/12/#fate-317891">Release
    Notes</link> to get the list of supported formats.
   </para>
   <para>
    Use <command>qemu-img info <replaceable>vmguest.img</replaceable></command>
    to get information about an existing image, such as: the format, the
    virtual size, the physical size, snapshots if available.
   </para>
   <note>
    <title>Performance</title>
    <para>
     It is recommended to convert the disk images to either raw or qcow2 to
     achieve good performance.
    </para>
   </note>
   <warning>
    <title>Encrypted Images Cannot be Compressed</title>
    <para>
     When you create an image, you cannot use compression (<option>-c</option>)
     in the output file together with the encryption option
     (<option>-e</option>).
    </para>
   </warning>
   <sect3 xml:id="sec.vt.best.img.imageformat.raw">
    <title>Raw Format</title>
    <para/>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       This format is simple and easily exportable to all other
       emulators/hypervisors.
      </para>
     </listitem>
     <listitem>
      <para>
       It provides best performance (least I/O overhead).
      </para>
     </listitem>
     <listitem>
      <para>
       If your file system supports holes (for example in Ext2 or Ext3 on
       Linux or NTFS on Windows*), then only the written sectors will
       reserve space.
      </para>
     </listitem>
     <listitem>
      <para>
       The raw format allows to copy a &vmguest; image to a physical device
       (<command>dd if=<replaceable>vmguest.raw</replaceable>
       of=<replaceable>/dev/sda</replaceable></command>).
      </para>
     </listitem>
     <listitem>
      <para>
       It is byte-for-byte the same as what the &vmguest; sees, so this
       wastes a lot of space.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="sec.vt.best.img.imageformat.qcow2">
    <title>qcow2 Format</title>
    <para/>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       Use this to have smaller images (useful if your file system does not
       supports holes, for example on Windows*).
      </para>
     </listitem>
     <listitem>
      <para>
       It has optional AES encryption.
      </para>
     </listitem>
     <listitem>
      <para>
       Zlib-based compression option.
      </para>
     </listitem>
     <listitem>
      <para>
       Support of multiple VM snapshots (internal, external).
      </para>
     </listitem>
     <listitem>
      <para>
       Improved performance and stability.
      </para>
     </listitem>
     <listitem>
      <para>
       Supports changing the backing file.
      </para>
     </listitem>
     <listitem>
      <para>
       Supports consistency checks.
      </para>
     </listitem>
     <listitem>
      <para>
       Less performance than raw format.
      </para>
     </listitem>
    </itemizedlist>
    <variablelist>
     <varlistentry>
      <term>l2-cache-size</term>
      <listitem>
       <para>
        qcow2 can provide the same performance for random read/write access as
        raw format, but it needs a well-sized cache size. By default cache size
        is set to 1 MB. This will give good performance up to a disk size of 8
        GB. If you need a bigger disk size, you need to adjust the cache
        size. For a disk size of 64 GB (64*1024 = 65536), you need 65536 /
        8192B = 8 MB of cache (<option>-drive
        format=qcow2,l2-cache-size=8M</option>).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Cluster Size</term>
      <listitem>
       <para>
        The qcow2 format offers the capability to change the cluster size. The
        value must be between 512&nbsp;KB and 2&nbsp;MB. Smaller cluster sizes
        can improve the image file size whereas larger cluster sizes generally
        provide better performance.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Preallocation</term>
      <listitem>
       <para>
        An image with preallocated metadata is initially larger but can
        improve performance when the image needs to grow.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Lazy Refcounts</term>
      <listitem>
       <para>
        Reference count updates are postponed with the goal of avoiding
        metadata I/O and improving performance. This is particularly beneficial
        with <option>cache=writethrough</option>, which does not batch metadata
        updates, but in case of host crash, the reference count tables must be
        rebuilt, this is done automatically at the next open with
        <command>qemu-img check -r all</command>, but this takes some time.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="sec.vt.best.img.imageformat.qed">
    <title>qed format</title>
    <para>
     qed is the next-generation qcow (&qemu; Copy On Write). Its
     characteristics include:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Strong data integrity because of simple design.
      </para>
     </listitem>
     <listitem>
      <para>
       Retains sparseness over non-sparse channels (for example HTTP).
      </para>
     </listitem>
     <listitem>
      <para>
       Supports changing the backing file.
      </para>
     </listitem>
     <listitem>
      <para>
       Supports consistency checks.
      </para>
     </listitem>
     <listitem>
      <para>
       Fully asynchronous I/O path.
      </para>
     </listitem>
     <listitem>
      <para>
       Does not support internal snapshots.
      </para>
     </listitem>
     <listitem>
      <para>
       Relies on the host file system and cannot be stored on a logical
       volume directly.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="sec.vt.best.img.imageformat.vmdk">
    <title>VMDK format</title>
    <para>
     VMware 3, 4, or 6 image format, for exchanging images with that
     product.
    </para>
   </sect3>
  </sect2>
  <sect2 xml:id="sec.vt.best.img.overlay">
   <title>Overlay Disk Images</title>
    <para>
     The qcow2 and qed formats provide a way to create a base image (also
     called backing file) and overlay images on top of the base image. A
     backing file is useful to be able to revert to a known state and discard
     the overlay. If you write to the image, the backing image will be
     untouched and all changes will be recorded in the overlay image file. The
     backing file will never be modified unless you use the
     <option>commit</option> monitor command (or <command>qemu-img
     commit</command>).
    </para>
    <para>
     To create an overlay image:
    </para>
   <screen>&prompt.root;qemu-img create -o<co xml:id="co.1.minoro"/>backing_file=vmguest.raw<co xml:id="co.1.backingfile"/>,backing_fmt=raw<co xml:id="co.1.backingfmt"/>\
     -f<co xml:id="co.1.minorf"/> qcow2 vmguest.cow<co xml:id="co.1.imagename"/></screen>
    <calloutlist>
     <callout arearefs="co.1.minoro">
      <para>
       Use <option>-o ?</option> for an overview of available options.
      </para>
     </callout>
     <callout arearefs="co.1.backingfile">
      <para>
       The backing file name.
      </para>
     </callout>
     <callout arearefs="co.1.backingfmt">
      <para>
       Specify the file format for the backing file.
      </para>
     </callout>
     <callout arearefs="co.1.minorf">
      <para>
       Specify the image format for the &vmguest;.
      </para>
     </callout>
     <callout arearefs="co.1.imagename">
      <para>
       Image name of the &vmguest;, it will only record the differences from
       the backing file.
      </para>
     </callout>
    </calloutlist>
    <warning>
     <title>Backing Image Path</title>
     <para>
      You should not change the path to the backing image, otherwise you
      will need to adjust it. The path is stored in the overlay image file.
      If you want to update the path, you should make a symbolic link from
      the original path to the new path and then use the
      <command>qemu-img</command> <option>rebase</option> option.
     </para>
     <screen>&prompt.root;ln -sf /var/lib/images/vmguest.raw  /var/lib/images/SLE12/vmguest.raw
&prompt.root;qemu-img rebase<co xml:id="co.2.rebase"/>-u<co xml:id="co.2.unsafe"/> -b<co xml:id="co.2.minorb"/> /var/lib/images/vmguest.raw /var/lib/images/SLE12/vmguest.cow<co xml:id="co.2.image"/></screen>
     <para>
      The <command>rebase</command> subcommand tells
      <command>qemu-img</command> to change the backing file image. The
      <option>-u</option> option activates the unsafe mode (see note below).
      The backing image to be used is specified with <option>-b</option> and
      the image path is the last argument of the command.
     </para>
     <para>
      There are two different modes in which <option>rebase</option> can
      operate:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        <emphasis>Safe</emphasis>: This is the default mode and performs a
        real rebase operation. The safe mode is a time-consuming operation.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis>Unsafe</emphasis>: The unsafe mode (<option>-u</option>)
        only changes the backing files name and the format of the file name
        without making any checks on the files contents. You should use this
        mode to rename or moving a backing file.
       </para>
      </listitem>
     </itemizedlist>
    </warning>
    <para>
     A common use is to initiate a new guest with the backing file. Let's
     assume we have a <filename>sle12_base.img</filename> &vmguest; ready to
     be used (fresh installation without any modification). This will be our
     backing file. Now you need to test a new package, on an updated system
     and on a system with a different kernel. We can use
     <filename>sle12_base.img</filename> to instantiate the new &sle;
     &vmguest; by creating a qcow2 overlay file pointing to this backing
     file (<filename>sle12_base.img</filename>).
    </para>
    <para>
     In our example we will use <filename>sle12_updated.qcow2</filename> for
     the updated system, and <filename>sle12_kernel.qcow2</filename> for the
     system with a different kernel.
    </para>
    <para>
     To create the two thin provisioned systems use the
     <command>qemu-img</command> command line with the <option>-b</option>
     option:
    </para>
<screen>&prompt.root;qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
/var/lib/libvirt/sle12_updated.qcow2
Formatting 'sle12_updated.qcow2', fmt=qcow2 size=17179869184
backing_file='sle12_base.img' encryption=off cluster_size=65536
lazy_refcounts=off nocow=off
&prompt.root;qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
/var/lib/libvirt/sle12_kernel.qcow2
Formatting 'sle12_kernel.qcow2', fmt=qcow2 size=17179869184
backing_file='vmguest-sle12_base.img' encryption=off cluster_size=65536
lazy_refcounts=off nocow=off</screen>
    <para>
     The images are now usable, and you can do your test without touching the
     initial <filename>sle12_base.img</filename> backing file, all changes will
     be stored in the new overlay images. Additionally, you can also use these
     new images as a backing file, and create a new overlay.
    </para>
   <screen>&prompt.root;qemu-img create -b sle12_kernel.qcow2 -f qcow2 sle12_kernel_TEST.qcow2</screen>
    <para>
     When using <command>qemu-img info</command> with the option
     <option>--backing-chain</option>, it will return all information about the
     entire backing chain recursively:
    </para>
<screen>&prompt.root;qemu-img info --backing-chain
/var/lib/libvirt/images/sle12_kernel_TEST.qcow2
image: sle12_kernel_TEST.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: sle12_kernel.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle12_kernel.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: SLE12.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle12_base.img
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 16G
cluster_size: 65536
Format specific information:
compat: 1.1
lazy refcounts: true</screen>
    <figure xml:id="fig.qemu-img.overlay">
     <title>Understanding Image Overlay</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
   </sect2>

<!--
       <sect2 id="vt.best.img.diskio">
       <title>Disk IO Modes</title>
       <table>
       <title>Notation Conventions</title>
       <tgroup cols="2">
       <colspec colnum="1" colname="1"/>
       <colspec colnum="2" colname="2"/>
       <thead>
       <row>
       <entry>
       <para>Mode</para>
       </entry>
       <entry>
       <para>Description</para>
       </entry>
       </row>
       </thead>
       <tbody>
       <row>
       <entry>
       <para>Disk IO Modes
       </para>
       </entry>
       <entry>
       <para>Native</para>
       </entry>
       </row>
       <row>
       <entry>
       <para>kernel asynchronous IO</para>
       </entry>
       <entry>
       <para>threads</para>
       </entry>
       </row>
       <row>
       <entry>
       <para>host user-mode based threads</para>
       </entry>
       <entry>
       <para>default, 'threads' mode in SLES</para>
       </entry>
       </row>
       </tbody>
       </tgroup>
       </table>
       </sect2>
   -->

<sect2 xml:id="sec.vt.best.img.open_img">
 <title>Opening a &vmguest; Image</title>
 <para>
  To access the file system of an image, use the
  <package>guestfs-tools</package>. If you do not have this tool installed on
  your system you can mount an image with other Linux tools, but you should
  avoid accessing an untrusted or unknown &vmguest;'s image system because this
  can lead to security issues (read <link
  xlink:href="https://www.berrange.com/posts/2013/02/20/a-reminder-why-you-should-never-mount-guest-disk-images-on-the-host-os/">D.
  Berrangé's post</link> for more information).
 </para>
 <sect3 xml:id="sec.vt.best.img.open_img.raw">
  <title>Opening a Raw Image</title>
  <procedure>
   <title>Mounting a Raw Image</title>
   <step>
    <para>
     To be able to mount the image, find a free loop device. The following
     command display the first unused loop device,
     <filename>/dev/loop1</filename> in this example.
    </para>
<screen>&prompt.root;losetup -f
/dev/loop1</screen>
   </step>
   <step>
    <para>
     Associate an image (<filename>SLE12.raw</filename> in this example) with
     the loop device:
    </para>
    <screen>&prompt.root;losetup /dev/loop1 SLE12.raw</screen>
   </step>
   <step>
    <para>
     Check whether the images has successfully been associated with the loop
     device by getting detailed information about the loop device:
    </para>
    <screen>&prompt.root;losetup -l
NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop1         0      0         0  0    /var/lib/libvirt/images/SLE12.raw</screen>
   </step>
   <step>
    <para>
     Check the image's partitions with <command>kpartx</command>:
    </para>
    <screen>&prompt.root;kpartx -a<co xml:id="co.kpartx.a"/> -v<co xml:id="co.kpartx.v"/> /dev/loop1
add map loop1p1 (254:1): 0 29358080 linear /dev/loop1 2048</screen>
    <calloutlist>
     <callout arearefs="co.kpartx.a">
      <para>
       Add partition device mappings.
      </para>
     </callout>
     <callout arearefs="co.kpartx.v">
      <para>
       Verbose mode.
      </para>
     </callout>
    </calloutlist>
   </step>
   <step>
    <para>
     Now mount the image partition(s) (to <filename>/mnt/sle12mount</filename>
     in the following example):
    </para>
    <screen>&prompt.root;mkdir /mnt/sle12mount
&prompt.root;mount /dev/mapper/loop1p1 /mnt/sle12mount</screen>
   </step>
  </procedure>
  <note>
   <title>Raw image with LVM</title>
   <para>
    If your raw image contains an LVM volume group you should use LVM
    tools to mount the partition. Refer to
    <xref linkend="sec.lvm.found"/>
   </para>
  </note>

  <procedure>
   <title>Unmounting a Raw Image</title>
   <step>
    <para>
     Unmount all mounted partitions of the image, for example:
    </para>
    <screen>&prompt.root;umount /mnt/sle12mount</screen>
   </step>
   <step xml:id="st.umount.raw">
    <para>
     Delete partition device mappings with <command>kpartx</command>:
    </para>
    <screen>&prompt.root;kpartx -d /dev/loop1</screen>
   </step>
   <step>
    <para>
     Detach the devices with <command>losetup</command>
    </para>
    <screen>&prompt.root;losetup -d /dev/loop1</screen>
   </step>
  </procedure>
 </sect3>

 <sect3 xml:id="sec.vt.best.img.open_img.qcow2">
  <title>Opening a qcow2 Image</title>
  <procedure>
   <title>Mounting a qcow2 Image</title>
   <step>
    <para>
     First you need to load the <literal>nbd</literal> (network block
     devices) module. The following example loads it with support for 16 block
     devices (<option>max_part=16</option>). Check with
     <command>dmesg</command> whether the operation was successful:
    </para>
<screen>&prompt.root;modprobe nbd max_part=16
&prompt.root;dmesg | grep nbd
[89155.142425] nbd: registered device at major 43</screen>
   </step>
   <step>
    <para>
     Connect the &vmguest; image (for example <filename>SLE12.qcow2</filename>)
     to an NBD device (<filename>/debv/nbd0</filename> in the following
     example) with the <command>qemu-nbd</command> command. Make sure to use a
     free NBD device:
    </para>
    <screen>&prompt.root;qemu-nbd -c<co xml:id="co.qemunbd.minusc"/> /dev/nbd0<co xml:id="co.qemunbd.device"/> SLE12.qcow2<co xml:id="co.qemunbd.image"/></screen>
    <calloutlist>
     <callout arearefs="co.qemunbd.minusc">
      <para>
       Connect <filename>SLE12.qcow2</filename> to the local NBD device
       <filename>/dev/nbd0</filename>
      </para>
     </callout>
     <callout arearefs="co.qemunbd.device">
      <para>
       NBD device to use
      </para>
     </callout>
     <callout arearefs="co.qemunbd.image">
      <para>
       &vmguest; image to use
      </para>
     </callout>
    </calloutlist>
    <tip>
      <title>Checking for a free NBD devices</title>
      <para>
       To check whether an NBD device is free run the following command:
      </para>
<screen>&prompt.root;lsof /dev/nbd0
COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF  NODE NAME
qemu-nbd 15149 root   10u   BLK   43,0      0t0 47347 /dev/nbd0</screen>
      <para>
       If the command produces an output like in the example above, the device
       is busy (not free). This can also be confirmed by the presence of the
       <filename>/sys/devices/virtual/block/nbd0/pid</filename> file.
      </para>
     </tip>
   </step>
   <step>
    <para>
     Inform the operating system about partition table changes with
     <command>partprobe</command>:
    </para>
    <screen>&prompt.root;partprobe /dev/nbd0 -s
/dev/nbd0: msdos partitions 1 2
&prompt.root;dmesg | grep nbd0 | tail -1
[89699.082206]  nbd0: p1 p2</screen>
   </step>
   <step>
    <para>
     In the example above, the <filename>SLE12.qcow2</filename> contains two
     partitions: <filename>/dev/nbd0p1</filename> and
     <filename>/dev/nbd0p2</filename>. Before mounting these partitions, use
     <command>vgscan</command> to check whether they belong to an LVM volume:
    </para>
    <screen>&prompt.root;vgscan -v
    Wiping cache of LVM-capable devices
    Wiping internal VG cache
    Reading all physical volumes. This may take a while...
    Using volume group(s) on command line.
    No volume groups found.</screen>
   </step>
   <step>
    <para>
     If no LVM volume has been found, you can mount the partition with
     <command>mount</command>:
    </para>
    <screen>&prompt.root;mkdir /mnt/nbd0p2
# mount /dev/nbd0p1 /mnt/nbd0p2</screen>
    <para>
     Refer to <xref linkend="sec.lvm.found"/> for information on how to
     handle LVM volumes.
    </para>
   </step>
  </procedure>

  <procedure>
   <title>Unmounting a qcow2 Image</title>
   <step>
    <para>
     Unmount all mounted partitions of the image, for example:
    </para>
    <screen>&prompt.root;umount /mnt/nbd0p2</screen>
   </step>
   <step xml:id="st.umount.qcow2">
    <para>
     Disconnect the image from the <filename>/dev/nbd0</filename> device
    </para>
    <screen>&prompt.root;qemu-nbd -d /dev/nbd0</screen>
   </step>
  </procedure>
 </sect3>

 <sect3 xml:id="sec.lvm.found">
  <title>Opening Images Containing LVM</title>

  <procedure>
   <title>Mounting Images Containing LVM</title>
   <step>
    <para>
     To check images for LVM groups use <command>vgscan -v</command>. If an
     image contains LVM groups, the output of the command looks like the
     following:
    </para>
    <screen>&prompt.root;vgscan -v
Wiping cache of LVM-capable devices
Wiping internal VG cache
Reading all physical volumes.  This may take a while...
Finding all volume groups
Finding volume group "system"
Found volume group "system" using metadata type lvm2</screen>
   </step>
   <step>
    <para>
     The <literal>system</literal> LVM volume group has been found on the
     system. You can get more information about this volume with
     <command>vgdisplay <replaceable>VOLUMEGROUPNAME</replaceable></command>
     (in our case <replaceable>VOLUMEGROUPNAME</replaceable> is
     <literal>system</literal>). You should activate this volume group to
     expose LVM partitions as devices so the system can mount them. Use
     <command>vgchange</command>:
  </para>
    <screen>&prompt.root;vgchange -ay -v
Finding all volume groups
Finding volume group "system"
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/home
Creating system-home
Loading system-home table (254:0)
Resuming system-home (254:0)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/root
Creating system-root
Loading system-root table (254:1)
Resuming system-root (254:1)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/swap
Creating system-swap
Loading system-swap table (254:2)
Resuming system-swap (254:2)
Activated 3 logical volumes in volume group system
    3 logical volume(s) in volume group "system" now active</screen>
   </step>
   <step>
    <para>
     All partitions in the volume group will be listed in the
     <filename>/dev/mapper</filename> directory. You can simply mount them
     now.
    </para>
    <screen>&prompt.root;ls /dev/mapper/system-*
/dev/mapper/system-home  /dev/mapper/system-root /dev/mapper/system-swap

&prompt.root;mkdir /mnt/system-root
&prompt.root;mount  /dev/mapper/system-root /mnt/system-root

&prompt.root;ls /mnt/system-root/
bin   dev  home  lib64       mnt  proc        root  sbin     srv  tmp  var
boot  etc  lib   lost+found  opt  read-write  run   selinux  sys  usr</screen>
   </step>
  </procedure>
  <procedure>
   <title>Unmounting Images Containing LVM</title>
   <step>
    <para>
     Unmount all partitions (with <command>umount</command>)
    </para>
    <screen>&prompt.root;umount /mnt/system-root</screen>
   </step>
   <step>
    <para>
     Deactivate the LVM volume group (with <command>vgchange -an
     <replaceable>VOLUMEGROUPNAME</replaceable></command>)
      </para>
    <screen>&prompt.root;vgchange -an -v system
Using volume group(s) on command line
Finding volume group "system"
Found volume group "system"
Removing system-home (254:0)
Found volume group "system"
Removing system-root (254:1)
Found volume group "system"
Removing system-swap (254:2)
Deactivated 3 logical volumes in volume group system
0 logical volume(s) in volume group "system" now active</screen>
   </step>
   <step>
    <para>
     Now you have two choices:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If case of a qcow2 image, proceed as described in
       <xref linkend="st.umount.qcow2"/> (<command>qemu-nbd -d
       /dev/nbd0</command>).
      </para>
     </listitem>
     <listitem>
      <para>
       In case of a raw image, proceeds as described in
       <xref linkend="st.umount.raw"/> (<command>kpartx -d
       /dev/loop1</command>; <command>losetup -d /dev/loop1</command>).
      </para>
     </listitem>
    </itemizedlist>
    <important>
     <title>Check for a Successful Unmount</title>
     <para>
      You should double-check that umounting succeeded by using a system
      command like <command>losetup</command>, <command>qemu-nbd</command>,
      <command>mount</command> or <command>vgscan</command>. If this is not the
      case you may have trouble using the &vmguest; because its system image is
      used in different places.
     </para>
    </important>
   </step>
  </procedure>
 </sect3>
</sect2>

<sect2 xml:id="sec.vt.best.img.share">
 <title>File System Sharing</title>
 <para>
  You can access a host directory in the &vmguest; using the
  <tag class="element">filesystem</tag> element. In the following example we
  will share the <filename>/data/shared</filename> directory and mount it
  in the &vmguest;. Note that the <tag class="attribute">accessmode</tag>
  parameter only works with <tag class="attribute">type='mount'</tag> for the
  &qemu;/&kvm; drive (most other values for <tag class="attribute">type</tag> are
  exclusively used for the LXC driver).
 </para>
 <screen>&lt;filesystem type='mount'<co xml:id="co.fs.mount"/> accessmode='mapped'<co xml:id="co.fs.mode"/>&gt;
   &lt;source dir='/data/shared'<co xml:id="co.fs.sourcedir"/>&gt;
   &lt;target dir='shared'<co xml:id="co.fs.targetdir"/>/&gt;
&lt;/filesystem&gt;</screen>
   <calloutlist>
    <callout arearefs="co.fs.mount">
     <para>
      A host directory to mount &vmguest;.
     </para>
    </callout>
    <callout arearefs="co.fs.mode">
     <para>
      Access mode (the security mode) set to <literal>mapped</literal>
      will give access with the permissions of the hypervisor. Use
      <literal>passthrough</literal> to access this share with the
      permissions of the user inside the &vmguest;.
     </para>
    </callout>
    <callout arearefs="co.fs.sourcedir">
     <para>
      Path to share with the &vmguest;.
     </para>
    </callout>
    <callout arearefs="co.fs.targetdir">
     <para>
      Name or label of the path for the mount command.
     </para>
    </callout>
   </calloutlist>
   <para>
    To mount the <literal>shared</literal> directory on the &vmguest;, use the
    following commands:
    Under the &vmguest; now you need to mount the <literal>target
    dir='shared'</literal>:
   </para>
   <screen>&prompt.root;mkdir /opt/mnt_shared
&prompt.root;mount shared -t 9p /opt/mnt_shared -o trans=virtio</screen>
   <para>
    See
    <link xlink:href="https://libvirt.org/formatdomain.html#elementsFilesystems">&libvirt;
    File System </link> and
    <link xlink:href="http://wiki.qemu.org/Documentation/9psetup">&qemu;
    9psetup</link> for more information.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.vt.best.vmguests">
  <title>&vmguest; Configuration</title>

  <sect2 xml:id="sec.vt.best.vmguests.virtio">
   <title>Virtio Driver</title>
   <para>
    To increase &vmguest; performance it is recommended to use paravirtualized
    drivers within the &vmguest;s. The virtualization standard for such drivers
    for &kvm; are the <literal>virtio</literal> drivers, that are designed for
    running in a virtual environment. &xen; is using similar paravirtualized
    device drivers (like <link
    xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link> in a
    Windows* guest). For a better understanding of this topic, refer to the
    <link
      xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_vt_io.html">I/O
    Virtualization</link> section in the official Virtualization Guide.
   </para>

   <sect3 xml:id="sec.vt.best.vmguests.virtio.virtio_blk">
    <title><literal>virtio blk</literal></title>
    <para>
     <literal>virtio_blk</literal> is the virtio block device for disk. To use
     the <literal>virtio blk</literal> driver for a block device, specify the
     <tag class="attribute">bus='virtio'</tag> attribute in the <tag
     class="element">disk</tag> definition:
    </para>
    <screen>&lt;disk type='....' device='disk'&gt;
    ....
    &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</screen>

    <important>
     <title>Disk Device Names</title>
     <para>
      <literal>virtio</literal> disk devices are named
      <literal>/dev/vd[a-z][1-9]</literal>. If you migrate a Linux guest from a
      non-virtio disk you need to adjust the <literal>root=</literal> parameter
      in the GRUB configuration, and regenerate the <filename>initrd</filename>
      file, otherwise the system cannot boot. On &vmguest;s with other
      operating systems, the boot loader may need to be adjusted or reinstalled
      accordingly, too.
     </para>
    </important>

    <important>
     <title>
      Using <literal>virtio</literal> disks with
      <command>qemu-system-ARCH</command>
     </title>
     <para>
      When running <command>qemu-system-ARCH</command>, use the
      <option>-drive</option> option to add a disk to the &vmguest;. See the
      <link
      xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/cha_qemu_guest_inst_qemu-kvm.html">Basic
      Installation with qemu-system-ARCH</link> section in the official
      Virtualization guide for an example. The <option>-hd[abcd]</option>
      option will not work for virtio disks.
     </para>
    </important>
   </sect3>

   <sect3 xml:id="sec.vt.best.vmguests.virtio.virtio_net">
    <title>virtio net</title>
    <para>
     <literal>virtio_net</literal> is the virtio network device. The
     kernel modules should be loaded automatically in the guest at boot
     time. You need to start the service to make the network available.
    </para>
<screen>&lt;interface type='network'&gt;
    ...
    &lt;model type='virtio' /&gt;
&lt;/interface&gt;</screen>
   </sect3>
   <sect3 xml:id="sec.vt.best.vmguests.virtio.balloon">
    <title>virtio balloon</title>
    <para>
     The virtio balloon is used for host memory over-commits for guests. For
     Linux guests, the balloon driver runs in the guest kernel, whereas for
     Windows guests, the balloon driver is in the VMDP package.
     <literal>virtio_balloon</literal> is a PV driver to give or take
     memory from a &vmguest;.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>Inflate balloon</emphasis>: Return memory from guest to
       host kernel (for &kvm;) or to hypervisor (for &xen;)
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Deflate balloon</emphasis>: Guest will have more available
       memory
      </para>
     </listitem>
    </itemizedlist>
    <para>
     It is controlled by the <literal>currentMemory</literal> and
     <literal>memory</literal> options.
    </para>
<screen>&lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;1048576&lt;/currentMemory&gt;
    [...]
    &lt;devices&gt;
        &lt;memballoon model='virtio'/&gt;
    &lt;/devices&gt;</screen>
    <para>
     You can also use <command>virsh</command> to change it:
    </para>
    <screen>&prompt.user;virsh setmem <replaceable>DOMAIN_ID</replaceable> <replaceable>MEMORY in KB</replaceable></screen>
   </sect3>
   <sect3 xml:id="sec.vt.best.vmguests.virtio.check">
    <title>Checking virtio Presence</title>
    <para>
     You can check the virtio block PCI with:
    </para>
    <screen>&prompt.user;find /sys/devices/ -name virtio*
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2</screen>
    <para>
     To find the block device associated with <filename>vdX</filename>:
    </para>
    <screen>&prompt.user;find /sys/devices/ -name virtio* -print  -exec ls {}/block 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2
vda</screen>
    <para>
     To get more information on the virtio block:
    </para>
    <screen>&prompt.user;udevadm info -p /sys/devices/pci0000:00/0000:00:08.0/virtio2
P: /devices/pci0000:00/0000:00:08.0/virtio2
E: DEVPATH=/devices/pci0000:00/0000:00:08.0/virtio2
E: DRIVER=virtio_blk
E: MODALIAS=virtio:d00000002v00001AF4
E: SUBSYSTEM=virtio</screen>
    <para>
     To check all virtio drivers being used:
    </para>
    <screen>&prompt.user;find /sys/devices/ -name virtio* -print  -exec ls -l {}/driver 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
lrwxrwxrwx 1 root root 0 Jun 17 15:48 /sys/devices/pci0000:00/0000:00:06.0/virtio0/driver -&gt; ../../../../bus/virtio/drivers/virtio_console
/sys/devices/pci0000:00/0000:00:07.0/virtio1
lrwxrwxrwx 1 root root 0 Jun 17 15:47 /sys/devices/pci0000:00/0000:00:07.0/virtio1/driver -&gt; ../../../../bus/virtio/drivers/virtio_balloon
/sys/devices/pci0000:00/0000:00:08.0/virtio2
lrwxrwxrwx 1 root root 0 Jun 17 14:35 /sys/devices/pci0000:00/0000:00:08.0/virtio2/driver -&gt; ../../../../bus/virtio/drivers/virtio_blk</screen>
   </sect3>
<!--
       <para>
       Currently performance is much better when using a host kernel configured with <literal>CONFIG_HIGH_RES_TIMERS</literal>. Another option is use HPET/RTC and <option>-clock=</option> &qemu; option.
       </para>
   -->
   <sect3 xml:id="sec.vt.best.vmguests.virtio.drv_opt">
    <title>Find Device Driver Options</title>
    <para>
     Virtio devices and other drivers have various options. To list all of
     them, use the <option>help</option> parameter of
     the<command>qemu-system-ARCH</command> command.
    </para>
    <screen>&prompt.user;qemu-system-x86_64 -device virtio-net,help
virtio-net-pci.ioeventfd=on/off
virtio-net-pci.vectors=uint32
virtio-net-pci.indirect_desc=on/off
virtio-net-pci.event_idx=on/off
virtio-net-pci.any_layout=on/off
.....</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.vt.best.perf.cirrus">
   <title>Cirrus Video Driver</title>
   <para>
    To get 16-bit color, high compatibility and better performance it is
    recommended to use the <literal>cirrus</literal> video driver.
   </para>
   <note>
    <title>&libvirt;</title>
    <para>
     &libvirt; ignores the <literal>vram</literal> value because video size has
     been hardcoded in &qemu;.
    </para>
   </note>
<screen>&lt;video&gt;
   &lt;model type='cirrus' vram='9216' heads='1'/&gt;
&lt;/video&gt;</screen>
  </sect2>

  <sect2 xml:id="sec.vt.best.entropy">
   <title>Better Entropy</title>
   <para>
    Virtio RNG (random number generator) is a paravirtualized device that is
    exposed as a hardware RNG device to the guest. On the host side, it can be
    wired up to one of several sources of entropy, including a real hardware
    RNG device and the host's <filename>/dev/random</filename> if hardware
    support does not exist. The Linux kernel contains the guest driver for the
    device from version 2.6.26 and higher.
   </para>
   <para>
    The system entropy is collected from various non-deterministic hardware
    events and is mainly used by cryptographic applications. The virtual
    random number generator device (paravirtualized device) allows the host
    to pass through entropy to &vmguest; operating systems. This results in
    a better entropy in the &vmguest;.
   </para>
   <para>
    To use Virtio RNG, add an <literal>RNG</literal> device in
    <command>virt-manager</command> or directly in the &vmguest;'s XML
    configuration:
   </para>
<screen>&lt;devices&gt;
   &lt;rng model='virtio'&gt;
       &lt;backend model='random'&gt;/dev/random&lt;/backend&gt;
   &lt;/rng&gt;
&lt;/devices&gt;</screen>
   <para>
    The host now should used <filename>/dev/random</filename>:
   </para>
   <screen>&prompt.user;lsof /dev/random
qemu-syst 4926 qemu    6r   CHR    1,8      0t0 8199 /dev/random</screen>
   <para>
    On the &vmguest;, the source of entropy can be checked with:
   </para>
   <screen>&prompt.user;cat /sys/devices/virtual/misc/hw_random/rng_available</screen>
   <para>
    The current device used for entropy can be checked with:
   </para>
   <screen>&prompt.user;cat /sys/devices/virtual/misc/hw_random/rng_current
virtio_rng.0</screen>
   <para>
    You should install the <package>rng-tools</package> package on the
    &vmguest;, enable the service, and start it. Under SLE12 do the
    following:
   </para>
   <screen>&prompt.root;zypper in rng-tools
&prompt.root;systemctl enable rng-tools
&prompt.root;systemctl start rng-tools</screen>
  </sect2>

  <sect2 xml:id="sec.vt.best.perf.disable">
   <title>Disable Unused Tools and Devices</title>
   <para>
    Per host, use one virtualization technology only. For example, do not
    use &kvm; and Containers on the same computer. Otherwise, you may find
    yourself with a reduced amount of available resources, increased
    security risk and a longer software update queue. Even when the amount
    of resources allocated to each of the technologies is configured
    carefully, the host may suffer from reduced overall availability and
    degraded performance.
   </para>
   <para>
    Minimize the amount of software and services available on hosts. Most
    default installations of operating systems are not optimized for VM
    usage. Install what you really need and remove all other components in
    the &vmguest;.
   </para>
   <para>
    Windows* Guest:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Disable the screen saver
     </para>
    </listitem>
    <listitem>
     <para>
      Remove all graphical effects
     </para>
    </listitem>
    <listitem>
     <para>
      Disable indexing of hard disks if not necessary
     </para>
    </listitem>
    <listitem>
     <para>
      Check the list of started services and disable the ones you do not
      need
     </para>
    </listitem>
    <listitem>
     <para>
      Check and remove all unneeded devices
     </para>
    </listitem>
    <listitem>
     <para>
      Disable system update if not needed, or configure it to avoid any
      delay while rebooting or shutting down the host
     </para>
    </listitem>
    <listitem>
     <para>
      Check the Firewall rules
     </para>
    </listitem>
    <listitem>
     <para>
      Schedule backups and anti-virus updates appropriately
     </para>
    </listitem>
    <listitem>
     <para>
      Install the
      <link xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link>
      paravirtualized driver for best performance
     </para>
    </listitem>
    <listitem>
     <para>
      Check the operating system recommendations, such as on the
      <link xlink:href="http://windows.microsoft.com/en-us/windows/optimize-windows-better-performance#optimize-windows-better-performance=windows-7">Microsoft
      Windows* 7 better performance</link> Web page.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Linux Guest:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Remove or do not start the X Window System if not necessary
     </para>
    </listitem>
    <listitem>
     <para>
      Check the list of started services and disable the ones you do not
      need
     </para>
    </listitem>
    <listitem>
     <para>
      Check the OS recommendations for kernel parameters that enable better
      performance
     </para>
    </listitem>
    <listitem>
     <para>
      Only install software that you really need
     </para>
    </listitem>
    <listitem>
     <para>
      Optimize the scheduling of predictable tasks (system updates, hard
      disk checks, etc.)
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec.vt.best.perf.mtype">
   <title>Updating the Guest Machine Type</title>
   <para>
    &qemu; machine types define details of the architecture that are
    particularly relevant for migration and session management. As changes or
    improvements to &qemu; are made, new machine types are added. Old machine
    types are still supported for compatibility reasons, but to take advantage
    of improvements, it is recommended to always migrate to the latest machine
    type when upgrading.
   </para>
   <para>
    Changing the guest's machine type for a Linux guest will mostly be
    transparent, whereas for Windows* guests, it is probably a good idea to
    take a snapshot or backup of the guest in case Windows* has issues with
    the changes it detects and subsequently the user decides to revert to
    the original machine type the guest was created with.
   </para>
   <note>
    <title>Changing the machine type</title>
    <para>
     Refer to the Virtualization guide section
     <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html#sec.libvirt.config.mahcinetype.virsh">Change
     Machine Type</link> for documentation.
    </para>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.vt.best.vm.setup.config">
  <title>&vmguest;-Specific Configurations and Settings</title>

  <sect2 xml:id="sec.vt.best.acpi">
   <title>ACPI Testing</title>
   <para>
    The ability to change a &vmguest;'s state heavily depends on the
    operating system. It is very important to test this feature before any use
    of your &vmguest;s in production. For example, most Linux operating system
    disable this capability by default, so this requires you to enable this
    operation (mostly through Policy Kit).
   </para>
   <para>
    ACPI must be enabled in the guest for a graceful shutdown to work. To
    check if ACPI is enabled, run:
   </para>
   <screen>&prompt.user;virsh dumpxml <replaceable>VMNAME</replaceable> | grep acpi</screen>
   <para>
    If nothing is printed, ACPI is not enabled for your machine. Use
    <command>virsh edit</command> to add the following XML under
    &lt;domain&gt;:
   </para>
<screen>&lt;features&gt;
   &lt;acpi/&gt;
&lt;/features&gt;</screen>
   <para>
    If ACPI was enabled during a Windows
    XP/Windows Server 2003 guest installation, turning it on in the &vmguest;
    configuration only is not sufficient. For more information, see:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <link xlink:href="https://support.microsoft.com/en-us/kb/314088"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://support.microsoft.com/en-us/kb/309283"/>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Regardless of the &vmguest;'s configuration, a graceful shutdown is always
    possible from within the guest operating system.
   </para>
  </sect2>

  <sect2 xml:id="sec.vt.best.guest.kbd">
   <title>Keyboard Layout</title>
   <para>
    Though it is possible to specify the keyboard layout from a
    <command>qemu-system-ARCH</command> command, it is recommended to
    configure it in the &libvirt; XML file. To change the keyboard
    layout while connecting to a remote &vmguest; using vnc, you should edit
    the &vmguest; XML configuration file.
    For example, to add an <literal>en-us</literal> keymap, add in the
    <literal>&lt;devices&gt;</literal> section:
   </para>
<screen>&lt;graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/&gt;</screen>
   <para>
    Check the <literal>vncdisplay</literal> configuration and connect to your
    &vmguest;:
   </para>
   <screen>&prompt.user;virsh vncdisplay sles12 127.0.0.1:0</screen>
  </sect2>

  <sect2 xml:id="sec.vt.best.spice_default_url">
   <title>Spice default listen URL</title>
   <para>
    If no network interface other than <literal>lo</literal> is assigned
    an IPv4 address on the host, the default address the spice server will
    listen on will not work. An error like the following one will occur:
   </para>
   <screen>&prompt.user;virsh start sles12
error: Failed to start domain sles12
error: internal error: process exited while connecting to monitor: ((null):26929): Spice-Warning **: reds.c:2330:reds_init_socket: getaddrinfo(127.0.0.1,5900): Address family for hostname not supported
2015-08-12T11:21:14.221634Z qemu-system-x86_64: failed to initialize spice server</screen>
   <para>
    To fix this, you can change the default <literal>spice_listen</literal>
    value in <filename>/etc/libvirt/qemu.conf</filename> using the local IPv6
    address <systemitem class="ipaddress">::1</systemitem>. The spice server
    listening address can also be changed on a per &vmguest; basis, use
    <command>virsh edit</command> to add the listen XML attribute to the
    <literal>graphics type='spice'</literal> element:
   </para>
<screen>&lt;graphics type='spice' listen='::1' autoport='yes'/&gt;></screen>
  </sect2>

  <sect2 xml:id="sec.vt.best.xml_to_qemu">
   <title>XML to &qemu; command line</title>
   <para>
    Sometimes it could be useful to get the &qemu; command line to launch the
    &vmguest; from the XML file.
   </para>
   <screen>&prompt.user;virsh domxml-to-native<co xml:id="co.domxml.native"/> qemu-argv<co xml:id="co.domxml.argv"/> SLE12.xml<co xml:id="co.domxml.file"/></screen>
   <calloutlist>
    <callout arearefs="co.domxml.native">
     <para>
      Convert the XML file in domain XML format to the native guest
      configuration
     </para>
    </callout>
    <callout arearefs="co.domxml.argv">
     <para>
      For the &qemu;/&kvm; hypervisor, the format argument needs be qemu-argv
     </para>
    </callout>
    <callout arearefs="co.domxml.file">
     <para>
      Domain XML file to use
     </para>
    </callout>
   </calloutlist>
   <screen>&prompt.user;&sudo; virsh domxml-to-native qemu-argv /etc/libvirt/qemu/SLE12.xml
LC_ALL=C PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
   QEMU_AUDIO_DRV=none /usr/bin/qemu-system-x86_64 -name SLE12 -machine \
   pc-i440fx-2.3,accel=kvm,usb=off -cpu SandyBridge -m 4048 -realtime \
   mlock=off -smp 4,sockets=4,cores=1,threads=1 -uuid 8616d00f-5f05-4244-97cc-86aeaed8aea7 \
   -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/SLE12.monitor,server,nowait \
   -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew \
   -global kvm-pit.lost_tick_policy=discard -no-hpet \
   -no-shutdown -global PIIX4_PM.disable_s3=1 -global PIIX4_PM.disable_s4=1 \
   -boot strict=on -device ich9-usb-ehci1,id=usb,bus=pci.0,addr=0x4.0x7 \
   -device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,multifunction=on,addr=0x4 \
   -device ich9-usb-uhci2,masterbus=usb.0,firstport=2,bus=pci.0,addr=0x4.0x1 \
   -device ich9-usb-uhci3,masterbus=usb.0,firstport=4,bus=pci.0,addr=0x4.0x2 \
   -drive file=/var/lib/libvirt/images/SLE12.qcow2,if=none,id=drive-virtio-disk0,format=qcow2,cache=none \
   -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=2 \
   -drive if=none,id=drive-ide0-0-1,readonly=on,format=raw  \
   -device ide-cd,bus=ide.0,unit=1,drive=drive-ide0-0-1,id=ide0-0-1 -netdev tap,id=hostnet0  \
   -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:28:04:a9,bus=pci.0,addr=0x3,bootindex=1 \
   -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 \
   -vnc 127.0.0.1:0 -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 \
   -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on</screen>
  </sect2>

  <sect2 xml:id="sec.vt.best.kernel.parameter">
   <title>Change Kernel Parameters at boot time</title>
   <sect3 xml:id="sec.vt.best.kernel.parameter.sle11">
    <title>&sle; 11</title>
    <para>
     To change the value for &slea; 11 products at boot time, you need to
     modify your <filename>/boot/grub/menu.lst</filename> file, adding the
     <option>OPTION=parameter</option>, and then reboot your system.
    </para>
   </sect3>
   <sect3 xml:id="sec.vt.best.kernel.parameter.sle12">
    <title>&sle; 12</title>
    <para>
     To change the value for &slea; 12 products at boot time, you need to
     modify your <filename>/etc/default/grub</filename> file. Find the
     variable starting with <option>GRUB_CMDLINE_LINUX_DEFAULT</option> and
     add at the end <option>OPTION=parameter</option> (or change it with the
     correct value if it is already available).
    </para>
    <para>
     Now you need to regenerate your <literal>grub2</literal> configuration:
    </para>
<screen># grub2-mkconfig -o /boot/grub2/grub.cfg</screen>
    <para>
     Then reboot your system
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.vt.best.guest.device_to_xml">
   <title>Add Device to an XML Configuration</title>
   <para>
    If you want to create a new &vmguest; based on an XML file, and you are
    not familiar with XML configuration syntax, you can specify the &qemu;
    command line using the special tag
    <literal>qemu:commandline</literal>. For example if you want to add a
    virtio-balloon-pci, add this block at the end of the XML configuration
    file (before the &lt;/domain&gt; tag).
   </para>
<screen>&lt;qemu:commandline&gt;
    &lt;qemu:arg value='-device'/&gt;
    &lt;qemu:arg value='virtio-balloon-pci,id=balloon0'/&gt;
&lt;/qemu:commandline&gt;</screen>
  </sect2>
 </sect1>
<!-- TODO
      <sect2 id="vt.best.guest.clock">
      <title>Clock Setting</title>
      <para>

      clock setting (common source)
      kvm clock and ntp
      </para>
      Use Time Stamp Counter (TSC) as
      clocksource (if host CPU supports TSC)

kvm-clock
hpet
acpi_pm
jiffies
tsc
</sect2>

<sect2 id="vt.guest.guest.bio">
<title>Bio-based</title>
<para>
bio-based driver on slow devices
</para>
</sect2>
 -->
<!--
     <sect1 id="vt.best.snapsav">
     <title>Saving, Migrating and Snapshoting</title>
     <para>
     Migration requirements/Recomendations
     save VM and start/boot (memory invalid)
     snapshot naming importance
     avoid qemu-img snapshot
     cache mode in live migration
     guestfs and live system
     </para>
     </sect1>


<sect1 id="vt.best.security">
<title>Security consideration</title>
<para>
Connection to guest: security policy
Authentication for libvirtd and VNC need to be configured separately
&qemu; Guest Agent
The VNC TLS (set at start)
</para>
</sect1>

<sect1 id="vt.best.pcipass">
<title>pcpipass</title>
<para>
Pci device (not online!, managed/unmanaged)
howto check SR-IOV capabilities
</para>
</sect1>
 -->
<!--
     <sect1 xml:id="sec.vt.best.net">
     <title>Network Tips</title>
     <para>
     </para>

<sect2 xml:id="sec.vt.best.net.vnic">


<title>Virtual NICs</title>
<para>
virtio-net (&kvm;) : multi-queue option
vhost-net (&kvm;) : Default vNIC, best performance
netbk (&xen;) : kernel threads vs. tasklets
</para>
</sect2>

<sect2 xml:id="sec.vt.best.net.enic">
<title>Emulated NICs</title>
<para>
e1000: Default and preferred emulated NIC
rtl8139
</para>
</sect2>

<sect2 xml:id="sec.vt.best.net.sharednic">
<title>Shared Physical NICs</title>
<para>
SR-IOV: macvtap
Physicial NICs : PCI pass-through
</para>
</sect2>

<sect2 xml:id="sec.vt.best.general">
<title>Network General</title>
<para>
use multi-network to avoid congestion
admin, storage, migration ...
use arp-filter to prevent arp flux

same MTU in all devices to avoid fragmentation
yast to configure bridge
Network MAC address
bridge configuration in bridge.conf file
PCI pass-through Vfio to improve performance
</para>
</sect2>
</sect1>
 -->
<!--
     <sect1 xml:id="sec.vt.best.debug">
     <title>Troubleshooting/Debugging</title>
     <para>
     </para>

<sect2 xml:id="sec.vt.best.debug.xen">
<title>&xen;</title>
<para>
</para>
<sect3 xml:id="sec.vt.best.debug.xen.log">
<title>&xen; Log Files</title>
<para>
libxl logs:
<filename>/var/log/xen/*</filename>
qemu-dm-domain.log, xl-domain.log
bootloader.log, vm-install, xen-hotplug
Process specific logs, often requiring debug log levels to be useful
Some logs require 'set -x' to be added to /etc/xen/scripts/*

libvirt logs:
<filename>/var/log/libvirt/libxl</filename>
libxl-driver.log
domain.log
</para>
</sect3>
<sect3 xml:id="sec.vt.best.debug.xen.hypervisor">
<title>Daemon and Hypervisor Logs</title>
<para>
View systemd journal for specific units/daemons: <command>journalctl
<command>journalctl [\-\-follow] –unit xencommons.service</command>
<command>journalctl /usr/sbin/xenwatchdogd</command>
xl dmesg
&xen; hypervisor logs
</para>
</sect3>

<sect3 xml:id="sec.vt.best.debug.xen.loglevel">
<title>Increasing Logging Levels</title>
<para>
Log levels are increased through xen parameters:
</para>
<screen>loglvl=all</screen>
<para>
Increased logging for &xen; hypervisor
</para>
<screen>guest_loglvl=all</screen>
<para>
Increased logging for guest domain actions Grub2 config:

Edit <filename>/etc/default/grub</filename>, then recreate <filename>grub.cfg</filename>:
GRUB_CMDLINE_XEN_DEFAULT=”loglvl=all guest_loglvl=all”
<command>grub2-mkconfig -o /boot/grub/grub.cfg</command>
</para>
</sect3>
</sect2>

<sect2 xml:id="sec.vt.best.debug.support">
<title>Support</title>
<para></para>
<sect3 xml:id="sec.vt.best.debug.support.config">
<title>Supportconfig and Virtualization</title>
<para>
Core files:
basic-environment.txt
Reports detected virtualization hypervisor
Under some hypervisors (xen), subsequent general checks might be incomplete

Hypervisor specific files:
kvm.txt, xen.txt
Both logs contain general information:
RPM version/verification of important packages
Kernel, hardware, network details
</para>
</sect3>

<sect3 xml:id="sec.vt.best.debug.support.kvm">
<title>kvm.txt</title>
<para>
libvirt details
General libvirt details
Libvirt daemon status
&kvm; statistics
virsh version, capabilities, nodeinfo, etc...

Domain list and configurations
Conf and log files
<filename>/etc/libvirt/libvirtd.conf</filename>
Last 500 lines from <filename>/var/log/libvirt/qemu/domain.log</filename>
</para>
</sect3>

<sect3 xml:id="sec.vt.best.debug.support.xen">
<title>xen.txt</title>
<para>
Daemon status
xencommons, xendomains and xen-watchdog daemons
grub/grub2 configuration (for xen.gz parameters)

libvirt details
Domain list and configurations

xl details
Domain list and configurations
Conf and Log files
<filename>/etc/xen/xl.conf</filename>, <filename>/etc/libvirt/libvirtd.conf</filename>
Last 500 lines from <filename>/var/log/xen/*</filename>,
<filename>/var/log/libvirt/libxl/*</filename>
Output of <command>xl dmesg</command> and <command>xl info</command>
</para>
</sect3>
</sect2>

<sect2 xml:id="sec.vt.best.debug.advanced">
<title>Advanced Debugging Options</title>
<para>
Serial console
</para>
<screen>GRUB_CMDLINE_XEN_DEFAULT=“loglvl=all guest_loglvl=all console=com1 com1=115200,8n1”</screen>
<screen>GRUB_CMDLINE_LINUX_DEFAULT=“console=ttyS0,115200”</screen>
<para>
Debug keys
<command>xl debug keys h; xl dmesg</command>
<command>xl debug keys q; xl dmesg</command>
Additional &xen; debug tools:
<command>xenstore-{ls,read,rm,watch,write}, xentrace, xentop</command>

Capturing Guest Logs
Capturing guest logs during triggered problem:
Connect to domain:
<command>virsh console domname</command>
Execute problem command
Capturing domain boot messages
</para>
<screen>xl create -c VM config file</screen>
<screen>virsh create VM config file \-\-console</screen>
</sect2>
<sect2 xml:id="sec.vt.best.trouble">
<title>Troubleshooting Installations</title>
<para>
virt-manager and virt-install logs:
Found in <filename>~/.cache/virt-manager</filename>

Debugging virt-manager:
<command>virt-manager \-\-no-fork</command>
Sends messages directly to screen and log file
</para>
<screen>LIBVIRT_DEBUG=1 virt-manager \-\-no-fork</screen>
<para>
See libvirt messages in <filename>/var/log/messages</filename>

Use <command>xl</command> to rule out libvirt layer
</para>
</sect2>
<sect2 xml:id="sec.vt.best.trouble.libvirt">
<title>Troubleshooting Libvirt</title>
<para>
Client side troubleshooting
</para>
<screen>LIBVIRT_DEBUG=1
1: debug, 2: info, 3: warning, 4: error</screen>
<para>
Server side troubleshooting
<filename>/etc/libvirt/libvirtd.conf</filename> (restart libvirtd after changes)
log_level = 1
log_output = “1:file:/var/log/libvirtd.log”
log_filters = “1:qemu 3:remote”
</para>
</sect2>

<sect2 xml:id="sec.vt.best.trouble.kernel">
<title>Kernel Cores</title>
<para>
Host cores -vs- guest domain cores
Host cores are enabled through Kdump YaST module
For &xen; dom0 cores, 'crashkernel=size@offset' should be added as a &xen; hypervisor parameter

Guest cores require:
on_crash[action]on_crash tag
Possible coredump actions are:
coredump-restart     Dump core, then restart the VM
coredump-destroy    Dup core, then terminate the VM
Crashes are written to:
<filename>/var/lib/libvirt/{libxl,qemu}/dump</filename>
<filename>/var/lib/xen/dump</filename>  (if using xl).
</para>
</sect2>


<sect2 xml:id="sec.vt.best.debug.other">
<title>Other</title>
<para>
VGA trouble debug
</para>
</sect2>
</sect1>
 -->
 <sect1 xml:id="sec.vt.best.hypervisors_containers">
  <title>Hypervisors Compared to Containers</title>

  <para/>

  <table>
   <title>Hypervisors Compared to Containers</title>
   <tgroup cols="3">
    <colspec colnum="1" colwidth="20%"/>
    <colspec colnum="2" colwidth="30%"/>
    <colspec colnum="3" colwidth="30%"/>
    <thead>
     <row>
      <entry>
       <para>
        Features
       </para>
      </entry>
      <entry>
       <para>
        Hypervisors
       </para>
      </entry>
      <entry>
       <para>
        Containers
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        Technologies
       </para>
      </entry>
      <entry>
       <para>
        Emulation of a physical computing environment
       </para>
      </entry>
      <entry>
       <para>
        Use kernel host
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        System layer level
       </para>
      </entry>
      <entry>
       <para>
        Managed by a virtualization layer (Hypervisor)
       </para>
      </entry>
      <entry>
       <para>
        Rely on kernel namespaces and cgroups
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Level (layer)
       </para>
      </entry>
      <entry>
       <para>
        Hardware level
       </para>
      </entry>
      <entry>
       <para>
        Software level
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Virtualization mode available
       </para>
      </entry>
      <entry>
       <para>
        FV or PV
       </para>
      </entry>
      <entry>
       <para>
        None, only user space
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Security
       </para>
      </entry>
      <entry>
       <para>
        Strong
       </para>
      </entry>
      <entry>
       <warning>
        <para>
         Security is very low
        </para>
       </warning>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Confinement
       </para>
      </entry>
      <entry>
       <para>
        Full isolation
       </para>
      </entry>
      <entry>
       <warning>
        <para>
         Host kernel (OS must be compatible with kernel version)
        </para>
       </warning>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Operating system
       </para>
      </entry>
      <entry>
       <para>
        Any operating system
       </para>
      </entry>
      <entry>
       <para>
        Only Linux (must be "kernel" compatible)
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Type of system
       </para>
      </entry>
      <entry>
       <para>
        Full OS needed
       </para>
      </entry>
      <entry>
       <para>
        Scope is an instance of Linux
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Boot time
       </para>
      </entry>
      <entry>
       <para>
        Slow to start (OS delay)
       </para>
      </entry>
      <entry>
       <para>
        Really quick start
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Overhead
       </para>
      </entry>
      <entry>
       <para>
        High
       </para>
      </entry>
      <entry>
       <para>
        Very low
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Efficiency
       </para>
      </entry>
      <entry>
       <para>
        Depends on OS
       </para>
      </entry>
      <entry>
       <para>
        Very efficient
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Sharing with host
       </para>
      </entry>
      <entry>
       <warning>
        <para>
         Complex because of isolation
        </para>
       </warning>
      </entry>
      <entry>
       <para>
        Sharing is easy (host sees everything; container sees its own
        objects)
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Migration
       </para>
      </entry>
      <entry>
       <para>
        Supports migration (live mode)
       </para>
      </entry>
      <entry>
       <warning>
        <para>
         Not possible
        </para>
       </warning>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <sect2 xml:id="sec.vt.best.hypervisors_containers.both">
   <title>Getting the Best of Both Worlds</title>
   <para>
    Even if the above table seems to indicate that running a single
    application in a highly secure way is not possible, starting with &sls; 12
    SP1 <command>virt-sandbox</command> will allow running a single
    application in a &kvm; guest. <command>virt-sandbox</command> bootstraps
    any command within a Linux kernel with a minimal root file system.
   </para>
   <para>
    The guest root file system can either be the root file system mounted
    read-only or a disk image. The following steps will show how to set up a
    sandbox with qcow2 disk image as root file system.
   </para>
   <procedure>
    <step>
     <para>
      Create the disk image using <command>qemu-img</command>:
     </para>
     <screen>&prompt.root;qemu-img create -f qcow2 rootfs.qcow2 6G</screen>
    </step>
    <step>
     <para>
      Format the disk image:
     </para>
     <screen>&prompt.root;modprobe nbd<co xml:id="co.vsmkfs.modprobe"/>
&prompt.root;/usr/bin/qemu-nbd --format qcow2 -n -c /dev/nbd0 $PWD/test-base.qcow2<co xml:id="co.vsmkfs.qemu-nbd"/>
&prompt.root;mkfs.ext3 /dev/nbd0<co xml:id="co.vsmkfs.do"/></screen>
     <calloutlist>
      <callout arearefs="co.vsmkfs.modprobe">
       <para>
        Make sure the nbd module is loaded: it is not loaded by default and
        will only be used to format the qcow image.
       </para>
      </callout>
      <callout arearefs="co.vsmkfs.qemu-nbd">
       <para>
        Create an NBD device for the qcow2 image. This device will then
        behave like any other block device. The example uses
        <replaceable>/dev/nbd0</replaceable> but any other free NBD device
        will work.
       </para>
      </callout>
      <callout arearefs="co.vsmkfs.do">
       <para>
        Format the disk image directly. Note that no partition table has
        been created: <command>virt-sandbox</command> considers the image to
        be a partition, not a disk.
       </para>
       <para>
        The partition formats that can be used are limited: the Linux kernel
        bootstrapping the sandbox needs to have the corresponding features
        built in. The Ext4 module is also available at the sandbox start-up
        time.
       </para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>
      Now populate the newly formatted image:
     </para>
     <screen>&prompt.root;guestmount -a base.qcow2 -m /dev/sda:/ /mnt<co
     xml:id="co.vsfs.mount"/>

&prompt.root;zypper --root /mnt ar cd:///?devices=/dev/dvd SLES12_DVD
&prompt.root;zypper --root /mnt in -t pattern Minimal<co xml:id="co.vsfs.install"/>

&prompt.root;guestunmount /mnt<co xml:id="co.vsfs.unmount"/></screen>
     <calloutlist>
      <callout arearefs="co.vsfs.mount">
       <para>
        Mount the qcow2 image using the <command>guestfs</command> tools.
       </para>
      </callout>
      <callout arearefs="co.vsfs.install">
       <para>
        Use Zypper with the <literal>--root</literal> parameter to add a &sls;
        repository and install the <literal>Minimal</literal> pattern in the
        disk image. Any additional package or configuration change should be
        performed in this step.
       </para>
       <note>
        <title>Using backing chains</title>
        <para>
         To share the root file system between several sandboxes, create qcow2
         images with a common disk image as backing chain as described in
         <xref linkend="sec.vt.best.img.overlay"/>.
        </para>
       </note>
      </callout>
      <callout arearefs="co.vsfs.unmount">
       <para>
        Unmount the qcow2 image.
       </para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>
      Run the sandbox, using <command>virt-sandbox</command>. This command has
      many interesting options, read its man page to discover them all. The
      command can be run as &rootuser; or as an unprivileged user.
     </para>
     <screen>&prompt.root;virt-sandbox -n <replaceable>name</replaceable> \
     -m host-image:/=$PWD/rootfs.qcow2 \ <co xml:id="co.vs.rootfs"/>
     -m host-bind:/srv/www=/guests/www \ <co xml:id="co.vs.bind"/>
     -m ram:/tmp=100MiB \
     -m ram:/run=100MiB \ <co xml:id="co.vs.tmpfs"/>
     -N source=default,address=192.168.122.12/24 \ <co xml:id="co.vs.net"/>
     -- \
     <replaceable>/bin/sh</replaceable></screen>
     <calloutlist>
      <callout arearefs="co.vs.rootfs">
       <para>
        Mount the created disk image as the root file system. Note that
        without any image being mounted as <filename>/</filename>, the host
        root file system is read-only mounted as the guest one.
       </para>
       <para>
        The host-image mount is not reserved for the root file system, it
        can be used to mount any disk image anywhere in the guest.
       </para>
      </callout>
      <callout arearefs="co.vs.bind">
       <para>
        The host-bind mount is pretty convenient for sharing files and
        directories between the host and the guest. In this example the host
        directory <filename>/guests/www</filename> is mounted as
        <filename>/srv/www</filename> in the sandbox.
       </para>
      </callout>
      <callout arearefs="co.vs.tmpfs">
       <para>
        The RAM mounts are defining <literal>tmpfs</literal> mounts in the
        sandbox.
       </para>
      </callout>
      <callout arearefs="co.vs.net">
       <para>
        The network uses a network defined in libvirt. When running as an
        unprivileged user, the source can be omitted, and the &kvm; user
        networking feature will be used. Using this option requires the
        <package>dhcp-client</package> and <package>iproute2</package>
        packages, which are part of the &sls; <literal>Minimal</literal>
        pattern.
       </para>
      </callout>
     </calloutlist>
    </step>
   </procedure>
  </sect2>
 </sect1>

<sect1 xml:id="sec.vt.best.xen_pv_fv">
  <title>&xen;: Moving from PV to FV</title>
  <para>
   This chapter explains how to convert a &xen; para-virtual machine into a
   &xen; fully virtualized machine.
  </para>
  <para>
   First you need to change to the <literal>-default</literal> kernel, if not
   already installed, you need to install it while in PV mode.
  </para>
  <para>
   In case you are using <literal>vda*</literal> disk naming, you need to
   change this to <literal>hd*</literal> in <filename>/etc/fstab</filename>,
   <filename>/boot/grub/menu.lst</filename> and
   <filename>/boot/grub/device.map</filename>.
  </para>
  <note>
   <title>Prefer UUIDs</title>
   <para>
    You should use UUIDs or logical volumes within your
    <filename>/etc/fstab</filename>. Using UUID simplifies attached network
    storage, multipathing, and virtualization.
   </para>
  </note>
  <para>
   Moving from PV to FV will lead to disk driver modules being missing from the
   initrd. The modules expected are <literal>xen-vbd</literal> (and
   <literal>xen-vnif</literal> for network). These are the PV drivers for a
   fully virtualized &vmguest;. All other modules like
   <literal>ata_piix</literal>, <literal>ata_generic</literal> and
   <literal>libata</literal> should be added automatically.
  </para>
  <tip>
   <title>Adding Modules to the initrd</title>
   <para>
    On &slsa; 11, you can add modules to the
    <literal>INITRD_MODULES</literal> line in the
    <filename>/etc/sysconfig/kernel</filename> file, for example:
   </para>
   <screen>INITRD_MODULES="xen-vbd xen-vnif"</screen>
   <para>
    Run <command>mkinitrd</command> to build a new initrd containing the
    modules.
   </para>
   <para>
    On &slsa; 12, open <filename>/etc/dracut.conf.d/01-dist.conf</filename>
    and add the modules with <literal>force-drivers</literal> by adding a
    line as in the example below:
   </para>
   <screen>force-drivers+="xen-vbd xen-vnif"</screen>
   <para>
    Run <command>dracut -f</command> to build a new initrd containing the
    modules.
   </para>
  </tip>
  <para>
   You need to change a few parameters in the XML configuration file to
   describe your &vmguest;:
  </para>
  <para>
   Set the OS section to something like:
  </para>
  <screen>&lt;os&gt;
    &lt;type arch='x86_64' machine='xenfv'&gt;hvm&lt;/type&gt;
    &lt;loader&gt;/usr/lib/xen/boot/hvmloader&lt;/loader&gt;
    &lt;boot dev='hd'/&gt;
&lt;/os&gt;</screen>
  <para>
    In the devices section, you need to add:
  </para>
  <screen>&lt;emulator&gt;/usr/lib/xen/bin/qemu-system-i386&lt;/emulator&gt;</screen>
  <para>
   Replace the <literal>xen</literal> disk bus with <literal>ide</literal>,
   and the <literal>vda</literal> target device with <literal>hda</literal>.
  </para>
  <note>
   <title>guestfs-tools</title>
   <para>
    If you want to script this process, or work on disk images directly, you
    can use the <link
    xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_guestfs_tools.html">guestfs-tools</link>
    suite, as numerous tools exist there to help to modify disk images.
   </para>
  </note>
 </sect1>

 <sect1 xml:id="sec.vt.best.refs">
  <title>External References</title>

  <para></para>

  <itemizedlist>
   <listitem>
    <para>
     <link xlink:href="https://kernel.org/doc/ols/2009/ols2009-pages-19-28.pdf">Increasing
     memory density using KSM</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://www.linux-kvm.org/page/KSM">linux-kvm.org
     KSM</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://www.kernel.org/doc/Documentation/vm/ksm.txt">KSM's
     kernel documentation</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/329123/">ksm - dynamic page
     sharing driver for linux v4</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://www.espenbraastad.no/post/memory-ballooning/">Memory
     Ballooning</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://wiki.libvirt.org/page/Virtio">libvirt
     virtio</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://www.kernel.org/doc/Documentation/block/cfq-iosched.txt">CFQ's
     kernel documentation</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="https://www.kernel.org/doc/Documentation/sysctl/kernel.txt">Documentation
     for sysctl</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/525459/">LWN Random
     Number</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://wiki.mikejung.biz/KVM_/_Xen">&kvm; / &xen;
     tweaks</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="http://events.linuxfoundation.org/sites/events/files/slides/CloudOpen2013_Khoa_Huynh_v3.pdf">Dr.
     Khoa Huynh, IBM Linux Technology Center</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
      xlink:href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/plain/Documentation/admin-guide/kernel-parameters.txt">Kernel
     Parameters</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://lwn.net/Articles/374424/">Huge pages
     Administration (Mel Gorman)</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt">kernel
     hugetlbpage</link>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</article>
