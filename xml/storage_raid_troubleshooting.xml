<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-raidtroubleshooting" xml:lang="en">
 <title>Troubleshooting software RAIDs</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
  <para>
   Check the <filename>/proc/mdstat</filename> file to find out whether a RAID
   partition has been damaged. If a disk fails, shut down your Linux system and
   replace the defective hard disk with a new one partitioned the same way.
   Then restart your system and enter the command <command>mdadm /dev/mdX --add
   /dev/sdX</command>. Replace <literal>X</literal> with your particular device
   identifiers. This integrates the hard disk automatically into the RAID
   system and fully reconstructs it (for all RAID levels except for
   RAID&nbsp;0).
  </para>

  <para>
   Although you can access all data during the rebuild, you might encounter
   some performance issues until the RAID has been fully rebuilt.
  </para>

  <sect1 xml:id="sec-raid-trouble-autorecovery">
   <title>Recovery after failing disk is back again</title>
   <para>
    There are several reasons a disk included in a RAID array may fail. Here is
    a list of the most common ones:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Problems with the disk media.
     </para>
    </listitem>
    <listitem>
     <para>
      Disk drive controller failure.
     </para>
    </listitem>
    <listitem>
     <para>
      Broken connection to the disk.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    In the case of disk media or controller failure, the device needs to be
    replaced or repaired. If a hot spare was not configured within the RAID,
    then manual intervention is required.
   </para>
   <para>
    In the last case, the failed device can be automatically re-added with the
    <command>mdadm</command> command after the connection is repaired (which
    might be automatic).
   </para>
   <para>
    Because <command>md</command>/<command>mdadm</command> cannot reliably
    determine what caused the disk failure, it assumes a serious disk error and
    treats any failed device as faulty until it is explicitly told that the
    device is reliable.
   </para>
   <para>
    Under some circumstances&mdash;such as storage devices with an internal
    RAID array&mdash;connection problems are very often the cause of the
    device failure. In such case, you can tell <command>mdadm</command> that it
    is safe to automatically <option>--re-add</option> the device after it
    appears. You can do this by adding the following line to
    <filename>/etc/mdadm.conf</filename>:
   </para>
<screen>POLICY action=re-add</screen>
   <para>
    Note that the device will be automatically re-added after re-appearing only
    if the <systemitem>udev</systemitem> rules cause <command>mdadm -I
    <replaceable>DISK_DEVICE_NAME</replaceable></command> to be run on any
    device that spontaneously appears (default behavior), and if write-intent
    bitmaps are configured (they are by default).
   </para>
   <para>
    If you want this policy to only apply to some devices and not to the
    others, then the <literal>path=</literal> option can be added to the
    <literal>POLICY</literal> line in <filename>/etc/mdadm.conf</filename> to
    restrict the non-default action to only selected devices. Wild cards can be
    used to identify groups of devices. See <command>man 5 mdadm.conf</command>
    for more information.
   </para>
  
 </sect1>
 
 </chapter>
