msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"POT-Creation-Date: 2018-08-18 22:20+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#. Put one translator per line, in the form NAME <EMAIL>, YEAR1, YEAR2
msgctxt "_"
msgid "translator-credits"
msgstr ""

#. (itstool) path: chapter/title
#: xml/tuning_network.xml:21
msgid "Tuning the Network"
msgstr ""

#. (itstool) path: chapter/para
#: xml/tuning_network.xml:28
msgid "The network subsystem is complex and its tuning highly depends on the system use scenario and on external factors such as software clients or hardware components (switches, routers, or gateways) in your network. The Linux kernel aims more at reliability and low latency than low overhead and high throughput. Other settings can mean less security, but better performance."
msgstr ""

#. (itstool) path: sect1/title
#: xml/tuning_network.xml:37
msgid "Configurable Kernel Socket Buffers"
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:39
msgid "Networking is largely based on the TCP/IP protocol and a socket interface for communication; for more information about TCP/IP, see <xref linkend=\"cha.basicnet\"/>. The Linux kernel handles data it receives or sends via the socket interface in socket buffers. These kernel socket buffers are tunable."
msgstr ""

#. (itstool) path: important/title
#: xml/tuning_network.xml:50
msgid "TCP Autotuning"
msgstr ""

#. (itstool) path: important/para
#: xml/tuning_network.xml:51
msgid "Since kernel version 2.6.17 full autotuning with 4 MB maximum buffer size exists. This means that manual tuning usually will not improve networking performance considerably. It is often the best not to touch the following variables, or, at least, to check the outcome of tuning efforts carefully."
msgstr ""

#. (itstool) path: important/para
#: xml/tuning_network.xml:58
msgid "If you update from an older kernel, it is recommended to remove manual TCP tunings in favor of the autotuning feature."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:64
msgid "The special files in the <filename>/proc</filename> file system can modify the size and behavior of kernel socket buffers; for general information about the <filename>/proc</filename> file system, see <xref linkend=\"sec.util.proc\"/>. Find networking related files in:"
msgstr ""

#. (itstool) path: sect1/screen
#: xml/tuning_network.xml:71
#, no-wrap
msgid ""
"/proc/sys/net/core\n"
"/proc/sys/net/ipv4\n"
"/proc/sys/net/ipv6"
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:75
msgid "General <systemitem>net</systemitem> variables are explained in the kernel documentation (<filename>linux/Documentation/sysctl/net.txt</filename>). Special <systemitem>ipv4</systemitem> variables are explained in <filename>linux/Documentation/networking/ip-sysctl.txt</filename> and <filename>linux/Documentation/networking/ipvs-sysctl.txt</filename>."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:84
msgid "In the <filename>/proc</filename> file system, for example, it is possible to either set the Maximum Socket Receive Buffer and Maximum Socket Send Buffer for all protocols, or both these options for the TCP protocol only (in <filename>ipv4</filename>) and thus overriding the setting for all protocols (in <filename>core</filename>)."
msgstr ""

#. (itstool) path: varlistentry/term
#: xml/tuning_network.xml:99
msgid "<filename>/proc/sys/net/ipv4/tcp_moderate_rcvbuf</filename>"
msgstr ""

#. (itstool) path: listitem/para
#: xml/tuning_network.xml:102
msgid "If <filename>/proc/sys/net/ipv4/tcp_moderate_rcvbuf</filename> is set to <literal>1</literal>, autotuning is active and buffer size is adjusted dynamically."
msgstr ""

#. (itstool) path: varlistentry/term
#: xml/tuning_network.xml:110
msgid "<filename>/proc/sys/net/ipv4/tcp_rmem</filename>"
msgstr ""

#. (itstool) path: listitem/para
#: xml/tuning_network.xml:113
msgid "The three values setting the minimum, initial, and maximum size of the Memory Receive Buffer per connection. They define the actual memory usage, not only TCP window size."
msgstr ""

#. (itstool) path: varlistentry/term
#: xml/tuning_network.xml:121
msgid "<filename>/proc/sys/net/ipv4/tcp_wmem</filename>"
msgstr ""

#. (itstool) path: listitem/para
#: xml/tuning_network.xml:124
msgid "The same as <filename>tcp_rmem</filename>, but for Memory Send Buffer per connection."
msgstr ""

#. (itstool) path: varlistentry/term
#: xml/tuning_network.xml:131
msgid "<filename>/proc/sys/net/core/rmem_max</filename>"
msgstr ""

#. (itstool) path: listitem/para
#: xml/tuning_network.xml:134
msgid "Set to limit the maximum receive buffer size that applications can request."
msgstr ""

#. (itstool) path: varlistentry/term
#: xml/tuning_network.xml:141
msgid "<filename>/proc/sys/net/core/wmem_max</filename>"
msgstr ""

#. (itstool) path: listitem/para
#: xml/tuning_network.xml:144
msgid "Set to limit the maximum send buffer size that applications can request."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:152
msgid "Via <filename>/proc</filename> it is possible to disable TCP features that you do not need (all TCP features are switched on by default). For example, check the following files:"
msgstr ""

#. (itstool) path: varlistentry/term
#: xml/tuning_network.xml:160
msgid "<filename>/proc/sys/net/ipv4/tcp_timestamps</filename>"
msgstr ""

#. (itstool) path: listitem/para
#: xml/tuning_network.xml:163
msgid "TCP time stamps are defined in RFC1323."
msgstr ""

#. (itstool) path: varlistentry/term
#: xml/tuning_network.xml:169
msgid "<filename>/proc/sys/net/ipv4/tcp_window_scaling</filename>"
msgstr ""

#. (itstool) path: listitem/para
#: xml/tuning_network.xml:172
msgid "TCP window scaling is also defined in RFC1323."
msgstr ""

#. (itstool) path: varlistentry/term
#: xml/tuning_network.xml:178
msgid "<filename>/proc/sys/net/ipv4/tcp_sack</filename>"
msgstr ""

#. (itstool) path: listitem/para
#: xml/tuning_network.xml:181
msgid "Select acknowledgments (SACKS)."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:188
msgid "Use <command>sysctl</command> to read or write variables of the <filename>/proc</filename> file system. <command>sysctl</command> is preferable to <command>cat</command> (for reading) and <command>echo</command> (for writing), because it also reads settings from <filename>/etc/sysctl.conf</filename> and, thus, those settings survive reboots reliably. With <command>sysctl</command> you can read all variables and their values easily; as <systemitem class=\"username\">root</systemitem> use the following command to list TCP related settings:"
msgstr ""

#. (itstool) path: sect1/screen
#: xml/tuning_network.xml:199
#, no-wrap
msgid ""
"sysctl -a | grep tcp"
msgstr ""

#. (itstool) path: note/title
#: xml/tuning_network.xml:205
msgid "Side-Effects of Tuning Network Variables"
msgstr ""

#. (itstool) path: note/para
#: xml/tuning_network.xml:206
msgid "Tuning network variables can affect other system resources such as CPU or memory use."
msgstr ""

#. (itstool) path: sect1/title
#: xml/tuning_network.xml:217
msgid "Detecting Network Bottlenecks and Analyzing Network Traffic"
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:219
msgid "Before starting with network tuning, it is important to isolate network bottlenecks and network traffic patterns. There are some tools that can help you with detecting those bottlenecks."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:225
msgid "The following tools can help analyzing your network traffic: <command>netstat</command>, <command>tcpdump</command>, and <command>wireshark</command>. Wireshark is a network traffic analyzer."
msgstr ""

#. (itstool) path: sect1/title
#: xml/tuning_network.xml:240
msgid "Netfilter"
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:242
msgid "The Linux firewall and masquerading features are provided by the Netfilter kernel modules. This is a highly configurable rule based framework. If a rule matches a packet, Netfilter accepts or denies it or takes special action (<quote>target</quote>) as defined by rules such as address translation."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:250
msgid "There are quite a lot of properties Netfilter can take into account. Thus, the more rules are defined, the longer packet processing may last. Also advanced connection tracking could be rather expensive and, thus, slowing down overall networking."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:259
msgid "When the kernel queue becomes full, all new packets are dropped, causing existing connections to fail. The 'fail-open' feature allows a user to temporarily disable the packet inspection and maintain the connectivity under heavy network traffic. For reference, see <link xlink:href=\"https://home.regit.org/netfilter-en/using-nfqueue-and-libnetfilter_queue/\"/>."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:267
msgid "For more information, see the home page of the Netfilter and iptables project, <link xlink:href=\"http://www.netfilter.org\"/>"
msgstr ""

#. (itstool) path: sect1/title
#: xml/tuning_network.xml:273
msgid "Improving the Network Performance with Receive Packet Steering (RPS)"
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:275
msgid "Modern network interface devices can move so many packets that the host can become the limiting factor for achieving maximum performance. To keep up, the system must be able to distribute the work across multiple CPU cores."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:282
msgid "Some modern network interfaces can help distribute the work to multiple CPU cores through the implementation of multiple transmission and multiple receive queues in hardware. However, others are only equipped with a single queue and the driver must deal with all incoming packets in a single, serialized stream. To work around this issue, the operating system must \"parallelize\" the stream to distribute the work across multiple CPUs. On <phrase role=\"productname\"><phrase os=\"osuse\">openSUSE Leap</phrase><phrase os=\"sles\">SUSE Linux Enterprise Server</phrase><phrase os=\"sled\">SUSE Linux Enterprise Desktop</phrase><phrase os=\"slerte\">SUSE Linux Enterprise Real Time Extension</phrase></phrase> this is done via Receive Packet Steering (RPS). RPS can also be used in virtual environments."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:293
msgid "RPS creates a unique hash for each data stream using IP addresses and port numbers. The use of this hash ensures that packets for the same data stream are sent to the same CPU, which helps to increase performance."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:299
msgid "RPS is configured per network device receive queue and interface. The configuration file names match the following scheme:"
msgstr ""

#. (itstool) path: sect1/screen
#: xml/tuning_network.xml:304
#, no-wrap
msgid ""
"/sys/class/net/<replaceable>&lt;device&gt;</replaceable>/queues/<replaceable>&lt;rx-queue&gt;</replaceable>/rps_cpus"
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:306
msgid "<replaceable>&lt;device&gt;</replaceable> stands for the network device, such as <literal>eth0</literal>, <literal>eth1</literal>. <replaceable>&lt;rx-queue&gt;</replaceable> stands for the receive queue, such as <literal>rx-0</literal>, <literal>rx-1</literal>."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:313
msgid "If the network interface hardware only supports a single receive queue, only <literal>rx-0</literal> will exist. If it supports multiple receive queues, there will be an rx-<replaceable>N</replaceable> directory for each receive queue."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:320
msgid "These configuration files contain a comma-delimited list of CPU bitmaps. By default, all bits are set to <literal>0</literal>. With this setting RPS is disabled and therefore the CPU that handles the interrupt will also process the packet queue."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:327
msgid "To enable RPS and enable specific CPUs to process packets for the receive queue of the interface, set the value of their positions in the bitmap to <literal>1</literal>. For example, to enable CPUs 0-3 to process packets for the first receive queue for eth0, set the bit positions 0-3 to 1 in binary: <literal>00001111</literal>. This representation then needs to be converted to hex—which results in <literal>F</literal> in this case. Set this hex value with the following command:"
msgstr ""

#. (itstool) path: sect1/screen
#: xml/tuning_network.xml:337
#, no-wrap
msgid ""
"echo \"f\" &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus"
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:339
msgid "If you wanted to enable CPUs 8-15:"
msgstr ""

#. (itstool) path: sect1/screen
#: xml/tuning_network.xml:343
#, no-wrap
msgid ""
"1111 1111 0000 0000 (binary)\n"
"15     15    0    0 (decimal)\n"
"F       F    0    0 (hex)"
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:347
msgid "The command to set the hex value of <literal>ff00</literal> would be:"
msgstr ""

#. (itstool) path: sect1/screen
#: xml/tuning_network.xml:351
#, no-wrap
msgid ""
"echo \"ff00\" &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus"
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:353
msgid "On NUMA machines, best performance can be achieved by configuring RPS to use the CPUs on the same NUMA node as the interrupt for the interface's receive queue."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:359
msgid "On non-NUMA machines, all CPUs can be used. If the interrupt rate is very high, excluding the CPU handling the network interface can boost performance. The CPU being used for the network interface can be determined from <filename>/proc/interrupts</filename>. For example:"
msgstr ""

#. (itstool) path: sect1/screen
#: xml/tuning_network.xml:366
#, no-wrap
msgid ""
"<prompt role=\"root\">root # </prompt>cat /proc/interrupts\n"
"            CPU0       CPU1       CPU2       CPU3\n"
"...\n"
"  51:  113915241          0          0          0      Phys-fasteoi   eth0\n"
"..."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:372
msgid "In this case, <literal>CPU 0</literal> is the only CPU processing interrupts for <literal>eth0</literal>, since only <literal>CPU0</literal> contains a non-zero value."
msgstr ""

#. (itstool) path: sect1/para
#: xml/tuning_network.xml:378
msgid "On x86 and AMD64/Intel 64 platforms, <command>irqbalance</command> can be used to distribute hardware interrupts across CPUs. See <command>man 1 irqbalance</command> for more details."
msgstr ""

#. (itstool) path: sect1/title
#: xml/tuning_network.xml:389
msgid "For More Information"
msgstr ""

#. (itstool) path: listitem/para
#: xml/tuning_network.xml:393
msgid "Eduardo Ciliendo, Takechika Kunimasa: <quote>Linux Performance and Tuning Guidelines</quote> (2007), esp. sections 1.5, 3.5, and 4.7: <link xlink:href=\"http://www.redbooks.ibm.com/redpapers/abstracts/redp4285.html\"/>"
msgstr ""

#. (itstool) path: listitem/para
#: xml/tuning_network.xml:400
msgid "John Heffner, Matt Mathis: <quote>Tuning TCP for Linux 2.4 and 2.6</quote> (2006): <link xlink:href=\"http://www.psc.edu/networking/projects/tcptune/#Linux\"/>"
msgstr ""

