<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.xen.ha">
 <title>&xen; as a High-Availability Virtualization Host</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
          </dm:bugtracker>
      </dm:docmanager>
    </info>
    <para>
  Setting up two &xen; hosts as a failover system has several advantages
  compared to a setup where every server runs on dedicated hardware.
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    Failure of a single server does not cause major interruption of the
    service.
   </para>
  </listitem>
  <listitem>
   <para>
    A single big machine is normally way cheaper than multiple smaller
    machines.
   </para>
  </listitem>
  <listitem>
   <para>
    Adding new servers as needed is a trivial task.
   </para>
  </listitem>
  <listitem>
   <para>
    The usage of the server is improved, which has positive effects on
    the power consumption of the system.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  The setup of migration for &xen; hosts is described in
  <xref linkend="sec.xen.manage.migrate"/>. In the following, several
  typical scenarios are described.
 </para>
 <sect1 xml:id="sec.xen.ha.remote_storage">
  <title>&xen; HA with Remote Storage</title>

<!--
  iSCSI, NBD
  FC/NPIV
  NFS
  -->

  <para>
   &xen; can directly provide several remote block devices to the
   respective &xen; guest systems. These include iSCSI, NPIV, and NBD.
   All of these may be used to do live migrations. When a storage system is
   already in place, first try to use the same device type you already used
   in the network.
  </para>

  <para>
   If the storage system cannot be used directly but provides a possibility
   to offer the needed space over NFS, it is also possible to create image
   files on NFS. If the NFS file system is available on all &xen; host
   systems, this method also allows live migrations of &xen; guests.
  </para>

  <para>
   When setting up a new system, one of the main considerations is whether a
   dedicated storage area network should be implemented. The following
   possibilities are available:
  </para>

<!-- method, complexity/cost, comments -->

  <table xml:id="tab.xen.remote_storage">
   <title>&xen; Remote Storage</title>
   <tgroup cols="3">
    <thead>
     <row>
      <entry>
       <para>
        Method
       </para>
      </entry>
      <entry>
       <para>
        Complexity
       </para>
      </entry>
      <entry>
       <para>
        Comments
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        Ethernet
       </para>
      </entry>
      <entry>
       <para>
        low
       </para>
      </entry>
      <entry>
       <para>
        Note that all block device traffic goes over the same Ethernet
        interface as the network traffic. This may be limiting the
        performance of the guest.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Ethernet dedicated to storage.
       </para>
      </entry>
      <entry>
       <para>
        medium
       </para>
      </entry>
      <entry>
       <para>
        Running the storage traffic over a dedicated Ethernet interface may
        eliminate a bottleneck on the server side. However, planning your
        own network with your own IP address range and possibly a VLAN
        dedicated to storage requires numerous considerations.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        NPIV
       </para>
      </entry>
      <entry>
       <para>
        high
       </para>
      </entry>
      <entry>
       <para>
        NPIV is a method to virtualize Fibre channel connections. This is
        available with adapters that support a data rate of at least 4
        Gbit/s and allows the setup of complex storage systems.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <para>
   Typically, a 1 Gbit/s Ethernet device can fully use a typical
   hard disk or storage system. When using very fast storage systems, such
   an Ethernet device will probably limit the speed of the system.
  </para>
 </sect1>
 <sect1 xml:id="sec.xen.ha.local_storage">
  <title>&xen; HA with Local Storage</title>

  <para>
   For space or budget reasons, it may be necessary to rely on storage that
   is local to the &xen; host systems. To still maintain the possibility
   of live migrations, it is necessary to build block devices that are
   mirrored to both &xen; hosts. The software that allows this is called
   Distributed Replicated Block Device (DRBD).
  </para>

  <para>
   If a system that uses DRBD to mirror the block devices or files between
   two &xen; hosts should be set up, both hosts should use the identical
   hardware. If one of the hosts has slower hard disks, both hosts will
   suffer from this limitation.
  </para>

  <para>
   During the setup, each of the required block devices should use its own
   DRBD device. The setup of such a system is quite a complex task.
  </para>
 </sect1>
 <sect1 xml:id="sec.xen.ha.private_bridge">
  <title>&xen; HA and Private Bridges</title>

  <para>
   When using several guest systems that need to communicate between each
   other, it is possible to do this over the regular interface. However, for
   security reasons it may be advisable to create a bridge that is only
   connected to guest systems.
  </para>

  <para>
   In an HA environment that also should support live migrations, such a
   private bridge must be connected to the other &xen; hosts. This is possible
   by using dedicated physical Ethernet devices and a dedicated network.
  </para>

  <para>
   A different implementation method is using VLAN interfaces. In that case,
   all the traffic goes over the regular Ethernet interface. However, the
   VLAN interface does not get the regular traffic, because only the VLAN
   packets that are tagged for the correct VLAN are forwarded.
  </para>

  <para>
   For more information about the setup of a VLAN interface see
   <xref linkend="sec.xen.net.vlan"/>.
  </para>
 </sect1>
</chapter>
