<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.xen.admin">
 <title>Administrative Tasks</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
          </dm:bugtracker>
      </dm:docmanager>
    </info>
    <para/>
 <sect1 xml:id="sec.xen.config.bootloader">
  <title>The Boot Loader Program</title>

  <remark condition="clarity">
    2014-02-10 - fs: FIXME: These entries needs to be adjusted to GRUB 2.
   </remark>

  <para>
   The boot loader controls how the virtualization software boots and runs.
   You can modify the boot loader properties by using &yast;, or by
   directly editing the boot loader configuration file.
  </para>

  <para>
   The &yast; boot loader program is located at <menuchoice>
   <guimenu>&yast;</guimenu> <guimenu>System</guimenu> <guimenu>Boot
   Loader</guimenu> </menuchoice>. Click the <guimenu>Bootloader
   Options</guimenu> tab and select the line containing the &xen; kernel
   as the <guimenu>Default Boot Section</guimenu>.
  </para>

  <figure>
   <title>Boot Loader Settings</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="xen_bootloader.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="xen_bootloader.png" width="80%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Confirm with <guimenu>OK</guimenu>. Next time you boot the host, it will
   be ready to provide the &xen; virtualization environment.
  </para>

  <para>
   You can use the Boot Loader program to specify functionality, such as:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Pass kernel command line parameters.
    </para>
   </listitem>
   <listitem>
    <para>
     Specify the kernel image and initial RAM disk.
    </para>
   </listitem>
   <listitem>
    <para>
     Select a specific hypervisor.
    </para>
   </listitem>
   <listitem>
    <para>
     Pass additional parameters to the hypervisor. See
     <link xlink:href="http://xenbits.xen.org/docs/unstable/misc/xen-command-line.html"/>
     for their complete list.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   You can customize your virtualization environment by editing the
   <filename>/etc/default/grub</filename> file. Add the following line to
   this file:
   <literal>GRUB_CMDLINE_XEN="&lt;boot_parameters&gt;"</literal>. Do not
   forget to run <command>grub2-mkconfig -o /boot/grub2/grub.cfg</command>
   after editing the file.
  </para>
 </sect1>
 <sect1 xml:id="sec.xen.config.sparse">
  <title>Sparse Image Files and Disk Space</title>

  <para>
   If the host’s physical disk reaches a state where it has no available
   space, a virtual machine using a virtual disk based on a sparse image
   file cannot write to its disk. Consequently, it reports I/O errors.
  </para>

  <para>
   If this situation
   occurs, you should free up available space on the physical disk, remount
   the virtual machine’s file system, and set the file system back to
   read-write.
  </para>

  <para>
   To check the actual disk requirements of a sparse image file, use the
   command <command>du -h &lt;image file&gt;</command>.
  </para>

  <para>
   To increase the available space of a sparse image file, first increase
   the file size and then the file system.
  </para>

  <warning>
   <title>Back Up Before Resizing</title>
   <para>
    Touching the sizes of partitions or sparse files always bears the risk
    of data failure. Do not work without a backup.
   </para>
  </warning>

  <para>
   The resizing of the image file can be done online, while the &vmguest;
   is running. Increase the size of a sparse image file with:
  </para>

<screen>&prompt.user;sudo dd if=/dev/zero of=&lt;image file&gt; count=0 bs=1M seek=&lt;new size in MB&gt;</screen>

  <para>
   For example, to increase the file
   <filename>/var/lib/xen/images/sles/disk0</filename> to a size of 16GB,
   use the command:
  </para>

<screen>&prompt.user;sudo dd if=/dev/zero of=/var/lib/xen/images/sles/disk0 count=0 bs=1M seek=16000</screen>

  <note>
   <title>Increasing Non-Sparse Images</title>
   <para>
    It is also possible to increase the image files of devices that are not
    sparse files. However, you must know exactly where the previous image
    ends. Use the seek parameter to point to the end of the image file and
    use a command similar to the following:
   </para>
<screen>&prompt.user;sudo dd if=/dev/zero of=/var/lib/xen/images/sles/disk0 seek=8000 bs=1M count=2000</screen>
  </note>

  <para>
   Be sure to use the right seek, else data loss may happen.
  </para>

  <para>
   If the &vmguest; is running during the resize operation, also resize
   the loop device that provides the image file to the &vmguest;. First
   detect the correct loop device with the command:
  </para>

<screen>&prompt.user;sudo losetup -j /var/lib/xen/images/sles/disk0</screen>

  <para>
   Then resize the loop device, for example <filename>/dev/loop0</filename>,
   with the following command:
  </para>

<screen>&prompt.user;sudo losetup -c /dev/loop0</screen>

  <para>
   Finally check the size of the block device inside the guest system with
   the command <command>fdisk -l /dev/xvdb</command>. The device name
   depends on the actually increased device.
  </para>

  <para>
   The resizing of the file system inside the sparse file involves tools that
   are depending on the actual file system. <phrase os="sles">This is
   described in detail in the <xref linkend="stor_admin"/>.</phrase>
  </para>
 </sect1>
 <sect1 xml:id="sec.xen.manage.migrate">
  <title>Migrating &xen; &vmguest; Systems</title>

  <para>
   With &xen; it is possible to migrate a &vmguest; system from one
   &vmhost; to another with almost no service interruption. This could be
   used for example to move a busy &vmguest; to a &vmhost; that has
   stronger hardware or is not yet loaded. Or, if a service of a &vmhost;
   is required, all &vmguest; systems running on this machine can be
   migrated to other machines to avoid interruption of service.
   These are only two examples&mdash;many more reasons may apply to your
   personal situation.
  </para>

  <para>
   Before starting, some preliminary considerations regarding the
   &vmhost; should be taken into account:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     All &vmhost; systems should use a similar CPU. The frequency is not
     so important, but they should be using the same CPU family. To get more
     information about the used CPU, see <command>cat
     /proc/cpuinfo</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     All resources that are used by a specific guest system must be
     available on all involved &vmhost; systems&mdash;for example all used block devices
     must exist on both &vmhost; systems.
    </para>
   </listitem>
   <listitem>
    <para>
     If the hosts included in the migration process run in different subnets,
     make sure that either DHCP relay is available to the guests,
     or for guests with static network configuration, set up the network manually.
    </para>
   </listitem>
   <listitem>
    <para>
     Using special features like <literal>&pciback;</literal> may be
     problematic. Do not implement these when deploying for an environment
     that should migrate &vmguest; systems between different &vmhost;
     systems.
    </para>
   </listitem>
   <listitem>
    <para>
     For fast migrations, a fast network is mandatory. If possible, use GB
     Ethernet and fast switches. Deploying VLAN might also help avoid
     collisions.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="sec.xen.manage.migrate.block">
   <title>Preparing Block Devices for Migrations</title>
   <para>
    The block devices needed by the &vmguest; system must be available on
    all involved &vmhost; systems. This is done by implementing some kind
    of shared storage that serves as container for the root file system of
    the migrated &vmguest; system. Common possibilities include:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <literal>iSCSI</literal> can be set up to give access to the same block
      devices from different systems at the same time. <phrase os="sles">For
      more information about iSCSI, see <xref linkend="cha.iscsi"/>.</phrase>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>NFS</literal> is a widely used root file system that can
      easily be accessed from different locations. For more information, see
      <xref linkend="cha.nfs"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>DRBD</literal> can be used if only two &vmhost; systems
      are involved. This gives some extra data security, because the used
      data is mirrored over the network. <phrase os="sles;sled">For more information, see the
      <citetitle>SUSE Linux Enterprise High Availability Extension
      &productnumber;</citetitle> documentation at
      <link xlink:href="http://www.suse.com/doc/"/></phrase>.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>SCSI</literal> can also be used if the available hardware
      permits shared access to the same disks.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>NPIV</literal> is a special mode to use Fibre channel disks.
      However, in this case all migration hosts must be attached to the same
      Fibre channel switch. For more information about NPIV, see
      <xref linkend="sec.xen.config.disk"/>. Commonly, this works if the
      Fibre channel environment supports 4 Gbit or faster connections.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec.xen.manage.migrate.xl">
   <title>Migrating &vmguest; Systems</title>
   <para>
    The actual migration of the &vmguest; system is done with the
    command:
   </para>
<screen>&prompt.user;sudo xl migrate &lt;domain_name&gt; &lt;host&gt;</screen>
   <para>
    The speed of the migration depends on how fast the memory print can be
    saved to disk, sent to the new &vmhost; and loaded there. This means
    that small &vmguest; systems can be migrated faster than big systems
    with a lot of memory.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.xen.monitor">
  <title>Monitoring &xen;</title>

  <para>
   For a regular operation of many virtual guests, having a possibility to
   check the sanity of all the different &vmguest; systems is
   indispensable. &xen; offers several tools besides the system tools to
   gather information about the system.
  </para>

  <tip>
   <title>Monitoring the &vmhost;</title>
   <para>
    Basic monitoring of the &vmhost; (I/O and CPU) is available via the
    &vmm;. Refer to
    <xref linkend="cha.libvirt.admin.monitor.virt-manager"/> for details.
   </para>
  </tip>

  <sect2 xml:id="sec.xen.monitor.xentop">
   <title>Monitor &xen; with <command>xentop</command></title>
   <para>
    The preferred terminal application to gather information about &xen;
    virtual environment is <command>xentop</command>. Unfortunately, this
    tool needs a rather broad terminal, else it inserts line breaks into the
    display.
   </para>
   <para>
    <command>xentop</command> has several command keys that can give you
    more information about the system that is monitored. Some of the more
    important are:
   </para>
   <variablelist>
    <varlistentry>
     <term>D</term>
     <listitem>
      <para>
       Change the delay between the refreshes of the screen.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>N</term>
     <listitem>
      <para>
       Also display network statistics. Note, that only standard
       configurations will be displayed. If you use a special configuration
       like a routed network, no network will be displayed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>B</term>
     <listitem>
      <para>
       Display the respective block devices and their cumulated usage count.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For more information about <command>xentop</command> see the manual page
    <command>man 1 xentop</command>.
   </para>

   <tip>
    <title><command>virt-top</command></title>
    <para>
     libvirt offers the hypervisor-agnostic tool <command>virt-top</command>,
     which is recommended for monitoring &vmguest;s. See <xref
     linkend="cha.libvirt.admin.monitor.virt-top"/> for details.
    </para>
   </tip>

  </sect2>

  <sect2 xml:id="sec.xen.monitor.tools">
   <title>Additional Tools</title>
   <para>
    There are many system tools that also help monitoring or debugging a
    running <phrase os="sles;sled">&sle;</phrase><phrase
    os="osuse">&opensuse;</phrase> system. Many of these are covered in
    <xref linkend="cha.util"/>. Especially useful for monitoring a
    virtualization environment are the following tools:
   </para>
   <variablelist>
    <varlistentry>
     <term>ip</term>
     <listitem>
      <para>
       The command line utility <command>ip</command> may be used to monitor
       arbitrary network interfaces. This is especially useful if you have
       set up a network that is routed or applied a masqueraded network. To
       monitor a network interface with the name
       <literal>&xenguest;.0</literal>, run the following command:
      </para>
<screen>&prompt.user;watch ip -s link show &xenguest;.0</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>brctl</term>
     <listitem>
      <para>
       In a standard setup, all the &xen; &vmguest; systems are
       attached to a virtual network bridge. <command>brctl</command> allows
       you to determine the connection between the bridge and the virtual
       network adapter in the &vmguest; system. For example, the output
       of <command>brctl show</command> may look like the following:
      </para>
<screen>bridge name     bridge id               STP enabled     interfaces
br0             8000.000476f060cc       no              eth0
                                                        vif1.0
br1             8000.00001cb5a9e7       no              vlan22 </screen>
      <para>
       This shows that there are two virtual bridges defined on the system.
       One is connected to the physical Ethernet device
       <literal>eth0</literal>, the other one is connected to a VLAN
       interface <literal>vlan22</literal>.
      </para>
      <para>
       There is only one guest interface active in this setup,
       <literal>vif1.0</literal>. This means that the guest with ID 1 has an
       Ethernet interface <literal>eth0</literal> assigned, that is
       connected to <literal>br0</literal> in the &vmhost;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>iptables-save</term>
     <listitem>
      <para>
       Especially when using masquerade networks, or if several Ethernet
       interfaces are set up together with a firewall setup, it may be
       helpful to check the current firewall rules.
      </para>
      <para>
       The command <command>iptables</command> may be used to check all the
       different firewall settings. To list all the rules of a chain, or
       even of the complete setup, you may use the commands
       <command>iptables-save</command> or <command>iptables -S</command>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.xen.admin.vhostmd">
  <title>Providing Host Information for &vmguest; Systems</title>

  <para>
   In a standard &xen; environment, the &vmguest; systems have only
   very limited information about the &vmhost; system they are running
   on. If a guest should know more about the &vmhost; it runs on,
   <systemitem>vhostmd</systemitem> can provide more information to selected
   guests. To set up your system to run <systemitem>vhostmd</systemitem>,
   proceed as follows:
  </para>

  <procedure>
   <step>
    <para>
     Install the package vhostmd on the &vmhost;.
    </para>
   </step>
   <step>
    <para>
     To add or remove <literal>metric</literal> sections from the
     configuration, edit the file
     <filename>/etc/vhostmd/vhostmd.conf</filename>. However, the default works
     well.
    </para>
   </step>
   <step>
    <para>
     Check the validity of the <filename>vhostmd.conf</filename>
     configuration file with the command:
    </para>
<screen>&prompt.user;cd /etc/vhostmd
&prompt.user;xmllint --postvalid --noout vhostmd.conf
    </screen>
   </step>
   <step>
    <para>
     Start the vhostmd daemon with the command <command>sudo systemctl start
     vhostmd</command>.
    </para>
    <para>
     If vhostmd should be started automatically during start-up of the
     system, run the command:
    </para>
<screen>&prompt.user;sudo systemctl enable vhostmd</screen>
   </step>
   <step>
    <para>
     Attach the image file <filename>/dev/shm/vhostmd0</filename> to the
     &vmguest; system named &xenguest; with the command:
    </para>
<screen>&prompt.user;xl block-attach opensuse /dev/shm/vhostmd0,,xvdb,ro</screen>
   </step>
   <step>
    <para>
     Log on the &vmguest; system.
    </para>
   </step>
   <step>
    <para>
     Install the client package <systemitem>vm-dump-metrics</systemitem>.
    </para>
   </step>
   <step>
    <para>
     Run the command <command>vm-dump-metrics</command>. To save the result to
     a file, use the option <systemitem>-d &lt;filename&gt;</systemitem>.
    </para>
   </step>
  </procedure>

  <para>
   The result of the <systemitem>vm-dump-metrics</systemitem> is an XML
   output. The respective metric entries follow the DTD
   <filename>/etc/vhostmd/metric.dtd</filename>.
  </para>

  <para>
   For more information, see the manual pages <command>man 8
   vhostmd</command> and <filename>/usr/share/doc/vhostmd/README</filename>
   on the &vmhost; system. On the guest, see the manual page <command>man
   1 vm-dump-metrics</command>.
  </para>
 </sect1>
</chapter>
