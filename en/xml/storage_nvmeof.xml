<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-nvmeof" xml:lang="en">
 <title>&nvmeof;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
  <abstract>
   <para>
    This chapter describes how to set up an &nvmeof; host and target.
   </para>
  </abstract>
 </info>
 <sect1 xml:id="cha-nvmeof-overview">
  <title>Overview</title>
  <para>
   <emphasis>NVM Express</emphasis> (<emphasis>&nvme;</emphasis>) is an
   interface standard for accessing non-volatile storage, commonly SSD
   disks. &nvme; supports much higher speeds and has a lower latency
   than SATA.
  </para>
  <para>
   <emphasis>&nvmeof;</emphasis> is an architecture to access &nvme;
   storage over different networking fabrics, for example
   <emphasis>RDMA</emphasis> or <emphasis>&nvme; over Fibre
   Channel</emphasis> (<emphasis>&fcnvme;</emphasis>). The role of
   &nvmeof; is similar to iSCSI. To increase the fault-tolerance,
   &nvmeof; has a built-in support for multipathing. The &nvmeof;
   multipathing is not based on the traditional &dmmpio;.
  </para>
  <para>
   The <emphasis>&nvme; host</emphasis> is the machine that connects to
   an &nvme; target. The <emphasis>&nvme; target</emphasis> is the
   machine that shares its &nvme; block devices.
  </para>
  <para>
   &nvme; is supported on &sls; &productnumber;. There are Kernel
   modules available for the &nvme; block storage and &nvmeof; target
   and host.
  </para>
  <para>
   To see if your hardware requires any special consideration, refer to
   <xref linkend="cha-nvmeof-hardware" />.
  </para>
 </sect1>
 <sect1 xml:id="sec-nvmeof-host-configuration">
  <title>Setting Up an &nvmeof; Host</title>
  <para>
   To use &nvmeof;, a target must be available with one of the
   supported networking methods. Supported are &nvme; over Fibre
   Channel and RDMA. The following sections describe how to connect a
   host to an &nvme; target.
  </para>
  <sect2 xml:id="sec-nvmeof-host-configuration-cli">
   <title>Installing Command Line Client</title>
   <para>
    To use &nvmeof;, you need the <command>nvme</command> command line
    tool. Install it with <command>zypper</command>:
   </para>
   <screen>&prompt.sudo;<command>zypper in nvme-cli</command></screen>
   <para>
    Use <command>nvme --help</command> to list all available
    subcommands. Man pages are available for <command>nvme</command>
    subcommands. Consult them by executing <command>man
    nvme-<replaceable>SUBCOMMAND</replaceable></command>. For example,
    to view the man page for the <option>discover</option> subcommand,
    execute <command>man nvme-discover</command>.
   </para>
  </sect2>
  <sect2 xml:id="sec-nvmeof-host-configuration-target-discovery">
   <title>Discovering &nvmeof; Targets</title>
   <para>
    To list available &nvme; subsystems on the &nvmeof; target, you need
    the discovery controller address and service ID.
   </para>
   <screen>&prompt.sudo;<command>nvme discover -t <replaceable>TRANSPORT</replaceable> -a <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> -s <replaceable>SERVICE_ID</replaceable></command></screen>
   <para>
    Replace <replaceable>TRANSPORT</replaceable> with the underlying
    transport medium: <option>loop</option>, <option>rdma</option> or
    <option>fc</option>. Replace
    <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> with the
    address of the discovery controller. For RDMA this should be an
    IPv4 address. Replace <replaceable>SERVICE_ID</replaceable> with
    the transport service ID. If the service is IP based, like RDMA,
    service ID specifies the port number. For Fibre Channel, the service
    ID is not required.
   </para>
   <para>
    The &nvme; hosts only see the subsystems they are allowed to connect
    to.
   </para>
   <para>
    Example:
   </para>
   <screen>&prompt.sudo;<command>nvme discover -t rdma -a 10.0.0.1 -s 4420</command></screen>
   <para>
    For more details, see <command>man nvme-discover</command>.
   </para>
  </sect2>
  <sect2 xml:id="sec-nvmeof-host-configuration-connect-target">
   <title>Connecting to &nvmeof; Targets</title>
   <para>
    After you have identified the &nvme; subsystem, you can connect it
    with the <command>nvme connect</command> command.
   </para>
   <screen>&prompt.sudo;<command>nvme connect -t <replaceable>transport</replaceable> -a <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> -s <replaceable>SERVICE_ID</replaceable> -n <replaceable>SUBSYSTTEM_NQN</replaceable></command></screen>
   <para>
    Replace <replaceable>TRANSPORT</replaceable> with the underlying
    transport medium: <option>loop</option>, <option>rdma</option> or
    <option>fc</option>.
    Replace <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> with
    the address of the discovery controller. For RDMA this should be an
    IPv4 address.
    Replace <replaceable>SERVICE_ID</replaceable> with the transport
    service ID. If the service is IP based, like RDMA, this specifies
    the port number.
    Replace <replaceable>SUBSYSTEM_NQN</replaceable> with the NVMe
    qualified name of the desired subsystem as found by the discovery
    command. <emphasis>NQN</emphasis> is the abbreviation for <emphasis>
    &nvme; Qualified Name</emphasis>. The NQN must be unique.
   </para>
   <para>Example:</para>
   <screen>&prompt.sudo;<command>nvme connect -t rdma -a 10.0.0.1 -s 4420 -n nqn.2014-08.com.example:nvme:nvm-subsystem-sn-d78432</command></screen>
   <para>
    Alternatively, use <command>nvme connect-all</command> to connect
    to all discovered namespaces. For advanced usage please see
    <command>man nvme-connect</command> and <command>man
    nvme-connect-all</command>.
   </para>
  </sect2>
  <sect2 xml:id="sec-nvmeof-host-configuration-multipathing">
   <title>Multipathing</title>
   <para>
    &nvme; native multipathing is enabled by default. To
    print the layout of the multipath devices, use the command
    <command>nvme list-subsys</command>. To disable NVMe native
    multipathing, add <option>nvme-core.multipath=N</option> as a boot
    parameter.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-nvmeof-target-configuration">
  <title>Setting Up an &nvmeof; Target</title>
  <sect2 xml:id="sec-nvmeof-target-configuration-cli">
   <title>Installing Command Line Client</title>
   <para>
    To configure an &nvmeof; target, you need the <command>nvmetcli</command> command line
    tool. Install it with <command>zypper</command>:
   </para>
   <screen>&prompt.sudo;<command>zypper in nvmetcli</command></screen>
   <para>
    The current documentation for <command>nvmetcli</command> is
    available at <link xlink:href="http://git.infradead.org/users/hch/nvmetcli.git/blob_plain/HEAD:/Documentation/nvmetcli.txt"/>.
   </para>
  </sect2>
  <sect2 xml:id="sec-nvmeof-target-configuration-steps">
   <title>Configuration Steps</title>
   <para>
    The following procedure provides an example of how to set up an
    &nvmeof; target.
   </para>
   <para>
    The configuration is stored in a tree structure. Use the command
    <command>cd</command> to navigate. Use <command>ls</command> to
    list objects. You can create new objects with
    <command>create</command>.
   </para>
   <procedure>
    <step>
     <para>
      Start the <command>nvmectli</command> interactive shell:
     </para>
     <screen>&prompt.sudo;<command>nvmetcli</command></screen>
    </step>
    <step>
     <para>
      Create a new port:
     </para>
<screen>&prompt.nvmetcli;<command>cd ports</command>
&prompt.nvmetcli;<command>create 1</command>
&prompt.nvmetcli;<command>ls 1/</command>
o- 1
  o- referrals
  o- subsystems</screen>
    </step>
    <step>
     <para>Create an &nvme; subsystem:</para>
<screen>&prompt.nvmetcli;<command>cd /subsystems</command>
&prompt.nvmetcli;<command>create nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82</command>
&prompt.nvmetcli;<command>cd nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82/</command>
&prompt.nvmetcli;<command>ls</command>
o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
  o- allowed_hosts
  o- namespaces</screen>
    </step>
    <step>
     <para>
      Create a new namespace and set an &nvme; device to it.
     </para>
<screen>&prompt.nvmetcli;<command>cd namespaces</command>
&prompt.nvmetcli;<command>create 1</command>
&prompt.nvmetcli;<command>cd 1</command>
&prompt.nvmetcli;<command>set device path=/dev/nvme0n1</command>
Parameter path is now '/dev/nvme0n1'.</screen>
    </step>
    <step>
     <para>
      Enable the previously created namespace:
     </para>
<screen>&prompt.nvmetcli;<command>cd ..</command>
&prompt.nvmetcli;<command>enable</command>
The Namespace has been enabled.</screen>
    </step>
    <step>
     <para>
      Display the created namespace:
     </para>
<screen>&prompt.nvmetcli;<command>cd ..</command>
&prompt.nvmetcli;<command>ls</command>
o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
  o- allowed_hosts
  o- namespaces
    o- 1</screen>
    </step>
    <step>
     <para>
      Allow all hosts to use the subsystem. Only do this in secure
      environments.
     </para>
<screen>&prompt.nvmetcli;<command>set attr allow_any_host=1</command>
Parameter allow_any_host is now '1'.</screen>
     <para>
      Alternatively, you can allow only specific hosts to connect:
     </para>
<screen>&prompt.nvmetcli;<command>cd nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82/allowed_hosts/</command>
&prompt.nvmetcli;<command>create hostnqn</command></screen>
    </step>
    <step>
     <para>
      List all created objects:
     </para>
<screen>&prompt.nvmetcli;<command>cd /</command>
&prompt.nvmetcli;<command>ls</command>
o- /
  o- hosts
  o- ports
  | o- 1
  |   o- referrals
  |   o- subsystems
  o- subsystems
    o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
      o- allowed_hosts
      o- namespaces
        o- 1</screen>
    </step>
    <step>
     <para>
      Make the target available via RDMA:
     </para>
<screen>&prompt.nvmetcli;<command>cd ports/1/</command>
&prompt.nvmetcli;<command>set addr adrfam=ipv4 trtype=rdma traddr=10.0.0.1 trsvcid=4420</command>
Parameter trtype is now 'rdma'.
Parameter adrfam is now 'ipv4'.
Parameter trsvcid is now '4420'.
Parameter traddr is now '10.0.0.1'.</screen>
     <para>
      Alternatively, you can make it available with Fibre Channel:
     </para>
<screen>&prompt.nvmetcli;<command>cd ports/1/</command>
&prompt.nvmetcli;<command>set addr adrfam=fc trtype=fc traddr=nn-0x1000000044001123:pn-0x2000000055001123 trsvcid=none</command></screen>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="sec-nvmeof-target-configuration-backup-configuration">
   <title>Back Up and Restore Target Configuration</title>
   <para>
    You can save the target configuration in a JSON file with the
    following commands:
   </para>
<screen>&prompt.sudo;<command>nvmetcli</command>
&prompt.nvmetcli;<command>saveconfig nvme-target-backup.json</command></screen>
    <para>
     To restore the configuration, use:
    </para>
    <screen>&prompt.nvmetcli;<command>restore nvme-target-backup.json</command></screen>
    <para>
     You can also wipe the current configuration:
    </para>
    <screen>&prompt.nvmetcli;<command>clear</command></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-nvmeof-hardware">
  <title>Special Hardware Configuration</title>
  <sect2 xml:id="cha-nvmeof-hardware-overview">
   <title>Overview</title>
   <para>
    Some hardware needs special configuration to work correctly. Skim
    the titles of the following sections to see if you are using any of
    the mentioned devices or vendors.
   </para>
  </sect2>
  <sect2 xml:id="cha-nvmeof-hardware-broadcom">
   <title>Broadcom</title>
   <para>
    If you are using the <emphasis>Broadcom Emulex LightPulse Fibre
    Channel SCSI</emphasis> driver, add a Kernel configuration
    parameter on the target and host for the <literal>lpfc</literal>
    module:
   </para>
   <screen>&prompt.sudo;<command>echo "options lpfc lpfc_enable_fc4_type=3" > /etc/modprobe.d/lpfc.conf</command></screen>
   <para>
    Make sure that the Broadcom adapter firmware has at least version
    11.2.156.27. Also make sure that you have the current versions of
    <package>nvme-cli</package>, <package>nvmetlci</package> and the
    Kernel installed.
   </para>
   <para>
    To enable a Fibre Channel port as an &nvme; target, set a module
    parameter <option>lpfc_enable_nvmet=<replaceable>
    COMMA_SEPARATED_WWPNS</replaceable></option>. Only listed WWPNs
    will be configured for target mode. A Fibre Channel port can either
    be configured as target <emphasis>or</emphasis> as initiator.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-nvmeof-more-information">
  <title>More Information</title>
  <para>
   For more details about the abilities of <command>nvme</command>,
   refer to <command>nvme nvme-help</command>.
  </para>
  <para>
   The following links provide a basic introduction to &nvme; and
   &nvmeof;:
  </para>
  <itemizedlist>
   <listitem>
    <para><link xlink:href="http://nvmexpress.org/"/></para>
   </listitem>
   <listitem>
    <para><link xlink:href="http://www.nvmexpress.org/wp-content/uploads/NVMe_Over_Fabrics.pdf"/></para>
   </listitem>
   <listitem>
    <para><link xlink:href="https://storpool.com/blog/demystifying-what-is-nvmeof"/></para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
