<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.tuning.network">
 <title>Tuning the Network</title>
 <para>
  The network subsystem is rather complex and its tuning highly depends on
  the system use scenario and also on external factors such as software
  clients or hardware components (switches, routers, or gateways) in your
  network. The Linux kernel aims more at reliability and low latency than
  low overhead and high throughput. Other settings can mean less security,
  but better performance.
 </para>
 <sect1 id="sec.tuning.network.buffers">
  <title>Configurable Kernel Socket Buffers</title>

  <para>
   Networking is largely based on the TCP/IP protocol and a socket interface
   for communication; for more information about TCP/IP, see
   <xref linkend="cha.basicnet" />. The Linux kernel handles data it
   receives or sends via the socket interface in socket buffers. These
   kernel socket buffers are tunable.
  </para>

<!-- http://www.psc.edu/networking/projects/tcptune/#Linux :-->

  <important>
   <title>TCP Autotuning</title>
   <para>
    Since kernel version 2.6.17 full autotuning with 4 MB maximum buffer
    size exists. This means that manual tuning in most cases will not
    improve networking performance considerably. It is often the best not to
    touch the following variables, or, at least, to check the outcome of
    tuning efforts carefully.
   </para>
   <para>
    If you update from an older kernel, it is recommended to remove manual
    TCP tunings in favor of the autotuning feature.
   </para>
  </important>

  <para>
   The special files in the <filename>/proc</filename> file system can
   modify the size and behavior of kernel socket buffers; for general
   information about the <filename>/proc</filename> file system, see
   <xref linkend="sec.util.proc" />. Find networking related files in:
  </para>

<screen>/proc/sys/net/core
/proc/sys/net/ipv4
/proc/sys/net/ipv6</screen>

  <para>
   General <systemitem>net</systemitem> variables are explained in the
   kernel documentation
   (<filename>linux/Documentation/sysctl/net.txt</filename>). Special
   <systemitem>ipv4</systemitem> variables are explained in
   <filename>linux/Documentation/networking/ip-sysctl.txt</filename> and
   <filename>linux/Documentation/networking/ipvs-sysctl.txt</filename>.
  </para>

  <para>
   In the <filename>/proc</filename> file system, for example, it is
   possible to either set the Maximum Socket Receive Buffer and Maximum
   Socket Send Buffer for all protocols, or both these options for the TCP
   protocol only (in <filename>ipv4</filename>) and thus overriding the
   setting for all protocols (in <filename>core</filename>).
  </para>

<!-- http://www.psc.edu/networking/projects/tcptune/#Linux :
       CHECKIT / FIXME
       setting xxx variable, will disable autotuning completely!
-->

  <variablelist>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_moderate_rcvbuf</filename>
    </term>
    <listitem>
     <para>
      If <filename>/proc/sys/net/ipv4/tcp_moderate_rcvbuf</filename> is set
      to <literal>1</literal>, autotuning is active and buffer size is
      adjusted dynamically.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_rmem</filename>
    </term>
    <listitem>
     <para>
      The three values setting the minimum, initial, and maximum size of the
      Memory Receive Buffer per connection. They define the actual memory
      usage, not only TCP window size.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_wmem</filename>
    </term>
    <listitem>
     <para>
      The same as <filename>tcp_rmem</filename>, but for Memory Send
      Buffer per connection.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/core/rmem_max</filename>
    </term>
    <listitem>
     <para>
      Set to limit the maximum receive buffer size that applications can
      request.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/core/wmem_max</filename>
    </term>
    <listitem>
     <para>
      Set to limit the maximum send buffer size that applications can
      request.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Via <filename>/proc</filename> it is possible to disable TCP features
   that you do not need (all TCP features are switched on by default). For
   example, check the following files:
  </para>

  <variablelist>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_timestamps</filename>
    </term>
    <listitem>
     <para>
      TCP time stamps are defined in RFC1323.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_window_scaling</filename>
    </term>
    <listitem>
     <para>
      TCP window scaling is also defined in RFC1323.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_sack</filename>
    </term>
    <listitem>
     <para>
      Select acknowledgments (SACKS).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Use <command>sysctl</command> to read or write variables of the
   <filename>/proc</filename> file system. <command>sysctl</command> is
   preferable to <command>cat</command> (for reading) and
   <command>echo</command> (for writing), because it also reads settings
   from <filename>/etc/sysctl.conf</filename> and, thus, those settings
   survive reboots reliably. With <command>sysctl</command> you can read all
   variables and their values easily; as &rootuser; use the following
   command to list TCP related settings:
  </para>

<screen>sysctl -a | grep tcp</screen>

<!--
  cf. id="sec.tuning.taskscheduler.cfs.tuning" -->

  <note>
   <title>Side-Effects of Tuning Network Variables</title>
   <para>
    Tuning network variables can affect other system resources such as CPU
    or memory use.
<!-- (p.124)-->
<!--
          Also see "Tuning TCP behavior", ibm p. 130
      -->
   </para>
  </note>
 </sect1>
 <sect1 id="sec.tuning.network.analyzing">
  <title>Detecting Network Bottlenecks and Analyzing Network Traffic</title>

  <para>
   Before starting with network tuning, it is important to isolate network
   bottlenecks and network traffic patterns. There are some tools that can
   help you with detecting those bottlenecks.
  </para>

  <para>
   The following tools can help analyzing your network traffic:
   <command>netstat</command>, <command>tcpdump</command>, and
   <command>wireshark</command>. Wireshark is a network traffic analyzer.
  </para>
 </sect1>
<!-- ibm 33:
      offload depends on the adapater's features.
      ibm 34:
     Bonding: Documentation/networking/bonding.txt.
     link aggregation and load balancing
     Check, whether this is already describe somewhere else!
     Obviously in ha and xen guides...
 -->
 <sect1>
  <title>Netfilter</title>

  <para>
   The Linux firewall and masquerading features are provided by the
   Netfilter kernel modules. This is a highly configurable rule based
   framework. If a rule matches a packet, Netfilter accepts or denies it or
   takes special action (<quote>target</quote>) as defined by rules such as
   address translation.
  </para>

  <para>
   There are quite some properties, Netfilter is able to take into account.
   Thus, the more rules are defined, the longer packet processing may last.
   Also advanced connection tracking could be rather expensive and, thus,
   slowing down overall networking.
  </para>

<!-- : security vs. performance -->

  <para>
   When the kernel queue becomes full, all new packets are dropped, causing
   existing connections to fail. The 'fail-open' feature, available since
   &sls; 11 SP3, allows a user to temporarily disable the packet inspection
   and maintain the connectivity under heavy network traffic. For reference,
   see
   <ulink url="https://home.regit.org/netfilter-en/using-nfqueue-and-libnetfilter_queue/"/>.
  </para>

  <para>
   For more information, see the home page of the Netfilter and iptables
   project, <ulink url="http://www.netfilter.org"/>
  </para>
 </sect1>
<!-- tuning NFS performance:
      http://blogs.techrepublic.com.com/opensource/?p=64&tag=rbxccnbtr1
 -->
<!-- apache is similar -->
 <sect1 id="sec.tuning.network.info">
  <title>For More Information</title>

  <itemizedlist>
   <listitem>
    <para>
     Eduardo Ciliendo, Takechika Kunimasa: <quote>Linux Performance and
     Tuning Guidelines</quote> (2007), esp. sections 1.5, 3.5, and 4.7:
     <ulink
    url="http://www.redbooks.ibm.com/redpapers/abstracts/redp4285.html"
    />
    </para>
   </listitem>
   <listitem>
    <para>
     John Heffner, Matt Mathis: <quote>Tuning TCP for Linux 2.4 and
     2.6</quote> (2006):
     <ulink url="http://www.psc.edu/networking/projects/tcptune/#Linux" />
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
