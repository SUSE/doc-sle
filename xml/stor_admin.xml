<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:novdoc-profile.xsl"
                 type="text/xml" 
                 title="Profiling step"?>
<!DOCTYPE book PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
<!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
]>
<?provo dirname="storage"?>
<book lang="en" id="stor_admin">
 <bookinfo>
  <title>&storage;</title>
  <productname>&productname;</productname>
  <productnumber>&productnumber;</productnumber>
  <date><?dbtimestamp format="B d, Y"?></date>
  <titleabbrev>&storage;</titleabbrev>
  <xi:include href="common_copyright_gfdl.xml"
	      xmlns:xi="http://www.w3.org/2001/XInclude"/>
  <abstract>
   <para>
    &abstract_storage;
   </para>
  </abstract>
 </bookinfo>
 <preface id="preface">
  <title>About This Guide</title>
  <para>
   This guide provides information about how to manage storage devices on a
   &productname; &productnumber; server.
  </para>
  <bridgehead id="audience">Audience</bridgehead>
  <para>
   This guide is intended for system administrators.
  </para>
  <bridgehead id="usercomments">Feedback</bridgehead>
  <para>
   We want to hear your comments and suggestions about this manual and the
   other documentation included with this product. Please use the User
   Comments feature at the bottom of each page of the online documentation,
   or go to <ulink url="http://www.suse.com/doc/feedback.html"/> and enter
   your comments there.
  </para>
  <bridgehead id="docupdates">Documentation Updates</bridgehead>
  <para>
   For the most recent version of the <citetitle>&productname; &productnumber;
   Storage Administration Guide</citetitle>, visit the <ulink
   url="http://www.suse.com/documentation/sles12">SUSE Documentation Web site
   for &productname; &productnumber;</ulink>.
  </para>
  <bridgehead id="additionaldoc">Additional Documentation</bridgehead>
  <para>
   For information about partitioning and managing devices, see <xref
   linkend="cha.advdisk"/>.
  </para>
 </preface>
 <chapter id="filesystems" lang="en">
  <title>Overview of File Systems in Linux</title>
  <para>
   SUSE Linux Enterprise Server ships with a number of different file
   systems from which to choose, including Btrfs, Ext3, Ext2, ReiserFS, and
   XFS. Each file system has its own advantages and disadvantages.
  </para>
  <para>
   Professional high-performance setups might require a highly available
   storage systems. To meet the requirements of high-performance clustering
   scenarios, SUSE Linux Enterprise Server includes OCFS2 (Oracle Cluster
   File System 2) and the Distributed Replicated Block Device (DRBD) in the
   SLES High-Availability Storage Infrastructure (HASI) release. These
   advanced storage systems are not covered in this guide. For information,
   see the
   <ulink url="http://www.suse.com/documentation/sle_ha/book_sleha/data/book_sleha.html"><citetitle>&productname; &productnumber; High Availability Extension
   Guide</citetitle></ulink>.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="sec_filesystems_glossary" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec_filesystems_major" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec_filesystems_add" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec_filesystems_lfs" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sect_stor_limits" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bwkbhpd" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec_filesystems_info" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="sec_filesystems_glossary">
   <title>Terminology</title>

   <variablelist>
    <varlistentry id="bgchzar">
     <term>metadata</term>
     <listitem>
      <para>
       A data structure that is internal to the file system. It assures that
       all of the on-disk data is properly organized and accessible.
       Essentially, it is <quote>data about the data.</quote> Almost every
       file system has its own structure of metadata, which is on reason
       that the file systems show different performance characteristics. It
       is extremely important to maintain metadata intact, because otherwise
       all data on the file system could become inaccessible.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="bgchzas">
     <term>inode</term>
     <listitem>
      <para>
       A data structure on a file system that contains various information
       about a file, including size, number of links, pointers to the disk
       blocks where the file contents are actually stored, and date and time
       of creation, modification, and access.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="bgchzat">
     <term>journal</term>
     <listitem>
      <para>
       In the context of a file system, a journal is an on-disk structure
       containing a type of log in which the file system stores what it is
       about to change in the file system’s metadata. Journaling greatly
       reduces the recovery time of a file system because it has no need for
       the lengthy search process that checks the entire file system at
       system startup. Instead, only the journal is replayed.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect1>
  <sect1 id="sec_filesystems_major">
   <title>Major File Systems in Linux</title>

   <para>
    SUSE Linux Enterprise Server offers a variety of file systems from which
    to choose. This section contains an overview of how these file systems
    work and which advantages they offer.
   </para>

   <para>
    It is very important to remember that no file system best suits all
    kinds of applications. Each file system has its particular strengths and
    weaknesses, which must be taken into account. In addition, even the most
    sophisticated file system cannot replace a reasonable backup strategy.
   </para>

   <para>
    The terms <emphasis>data integrity</emphasis> and <emphasis>data
    consistency</emphasis>, when used in this section, do not refer to the
    consistency of the user space data (the data your application writes to
    its files). Whether this data is consistent must be controlled by the
    application itself.
   </para>

   <important>
    <para>
     Unless stated otherwise in this section, all the steps required to set
     up or change partitions and file systems can be performed by using
     YaST.
    </para>
   </important>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bwk8gda" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_filesystems_major_ext2" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_filesystems_major_ext3" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_filesystems_major_reiser" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_filesystems_major_xfs" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwkryet" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bwk8gda">
    <title>Btrfs</title>
    <para>
     Btrfs is a copy-on-write (COW) file system developed by Chris Mason. It
     is based on COW-friendly B-trees developed by Ohad Rodeh. Btrfs is a
     logging-style file system. Instead of journaling the block changes, it
     writes them in a new location, then links the change in. Until the last
     write, the new changes are not committed.
    </para>
    <important>
     <para>
      Because Btrfs is capable of storing snapshots of the file system, it
      is advisable to reserve twice the amount of disk space than the
      standard storage proposal. This is done automatically by the YaST
      Partitioner in the Btrfs storage proposal for the root file system.
     </para>
    </important>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="b15tkq5m" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkq5n" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5j" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5k" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5l" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5m" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5n" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5o" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5p" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="b15tkq5m">
     <title>Key Features</title>
     <para>
      Btrfs provides fault tolerance, repair, and easy management features,
      such as the following:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Writable snapshots that allow you to easily roll back your system if
        needed after applying updates, or to back up files.
       </para>
      </listitem>
      <listitem>
       <para>
        Multiple device support that allows you to grow or shrink the file
        system. The feature is planned to be available in a future release
        of the YaST Partitioner.
       </para>
       <para>
        Use Btrfs commands to set up transparent compression. Compression
        and Encryption functionality for Btrfs is currently under
        development and is currently not supported on &productname;.
       </para>
      </listitem>
      <listitem>
       <para>
        Different RAID levels for metadata and user data.
       </para>
      </listitem>
      <listitem>
       <para>
        Different checksums for metadata and user data to improve error
        detection.
       </para>
      </listitem>
      <listitem>
       <para>
        Integration with Linux Logical Volume Manager (LVM) storage objects.
       </para>
      </listitem>
      <listitem>
       <para>
        Integration with the YaST Partitioner and AutoYaST on SUSE Linux.
       </para>
      </listitem>
      <listitem>
       <para>
        Offline migration from existing Ext2, Ext3, and Ext4 file systems.
       </para>
      </listitem>
     </itemizedlist>
    </sect3>
    <sect3 id="b15tkq5n">
     <title>Bootloader Support</title>
     <para>
      Bootloader support for <filename>/boot</filename> on Btrfs is planned
      to be available beginning in SUSE Linux Enterprise 12.
     </para>
    </sect3>
    <sect3 id="b15tkr5j">
     <title>Btrfs Subvolumes</title>
     <para>
      Btrfs creates a default subvolume in its assigned pool of space. It
      allows you to create additional subvolumes that act as individual file
      systems within the same pool of space. The number of subvolumes is
      limited only by the space allocated to the pool.
     </para>
     <para>
      If Btrfs is used for the root (<filename>/</filename>) file system,
      the YaST Partitioner automatically prepares the Btrfs file system for
      use with Btrfs subvolumes. You can cover any subdirectory as a
      subvolume. For example,
      <xref linkend="b11qhfrl" xrefstyle="TableTitleOnPage"/> identifies the
      subdirectories that we recommend you treat as subvolumes because they
      contain files that you should not snapshot for the reasons given:
     </para>
     <table id="b11qhfrl" frame="topbot" rowsep="1" pgwide="0" >
      <title>Default Subvolume Handling for Btrfs in YaST</title>
      <tgroup cols="2">
       <colspec colnum="1" colname="1" colwidth="2381*"/>
       <colspec colnum="2" colname="2" colwidth="7620*"/>
       <thead>
        <row id="bwk8pe0">
         <entry>
          <para>
           Path
          </para>
         </entry>
         <entry>
          <para>
           Reason to Cover as a Subvolume
          </para>
         </entry>
        </row>
       </thead>
       <tbody>
        <row id="b11qhf62">
         <entry>
          <para>
           <filename>/opt</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains third-party add-on application software packages.
          </para>
         </entry>
        </row>
        <row id="b11qhf63">
         <entry>
          <para>
           <filename>/srv</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains <filename>http</filename> and <filename>ftp</filename>
           files.
          </para>
         </entry>
        </row>
        <row id="b11qhf64">
         <entry>
          <para>
           <filename>/tmp</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains temporary files.
          </para>
         </entry>
        </row>
        <row id="b11qhf65">
         <entry>
          <para>
           <filename>/var/crash</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains memory dumps of crashed kernels.
          </para>
         </entry>
        </row>
        <row id="b11qhf66">
         <entry>
          <para>
           <filename>/var/log</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains system and applications’ log files, which should never
           be rolled back.
          </para>
         </entry>
        </row>
        <row id="b11qhf67">
         <entry>
          <para>
           <filename>/var/run</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains run-time variable data.
          </para>
         </entry>
        </row>
        <row id="b11qhf68">
         <entry>
          <para>
           <filename>/var/spool</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains data that is awaiting processing by a program, user, or
           administrator, such as news, mail, and printer queues.
          </para>
         </entry>
        </row>
        <row id="b11qhf69">
         <entry>
          <para>
           <filename>/var/tmp</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains temporary files or directories that are preserved
           between system reboots.
          </para>
         </entry>
        </row>
       </tbody>
      </tgroup>
     </table>
     <para>
      After the installation, you can add or remove Btrfs subvolumes by using
      the YaST Expert Partitioner. For information, see <xref
      linkend="yast2.btrfs.yast"/>.
     </para>
    </sect3>
    <sect3 id="b15tkr5k">
     <title>Snapshots for the Root File System</title>
     <para>
      Btrfs provides writable snapshots with the SUSE Snapper infrastructure
      that allow you to easily roll back your system if needed after
      applying updates, or to back up files. Snapper allows you to create
      and delete snapshots, and to compare snapshots and revert the
      differences between them. If Btrfs is used for the root
      (<filename>/</filename>) file system, YaST automatically enables
      snapshots for the root file system.
     </para>
     <para>
      For information about Snapper and its integration in ZYpp
      (<filename>snapper-zypp-plugin</filename>) and YaST
      (<filename>yast2-snapper</filename>), see <xref linkend="cha.snapper"/>.
     </para>
     <para>
      To prevent snapshots from filling up the system disk, you can change
      the Snapper cleanup defaults to be more aggressive in the
      <filename>/etc/snapper/configs/root</filename> configuration file, or
      for other mount points. Snapper provides three algorithms to clean up
      old snapshots that are executed in a daily cron-job. The cleanup
      frequency is defined in the Snapper configuration for the mount point.
      Lower the TIMELINE_LIMIT parameters for daily, monthly, and yearly to
      reduce how long and the number of snapshots to be retained. For
      information, see <xref linkend="sec.snapper.config"/>.
     </para>
     <para>
      For information about the SUSE Snapper project, see the
      <ulink url="http://en.opensuse.org/Portal:Snapper">Snapper Portal wiki
      at OpenSUSE.org</ulink>.
     </para>
    </sect3>
    <sect3 id="b15tkr5l">
     <title>Online Check and Repair Functionality</title>
     <para>
      The <command>scrub</command> check and repair functionality is
      available as part of the Btrfs command line tools. It verifies the
      integrity of data and metadata, assuming the tree structures is fine.
      You can run <command>scrub</command> periodically on a mounted file
      system; it runs as a background process during normal operation.
     </para>
    </sect3>
    <sect3 id="b15tkr5m">
     <title>RAID and Multipath Support</title>
     <para>
      You can create Btrfs on Multiple Devices (MD) and Device Mapper (DM)
      storage configurations by using the YaST Partitioner.
     </para>
    </sect3>
    <sect3 id="b15tkr5n">
     <title>Migration from Ext File Systems to Btrfs</title>
     <para>
      You can migrate data volumes from existing Ext file systems (Ext2,
      Ext3, or Ext4) to the Btrfs file system. The conversion process occurs
      offline and in place on the device. The file system needs least 15% of
      available free space on the device.
     </para>
     <para>
      To convert the Ext file system to Btrfs, take the file system offline,
      then enter:
     </para>
<screen>
btrfs-convert &lt;<replaceable>device</replaceable>&gt;
</screen>
     <para>
      To roll back the migration to the original Ext file system, take the
      file system offline, then enter:
     </para>
<screen>
btrfs-convert -r &lt;<replaceable>device</replaceable>&gt;
</screen>
     <important>
      <para>
       When rolling back to the original Ext file system, all data will be
       lost that you added after the conversion to Btrfs. That is, only the
       original data is converted back to the Ext file system.
      </para>
     </important>
    </sect3>
    <sect3 id="b15tkr5o">
     <title>Btrfs Administration</title>
     <para>
      Btrfs is integrated in the YaST Partitioner and AutoYaST. It is
      available during the installation to allow you to set up a solution
      for the root file system. You can use the YaST Partitioner after the
      install to view and manage Btrfs volumes.
     </para>
     <para>
      Btrfs administration tools are provided in the
      <filename>btrfsprogs</filename> package. For information about using
      Btrfs commands, see the <command>btrfs(8)</command>,
      <command>btrfsck(8)</command>, <command>mkfs.btrfs(8)</command>, and
      <command>btrfsctl(8)</command> man pages. For information about Btrfs
      features, see the
      <ulink url="http://btrfs.wiki.kernel.org"><citetitle>Btrfs
      wiki</citetitle></ulink>.
     </para>
     <para>
      The <command>fsck.btrfs(8)</command> tool will soon be available in
      the SUSE Linux Enterprise update repositories.
     </para>
    </sect3>
    <sect3 id="b15tkr5p">
     <title>Btrfs Quota Support for Subvolumes</title>
     <para>
      The Btrfs <systemitem>root</systemitem> file system subvolumes
      <filename>/var/log</filename>, <filename>/var/crash</filename> and
      <filename>/var/cache</filename> can use all of the available disk
      space during normal operation, and cause a system malfunction. To help
      avoid this situation, SUSE Linux Enterprise now offers Btrfs quota
      support for subvolumes. See the <filename>btrfs(8)</filename> manual
      page for more details.
     </para>
    </sect3>
   </sect2>

   <sect2 id="sec_filesystems_major_ext2">
    <title>Ext2</title>
    <para>
     The origins of Ext2 go back to the early days of Linux history. Its
     predecessor, the Extended File System, was implemented in April 1992
     and integrated in Linux 0.96c. The Extended File System underwent a
     number of modifications and, as Ext2, became the most popular Linux
     file system for years. With the creation of journaling file systems and
     their short recovery times, Ext2 became less important.
    </para>
    <para>
     A brief summary of Ext2’s strengths might help understand why it
     was&mdash;and in some areas still is&mdash;the favorite Linux file
     system of many Linux users.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bi6ymkx" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzb1" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bi6ymkx">
     <title>Solidity and Speed</title>
     <para>
      Being quite an <quote>old-timer,</quote> Ext2 underwent many
      improvements and was heavily tested. This might be the reason why
      people often refer to it as rock-solid. After a system outage when the
      file system could not be cleanly unmounted, e2fsck starts to analyze
      the file system data. Metadata is brought into a consistent state and
      pending files or data blocks are written to a designated directory
      (called <filename>lost+found</filename>). In contrast to journaling
      file systems, e2fsck analyzes the entire file system and not only the
      recently modified bits of metadata. This takes significantly longer
      than checking the log data of a journaling file system. Depending on
      file system size, this procedure can take half an hour or more.
      Therefore, it is not desirable to choose Ext2 for any server that
      needs high availability. However, because Ext2 does not maintain a
      journal and uses significantly less memory, it is sometimes faster
      than other file systems.
     </para>
    </sect3>
    <sect3 id="bgchzb1">
     <title>Easy Upgradability</title>
     <para>
      Because Ext3 is based on the Ext2 code and shares its on-disk format
      as well as its metadata format, upgrades from Ext2 to Ext3 are very
      easy.
     </para>
    </sect3>
   </sect2>

   <sect2 id="sec_filesystems_major_ext3">
    <title>Ext3</title>
    <para>
     Ext3 was designed by Stephen Tweedie. Unlike all other next-generation
     file systems, Ext3 does not follow a completely new design principle.
     It is based on Ext2. These two file systems are very closely related to
     each other. An Ext3 file system can be easily built on top of an Ext2
     file system. The most important difference between Ext2 and Ext3 is
     that Ext3 supports journaling. In summary, Ext3 has three major
     advantages to offer:
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bgchzb3" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzb4" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="sec.filesystems.major.ext22ext3" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="sec_fileystems_major_ext3inodesize" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bgchzb3">
     <title>Easy and Highly Reliable Upgrades from Ext2</title>
     <para>
      The code for Ext2 is the strong foundation on which Ext3 could become
      a highly-acclaimed next-generation file system. Its reliability and
      solidity are elegantly combined in Ext3 with the advantages of a
      journaling file system. Unlike transitions to other journaling file
      systems, such as ReiserFS or XFS, which can be quite tedious (making
      backups of the entire file system and recreating it from scratch), a
      transition to Ext3 is a matter of minutes. It is also very safe,
      because re-creating an entire file system from scratch might not work
      flawlessly. Considering the number of existing Ext2 systems that await
      an upgrade to a journaling file system, you can easily see why Ext3
      might be of some importance to many system administrators. Downgrading
      from Ext3 to Ext2 is as easy as the upgrade. Perform a clean unmount
      of the Ext3 file system and remount it as an Ext2 file system.
     </para>
    </sect3>
    <sect3 id="bgchzb4">
     <title>Reliability and Performance</title>
     <para>
      Some other journaling file systems follow the
      <quote>metadata-only</quote> journaling approach. This means your
      metadata is always kept in a consistent state, but this cannot be
      automatically guaranteed for the file system data itself. Ext3 is
      designed to take care of both metadata and data. The degree of
      <quote>care</quote> can be customized. Enabling Ext3 in the
      <option>data=journal</option> mode offers maximum security (data
      integrity), but can slow down the system because both metadata and
      data are journaled. A relatively new approach is to use the
      <option>data=ordered</option> mode, which ensures both data and
      metadata integrity, but uses journaling only for metadata. The file
      system driver collects all data blocks that correspond to one metadata
      update. These data blocks are written to disk before the metadata is
      updated. As a result, consistency is achieved for metadata and data
      without sacrificing performance. A third option to use is
      <option>data=writeback</option>, which allows data to be written into
      the main file system after its metadata has been committed to the
      journal. This option is often considered the best in performance. It
      can, however, allow old data to reappear in files after crash and
      recovery while internal file system integrity is maintained. Ext3 uses
      the <option>data=ordered</option> option as the default.
     </para>
    </sect3>
    <sect3 id="sec.filesystems.major.ext22ext3">
     <title>Converting an Ext2 File System into Ext3</title>
     <para>
      To convert an Ext2 file system to Ext3:
     </para>
     <procedure id="bgchzb5">
      <step id="bgchzb6">
       <para>
        Create an Ext3 journal by running <command>tune2fs -j</command> as
        the <systemitem>root</systemitem> user.
       </para>
       <para>
        This creates an Ext3 journal with the default parameters.
       </para>
       <para>
        To specify how large the journal should be and on which device it
        should reside, run <command>tune2fs <option>-J</option></command>
        instead together with the desired journal options
        <option>size=</option> and <option>device=</option>. More
        information about the <command>tune2fs</command> program is
        available in the <command>tune2fs</command> man page.
       </para>
      </step>
      <step id="bgchzb7">
       <para>
        Edit the file <filename>/etc/fstab</filename> as the
        <systemitem>root</systemitem> user to change the file system type
        specified for the corresponding partition from
        <literal>ext2</literal> to <literal>ext3</literal>, then save the
        changes.
       </para>
       <para>
        This ensures that the Ext3 file system is recognized as such. The
        change takes effect after the next reboot.
       </para>
      </step>
      <step id="bgchzb8">
       <para>
        To boot a root file system that is set up as an Ext3 partition,
        include the modules <literal>ext3</literal> and
        <literal>jbd</literal> in the <filename>initrd</filename>.
       </para>
       <substeps>
        <step id="bi6z1gc">
         <para>
          Edit <filename>/etc/dracut.conf.d/01-dist.conf</filename>, the
	  following line:
	 </para>
	 <screen>force_drivers+="ext3 jbd"</screen>
        </step>
        <step id="bi6z1rw">
         <para>
          Run the <command>dracut <option>-f</option></command> command.
         </para>
         <para>
          This builds a new initrd and prepares it for use.
         </para>
        </step>
       </substeps>
      </step>
      <step id="bi6z29v">
       <para>
        Reboot the system.
       </para>
      </step>
     </procedure>
    </sect3>
<!--  <sect2 id="sec.filesystems.major.reiser4">   <title>Reiser4</title>   <indexterm class="startofrange" id="idx.file_systems_Reiser4">    <primary>file systems</primary>    <secondary>Reiser4</secondary>   </indexterm>   <para>    Right after kernel 2.6 had been released, the family of journaling file    systems was joined by another member: Reiser4. Reiser4 is fundamentally    different from its predecessor ReiserFS (version 3.6). It introduces the    concept of plug-ins to tweak the file system functionality and a finer    grained security concept.     </para>   <variablelist>   <varlistentry>    <term>Fine Grained Security Concept</term>    <listitem>     <para>      In designing Reiser4, its developers put an emphasis on the      implementation of security-relevant features. Reiser4 therefore comes      with a set of dedicated security plug-ins. The most important one      introduces the concept of file <quote>items.</quote> Currently, file      access controls are defined per file. If there is a large file      containing information relevant to several users, groups, or applications,      the access rights had be fairly imprecise to include all parties      involved. In Reiser4, you can split those files into smaller portions      (the <quote>items</quote>). Access rights can then be set for      each item and each user separately, allowing a much more precise file      security management. A perfect example would be      <filename>/etc/passwd</filename>. To date, only <systemitem       class="username">root</systemitem> can read and edit the file while      non-<systemitem class="username">root</systemitem> users only get read      access to this file. Using the item concept of Reiser4, you could split      this file in a set of items (one item per user) and allow users or      applications to modify their own data but not      access other users' data. This concept adds both to security and       flexibility.     </para>    </listitem>   </varlistentry>    <varlistentry>     <term>Extensibility through Plug-Ins</term>     <listitem>      <para>       Many file system functions and external functions normally used by a       file system are implemented as plug-ins in Reiser4. These plug-ins can       easily be added to the base system. You no longer need to recompile the       kernel or reformat the hard disk to add new functions to your       file system.      </para>     </listitem>    </varlistentry>   <varlistentry>    <term>Better File System Layout through Delayed Allocation</term>    <listitem>     <para>      Like XFS, Reiser4 supports delayed allocation. See <xref       linkend="sec.filesystems.major.xfs"/>. Using delayed allocation even      for metadata can result in better overall layout.     </para>    </listitem>   </varlistentry>  </variablelist>   <indexterm class="endofrange" startref="idx.file_systems_Reiser4"/>  </sect2>-->
    <sect3 id="sec_fileystems_major_ext3inodesize">
     <title>Ext3 File System Inode Size and Number of Inodes</title>
     <para>
      An inode stores information about the file and its block location in
      the file system. To allow space in the inode for extended attributes
      and ACLs, the default inode size for Ext3 was increased from 128 bytes
      on SLES 10 to 256 bytes on SLES 11. As compared to SLES 10, when you
      make a new Ext3 file system on SLES 11, the default amount of space
      pre-allocated for the same number of inodes is doubled, and the usable
      space for files in the file system is reduced by that amount. Thus,
      you must use larger partitions to accommodate the same number of
      inodes and files than were possible for an Ext3 file system on SLES
      10.
     </para>
     <para>
      When you create a new Ext3 file system, the space in the inode table
      is pre-allocated for the total number of inodes that can be created.
      The bytes-per-inode ratio and the size of the file system determine
      how many inodes are possible. When the file system is made, an inode
      is created for every bytes-per-inode bytes of space:
     </para>
<screen>
number of inodes = total size of the file system divided by the number of bytes per inode
</screen>
     <para>
      The number of inodes controls the number of files you can have in the
      file system: one inode for each file. To address the increased inode
      size and reduced usable space available, the default for the
      bytes-per-inode ratio was increased from 8192 bytes on SLES 10 to
      16384 bytes on SLES 11. The doubled ratio means that the number of
      files that can be created is one-half of the number of files possible
      for an Ext3 file system on SLES 10.
     </para>
     <important>
      <para>
       After the inodes are allocated, you cannot change the settings for
       the inode size or bytes-per-inode ratio. No new inodes are possible
       without recreating the file system with different settings, or unless
       the file system gets extended. When you exceed the maximum number of
       inodes, no new files can be created on the file system until some
       files are deleted.
      </para>
     </important>
     <para>
      When you make a new Ext3 file system, you can specify the inode size
      and bytes-per-inode ratio to control inode space usage and the number
      of files possible on the file system. If the blocks size, inode size,
      and bytes-per-inode ratio values are not specified, the default values
      in the <filename>/etc/mked2fs.conf</filename> file are applied. For
      information, see the <filename>mke2fs.conf(5)</filename> man page.
     </para>
     <para>
      Use the following guidelines:
     </para>
     <itemizedlist>
      <listitem>
       <formalpara id="b14jepus">
        <title>Inode size:</title>
        <para>
         The default inode size is 256 bytes. Specify a value in bytes that
         is a power of 2 and equal to 128 or larger in bytes and up to the
         block size, such as 128, 256, 512, and so on. Use 128 bytes only if
         you do not use extended attributes or ACLs on your Ext3 file
         systems.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="b14jeput">
        <title>Bytes-per-inode ratio:</title>
        <para>
         The default bytes-per-inode ratio is 16384 bytes. Valid
         bytes-per-inode ratio values must be a power of 2 equal to 1024 or
         greater in bytes, such as 1024, 2048, 4096, 8192, 16384, 32768, and
         so on. This value should not be smaller than the block size of the
         file system, because the block size is the smallest chunk of space
         used to store data. The default block size for the Ext3 file system
         is 4 KB.
        </para>
       </formalpara>
       <para>
        In addition, you should consider the number of files and the size of
        files you need to store. For example, if your file system will have
        many small files, you can specify a smaller bytes-per-inode ratio,
        which increases the number of inodes. If your file system will have
        a very large files, you can specify a larger bytes-per-inode ratio,
        which reduces the number of possible inodes.
       </para>
       <para>
        Generally, it is better to have too many inodes than to run out of
        them. If you have too few inodes and very small files, you could
        reach the maximum number of files on a disk that is practically
        empty. If you have too many inodes and very large files, you might
        have free space reported but be unable to use it because you cannot
        create new files in space reserved for inodes.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      If you do not use extended attributes or ACLs on your Ext3 file
      systems, you can restore the SLES 10 behavior specifying 128 bytes as
      the inode size and 8192 bytes as the bytes-per-inode ratio when you
      make the file system. Use any of the following methods to set the
      inode size and bytes-per-inode ratio:
     </para>
     <itemizedlist>
      <listitem>
       <formalpara id="b14irnj5">
        <title>Modifying the default settings for all new Ext3 files:</title>
        <para>
         In a text editor, modify the <literal>defaults</literal> section of
         the <filename>/etc/mke2fs.conf</filename> file to set the
         <literal>inode_size</literal> and <literal>inode_ratio</literal> to
         the desired default values. The values apply to all new Ext3 file
         systems. For example:
        </para>
       </formalpara>
<screen>
blocksize = 4096
inode_size = 128
inode_ratio = 8192
</screen>
      </listitem>
      <listitem>
       <formalpara id="b14irnj6">
        <title>At the command line:</title>
        <para>
         Pass the inode size (<literal>-I 128</literal>) and the
         bytes-per-inode ratio (<literal>-i 8192</literal>) to the
         <command>mkfs.ext3(8)</command> command or the
         <command>mke2fs(8)</command> command when you create a new Ext3
         file system. For example, use either of the following commands:
        </para>
       </formalpara>
<screen>
mkfs.ext3 -b 4096 -i 8092 -I 128 /dev/sda2

mke2fs -t ext3 -b 4096 -i 8192 -I 128 /dev/sda2
</screen>
      </listitem>
      <listitem>
       <formalpara id="b12dp9td">
        <title>During installation with YaST:</title>
        <para>
         Pass the inode size and bytes-per-inode ratio values when you
         create a new Ext3 file system during the installation. In the YaST
         Partitioner on the <guimenu>Edit Partition</guimenu> page
         under<guimenu> Formatting Options</guimenu>, select <guimenu>Format
         partition</guimenu><guimenu>Ext3</guimenu>, then click
         <guimenu>Options</guimenu>. In the <guimenu>File system
         options</guimenu> dialog box, select the desired values from the
         <guimenu>Block Size in Bytes</guimenu>,
         <guimenu>Bytes-per-inode</guimenu>, and <guimenu>Inode
         Size</guimenu> drop-down lists.
        </para>
       </formalpara>
       <para>
        For example, select 4096 for the <guimenu>Block Size in
        Bytes</guimenu> drop-down list, select 8192 from the <guimenu>Bytes
        per inode</guimenu> drop-down list, select 128 from the
        <guimenu>Inode Size</guimenu> drop-down list, then click
        <guimenu>OK</guimenu>.
       </para>
       <informalfigure pgwide="0">
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="ext3_inode_yast_a.png" width="305pt" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="ext3_inode_yast_a.png" width="305pt" format="PNG"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
      </listitem>
      <listitem>
       <formalpara id="b12dpiab">
        <title>During installation with autoyast:</title>
        <para>
         In an autoyast profile, you can use the <literal>fs_options
         </literal>tag to set the <literal>opt_bytes_per_inode</literal>
         ratio value of 8192 for -i and the
         <literal>opt_inode_density</literal> value of 128 for -I:
        </para>
       </formalpara>
<screen>
&lt;partitioning config:type="list"&gt;
    &lt;drive&gt;
      &lt;device&gt;/dev/sda&lt;/device&gt;
      &lt;initialize config:type="boolean"&gt;true&lt;/initialize&gt;
      &lt;partitions config:type="list"&gt;
        &lt;partition&gt;
          &lt;filesystem config:type="symbol"&gt;ext3&lt;/filesystem&gt;
          &lt;format config:type="boolean"&gt;true&lt;/format&gt;
          &lt;fs_options&gt;
            &lt;opt_bytes_per_inode&gt;
              &lt;option_str&gt;-i&lt;/option_str&gt;
              &lt;option_value&gt;8192&lt;/option_value&gt;
            &lt;/opt_bytes_per_inode&gt;
            &lt;opt_inode_density&gt;
              &lt;option_str&gt;-I&lt;/option_str&gt;
              &lt;option_value&gt;128&lt;/option_value&gt;
            &lt;/opt_inode_density&gt;
          &lt;/fs_options&gt;
          &lt;mount&gt;/&lt;/mount&gt;
          &lt;partition_id config:type="integer"&gt;131&lt;/partition_id&gt;
          &lt;partition_type&gt;primary&lt;/partition_type&gt;
          &lt;size&gt;25G&lt;/size&gt;
        &lt;/partition&gt;
</screen>
      </listitem>
     </itemizedlist>
     <para>
      For information,
      see<ulink url="http://www.novell.com/support/kb/doc.php?id=7009075"><citetitle>
      SLES11 ext3 partitions can only store 50% of the files that can be
      stored on SLES10</citetitle> [Technical Information Document
      7009075]</ulink>.
     </para>
    </sect3>
   </sect2>

   <sect2 id="sec_filesystems_major_reiser">
    <title>ReiserFS</title>
    <para>
     Officially one of the key features of the 2.4 kernel release, ReiserFS
     has been available as a kernel patch for 2.2.x SUSE kernels since
     version 6.4. ReiserFS was designed by Hans Reiser and the Namesys
     development team. It has proven itself to be a powerful alternative to
     Ext2. Its key assets are better disk space utilization, better disk
     access performance, faster crash recovery, and reliability through data
     journaling.
    </para>
    <important>
     <para>
      Existing ReiserFS partitions are supported for the lifetime of SUSE
      Linux Enterprise Server 12 specifically for migration purposes. Support
      for creating new ReiserFS file systems has been removed
      starting with SUSE Linux Enterprise Server 12.
     </para>
    </important>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bgchzav" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzaw" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzax" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzay" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bgchzav">
     <title>Better Disk Space Utilization</title>
     <para>
      In ReiserFS, all data is organized in a structure called a B*-balanced
      tree. The tree structure contributes to better disk space utilization
      because small files can be stored directly in the B* tree leaf nodes
      instead of being stored elsewhere and maintaining a pointer to the
      actual disk location. In addition to that, storage is not allocated in
      chunks of 1 or 4 KB, but in portions of the exact size needed. Another
      benefit lies in the dynamic allocation of inodes. This keeps the file
      system more flexible than traditional file systems, like Ext2, where
      the inode density must be specified at file system creation time.
     </para>
    </sect3>
    <sect3 id="bgchzaw">
     <title>Better Disk Access Performance</title>
     <para>
      For small files, file data and <quote>stat_data</quote> (inode)
      information are often stored next to each other. They can be read with
      a single disk I/O operation, meaning that only one access to disk is
      required to retrieve all the information needed.
     </para>
    </sect3>
    <sect3 id="bgchzax">
     <title>Fast Crash Recovery</title>
     <para>
      Using a journal to keep track of recent metadata changes makes a file
      system check a matter of seconds, even for huge file systems.
     </para>
    </sect3>
    <sect3 id="bgchzay">
     <title>Reliability through Data Journaling</title>
     <para>
      ReiserFS also supports data journaling and ordered data modes similar
      to the concepts outlined in
      <xref linkend="sec_filesystems_major_ext3" xrefstyle="HeadingOnPage"/>.
      The default mode is <literal>data=ordered</literal>, which ensures
      both data and metadata integrity, but uses journaling only for
      metadata.
     </para>
    </sect3>
   </sect2>

   <sect2 id="sec_filesystems_major_xfs">
    <title>XFS</title>
    <para>
     Originally intended as the file system for their IRIX OS, SGI started
     XFS development in the early 1990s. The idea behind XFS was to create a
     high-performance 64-bit journaling file system to meet extreme
     computing challenges. XFS is very good at manipulating large files and
     performs well on high-end hardware. However, even XFS has a drawback.
     Like ReiserFS, XFS takes great care of metadata integrity, but less
     care of data integrity.
    </para>
    <para>
     A quick review of XFS’s key features explains why it might prove to
     be a strong competitor for other journaling file systems in high-end
     computing.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bgchzba" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzbb" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzbc" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bgchzba">
     <title>High Scalability through the Use of Allocation Groups</title>
     <para>
      At the creation time of an XFS file system, the block device
      underlying the file system is divided into eight or more linear
      regions of equal size. Those are referred to as <emphasis>allocation
      groups</emphasis>. Each allocation group manages its own inodes and
      free disk space. Practically, allocation groups can be seen as file
      systems in a file system. Because allocation groups are rather
      independent of each other, more than one of them can be addressed by
      the kernel simultaneously. This feature is the key to XFS’s great
      scalability. Naturally, the concept of independent allocation groups
      suits the needs of multiprocessor systems.
     </para>
    </sect3>
    <sect3 id="bgchzbb">
     <title>High Performance through Efficient Management of Disk Space</title>
     <para>
      Free space and inodes are handled by B<superscript>+</superscript>
      trees inside the allocation groups. The use of
      B<superscript>+</superscript> trees greatly contributes to XFS’s
      performance and scalability. XFS uses <emphasis>delayed
      allocation</emphasis>, which handles allocation by breaking the
      process into two pieces. A pending transaction is stored in RAM and
      the appropriate amount of space is reserved. XFS still does not decide
      where exactly (in file system blocks) the data should be stored. This
      decision is delayed until the last possible moment. Some short-lived
      temporary data might never make its way to disk, because it is
      obsolete by the time XFS decides where actually to save it. In this
      way, XFS increases write performance and reduces file system
      fragmentation. Because delayed allocation results in less frequent
      write events than in other file systems, it is likely that data loss
      after a crash during a write is more severe.
     </para>
    </sect3>
    <sect3 id="bgchzbc">
     <title>Preallocation to Avoid File System Fragmentation</title>
     <para>
      Before writing the data to the file system, XFS
      <emphasis>reserves</emphasis> (preallocates) the free space needed for
      a file. Thus, file system fragmentation is greatly reduced.
      Performance is increased because the contents of a file are not
      distributed all over the file system.
     </para>
    </sect3>
   </sect2>

   <sect2 id="bwkryet">
    <title>File System Feature Comparison</title>
    <para>
     For a side-by-side feature comparison of the major operating systems in
     SUSE Linux Enterprise Server,
     see<ulink url="http://www.suse.com/products/server/technical-information/#FileSystem">
     File System Support and Sizes</ulink> on the
     <ulink url="http://www.suse.com/products/server/technical-information/">SUSE
     Linux Enterprise Server Technical Information Web site</ulink>.
    </para>
   </sect2>
  </sect1>
  <sect1 id="sec_filesystems_add">
   <title>Other Supported File Systems</title>

   <para role="intro">
    <xref linkend="tab.dateisysteme" xrefstyle="TableXRef"/> summarizes some
    other file systems supported by Linux. They are supported mainly to
    ensure compatibility and interchange of data with different kinds of
    media or foreign operating systems.
   </para>

   <table id="tab.dateisysteme" frame="topbot" rowsep="1" pgwide="0">
    <title>File System Types in Linux</title>
    <tgroup cols="2">
     <colspec colnum="1" colname="1" colwidth="2381*"/>
     <colspec colnum="2" colname="2" colwidth="7620*"/>
     <thead>
      <row id="bi6z95b">
       <entry>
        <para>
         File System Type
        </para>
       </entry>
       <entry>
        <para>
         Description
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row id="bwk81ba">
       <entry>
        <para>
         <systemitem>cramfs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Compressed ROM file system: A compressed read-only file system for
         ROMs.
        </para>
       </entry>
      </row>
      <row id="bwk81bb">
       <entry>
        <para>
         <systemitem>hpfs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         High Performance File System: The IBM OS/2 standard file system.
         Only supported in read-only mode.
        </para>
       </entry>
      </row>
      <row id="bwk81bc">
       <entry>
        <para>
         <systemitem>iso9660</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Standard file system on CD-ROMs.
        </para>
       </entry>
      </row>
      <row id="bwk81bd">
       <entry>
        <para>
         <systemitem>minix</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         This file system originated from academic projects on operating
         systems and was the first file system used in Linux. Today, it is
         used as a file system for floppy disks.
        </para>
       </entry>
      </row>
      <row id="bwk81be">
       <entry>
        <para>
         <systemitem>msdos</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         <filename>fat</filename>, the file system originally used by DOS,
         is today used by various operating systems.
        </para>
       </entry>
      </row>
      <row id="bwk81bf">
       <entry>
        <para>
         <systemitem>ncpfs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         File system for mounting Novell volumes over networks.
        </para>
       </entry>
      </row>
      <row id="bwk81bg">
       <entry>
        <para>
         <systemitem>nfs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Network File System: Here, data can be stored on any machine in a
         network and access might be granted via a network.
        </para>
       </entry>
      </row>
      <row id="bwk81bh">
       <entry>
        <para>
         <systemitem>ntfs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Windows&nbsp;NT file system; read-only.
        </para>
       </entry>
      </row>
      <row id="bwk81bi">
       <entry>
        <para>
         <systemitem>smbfs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Server Message Block is used by products such as Windows to enable
         file access over a network.
        </para>
       </entry>
      </row>
      <row id="bwk81bj">
       <entry>
        <para>
         <systemitem>sysv</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Used on SCO UNIX, Xenix, and Coherent (commercial UNIX systems for
         PCs).
        </para>
       </entry>
      </row>
      <row id="bwk81bk">
       <entry>
        <para>
         <systemitem>ufs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Used by BSD, SunOS, and NextStep. Only supported in read-only mode.
        </para>
       </entry>
      </row>
      <row id="bwk81bl">
       <entry>
        <para>
         <systemitem>umsdos</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         UNIX on MS-DOS: Applied on top of a standard
         <filename>fat</filename> file system, achieves UNIX functionality
         (permissions, links, long filenames) by creating special files.
        </para>
       </entry>
      </row>
      <row id="bwk81bm">
       <entry>
        <para>
         <systemitem>vfat</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Virtual FAT: Extension of the <literal>fat</literal> file system
         (supports long filenames).
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </sect1>
  <sect1 id="sec_filesystems_lfs">
   <title>Large File Support in Linux</title>

   <para>
    Originally, Linux supported a maximum file size of 2&nbsp;GiB
    (2<superscript>31</superscript> bytes). Unless a file system comes with
    large file support, the maximum file size on a 32-bit system is 2 GiB.
   </para>

   <para>
    Currently, all of our standard file systems have LFS (large file
    support), which gives a maximum file size of
    2<superscript>63</superscript> bytes in theory.
    <xref linkend="tab.maxsize" xrefstyle="TableXRef"/> offers an overview
    of the current on-disk format limitations of Linux files and file
    systems. The numbers in the table assume that the file systems are using
    4 KiB block size, which is a common standard. When using different block
    sizes, the results are different. The maximum file sizes in
    <xref linkend="tab.maxsize" xrefstyle="TableXRef"/> can be larger than
    the file system's actual size when using sparse blocks.
   </para>

   <note>
    <para>
     In this document: 1024 Bytes = 1 KiB; 1024 KiB = 1 MiB; 1024 MiB = 1
     GiB; 1024 GiB = 1 TiB; 1024 TiB = 1 PiB; 1024 PiB = 1 EiB (see also
     <ulink url="http://physics.nist.gov/cuu/Units/binary.html"><citetitle>NIST:
     Prefixes for Binary Multiples</citetitle></ulink>.
    </para>
   </note>

   <table id="tab.maxsize" frame="topbot" rowsep="1" pgwide="0">
    <title>Maximum Sizes of Files and File Systems (On-Disk Format, 4 KiB Block Size)</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="3334*"/>
     <colspec colnum="2" colname="2" colwidth="3334*"/>
     <colspec colnum="3" colname="3" colwidth="3334*"/>
     <thead>
      <row id="b12r1f2q">
       <entry>
        <para>
         File System (4 KiB Block Size)
        </para>
       </entry>
       <entry>
        <para>
         Maximum File System Size
        </para>
       </entry>
       <entry>
        <para>
         Maximum File Size
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row id="b12r1f2r">
       <entry>
        <para>
         Btrfs
        </para>
       </entry>
       <entry>
        <para>
         16 EiB
        </para>
       </entry>
       <entry>
        <para>
         16 EiB
        </para>
       </entry>
      </row>
      <row id="b12r1f2s">
       <entry>
        <para>
         Ext3
        </para>
       </entry>
       <entry>
        <para>
         16 TiB
        </para>
       </entry>
       <entry>
        <para>
         2 TiB
        </para>
       </entry>
      </row>
      <row id="b12r1f2t">
       <entry>
        <para>
         OCFS2 (a cluster-aware file system available in the High
         Availability Extension)
        </para>
       </entry>
       <entry>
        <para>
         16 TiB
        </para>
       </entry>
       <entry>
        <para>
         1 EiB
        </para>
       </entry>
      </row>
      <row id="b12r1f2u">
       <entry>
        <para>
         ReiserFS v3.6
        </para>
       </entry>
       <entry>
        <para>
         16 TiB
        </para>
       </entry>
       <entry>
        <para>
         1 EiB
        </para>
       </entry>
      </row>
      <row id="b12r1f2v">
       <entry>
        <para>
         XFS
        </para>
       </entry>
       <entry>
        <para>
         8 EiB
        </para>
       </entry>
       <entry>
        <para>
         8 EiB
        </para>
       </entry>
      </row>
      <row id="b12r1f2w">
       <entry>
        <para>
         NFSv2 (client side)
        </para>
       </entry>
       <entry>
        <para>
         8 EiB
        </para>
       </entry>
       <entry>
        <para>
         2 GiB
        </para>
       </entry>
      </row>
      <row id="b12r1f2x">
       <entry>
        <para>
         NFSv3 (client side)
        </para>
       </entry>
       <entry>
        <para>
         8 EiB
        </para>
       </entry>
       <entry>
        <para>
         8 EiB
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>

   <important>
    <para>
     <xref linkend="tab.maxsize" xrefstyle="TableXRef"/> describes the
     limitations regarding the on-disk format. The Linux kernel imposes its
     own limits on the size of files and file systems handled by it. These
     are as follows:
    </para>
    <variablelist>
     <varlistentry id="bgchzc6">
      <term>File Size</term>
      <listitem>
       <para>
        On 32-bit systems, files cannot exceed 2 TiB
        (2<superscript>41</superscript> bytes).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bgchzc7">
      <term>File System Size</term>
      <listitem>
       <para>
        File systems can be up to 2<superscript>73</superscript> bytes in
        size. However, this limit is still out of reach for the currently
        available hardware.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </important>
  </sect1>
  <sect1 id="sect_stor_limits">
   <title>Linux Kernel Storage Limitations</title>

   <para>
    <xref linkend="b12o27j6" xrefstyle="TableXRef"/> summarizes the kernel
    limits for storage associated with &productname;.
   </para>

   <table id="b12o27j6"  frame="topbot" rowsep="1" pgwide="0">
    <title>Storage Limitations</title>
    <tgroup cols="2">
     <colspec colnum="1" colname="1" colwidth="5001*"/>
     <colspec colnum="2" colname="2" colwidth="5001*"/>
     <thead>
      <row id="b12o27j7">
       <entry>
        <para>
         Storage Feature
        </para>
       </entry>
       <entry>
        <para>
         Limitation
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row id="b12o27j8">
       <entry>
        <para>
         Maximum number of LUNs supported
        </para>
       </entry>
       <entry>
        <para>
         16384 LUNs per target.
        </para>
       </entry>
      </row>
      <row id="b12o27j9">
       <entry>
        <para>
         Maximum number of paths per single LUN
        </para>
       </entry>
       <entry>
        <para>
         No limit per se. Each path is treated as a normal LUN.
        </para>
        <para>
         The actual limit is given by the number of LUNs per target and the
         number of targets per HBA (16777215 for a Fibre Channel HBA).
        </para>
       </entry>
      </row>
      <row id="b12o27ja">
       <entry>
        <para>
         Maximum number of HBAs
        </para>
       </entry>
       <entry>
        <para>
         Unlimited. The actual limit is determined by the amount of PCI
         slots of the system.
        </para>
       </entry>
      </row>
      <row id="b12o27jb">
       <entry>
        <para>
         Maximum number of paths with device-mapper-multipath (in total) per
         operating system
        </para>
       </entry>
       <entry>
        <para>
         Approximately1024. The actual number depends on the length of the
         device number strings. It is a compile-time variable within
         multipath-tools, which can be raised if this limit poses to be a
         problem.
        </para>
       </entry>
      </row>
      <row id="b12r1mnq">
       <entry>
        <para>
         Maximum size per block device
        </para>
       </entry>
       <entry>
        <para>
         For X86, up to 16 TiB.
        </para>
        <para>
         For x86_64, ia64, s390x, and ppc64, up to 8 EiB.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </sect1>
  <sect1 id="bwkbhpd">
   <title>Managing Devices with the YaST Partitioner</title>

   <para>
    You can use the YaST Partitioner to create and manage file systems and
    RAID devices. For information, see <xref linkend="cha.advdisk"/>.
   </para>
  </sect1>
  <sect1 id="sec_filesystems_info">
   <title>Additional Information</title>

   <para>
    Each of the file system projects described above maintains its own home
    page on which to find mailing list information, further documentation,
    and FAQs:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      <ulink url="http://e2fsprogs.sourceforge.net/"><citetitle>E2fsprogs:
      Ext2/3/4 File System Utilities</citetitle></ulink>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="http://www.ibm.com/developerworks/linux/library/l-fs7/"><citetitle>Introducing
      Ext3</citetitle></ulink>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="http://oss.sgi.com/projects/xfs/"><citetitle>XFS: A
      High-Performance Journaling Filesytem</citetitle></ulink>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="http://oss.oracle.com/projects/ocfs2/"><citetitle>OCFS2
      Project</citetitle></ulink>
     </para>
    </listitem>
   </itemizedlist>


   <para>
    A comprehensive multipart tutorial about Linux file systems can be found
    at IBM&nbsp;developerWorks in the
    <ulink url="https://www.ibm.com/developerworks/linux/library/l-fs/"><citetitle>Advanced
    File System Implementor’s Guide</citetitle></ulink>.
   </para>

   <para>
    An in-depth comparison of file systems (not only Linux file systems) is
    available from the Wikipedia project in
    <ulink url="http://en.wikipedia.org/wiki/Comparison_of_file_systems#Comparison"><citetitle>Comparison
    of File Systems</citetitle></ulink>.
   </para>
  </sect1>
 </chapter>
 <chapter id="storwhatsnew" lang="en">
  <title>What’s New for Storage in SLES 11</title>
  <para>
   The features and behavior changes noted in this section were made for
   SUSE Linux Enterprise Server 11.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="new_sles11sp3" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bwkb7my" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bndgyod" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bndgyoe" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="new_sles11sp3">
   <title>What’s New in SLES 11 SP3</title>

   <para>
    In addition to bug fixes, the features and behavior changes in this
    section were made for the SUSE Linux Enterprise Server 11 SP3 release:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      <xref linkend="b15ds08f" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b15ds08d" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b15ds08c" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b15ds08g" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b15ds08e" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b15ds08f">
    <title>Btrfs Quotas</title>
    <para>
     Btrfs quota support for subvolumes on the <systemitem>root</systemitem>
     file system has been added in the <command>btrfs(8)</command> command.
    </para>
   </sect2>

   <sect2 id="b15ds08d">
    <title>iSCSI LIO Target Server</title>
    <para>
     YaST supports the iSCSI LIO Target Server software. For information,
     see <xref linkend="cha_iscsi_lio" xrefstyle="ChapTitleOnPage"/>.
    </para>
   </sect2>

   <sect2 id="b15ds08c">
    <title>Linux Software RAIDs</title>
    <para>
     The following enhancements were added for Linux software RAIDs:
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="b15jy5sx" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15jy5sy" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15jy5sz" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="b15jy5sx">
     <title>Support for Intel RSTe+</title>
     <para>
      The software RAID provides improved support on the Intel RSTe+ (Rapid
      Storage Technology Enterprise) platform to support RAID level 0, 1, 4,
      5, 6, and 10.
     </para>
    </sect3>
    <sect3 id="b15jy5sy">
     <title>LEDMON Utility</title>
     <para>
      The LEDMON utility supports PCIe-SSD enclosure LEDs for MD software
      RAIDs. For information, see
      <xref linkend="cha_raids_lee" xrefstyle="ChapTitleOnPage"/>.
     </para>
    </sect3>
    <sect3 id="b15jy5sz">
     <title>Device Order in the Software RAID</title>
     <para>
      In the <guimenu>Add RAID</guimenu> wizard in the YaST Partitioner, the
      <guimenu>Classify</guimenu> option allows you to specify the order in
      which the selected devices in a Linux software RAID will be used to
      ensure that one half of the array resides on one disk subsystem and
      the other half of the array resides on a different disk subsystem. For
      example, if one disk subsystem fails, the system keeps running from
      the second disk subsystem. For information, see
      <xref linkend="b14dtk77" xrefstyle="StepXRef"/> in
      <xref linkend="b14drcbo" xrefstyle="SectTitleOnPage"/>.
     </para>
    </sect3>
   </sect2>

   <sect2 id="b15ds08g">
    <title>LVM2</title>
    <para>
     The following enhancements were added for LVM2:
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="b15jy33d" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15jy33e" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="b15jy33d">
     <title>Thin Pool and Thin Volumes</title>
     <para>
      LVM logical volumes can be thinly provisioned. For information, see
      <xref linkend="bgchx8c" xrefstyle="SectTitleOnPage"/>.
     </para>
     <itemizedlist>
      <listitem>
       <formalpara id="b15jy6mq">
        <title>Thin pool:</title>
        <para>
         The logical volume is a pool of space that is reserved for use with
         thin volumes. The thin volumes can allocate their needed space from
         it on demand.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="b15jy6mr">
        <title>Thin volume:</title>
        <para>
         The volume is created as a sparse volume. The volume allocates
         needed space on demand from a thin pool.
        </para>
       </formalpara>
      </listitem>
     </itemizedlist>
    </sect3>
    <sect3 id="b15jy33e">
     <title>Thin Snapshots</title>
     <para>
      LVM logical volume snapshots can be thinly provisioned. Thin
      provisioning is assumed if you to create a snapshot without a
      specified size. For information, see
      <xref linkend="bi7uttc" xrefstyle="SectTitleOnPage"/>.
     </para>
    </sect3>
   </sect2>

   <sect2 id="b15ds08e">
    <title>Multipath I/O</title>
    <para>
     The following changes and enhancements were made for multipath I/O:
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="b15jxva2" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15jxva1" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15jxva0" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="b15jxva2">
     <title>mpathpersist(8)</title>
     <para>
      The <command>mpathpersist(8)</command> utility is new. It can be used
      to manage SCSI persistent reservations on Device Mapper Multipath
      devices. For information, see
      <xref linkend="b15jw320" xrefstyle="SectTitleOnPage"/>.
     </para>
    </sect3>
    <sect3 id="b15jxva1">
     <title>multipath(8)</title>
     <para>
      The following enhancement was added to the
      <command>multipath(8)</command> command:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        The <literal>-r</literal> option allows you to force a device map
        reload.
       </para>
      </listitem>
     </itemizedlist>
    </sect3>
    <sect3 id="b15jxva0">
     <title>/etc/multipath.conf</title>
     <para>
      The Device Mapper - Multipath tool added the following enhancements
      for the <filename>/etc/multipath.conf</filename> file:
     </para>
     <itemizedlist>
      <listitem>
       <formalpara id="b15jxy6m" role="intro">
        <title>udev_dir</title>
        <para/>
       </formalpara>
       <para>
        The <literal>udev_dir</literal> attribute is deprecated. After you
        upgrade to SLES 11 SP3, you can remove the following line from the
        <literal>defaults</literal> section of your
        <filename>/etc/multipath.conf</filename> file:
       </para>
<screen>
udev_dir /dev
</screen>
      </listitem>
      <listitem>
       <formalpara id="b15jxy6n" role="intro">
        <title>getuid_callout</title>
        <para/>
       </formalpara>
       <para>
        In the <literal>defaults</literal> section of the
        <filename>/etc/multipath.conf</filename> file, the
        <literal>getuid_callout </literal>attribute is deprecated and
        replaced by the <literal>uid_attribute</literal> parameter. This
        parameter is a udev attribute that provides a unique path
        identifier. The default value is <literal>ID_SERIAL</literal>.
       </para>
       <para>
        After you upgrade to SLES 11 SP3, you can modify the attributes in
        the <literal>defaults</literal> section of your
        <filename>/etc/multipath.conf</filename> file:
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Remove the following line from the <literal>defaults</literal>
          section:
         </para>
<screen>
  getuid_callout "/lib/udev/scsi_id --whitelisted --device=/dev/%n"
</screen>
        </listitem>
        <listitem>
         <para>
          Add the following line to the <literal>defaults</literal> section:
         </para>
<screen>
  uid_attribute "ID_SERIAL"
</screen>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <formalpara id="b15jxy6o" role="intro">
        <title>path_selector</title>
        <para/>
       </formalpara>
       <para>
        In the <literal>defaults</literal> section of the
        <filename>/etc/multipath.conf</filename> file, the default value for
        the <literal>path_selector </literal>attribute was changed from
        <literal>"round-robin 0"</literal> to <literal>"service-time
        0"</literal>. The <literal>service-time</literal> option chooses the
        path for the next bunch of I/O based on the amount of outstanding
        I/O to the path and its relative throughput.
       </para>
       <para>
        After you upgrade to SLES 11 SP3, you can modify the attribute value
        in the <literal>defaults</literal> section of your
        <filename>/etc/multipath.conf</filename> file to use the recommended
        default:
       </para>
<screen>
  path_selector "service-time 0"
</screen>
      </listitem>
      <listitem>
       <formalpara id="b15jxy6p" role="intro">
        <title>user_friendly_names</title>
        <para/>
       </formalpara>
       <para>
        The <literal>user_friendly_names</literal> attribute can be
        configured in the <literal>devices</literal> section and in the
        <literal>multipaths</literal> section.
       </para>
      </listitem>
      <listitem>
       <formalpara id="b15jxy6q" role="intro">
        <title>max_fds</title>
        <para/>
       </formalpara>
       <para role="intro">
        The default setting for the <literal>max_fds</literal> attribute was
        changed to <literal>max</literal>. This allows the multipath daemon
        to open as many file descriptors as the system allows when it is
        monitoring paths.
       </para>
       <para role="intro">
        After you upgrade to SLES 11 SP3, you can modify the attribute value
        in your <filename>/etc/multipath.conf</filename> file:
       </para>
<screen>
max_fds "max"
</screen>
      </listitem>
      <listitem>
       <formalpara id="b15jxy6r" role="intro">
        <title>reservation_key</title>
        <para/>
       </formalpara>
       <para role="intro">
        In the <literal>defaults</literal> section or
        <literal>multipaths</literal> section of the
        <filename>/etc/multipath.conf</filename> file, the
        <literal>reservation_key</literal> attribute can be used to assign a
        Service Action Reservation Key that is used with the
        <command>mpathpersist(8)</command> utility to manage persistent
        reservations for Device Mapper Multipath devices. The attribute is
        not used by default. If it is not set, the
        <command>multipathd</command> daemon does not check for persistent
        reservation for newly discovered paths or reinstated paths.
       </para>
<screen>
reservation_key &lt;<replaceable>reservation key</replaceable>&gt;
</screen>
       <para>
        For example:
       </para>
<screen>
multipaths {
        multipath {
                          wwid   XXXXXXXXXXXXXXXX
                         alias      yellow
                         reservation_key  0x123abc
      }
}
</screen>
       <para>
        For information about setting persistent reservations, see
        <xref linkend="b15jw320" xrefstyle="SectTitleOnPage"/>.
       </para>
      </listitem>
      <listitem>
       <formalpara id="b15jxy6s" role="intro">
        <title>hardware_handler</title>
        <para/>
       </formalpara>
       <para>
        Four SCSI hardware handlers were added in the SCSI layer that can be
        used with DM-Multipath:
       </para>
       <simplelist>
        <member><filename>scsi_dh_alua</filename>
        </member>
        <member><filename>scsi_dh_rdac</filename>
        </member>
        <member><literal>scsi_dh_hp_sw</literal>
        </member>
        <member><literal>scsi_dh_emc</literal>
        </member>
       </simplelist>
       <para>
        These handlers are modules created under the SCSI directory in the
        Linux kernel. Previously, the hardware handler in the Device Mapper
        layer was used.
       </para>
       <para>
        Add the modules to the <filename>initrd</filename> image, then
        specify them in the <filename>/etc/multipath.conf</filename> file as
        hardware handler types <literal>alua</literal>,
        <literal>rdac</literal>, <literal>hp_sw</literal>, and
        <literal>emc</literal>. For information about adding the device
        drivers to the <filename>initrd</filename> image, see
        <xref linkend="mpiosysconfsvr" xrefstyle="SectTitleOnPage"/>.
       </para>
      </listitem>
     </itemizedlist>
    </sect3>
   </sect2>
  </sect1>
  <sect1 id="bwkb7my">
   <title>What’s New in SLES 11 SP2</title>

   <para>
    In addition to bug fixes, the features and behavior changes in this
    section were made for the SUSE Linux Enterprise Server 11 SP2 release:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      Btrfs File System. See
      <xref linkend="bwk8gda" xrefstyle="SectTitleOnPage"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Open Fibre Channel over Ethernet. See
      <xref linkend="cha_fcoe" xrefstyle="ChapTitleOnPage"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Tagging for LVM storage objects. See
      <xref linkend="lvmtagging" xrefstyle="SectTitleOnPage"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      NFSv4 ACLs tools. See
      <xref linkend="nfsv4acls" xrefstyle="ChapTitleOnPage"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <command>--assume-clean</command> option for <command>mdadm
      resize</command> command. See
      <xref linkend="resizeincrraid" xrefstyle="SectTitleOnPage"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      In the <literal>defaults</literal> section of the
      <filename>/etc/multipath.conf</filename> file, the
      <literal>default_getuid</literal> parameter was obsoleted and replaced
      by the <literal>getuid_callout </literal>parameter:
     </para>
     <para>
      The line changed from this:
     </para>
<screen>
  default_getuid "/sbin/scsi_id -g -u -s /block/%n"
</screen>
     <para>
      to this:
     </para>
<screen>
  getuid_callout "/lib/udev/scsi_id --whitelisted --device=/dev/%n"
</screen>
    </listitem>
   </itemizedlist>
  </sect1>
  <sect1 id="bndgyod">
   <title>What’s New in SLES 11 SP1</title>

   <para>
    In addition to bug fixes, the features and behavior changes noted in
    this section were made for the SUSE Linux Enterprise Server 11 SP1
    release.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bnmqoyw" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bnmr1a1" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bnmqre5" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bnmti6z" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bnmtvr8" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bnmrccr" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bnmrfnk" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bnmtwna" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bnmss2g" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bnmszsy" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bnmtqql" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bnmqoyw">
    <title>Saving iSCSI Target Information</title>
    <remark>#301932: Export credentials for target of iSCSI target</remark>
    <para>
     In the
     <link linkend="sec_inst_system_iscsi_target_yast"><guimenu>YaST</guimenu><guimenu>Network
     Services</guimenu><guimenu>iSCSI Target</guimenu></link> function, a
     <guimenu>Save</guimenu> option was added that allows you to export the
     iSCSI target information. This makes it easier to provide information
     to consumers of the resources.
    </para>
   </sect2>

   <sect2 id="bnmr1a1">
    <title>Modifying Authentication Parameters in the iSCSI Initiator</title>
    <remark>#305306: [YaST2] allow modification of authentication in iSCSI initiator</remark>
    <para>
     In the
     <link linkend="sec_inst_system_iscsi_target_yast"><guimenu>YaST</guimenu><guimenu>Network
     Services</guimenu><guimenu>iSCSI Initiator</guimenu></link> function,
     you can modify the authentication parameters for connecting to a target
     devices. Previously, you needed to delete the entry and re-create it in
     order to change the authentication information.
    </para>
   </sect2>

   <sect2 id="bnmqre5">
    <title>Allowing Persistent Reservations for MPIO Devices</title>
    <remark>#301980: SCSI persistent reservations on MPIO devices</remark>
    <para>
     A SCSI initiator can issue SCSI reservations for a shared storage
     device, which locks out SCSI initiators on other servers from accessing
     the device. These reservations persist across SCSI resets that might
     happen as part of the SCSI exception handling process.
    </para>
    <para>
     The following are possible scenarios where SCSI reservations would be
     useful:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       In a simple SAN environment, persistent SCSI reservations help
       protect against administrator errors where a LUN is attempted to be
       added to one server but it is already in use by another server, which
       might result in data corruption. SAN zoning is typically used to
       prevent this type of error.
      </para>
     </listitem>
     <listitem>
      <para>
       In a high-availability environment with failover set up, persistent
       SCSI reservations help protect against errant servers connecting to
       SCSI devices that are reserved by other servers.
      </para>
     </listitem>
    </itemizedlist>
   </sect2>

   <sect2 id="bnmti6z">
    <title>MDADM 3.0.2</title>
    <remark>#307159: MDADM update to most recent version</remark>
    <para>
     Use the latest version of the Multiple Devices Administration (MDADM,
     <command>mdadm</command>) utility to take advantage of bug fixes and
     improvements.
    </para>
   </sect2>

   <sect2 id="bnmtvr8">
    <title>Boot Loader Support for MDRAID External Metadata</title>
    <remark>#307447: Boot support for MDRAID External Metadata (ISW)</remark>
    <para>
     Support was added to use the external metadata capabilities of the
     MDADM utility version 3.0 to install and run the operating system from
     RAID volumes defined by the Intel Matrix Storage Technology metadata
     format. This moves the functionality from the Device Mapper RAID
     (DMRAID) infrastructure to the Multiple Devices RAID (MDRAID)
     infrastructure, which offers the more mature RAID 5 implementation and
     offers a wider feature set of the MD kernel infrastructure. It allows a
     common RAID driver to be used across all metadata formats, including
     Intel, DDF (common RAID disk data format), and native MD metadata.
    </para>
   </sect2>

   <sect2 id="bnmrccr">
    <title>YaST Install and Boot Support for MDRAID External Metadata</title>
    <remark>#305883: Install and boot support for MDRAID External Metadata (ISW)</remark>
    <para>
     The YaST installer tool added support for MDRAID External Metadata for
     RAID 0, 1, 10, 5, and 6. The installer can detect RAID arrays and
     whether the platform RAID capabilities are enabled. If multipath RAID
     is enabled in the platform BIOS for Intel Matrix Storage Manager, it
     offers options for DMRAID, MDRAID (recommended), or none. The
     <filename>initrd</filename> was also modified to support assembling
     BIOS-based RAID arrays.
    </para>
   </sect2>

   <sect2 id="bnmrfnk">
    <title>Improved Shutdown for MDRAID Arrays that Contain the Root File System</title>
    <remark>#305885: Handle the metadata "dirty" bit when an ISW array contains the root file system</remark>
    <remark>#306823: Handle the metadata "dirty" bit when an ISW array contains the root filesystem</remark>
    <para>
     Shutdown scripts were modified to wait until all of the MDRAID arrays
     are marked clean. The operating system shutdown process now waits for a
     dirty-bit to be cleared until all MDRAID volumes have finished write
     operations.
    </para>
    <para>
     Changes were made to the startup script, shutdown script, and the
     <command>initrd</command> to consider whether the root
     (<filename>/</filename>) file system (the system volume that contains
     the operating system and application files) resides on a software RAID
     array. The metadata handler for the array is started early in the
     shutdown process to monitor the final root file system environment
     during the shutdown. The handler is excluded from the general
     <command>killall</command> events. The process also allows for writes
     to be quiesced and for the array’s metadata dirty-bit (which
     indicates whether an array needs to be resynchronized) to be cleared at
     the end of the shutdown.
    </para>
   </sect2>

   <sect2 id="bnmtwna">
    <title>MD over iSCSI Devices</title>
    <remark>#307478: md over iscsi devices</remark>
    <para>
     The YaST installer now allows MD to be configured over iSCSI devices.
    </para>
    <para>
     If RAID arrays are needed on boot, the iSCSI initiator software is
     loaded before <filename>boot.md</filename> so that the iSCSI targets
     are available to be auto-configured for the RAID.
    </para>
    <para>
     For a new install, Libstorage creates an
     <filename>/etc/mdadm.conf</filename> file and adds the line
     <literal>AUTO -all</literal>. During an update, the line is not added.
     If <filename>/etc/mdadm.conf</filename> contains the line
    </para>
<screen>
AUTO -all
</screen>
    <para>
     then no RAID arrays are auto-assembled unless they are explicitly
     listed in <filename>/etc/mdadm.conf</filename>.
    </para>
   </sect2>

   <sect2 id="bnmss2g">
    <title>MD-SGPIO</title>
    <remark>#305886: SGPIO support for MD</remark>
    <para>
     The MD-SGPIO utility is a standalone application that monitors RAID
     arrays via <command>sysfs(2)</command>. Events trigger an LED change
     request that controls blinking for LED lights that are associated with
     each slot in an enclosure or a drive bay of a storage subsystem. It
     supports two types of LED systems:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       2-LED systems (Activity LED, Status LED)
      </para>
     </listitem>
     <listitem>
      <para>
       3-LED systems (Activity LED, Locate LED, Fail LED)
      </para>
     </listitem>
    </itemizedlist>
   </sect2>

   <sect2 id="bnmszsy">
    <title>Resizing LVM 2 Mirrors</title>
    <remark>#306518: LVM mirror resize</remark>
    <para>
     The <command>lvresize</command>, <command>lvextend</command>, and
     <command>lvreduce</command> commands that are used to resize logical
     volumes were modified to allow the resizing of LVM 2 mirrors.
     Previously, these commands reported errors if the logical volume was a
     mirror.
    </para>
   </sect2>

   <sect2 id="bnmtqql">
    <title>Updating Storage Drivers for Adapters on IBM Servers</title>
    <remark>#307414: Storage driver updates</remark>
    <para>
     Update the following storage drivers to use the latest available
     versions to support storage adapters on IBM servers:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Adaptec: <filename>aacraid</filename>, <filename>aic94xx</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       Emulex: <filename>lpfc</filename>
      </para>
     </listitem>
     <listitem>
      <para>
       LSI: <filename>mptas</filename>, <filename>megaraid_sas</filename>
      </para>
      <para>
       The <filename>mptsas</filename> driver now supports native EEH
       (Enhanced Error Handler) recovery, which is a key feature for all of
       the IO devices for Power platform customers.
      </para>
     </listitem>
     <listitem>
      <para>
       qLogic: <filename>qla2xxx</filename>, <filename>qla3xxx</filename>,
       <filename>qla4xxx</filename>
      </para>
     </listitem>
    </itemizedlist>
   </sect2>
  </sect1>
  <sect1 id="bndgyoe">
   <title>What’s New in SLES 11</title>

   <para>
    The features and behavior changes noted in this section were made for
    the SUSE Linux Enterprise Server 11 release.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bi72tak" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bi72zf2" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b12xr11r" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="biu3u2m" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="biu3v96" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bin8klb" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="binagtl" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="binahn3" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bin9wm3" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bmqiv07" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bmy1edt" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bn4yxd1" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bvc111p" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bi72tak">
    <title>EVMS2 Is Deprecated</title>
    <para>
     The Enterprise Volume Management Systems (EVMS2) storage management
     solution is deprecated. All EVMS management modules have been removed
     from the SUSE Linux Enterprise Server 11 packages. Your non-system
     EVMS-managed devices should be automatically recognized and managed by
     Linux Volume Manager 2 (LVM2) when you upgrade your system.
<!--
      taroth 2014-05-09: commenting because redirects only to
      http://www.suse.com/, without any specific information on the topic:
      For more information, see
     <ulink url="http://www.novell.com/linux/volumemanagement/strategy.html">
     <citetitle>Evolution of Storage and Volume Management in SUSE Linux
     Enterprise</citetitle></ulink>.-->
    </para>
    <para>
     If you have EVMS managing the system device (any device that contains
     the root (<filename>/</filename>), <filename>/boot</filename>, or
     <filename>swap</filename>), try these things to prepare the SLES 10
     server before you reboot the server to upgrade:
    </para>
    <procedure id="b176hy1d">
     <step id="b176hy1e">
      <para>
       In the /etc/fstab file, modify the boot and swap disks to the default
       <filename>/dev/system/sys_lx</filename> directory:
      </para>
      <substeps>
       <step id="b176hy1f">
        <para>
         Remove <filename>/evms/lvm2</filename> from the path for the
         <filename>swap</filename> and root (<filename>/</filename>)
         partitions.
        </para>
       </step>
       <step id="b176hy1g">
        <para>
         Remove <filename>/evms</filename> from the path for
         <filename>/boot</filename> partition.
        </para>
       </step>
      </substeps>
     </step>
     <step id="b176hy1h">
      <para>
       In the <filename>/boot/grub/menu.lst</filename> file, remove
       <filename>/evms/lvm2</filename> from the path.
      </para>
     </step>
     <step id="b176hy1i">
      <para>
       In the <filename>/etc/sysconfig/bootloader</filename> file, verify
       that the path for the boot device is the <filename>/dev</filename>
       directory.
      </para>
     </step>
     <step id="b176hy1j">
      <para>
       Ensure that <filename>boot.lvm</filename> and
       <filename>boot.md</filename> are enabled:
      </para>
      <substeps>
       <step id="b176hy1k">
        <para>
<!--<remark>taroth 2014-02-27: systemd: CAVE - the following can stay as it is
         because it refers to the preparation of a  SLE *10* machine before
         upgrading it to a newer version</remark>-->
         In YaST, click <guimenu>System</guimenu><guimenu>Runlevel
         Editor</guimenu><guimenu>Expert Mode</guimenu>.
        </para>
       </step>
       <step id="b176hy1l">
        <para>
         Select <filename>boot.lvm</filename>.
        </para>
       </step>
       <step id="b176hy1m">
        <para>
         Click <guimenu>Set/Reset</guimenu><guimenu>Enable the
         Service</guimenu>.
        </para>
       </step>
       <step id="b176hy1n">
        <para>
         Select <filename>boot.md</filename>.
        </para>
       </step>
       <step id="b176hy1o">
        <para>
         Click <guimenu>Set/Reset</guimenu><guimenu>Enable the
         Service</guimenu>.
        </para>
       </step>
       <step id="b176hy1p">
        <para>
         Click <guimenu>Finish</guimenu>, then click <guimenu>Yes</guimenu>.
        </para>
       </step>
      </substeps>
     </step>
     <step id="b176hy1q">
      <para>
       Reboot and start the upgrade.
      </para>
     </step>
    </procedure>
    <para>
     For information about managing storage with EVMS2 on SUSE Linux
     Enterprise Server 10, see the
     <ulink url="http://www.suse.com/documentation/sles10/stor_admin/data/bookinfo.html"><citetitle>SUSE
     Linux Enterprise Server 10 SP3: Storage Administration
     Guide</citetitle></ulink>.
    </para>
   </sect2>

   <sect2 id="bi72zf2">
    <title>Ext3 as the Default File System</title>
    <para>
     The Ext3 file system has replaced ReiserFS as the default file system
     recommended by the YaST tools at installation time and when you create
     file systems. ReiserFS is still supported. For more information, see
     <ulink url="http://www.suse.com/products/server/technical-information/?tab=2"><citetitle>File
     System Support</citetitle></ulink> on the <citetitle>SUSE Linux
     Enterprise 11 Tech Specs</citetitle> Web page.
    </para>
   </sect2>

   <sect2 id="b12xr11r">
    <title>Default Inode Size Increased for Ext3</title>
    <para>
     <remark>Bug 708669</remark>
     To allow space for extended attributes and ACLs for a file on Ext3 file
     systems, the default inode size for Ext3 was increased from 128 bytes
     on SLES 10 to 256 bytes on SLES11. For information, see
     <xref linkend="sec_fileystems_major_ext3inodesize" xrefstyle="HeadingOnPage"/>.
    </para>
   </sect2>

   <sect2 id="biu3u2m">
    <title>JFS File System Is Deprecated</title>
    <para>
     The JFS file system is no longer supported. The JFS utilities were
     removed from the distribution.
    </para>
   </sect2>

   <sect2 id="biu3v96">
    <title>OCFS2 File System Is in the High Availability Release</title>
    <para>
     The OCFS2 file system is fully supported as part of the SUSE Linux
     Enterprise High Availability Extension.
    </para>
   </sect2>

   <sect2 id="bin8klb">
    <title>/dev/disk/by-name Is Deprecated</title>
    <para>
     The <filename>/dev/disk/by-name</filename> path is deprecated in SUSE
     Linux Enterprise Server 11 packages.
    </para>
   </sect2>

   <sect2 id="binagtl">
    <title>Device Name Persistence in the /dev/disk/by-id Directory</title>
    <para>
     In SUSE Linux Enterprise Server 11, the default multipath setup relies
     on <command>udev</command> to overwrite the existing symbolic links in
     the <filename>/dev/disk/by-id</filename> directory when multipathing is
     started. Before you start multipathing, the link points to the SCSI
     device by using its <filename>scsi-xxx</filename> name. When
     multipathing is running, the symbolic link points to the device by
     using its <filename>dm-uuid-xxx</filename> name. This ensures that the
     symbolic links in the <filename>/dev/disk/by-id</filename> path
     persistently point to the same device regardless of whether
     multipathing is started or not. The configuration files (such as
     <filename>lvm.conf</filename> and <filename>md.conf</filename>) do not
     need to be modified because they automatically point to the correct
     device.
    </para>
    <para>
     See the following sections for more information about how this behavior
     change affects other features:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <xref linkend="binahn3" xrefstyle="SectTitleOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bin9wm3" xrefstyle="SectTitleOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
   </sect2>

   <sect2 id="binahn3">
    <title>Filters for Multipathed Devices</title>
    <para>
     The deprecation of the <filename>/dev/disk/by-name</filename> directory
     (as described in <xref linkend="bin8klb" xrefstyle="SectTitleOnPage"/>)
     affects how you set up filters for multipathed devices in the
     configuration files. If you used the
     <filename>/dev/disk/by-name</filename> device name path for the
     multipath device filters in the <filename>/etc/lvm/lvm.conf</filename>
     file, you need to modify the file to use the
     <filename>/dev/disk/by-id</filename> path. Consider the following when
     setting up filters that use the <filename>by-id</filename> path:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The <filename>/dev/disk/by-id/scsi-*</filename> device names are
       persistent and created for exactly this purpose.
      </para>
     </listitem>
     <listitem>
      <para>
       Do not use the <filename>/dev/disk/by-id/dm-*</filename> name in the
       filters. These are symbolic links to the Device-Mapper devices, and
       result in reporting duplicate PVs in response to a
       <filename>pvscan</filename> command. The names appear to change from
       <filename>LVM-pvuuid</filename> to <filename>dm-uuid</filename> and
       back to <filename>LVM-pvuuid</filename>.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     For information about setting up filters, see
     <xref linkend="mpiousinglvm" xrefstyle="SectTitleOnPage"/>.
    </para>
   </sect2>

   <sect2 id="bin9wm3">
    <title>User-Friendly Names for Multipathed Devices</title>
    <para>
     A change in how multipathed device names are handled in the
     <filename>/dev/disk/by-id</filename> directory (as described in
     <xref linkend="binagtl" xrefstyle="SectTitleOnPage"/>) affects your
     setup for user-friendly names because the two names for the device
     differ. You must modify the configuration files to scan only the device
     mapper names after multipathing is configured.
    </para>
    <para>
     For example, you need to modify the <filename>lvm.conf</filename> file
     to scan using the multipathed device names by specifying the
     <filename>/dev/disk/by-id/dm-uuid-.*-mpath-.*</filename> path instead
     of <filename>/dev/disk/by-id</filename>.
    </para>
   </sect2>

   <sect2 id="bmqiv07">
    <title>Advanced I/O Load-Balancing Options for Multipath</title>
    <para>
     The following advanced I/O load-balancing options are available for
     Device Mapper Multipath, in addition to round-robin:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Least-pending
      </para>
     </listitem>
     <listitem>
      <para>
       Length-load-balancing
      </para>
     </listitem>
     <listitem>
      <para>
       Service-time
      </para>
     </listitem>
    </itemizedlist>
    <para>
     For information, see <xref linkend="b122sbgu" xrefstyle="RefNoPage"/>
     in <xref linkend="beg263n" xrefstyle="HeadingOnPage"/>.
    </para>
   </sect2>

   <sect2 id="bmy1edt">
    <title>Location Change for Multipath Tool Callouts</title>
    <para>
     The <filename>mpath_*</filename> prio_callouts for the Device Mapper
     Multipath tool have been moved to shared libraries
     in<filename>/lib/libmultipath/lib*</filename>. By using shared
     libraries, the callouts are loaded into memory on daemon startup. This
     helps avoid a system deadlock on an all-paths-down scenario where the
     programs need to be loaded from the disk, which might not be available
     at this point.
    </para>
   </sect2>

   <sect2 id="bn4yxd1">
    <title>Change from mpath to multipath for the mkinitrd -f Option</title>
    <para>
     The option for adding Device Mapper Multipath services to the
     <filename>initrd</filename> has changed from <literal>-f
     mpath</literal> to <literal>-f multipath</literal>.
    </para>
    <para>
     To make a new <filename>initrd</filename>, the command is now:
    </para>
<screen>
mkinitrd -f multipath
</screen>
   </sect2>

   <sect2 id="bvc111p">
    <title>Change from Multibus to Failover as the Default Setting for the MPIO Path Grouping Policy</title>
    <para>
     The default setting for the <literal>path_grouping_policy</literal> in
     the <filename>/etc/multipath.conf </filename>file has changed from
     <literal>multibus</literal> to <literal>failover</literal>.
    </para>
    <para>
     For information about configuring the path_grouping_policy, see
     <xref linkend="bbi89rh" xrefstyle="SectTitleOnPage"/>.
    </para>
   </sect2>
  </sect1>
 </chapter>
 <chapter id="storplan" lang="en">
  <title>Planning a Storage Solution</title>
  <para>
   Consider what your storage needs are and how you can effectively manage
   and divide your storage space to best meet your needs. Use the
   information in this section to help plan your storage deployment for file
   systems on your &productname; server.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="bi72kf0" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bi72p1s" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bi72o4h" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bi72q5v" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bi739fa" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="bi72kf0">
   <title>Partitioning Devices</title>

   <para>
    For information about using the YaST Expert Partitioner, see
    <xref linkend="sec.yast2.i_y2_part_expert"/>.
   </para>
  </sect1>
  <sect1 id="bi72p1s">
   <title>Multipath Support</title>

   <para>
    Linux supports using multiple I/O paths for fault-tolerant connections
    between the server and its storage devices. Linux multipath support is
    disabled by default. If you use a multipath solution that is provided by
    your storage subsystem vendor, you do not need to configure the Linux
    multipath separately.
   </para>
  </sect1>
  <sect1 id="bi72o4h">
   <title>Software RAID Support</title>

   <para>
    Linux supports hardware and software RAID devices. If you use hardware
    RAID devices, software RAID devices are unnecessary. You can use both
    hardware and software RAID devices on the same server.
   </para>

   <para>
    To maximize the performance benefits of software RAID devices,
    partitions used for the RAID should come from different physical
    devices. For software RAID 1 devices, the mirrored partitions cannot
    share any disks in common.
   </para>
  </sect1>
  <sect1 id="bi72q5v">
   <title>File System Snapshots</title>

   <para>
    Linux supports file system snapshots.
   </para>
  </sect1>
  <sect1 id="bi739fa">
   <title>Backup and Antivirus Support</title>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="biu2e2j" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="biu2e2k" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="biu2e2j">
    <title>Open Source Backup</title>
    <para>
     Open source tools for backing up data on Linux include
     <command>tar</command>, <command>cpio</command>, and
     <command>rsync</command>. See the man pages for these tools for more
     information.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       PAX: POSIX File System Archiver. It supports <command>cpio</command>
       and <command>tar</command>, which are the two most common forms of
       standard archive (backup) files. See the man page for more
       information.
      </para>
     </listitem>
     <listitem>
      <para>
       Amanda: The Advanced Maryland Automatic Network Disk Archiver. See
       <ulink url="http://www.amanda.org/">www.amanda.org</ulink>.
      </para>
     </listitem>
    </itemizedlist>
   </sect2>

   <sect2 id="biu2e2k">
    <title>Commercial Backup and Antivirus Support</title>
    <para>
     Novell Open Enterprise Server (OES) 2 for Linux is a product that
     includes SUSE Linux Enterprise Server (SLES) 10. Antivirus and backup
     software vendors who support OES 2 also support SLES 10. You can visit
     the vendor Web sites to find out about their scheduled support of SLES
     11.
    </para>
    <para>
     For a current list of possible backup and antivirus software vendors,
     see
     <ulink url="http://www.novell.com/products/openenterpriseserver/partners_communities.html"><citetitle>Novell
     Open Enterprise Server Partner Support: Backup and Antivirus
     Support</citetitle></ulink>. This list is updated quarterly.
    </para>
   </sect2>
  </sect1>
 </chapter>
 <chapter id="lvm" lang="en">
  <title>LVM Configuration</title>
  <para>
   This section briefly describes the principles behind Logical Volume
   Manager (LVM) and its basic features that make it useful under many
   circumstances. The YaST LVM configuration can be reached from the YaST
   Expert Partitioner. This partitioning tool enables you to edit and delete
   existing partitions and create new ones that should be used with LVM.
  </para>
  <warning>
   <para>
    Using LVM might be associated with increased risk, such as data loss.
    Risks also include application crashes, power failures, and faulty
    commands. Save your data before implementing LVM or reconfiguring
    volumes. Never work without a backup.
   </para>
  </warning>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="sec_yast2_system_lvm_explanation" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bi706ct" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bgchx8a" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bgchx8b" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bgchx8c" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="lvm_activate_vgs" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="lvmtagging" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="brgpqe2" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="brgqpfy" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="biuz4lz" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="brgpmw3" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="brgob84" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="btx9jxu" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="sec_yast2_system_lvm_explanation">
   <title>Understanding the Logical Volume Manager</title>

   <para>
    LVM enables flexible distribution of hard disk space over several file
    systems. It was developed because the need to change the segmentation of
    hard disk space might arise only after the initial partitioning has
    already been done during installation. Because it is difficult to modify
    partitions on a running system, LVM provides a virtual pool (volume
    group or VG) of memory space from which logical volumes (LVs) can be
    created as needed. The operating system accesses these LVs instead of
    the physical partitions. Volume groups can span more than one disk, so
    that several disks or parts of them can constitute one single VG. In
    this way, LVM provides a kind of abstraction from the physical disk
    space that allows its segmentation to be changed in a much easier and
    safer way than through physical repartitioning.
   </para>

   <para role="intro">
    <xref linkend="bo1t1qx" xrefstyle="FigureXRef"/> compares physical
    partitioning (left) with LVM segmentation (right). On the left side, one
    single disk has been divided into three physical partitions (PART), each
    with a mount point (MP) assigned so that the operating system can access
    them. On the right side, two disks have been divided into two and three
    physical partitions each. Two LVM volume groups (VG&nbsp;1 and
    VG&nbsp;2) have been defined. VG&nbsp;1 contains two partitions from
    DISK&nbsp;1 and one from DISK&nbsp;2. VG&nbsp;2 contains the remaining
    two partitions from DISK&nbsp;2.
   </para>

   <figure pgwide="0" id="bo1t1qx">
    <title>Physical Partitioning versus LVM</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="lvm.svg" width="445pt" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="lvm.png" width="445pt" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>

   <para>
    In LVM, the physical disk partitions that are incorporated in a volume
    group are called physical volumes (PVs). Within the volume groups in
    <xref linkend="bo1t1qx" xrefstyle="FigureXRef"/>, four logical volumes
    (LV&nbsp;1 through LV&nbsp;4) have been defined, which can be used by
    the operating system via the associated mount points. The border between
    different logical volumes need not be aligned with any partition border.
    See the border between LV&nbsp;1 and LV&nbsp;2 in this example.
   </para>

   <para>
    LVM features:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      Several hard disks or partitions can be combined in a large logical
      volume.
     </para>
    </listitem>
    <listitem>
     <para>
      Provided the configuration is suitable, an LV (such as
      <filename>/usr</filename>) can be enlarged when the free space is
      exhausted.
     </para>
    </listitem>
    <listitem>
     <para>
      Using LVM, it is possible to add hard disks or LVs in a running
      system. However, this requires hot-swappable hardware that is capable
      of such actions.
     </para>
    </listitem>
    <listitem>
     <para>
      It is possible to activate a <emphasis>striping mode</emphasis> that
      distributes the data stream of a logical volume over several physical
      volumes. If these physical volumes reside on different disks, this can
      improve the reading and writing performance like RAID&nbsp;0.
     </para>
    </listitem>
    <listitem>
     <para>
      The snapshot feature enables consistent backups (especially for
      servers) in the running system.
     </para>
    </listitem>
   </itemizedlist>

   <para>
    With these features, using LVM already makes sense for heavily used home
    PCs or small servers. If you have a growing data stock, as in the case
    of databases, music archives, or user directories, LVM is especially
    useful. It allows file systems that are larger than the physical hard
    disk. Another advantage of LVM is that up to 256 LVs can be added.
    However, keep in mind that working with LVM is different from working
    with conventional partitions.
   </para>

   <para>
    Starting from kernel version&nbsp;2.6, LVM version&nbsp;2 is available,
    which is downward-compatible with the previous LVM and enables the
    continued management of old volume groups. When creating new volume
    groups, decide whether to use the new format or the downward-compatible
    version. LVM&nbsp;2 does not require any kernel patches. It makes use of
    the device mapper integrated in kernel 2.6. This kernel only supports
    LVM version&nbsp;2. Therefore, when talking about LVM, this section
    always refers to LVM version&nbsp;2.
   </para>

   <para>
    You can manage new or existing LVM storage objects by using the YaST
    Partitioner. Instructions and further information about configuring LVM
    is available in the official
    <ulink url="http://tldp.org/HOWTO/LVM-HOWTO/"><citetitle>LVM
    HOWTO</citetitle></ulink>.
   </para>

   <important>
    <para>
     If you add multipath support after you have configured LVM, you must
     modify the <filename>/etc/lvm/lvm.conf</filename> file to scan only the
     multipath device names in the <filename>/dev/disk/by-id</filename>
     directory as described in
     <xref linkend="mpiousinglvm" xrefstyle="SectTitleOnPage"/>, then reboot
     the server.
    </para>
   </important>
  </sect1>
  <sect1 id="bi706ct">
   <title>Creating LVM Partitions</title>

   <para>
    For each disk, partition the free space that you want to use for LVM as
    <literal>0x8E Linux LVM</literal>. You can create one or multiple LVM
    partitions on a single device. It is not necessary for all of the
    partitions on a device to be LVM partitions.
   </para>

   <para>
    You can use the Volume Group function to group one or more LVM
    partitions into a logical pool of space called a volume group, then
    carve out one or more logical volumes from the space in the volume
    group.
   </para>

   <para>
    In the YaST Partitioner, only the free space on the disk is made
    available to you as you are creating LVM partitions. If you want to use
    the entire disk for a single LVM partition and other partitions already
    exists on the disk, you must first remove all of the existing partitions
    to free the space before you can use that space in an LVM partition.
   </para>

   <warning>
    <para>
     Deleting a partition destroys all of the data in the partition.
    </para>
   </warning>

   <procedure id="bq2fjmu">
    <step id="bq2fjmv">
     <para>
      Launch YaST as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="bq2fk0j">
     <para>
      In YaST, open the <guimenu>Partitioner</guimenu>.
     </para>
    </step>
    <step id="brgn1yt">
     <para>
      (Optional) Remove one or more existing partitions to free that space
      and make it available for the LVM partition you want to create.
     </para>
     <para>
      For information, see
      <xref linkend="brgob84" xrefstyle="SectTitleOnPage"/>.
     </para>
    </step>
    <step id="bq2fk9e">
     <para>
      On the Partitions page, click <guimenu>Add</guimenu>.
     </para>
    </step>
    <step id="brgodov">
     <para>
      Under <guimenu>New Partition Type</guimenu>, select <guimenu>Primary
      Partition</guimenu> or<guimenu> Extended Partition</guimenu>, then
      click <guimenu>Next</guimenu>.
     </para>
    </step>
    <step id="brgoefx">
     <para role="intro">
      Specify the <guimenu>New Partition Size</guimenu>, then click
      <guimenu>Next</guimenu>.
     </para>
     <informaltable frame="topbot" rowsep="0" pgwide="0">
      <tgroup cols="2">
       <colspec colnum="1" colname="1" colwidth="5001*"/>
       <colspec colnum="2" colname="2" colwidth="5001*"/>
       <tbody>
        <row id="brgpexh">
         <entry>
          <mediaobject>
           <imageobject role="fo">
            <imagedata fileref="yast2_lvm2_a.png" width="151pt" format="PNG"/>
           </imageobject>
           <imageobject role="html">
            <imagedata fileref="yast2_lvm2_a.png" width="151pt" format="PNG"/>
           </imageobject>
          </mediaobject>
         </entry>
         <entry>
          <itemizedlist>
           <listitem>
            <formalpara id="brgof07">
             <title>Maximum Size:</title>
             <para>
              Use all of the free available space on the disk.
             </para>
            </formalpara>
           </listitem>
           <listitem>
            <formalpara id="brgooyf">
             <title>Custom Size:</title>
             <para>
              Specify a size up the amount of free available space on the
              disk.
             </para>
            </formalpara>
           </listitem>
           <listitem>
            <formalpara id="brgooyg">
             <title>Custom Region:</title>
             <para>
              Specify the start and end cylinder of the free available space
              on the disk.
             </para>
            </formalpara>
           </listitem>
          </itemizedlist>
         </entry>
        </row>
       </tbody>
      </tgroup>
     </informaltable>
    </step>
    <step id="brgpahi">
     <para role="intro">
      Configure the partition format:
     </para>
     <informaltable frame="topbot" rowsep="0" pgwide="0">
      <tgroup cols="2">
       <colspec colnum="1" colname="1" colwidth="5001*"/>
       <colspec colnum="2" colname="2" colwidth="5001*"/>
       <tbody>
        <row id="brgpi5z">
         <entry>
          <mediaobject>
           <imageobject role="fo">
            <imagedata fileref="yast2_lvm3_a.png" width="139pt" format="PNG"/>
           </imageobject>
           <imageobject role="html">
            <imagedata fileref="yast2_lvm3_a.png" width="139pt" format="PNG"/>
           </imageobject>
          </mediaobject>
         </entry>
         <entry>
          <orderedlist>
           <listitem>
            <para>
             Under <guimenu>Formatting Options</guimenu>, select <guimenu>Do
             not format</guimenu>.
            </para>
           </listitem>
           <listitem>
            <para>
             From the <guimenu>File System ID</guimenu> drop-down list,
             select <guimenu>0x8E Linux LVM</guimenu> as the partition
             identifier.
            </para>
           </listitem>
           <listitem>
            <para>
             Under <guimenu>Mounting Options</guimenu>, select <guimenu>Do
             not mount partition</guimenu>.
            </para>
           </listitem>
          </orderedlist>
         </entry>
        </row>
       </tbody>
      </tgroup>
     </informaltable>
    </step>
    <step id="brgosll">
     <para>
      Click <guimenu>Finish</guimenu>.
     </para>
     <para>
      The partitions are not actually created until you click
      <guimenu>Next</guimenu> and <guimenu>Finish</guimenu> to exit the
      partitioner.
     </para>
    </step>
    <step id="bq2fkxr">
     <para>
      Repeat <xref linkend="bq2fk9e" xrefstyle="StepXRef"/> through
      <xref linkend="brgosll" xrefstyle="StepXRef"/> for each Linux LVM
      partition you want to add.
     </para>
    </step>
    <step id="brgouy6">
     <para>
      Click <guimenu>Next</guimenu>, verify that the new Linux LVM
      partitions are listed, then click <guimenu>Finish</guimenu> to exit
      the partitioner.
     </para>
    </step>
    <step id="bq2fl52">
     <para>
      (Optional) Continue with the Volume Group configuration as described
      in <xref linkend="bgchx8a" xrefstyle="SectTitleOnPage"/>.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="bgchx8a">
   <title>Creating Volume Groups</title>

   <para>
    An LVM volume group organizes the Linux LVM partitions into a logical
    pool of space. You can carve out logical volumes from the available
    space in the group. The Linux LVM partitions in a group can be on the
    same or different disks. You can add LVM partitions from the same or
    different disks to expand the size of the group. Assign all partitions
    reserved for LVM to a volume group. Otherwise, the space on the
    partition remains unused.
   </para>

   <procedure id="brgncwg">
    <step id="brgnjca">
     <para>
      Launch YaST as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="brgnjcb">
     <para>
      In YaST, open the <guimenu>Partitioner</guimenu>.
     </para>
    </step>
    <step id="brgncwh">
     <para>
      In the left panel, click <guimenu>Volume Management</guimenu>.
     </para>
     <para>
      A list of existing Volume Groups are listed in the right panel.
     </para>
    </step>
    <step id="brgnd6f">
     <para role="intro">
      At the lower left of the Volume Management page, click <guimenu>Add
      Volume Group</guimenu>.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_lvm4_a.png" width="329pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_lvm4_a.png" width="329pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="brgndwo">
     <para>
      Specify the <guimenu>Volume Group Name</guimenu>.
     </para>
     <para>
      If you are creating a volume group at install time, the name
      <literal>system</literal> is suggested for a volume group that will
      contain the SUSE Linux Enterprise Server system files.
     </para>
    </step>
    <step id="brgnght">
     <para>
      Specify the <guimenu>Physical Extent Size</guimenu>.
     </para>
     <para>
      The <guimenu>Physical Extent Size</guimenu> defines the size of a
      physical block in the volume group. All the disk space in a volume
      group is handled in chunks of this size. Values can be from 1 KB to 16
      GB in powers of 2. This value is normally set to 4&nbsp;MB.
     </para>
     <para>
      In LVM1, a 4 MB physical extent allowed a maximum LV size of 256 GB
      because it supports only up to 65534 extents per LV. VM2 does not
      restrict the number of physical extents. Having a large number of
      extents has no impact on I/O performance to the logical volume, but it
      slows down the LVM tools.
     </para>
     <important>
      <para>
       Different physical extent sizes should not be mixed in a single VG.
       The extent should not be modified after the initial setup.
      </para>
     </important>
    </step>
    <step id="brgnhd9">
     <para>
      In the <guimenu>Available Physical Volumes</guimenu> list, select the
      Linux LVM partitions that you want to make part of this volume group,
      then click <guimenu>Add</guimenu> to move them to the
      <guimenu>Selected Physical Volumes</guimenu> list.
     </para>
    </step>
    <step id="brgnihe">
     <para>
      Click <guimenu>Finish</guimenu>.
     </para>
     <para>
      The new group appears in the <guimenu>Volume Groups</guimenu> list.
     </para>
    </step>
    <step id="brgnk9f">
     <para>
      On the Volume Management page, click <guimenu>Next</guimenu>, verify
      that the new volume group is listed, then click
      <guimenu>Finish</guimenu>.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="bgchx8b">
   <title>Configuring Physical Volumes</title>

   <para role="intro">
    When the Linux LVM partitions are assigned to a volume group, the
    partitions are then referred to as a physical volumes.
   </para>

   <figure pgwide="0" id="fig.yast2.lvm5">
    <title>Physical Volumes in the Volume Group Named Home</title>
<!--Physical Volume Setup-->
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast2_lvm5_a.png" width="329pt" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast2_lvm5_a.png" width="329pt" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>

   <para>
    To add more physical volumes to an existing volume group:
   </para>

   <procedure id="brgq6lc">
    <step id="brgq6ld">
     <para>
      Launch YaST as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="brgq6le">
     <para>
      In YaST, open the <guimenu>Partitioner</guimenu>.
     </para>
    </step>
    <step id="brgq6lf">
     <para>
      In the left panel, select Volume Management and expand the list of
      groups.
     </para>
    </step>
    <step id="brgq6lg">
     <para>
      Under Volume Management, select the volume group, then click the
      <guimenu>Overview</guimenu> tab.
     </para>
    </step>
    <step id="brgq6lh">
     <para role="intro">
      At the bottom of the page, click <guimenu>Resize</guimenu>.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_lvm8_a.png" width="329pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_lvm8_a.png" width="329pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="brgq6li">
     <para>
      Select a physical volume (LVM partitions) from the <guimenu>Available
      Physical Volumes</guimenu> list then click Add to move it to the
      <guimenu>Selected Physical Volumes</guimenu> list.
     </para>
    </step>
    <step id="brgq6lj">
     <para>
      Click <guimenu>Finish</guimenu>.
     </para>
    </step>
    <step id="brgq6lk">
     <para>
      Click <guimenu>Next</guimenu>, verify that the changes are listed,
      then click <guimenu>Finish</guimenu>.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="bgchx8c">
   <title>Configuring Logical Volumes</title>

   <para>
    After a volume group has been filled with physical volumes, use the
    Logical Volumes dialog box (see
    <xref linkend="fig.yast2.lvm6" xrefstyle="FigureXRef"/>) to define and
    manage the logical volumes that the operating system should use. This
    dialog lists all of the logical volumes in that volume group. You can
    use <guimenu>Add</guimenu>, <guimenu>Edit</guimenu>, and
    <guimenu>Remove</guimenu> options to manage the logical volumes. Assign
    at least one logical volume to each volume group. You can create new
    logical volumes as needed until all free space in the volume group has
    been exhausted.
   </para>

   <para>
    Beginning in SLES 11 SP3, an LVM logical volume can be thinly
    provisioned. Thin provisioning allows you to create logical volumes with
    sizes that overbook the available free space. You create a thin pool
    that contains unused space reserved for use with an arbitrary number of
    thin volumes. A thin volume is created as a sparse volume and space is
    allocated from a thin pool as needed. The thin pool can be expanded
    dynamically when needed for cost-effective allocation of storage space.
   </para>

   <important>
    <para>
     To use thinly provisioned volumes in a cluster, the thin pool and the
     thin volumes that use it must be managed in a single cluster resource.
     This allows the thin volumes and thin pool to always be mounted
     exclusively on the same node.
    </para>
   </important>

   <figure pgwide="0" id="fig.yast2.lvm6">
    <title>Logical Volume Management</title>
<!--Logical Volume Management-->
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast2_lvm6_a.png" width="329pt" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast2_lvm6_a.png" width="329pt" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>

   <para>
    It is possible to distribute the data stream in the logical volume among
    several physical volumes (striping). If these physical volumes reside on
    different hard disks, this generally results in a better reading and
    writing performance (like RAID&nbsp;0). However, a striping LV with
    <literal>n</literal> stripes can only be created correctly if the hard
    disk space required by the LV can be distributed evenly to
    <literal>n</literal> physical volumes. For example, if only two physical
    volumes are available, a logical volume with three stripes is
    impossible.
   </para>

   <procedure id="brgq5hr">
    <step id="brgq6ll">
     <para>
      Launch YaST as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="brgq6lm">
     <para>
      In YaST, open the <guimenu>Partitioner</guimenu>.
     </para>
    </step>
    <step id="brgq6ln">
     <para>
      In the left panel, select Volume Management and expand it to see the
      list of volume groups.
     </para>
    </step>
    <step id="brgq6lo">
     <para>
      Under Volume Management, select the volume group, then click the
      <guimenu>Logical Volumes</guimenu> tab.
     </para>
    </step>
    <step id="brgq5hs">
     <para>
      In the lower left, click <guimenu>Add</guimenu> to open the Add
      Logical Volume dialog box.
     </para>
    </step>
    <step id="brgq8gl">
     <para role="intro">
      Specify the <guimenu>Name</guimenu> for the logical volume, then click
      <guimenu>Next</guimenu>.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_lvm9_a.png" width="137pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_lvm9_a.png" width="137pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b15cmz3e">
     <para>
      Specify the type of LVM volume:
     </para>
     <itemizedlist>
      <listitem>
       <formalpara id="b15cmz3f">
        <title>Normal volume:</title>
        <para>
         (Default) The volume’s space is allocated immediately.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="b15cmz3g">
        <title>Thin pool:</title>
        <para>
         The logical volume is a pool of space that is reserved for use with
         thin volumes. The thin volumes can allocate their needed space from
         it on demand.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="b15cmz3h">
        <title>Thin volume:</title>
        <para>
         The volume is created as a sparse volume. The volume allocates
         needed space on demand from a thin pool.
        </para>
       </formalpara>
      </listitem>
     </itemizedlist>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_lvm9a_a.png" width="137pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_lvm9a_a.png" width="137pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="brgqd13">
     <para role="intro">
      Specify the size of the volume and whether to use multiple stripes.
     </para>
     <informaltable frame="topbot" rowsep="0" pgwide="0">
      <tgroup cols="2">
       <colspec colnum="1" colname="1" colwidth="5001*"/>
       <colspec colnum="2" colname="2" colwidth="5001*"/>
       <tbody>
        <row id="brgqgrg">
         <entry>
          <mediaobject>
           <imageobject role="fo">
            <imagedata fileref="yast2_lvm10_a.png" width="155pt" format="PNG"/>
           </imageobject>
           <imageobject role="html">
            <imagedata fileref="yast2_lvm10_a.png" width="155pt" format="PNG"/>
           </imageobject>
          </mediaobject>
         </entry>
         <entry>
          <orderedlist>
           <listitem>
            <para>
             Specify the size of the logical volume, up to the maximum size
             available.
            </para>
            <para>
             The amount of free space in the current volume group is shown
             next to the <guimenu>Maximum Size</guimenu> option.
            </para>
           </listitem>
           <listitem>
            <para>
             Specify the number of stripes.
            </para>
            <warning>
             <para>
              YaST has no chance at this point to verify the correctness of
              your entries concerning striping. Any mistake made here is
              apparent only later when the LVM is implemented on disk.
             </para>
            </warning>
           </listitem>
          </orderedlist>
         </entry>
        </row>
       </tbody>
      </tgroup>
     </informaltable>
    </step>
    <step id="brgqevk">
     <para>
      Specify the formatting options for the logical volume:
     </para>
     <informaltable frame="topbot" rowsep="1" pgwide="0">
      <tgroup cols="2">
       <colspec colnum="1" colname="1" colwidth="5001*"/>
       <colspec colnum="2" colname="2" colwidth="5001*"/>
       <tbody>
        <row id="brgqevl">
         <entry>
          <mediaobject>
           <imageobject role="fo">
            <imagedata fileref="yast2_lvm11_a.png" width="129pt" format="PNG"/>
           </imageobject>
           <imageobject role="html">
            <imagedata fileref="yast2_lvm11_a.png" width="129pt" format="PNG"/>
           </imageobject>
          </mediaobject>
         </entry>
         <entry>
          <orderedlist>
           <listitem>
            <para>
             Under <guimenu>Formatting Options</guimenu>, select
             <guimenu>Format partition</guimenu>, then select the format
             type from the <guimenu>File system</guimenu> drop-down list,
             such as Ext3.
            </para>
           </listitem>
           <listitem>
            <para>
             Under <guimenu>Mounting Options</guimenu>, select
             <guimenu>Mount partition</guimenu>, then select the mount
             point.
            </para>
            <para>
             The files stored on this logical volume can be found at this
             mount point on the installed system.
            </para>
           </listitem>
           <listitem>
            <para>
             Click <guimenu>Fstab Options</guimenu> to add special mounting
             options for the volume.
            </para>
           </listitem>
          </orderedlist>
         </entry>
        </row>
       </tbody>
      </tgroup>
     </informaltable>
    </step>
    <step id="brgqmpf">
     <para>
      Click <guimenu>Finish</guimenu>.
     </para>
    </step>
    <step id="brgqn20">
     <para>
      Click <guimenu>Next</guimenu>, verify that the changes are listed,
      then click <guimenu>Finish</guimenu>.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="lvm_activate_vgs">
   <title>Automatically Activating Non-Root LVM Volume Groups</title>

   <para>
    Activation behavior for non-root LVM volume groups is controlled by
    parameter settings in the <literal>/etc/sysconfig/lvm</literal> file.
   </para>

   <para>
    By default, non-root LVM volume groups are automatically activated on
    system restart by <filename>/etc/rc.d/boot.lvm</filename>, according to
    the setting for the <literal>LVM_VGS_ACTIVATED_ON_BOOT</literal>
    parameter in the <literal>/etc/sysconfig/lvm</literal> file. This
    parameter allows you to activate all volume groups on system restart, or
    to activate only specified non-root LVM volume groups.
   </para>

   <para>
    To activate all non-root LVM volume groups on system restart, ensure
    that the value for the <literal>LVM_VGS_ACTIVATED_ON_BOOT</literal>
    parameter in the <filename>/etc/sysconfig/lvm</filename> file is empty
    (<literal>""</literal>). This is the default setting. For almost all
    standard LVM installations, it can safely stay empty.
   </para>

<screen>
LVM_VGS_ACTIVATED_ON_BOOT="" 
</screen>

   <para>
    To activate only a specified non-root LVM volume group on system
    restart, specify the volume group name as the value for the
    <literal>LVM_VGS_ACTIVATED_ON_BOOT</literal> parameter:
   </para>

<screen>
LVM_VGS_ACTIVATED_ON_BOOT="vg1" 
</screen>

   <para>
    By default, newly discovered LVM volume groups are not automatically
    activated. The <literal>LVM_ACTIVATED_ON_DISCOVERED</literal> parameter
    is disabled in the <filename>/etc/sysconfig/lvm</filename> file:
   </para>

<screen>
LVM_ACTIVATED_ON_DISCOVERED="disable"
</screen>

   <para>
    You can enable the <literal>LVM_ACTIVATED_ON_DISCOVERED</literal>
    parameter to allow newly discovered LVM volume groups to be activated
    via udev rules:
   </para>

<screen>
LVM_ACTIVATED_ON_DISCOVERED="enable"
</screen>
  </sect1>
  <sect1 id="lvmtagging">
   <title>Tagging LVM2 Storage Objects</title>

   <para role="intro">
    A tag is an unordered keyword or term assigned to the metadata of a
    storage object. Tagging allows you to classify collections of LVM
    storage objects in ways that you find useful by attaching an unordered
    list of tags to their metadata.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bwkc8pi" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwkc1zg" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwkc1ta" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwkepmz" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwkdms3" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwkdqfz" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bwkc8pi">
    <title>Using LVM2 Tags</title>
    <para role="intro">
     After you tag the LVM2 storage objects, you can use the tags in
     commands to accomplish the following tasks:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Select LVM objects for processing according to the presence or
       absence of specific tags.
      </para>
     </listitem>
     <listitem>
      <para>
       Use tags in the configuration file to control which volume groups and
       logical volumes are activated on a server.
      </para>
     </listitem>
     <listitem>
      <para>
       Override settings in a global configuration file by specifying tags
       in the command.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     A tag can be used in place of any command line LVM object reference
     that accepts:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       a list of objects
      </para>
     </listitem>
     <listitem>
      <para>
       a single object as long as the tag expands to a single object
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Replacing the object name with a tag is not supported everywhere yet.
     After the arguments are expanded, duplicate arguments in a list are
     resolved by removing the duplicate arguments, and retaining the first
     instance of each argument.
    </para>
    <para>
     Wherever there might be ambiguity of argument type, you must prefix a
     tag with the commercial at sign (@) character, such as
     <literal>@mytag</literal>. Elsewhere, using the <quote>@</quote> prefix
     is optional.
    </para>
   </sect2>

   <sect2 id="bwkc1zg">
    <title>Requirements for Creating LVM2 Tags</title>
    <para role="intro">
     Consider the following requirements when using tags with LVM:
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bwkc4lx" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bwkc75c" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bwkc4lx">
     <title>Supported Characters</title>
     <para>
      An LVM tag word can contain the ASCII uppercase characters A to Z,
      lowercase characters a to z, numbers 0 to 9, underscore (_), plus (+),
      hyphen (-), and period (.). The word cannot begin with a hyphen. The
      maximum length is 128 characters.
     </para>
    </sect3>
    <sect3 id="bwkc75c">
     <title>Supported Storage Objects</title>
     <para>
      You can tag LVM2 physical volumes, volume groups, logical volumes, and
      logical volume segments. PV tags are stored in its volume group’s
      metadata. Deleting a volume group also deletes the tags in the
      orphaned physical volume. Snapshots cannot be tagged, but their origin
      can be tagged.
     </para>
     <para>
      LVM1 objects cannot be tagged because the disk format does not support
      it.
     </para>
    </sect3>
   </sect2>

   <sect2 id="bwkc1ta">
    <title>Command Line Tag Syntax</title>
    <variablelist>
     <varlistentry id="bwkddun">
      <term>--addtag &lt;<replaceable>tag_info</replaceable>&gt;</term>
      <listitem>
       <para>
        Add a tag to (or <emphasis>tag</emphasis>) an LVM2 storage object.
       </para>
       <formalpara id="bwkdj1p">
        <title>Example</title>
        <para/>
       </formalpara>
<screen>
vgchange --addtag @db1 vg1
</screen>
      </listitem>
     </varlistentry>
     <varlistentry id="bwkdh2t">
      <term>--deltag &lt;<replaceable>tag_info</replaceable>&gt;</term>
      <listitem>
       <para>
        Remove a tag from (or <emphasis>untag</emphasis>) an LVM2 storage
        object.
       </para>
       <formalpara id="bwkdjin">
        <title>Example</title>
        <para/>
       </formalpara>
<screen>
vgchange --deltag @db1 vg1
</screen>
      </listitem>
     </varlistentry>
     <varlistentry id="bwkdh2u">
      <term>--tag &lt;<replaceable>tag_info</replaceable>&gt;</term>
      <listitem>
       <para>
        Specify the tag to use to narrow the list of volume groups or
        logical volumes to be activated or deactivated.
       </para>
       <formalpara id="bwkdg98">
        <title>Example</title>
        <para/>
       </formalpara>
       <para>
        Enter the following to activate it the volume if it has a tag that
        matches the tag provided:
       </para>
<screen>
lvchange -ay --tag @db1 vg1/vol2
</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect2>

   <sect2 id="bwkepmz">
    <title>Configuration File Syntax</title>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bwkcqoj" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bwkeuio" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bwkctd3" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bwkcwqd" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bwkcqoj">
     <title>Enabling Hostname Tags in the lvm.conf File</title>
     <para>
      Add the following code to the <filename>/etc/lvm/lvm.conf</filename>
      file to enable host tags that are defined separately on host in a
      <filename>/etc/lvm/lvm_&lt;<replaceable>hostname</replaceable>&gt;.conf</filename>
      file.
     </para>
<screen>
tags {
   # Enable hostname tags
   hosttags = 1
}
</screen>
     <para>
      You place the activation code in the
      <filename>/etc/lvm/lvm_&lt;<replaceable>hostname</replaceable>&gt;.conf</filename>
      file on the host. See
      <xref linkend="bwkctd3" xrefstyle="HeadingOnPage"/>.
     </para>
    </sect3>
    <sect3 id="bwkeuio">
     <title>Defining Tags for Hostnames in the lvm.conf File</title>
<screen>
tags {

   tag1 { }
      # Tag does not require a match to be set.

   tag2 {
      # If no exact match, tag is not set.
      host_list = [ "hostname1", "hostname2" ]
   }
}
</screen>
    </sect3>
    <sect3 id="bwkctd3">
     <title>Defining Activation</title>
     <para>
      You can modify the <filename>/etc/lvm/lvm.conf</filename> file to
      activate LVM logical volumes based on tags.
     </para>
     <para>
      In a text editor, add the following code to the file:
     </para>
<screen>
  activation {
      volume_list = [ "vg1/lvol0", "@database" ]
  }
</screen>
     <para>
      Replace <literal>@database</literal> with the your tag. Use
      <literal>"@*"</literal> to match the tag against any tag set on the
      host.
     </para>
     <para>
      The activation command matches against
      <replaceable>vgname</replaceable>,
      <replaceable>vgname/lvname</replaceable>, or
      @<replaceable>tag</replaceable> set in the metadata of volume groups
      and logical volumes. A volume group or logical volume is activated
      only if a metadata tag matches. The default if there is no match is
      not to activate.
     </para>
     <para>
      If <literal>volume_list</literal> is not present and any tags are
      defined on the host, then it activates the volume group or logical
      volumes only if a host tag matches a metadata tag.
     </para>
     <para>
      If <literal>volume_list</literal> is not present and no tags are
      defined on the host, then it does activate.
     </para>
    </sect3>
    <sect3 id="bwkcwqd">
     <title>Defining Activation in Multiple Hostname Configuration Files</title>
     <para>
      You can use the activation code in a host’s configuration file
      (<filename>/etc/lvm/lvm_&lt;<replaceable>host_tag</replaceable>&gt;.conf</filename>)
      when host tags are enabled in the <filename>lvm.conf</filename> file.
      For example, a server has two configuration files in the
      <filename>/etc/lvm/</filename> folder:
     </para>
     <simplelist>
      <member><filename>lvm.conf</filename>
      </member>
      <member><filename>lvm_&lt;<replaceable>host_tag</replaceable>&gt;.conf</filename>
      </member>
     </simplelist>
     <para>
      At startup, load the <filename>/etc/lvm/lvm.conf</filename> file, and
      process any tag settings in the file. If any host tags were defined,
      it loads the related
      <filename>/etc/lvm/lvm_&lt;<replaceable>host_tag</replaceable>&gt;.conf</filename>
      file.When it searches for a specific configuration file entry, it
      searches the host tag file first, then the <filename>lvm.conf
      </filename>file, and stops at the first match.Within the
      <filename>lvm_&lt;<replaceable>host_tag</replaceable>&gt;.conf</filename>
      file, use the reverse order that tags were set. This allows the file
      for the last tag set to be searched first. New tags set in the host
      tag file will trigger additional configuration file loads.
     </para>
    </sect3>
   </sect2>

   <sect2 id="bwkdms3">
    <title>Using Tags for a Simple Activation Control in a Cluster</title>
    <para>
     You can set up a simple hostname activation control by enabling the
     <literal>hostname_tags</literal> option in a the
     <filename>/etc/lvm/lvm.conf</filename> file. Use the same file on every
     machine in a cluster so that it is a global setting.
    </para>
    <procedure id="bwkel6y">
     <step id="bwkel6z">
      <para>
       In a text editor, add the following code to the
       <filename>/etc/lvm/lvm.conf</filename> file:
      </para>
<screen>
tags {
   hostname_tags = 1
}
</screen>
     </step>
     <step id="bwkelf9">
      <para>
       Replicate the file to all hosts in the cluster.
      </para>
     </step>
     <step id="bwkelow">
      <para>
       From any machine in the cluster, add <literal>db1</literal> to the
       list of machines that activate <filename>vg1/lvol2</filename>:
      </para>
<screen>
lvchange --addtag @db1 vg1/lvol2
</screen>
     </step>
     <step id="bwkelx8">
      <para>
       On the <filename>db1</filename> server, enter the following to
       activate it:
      </para>
<screen>
lvchange -ay vg1/vol2
</screen>
     </step>
    </procedure>
   </sect2>

   <sect2 id="bwkdqfz">
    <title>Using Tags to Activate On Preferred Hosts in a Cluster</title>
    <para>
     The examples in this section demonstrate two methods to accomplish the
     following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Activate volume group <filename>vg1</filename> only on the database
       hosts <filename>db1</filename> and <filename>db2</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       Activate volume group <filename>vg2</filename> only on the file
       server host <filename>fs1</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       Activate nothing initially on the file server backup host
       <filename>fsb1</filename>, but be prepared for it to take over from
       the file server host <filename>fs1</filename>.
      </para>
     </listitem>
    </itemizedlist>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bwkdsty" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bwkdtdw" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bwkdsty">
     <title>Option 1: Centralized Admin and Static Configuration Replicated Between Hosts</title>
     <para>
      In the following solution, the single configuration file is replicated
      among multiple hosts.
     </para>
     <procedure id="bwkdvwj">
      <step id="bwkdvwk">
       <para>
        Add the <literal>@database</literal> tag to the metadata of volume
        group <filename>vg1</filename>. In a terminal console, enter
       </para>
<screen>
vgchange --addtag @database vg1
</screen>
      </step>
      <step id="bwke0dt">
       <para>
        Add the <literal>@fileserver</literal> tag to the metadata of volume
        group <filename>vg2</filename>. In a terminal console, enter
       </para>
<screen>
vgchange --addtag @fileserver vg2
</screen>
      </step>
      <step id="bwke0du">
       <para>
        In a text editor, modify the<filename> /etc/lvm/lvm.conf</filename>
        file with the following code to define the
        <literal>@database</literal>, <literal>@fileserver</literal>,
        <literal>@fileserverbackup</literal> tags.
       </para>
<screen>
tags {
   database {
      host_list = [ "db1", "db2" ]
   }
   fileserver {
      host_list = [ "fs1" ]
   }
   fileserverbackup {
      host_list = [ "fsb1" ]
   }
}

activation {
   # Activate only if host has a tag that matches a metadata tag
   volume_list = [ "@*" ]
}
</screen>
      </step>
      <step id="bwke0dv">
       <para>
        Replicate the modified<filename> /etc/lvm/lvm.conf</filename> file
        to the four hosts: <filename>db1</filename>,
        <filename>db2</filename>, <filename>fs1</filename>, and
        <filename>fsb1</filename>.
       </para>
      </step>
      <step id="bwke2yh">
       <para>
        If the file server host goes down, <filename>vg2</filename> can be
        brought up on <filename>fsb1</filename> by entering the following
        commands in a terminal console on any node:
       </para>
<screen>
vgchange --addtag @fileserverbackup vg2
vgchange -ay vg2
</screen>
      </step>
     </procedure>
    </sect3>
    <sect3 id="bwkdtdw">
     <title>Option 2: Localized Admin and Configuration</title>
     <para>
      In the following solution, each host holds locally the information
      about which classes of volume to activate.
     </para>
     <procedure id="bwke3z3">
      <step id="bwke8yi">
       <para role="intro">
        Add the <literal>@database</literal> tag to the metadata of volume
        group <filename>vg1</filename>. In a terminal console, enter
       </para>
<screen>
vgchange --addtag @database vg1
</screen>
      </step>
      <step id="bwke8yj">
       <para>
        Add the <literal>@fileserver</literal> tag to the metadata of volume
        group <filename>vg2</filename>. In a terminal console, enter
       </para>
<screen>
vgchange --addtag @fileserver vg2
</screen>
      </step>
      <step id="bwke8yk">
       <para>
        Enable host tags in the <filename>/etc/lvm/lvm.conf</filename> file:
       </para>
       <substeps>
        <step id="bwke8dm">
         <para>
          In a text editor, modify the
          <filename>/etc/lvm/lvm.conf</filename> file with the following
          code to enable host tag configuration files.
         </para>
<screen>
tags {
   hosttags = 1
}
</screen>
        </step>
        <step id="bwke8dn">
         <para>
          Replicate the modified <filename>/etc/lvm/lvm.conf</filename> file
          to the four hosts: <filename>db1</filename>,
          <filename>db2</filename>, <filename>fs1</filename>, and
          <filename>fsb1</filename>.
         </para>
        </step>
       </substeps>
      </step>
      <step id="bwke7w8">
       <para>
        On host <filename>db1</filename>, create an activation configuration
        file for the database host <filename>db1</filename>. In a text
        editor, create <filename>a /etc/lvm/lvm_db1.conf</filename> file and
        add the following code:
       </para>
<screen>
activation {
   volume_list = [ "@database" ]
}
</screen>
      </step>
      <step id="bwkeef1">
       <para>
        On host <filename>db2</filename>, create an activation configuration
        file for the database host <filename>db2</filename>. In a text
        editor, create <filename>a /etc/lvm/lvm_db2.conf</filename> file and
        add the following code:
       </para>
<screen>
activation {
   volume_list = [ "@database" ]
}
</screen>
      </step>
      <step id="bwkeef2">
       <para>
        On host fs1, create an activation configuration file for the file
        server host <filename>fs1</filename>.In a text editor, create
        <filename>a /etc/lvm/lvm_fs1.conf</filename> file and add the
        following code:
       </para>
<screen>
activation {
   volume_list = [ "@fileserver" ]
}
</screen>
      </step>
      <step id="bwkeef3">
       <para>
        If the file server host <filename>fs1</filename> goes down, to bring
        up a spare file server host fsb1 as a file server:
       </para>
       <substeps>
        <step id="bwkeef4">
         <para>
          On host <filename>fsb1</filename>, create an activation
          configuration file for the host <filename>fsb1</filename>. In a
          text editor, create <filename>a /etc/lvm/lvm_fsb1.conf</filename>
          file and add the following code:
         </para>
<screen>
activation {
   volume_list = [ "@fileserver" ]
}
</screen>
        </step>
        <step id="bwkeef5">
         <para>
          In a terminal console, enter one of the following commands:
         </para>
<screen>
vgchange -ay vg2

vgchange -ay @fileserver
</screen>
        </step>
       </substeps>
      </step>
     </procedure>
    </sect3>
   </sect2>
  </sect1>
  <sect1 id="brgpqe2">
   <title>Resizing a Volume Group</title>

   <para role="intro">
    You can add and remove Linux LVM partitions from a volume group to
    expand or reduce its size.
   </para>

   <warning>
    <para>
     Removing a partition can result in data loss if the partition is in use
     by a logical volume.
    </para>
   </warning>

   <procedure id="brgpt5w">
    <step id="brgpt5x">
     <para>
      Launch YaST as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="brgpt5y">
     <para>
      In YaST, open the <guimenu>Partitioner</guimenu>.
     </para>
    </step>
    <step id="brgpt5z">
     <para>
      In the left panel, select Volume Management and expand it to see the
      list of volume groups.
     </para>
    </step>
    <step id="brgpt60">
     <para>
      Under Volume Management, select the volume group, then click the
      <guimenu>Overview</guimenu> tab.
     </para>
    </step>
    <step id="brgpt61">
     <para role="intro">
      At the bottom of the page, click <guimenu>Resize</guimenu>.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_lvm8_a.png" width="329pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_lvm8_a.png" width="329pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="brgptkf">
     <para>
      Do one of the following:
     </para>
     <itemizedlist>
      <listitem>
       <formalpara id="brgpw44">
        <title>Add:</title>
        <para>
         Expand the size of the volume group by moving one or more physical
         volumes (LVM partitions) from the <guimenu>Available Physical
         Volumes</guimenu> list to the <guimenu>Selected Physical
         Volumes</guimenu> list.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="brgpxfv">
        <title>Remove:</title>
        <para>
         Reduce the size of the volume group by moving Lone or more physical
         volumes (LVM partitions) from the <guimenu>Selected Physical
         Volumes</guimenu> list to the <guimenu>Available Physical
         Volumes</guimenu> list.
        </para>
       </formalpara>
      </listitem>
     </itemizedlist>
    </step>
    <step id="brgptf8">
     <para>
      Click <guimenu>Finish</guimenu>.
     </para>
    </step>
    <step id="brgpt62">
     <para>
      Click <guimenu>Next</guimenu>, verify that the changes are listed,
      then click <guimenu>Finish</guimenu>.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="brgqpfy">
   <title>Resizing a Logical Volume with YaST</title>

   <procedure id="brgqpvs">
    <step id="brgqsmd">
     <para>
      Launch YaST as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="brgqsme">
     <para>
      In YaST, open the <guimenu>Partitioner</guimenu>.
     </para>
    </step>
    <step id="brgqsmf">
     <para>
      In the left panel, select Volume Management and expand it to see the
      list of volume groups.
     </para>
    </step>
    <step id="brgqsmg">
     <para>
      Under Volume Management, select the volume group, then click the
      <guimenu>Logical Volumes</guimenu> tab.
     </para>
    </step>
    <step id="brgqsmh">
     <para>
      At the bottom of the page, click <guimenu>Resize</guimenu> to open the
      Resize Logical Volume dialog box.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_lvm12_a.png" width="244pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_lvm12_a.png" width="244pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="brgqpvt">
     <para>
      Use the slider to expand or reduce the size of the logical volume.
     </para>
     <warning>
      <para>
       Reducing the size of a logical volume that contains data can cause
       data corruption.
      </para>
     </warning>
    </step>
    <step id="brgqueh">
     <para>
      Click <guimenu>OK</guimenu>.
     </para>
    </step>
    <step id="brgqulm">
     <para>
      Click <guimenu>Next</guimenu>, verify that the change is listed, then
      click <guimenu>Finish</guimenu>.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="biuz4lz">
   <title>Resizing a Logical Volume with Commands</title>

   <para>
    The <command>lvresize</command>, <command>lvextend</command>, and
    <command>lvreduce</command> commands are used to resize logical volumes.
    See the man pages for each of these commands for syntax and options
    information.
   </para>

   <para>
    You can also increase the size of a logical volume by using the YaST
    Partitioner. YaST uses <command>parted(8)</command> to grow the
    partition.
   </para>

   <para>
    To extend an LV there must be enough unallocated space available on the
    VG.
   </para>

   <para>
    LVs can be extended or shrunk while they are being used, but this may
    not be true for a file system on them. Extending or shrinking the LV
    does not automatically modify the size of file systems in the volume.
    You must use a different command to grow the file system afterwards. For
    information about resizing file systems, see
    <xref linkend="biuymaa" xrefstyle="ChapTitleOnPage"/>.
   </para>

   <para>
    Ensure that you use the right sequence:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      If you extend an LV, you must extend the LV before you attempt to grow
      the file system.
     </para>
    </listitem>
    <listitem>
     <para>
      If you shrink an LV, you must shrink the file system before you
      attempt to shrink the LV.
     </para>
    </listitem>
   </itemizedlist>

   <para>
    To extend the size of a logical volume:
   </para>

   <procedure id="biuz4s4">
    <step id="biuz7nu">
     <para>
      Open a terminal console, log in as the <systemitem>root</systemitem>
      user.
     </para>
    </step>
    <step id="biuz81p">
     <para>
      If the logical volume contains file systems that are hosted for a
      virtual machine (such as a Xen VM), shut down the VM.
     </para>
    </step>
    <step id="biuz7ph">
     <para>
      Dismount the file systems on the logical volume.
     </para>
    </step>
    <step id="biuz4s5">
     <para>
      At the terminal console prompt, enter the following command to grow
      the size of the logical volume:
     </para>
<screen>
lvextend -L +<replaceable>size</replaceable> <replaceable>/dev/vgname/lvname</replaceable>
</screen>
     <para>
      For <replaceable>size</replaceable>, specify the amount of space you
      want to add to the logical volume, such as 10GB. Replace
      <filename><replaceable>/dev/vgname/lvname</replaceable></filename>
      with the Linux path to the logical volume, such as
      <filename>/dev/vg1/v1</filename>. For example:
     </para>
<screen>
lvextend -L +10GB /dev/vg1/v1<replaceable/>
</screen>
    </step>
   </procedure>

   <para>
    For example, to extend an LV with a (mounted and active) ReiserFS on it
    by 10GB:
   </para>

<screen>
lvextend −L +10G <replaceable>/dev/vgname/lvname</replaceable>
resize_reiserfs −s +10GB −f <replaceable>/dev/vg−name/lv−name</replaceable>
</screen>

   <para>
    For example, to shrink an LV with a ReiserFS on it by 5GB:
   </para>

<screen>
umount <replaceable>/mountpoint−of−LV</replaceable>
resize_reiserfs −s −5GB <replaceable>/dev/vgname/lvname</replaceable>
lvreduce <replaceable>/dev/vgname/lvname</replaceable>
mount <replaceable>/dev/vgname/lvname</replaceable> <replaceable>/mountpoint−of−LV</replaceable>
</screen>
  </sect1>
  <sect1 id="brgpmw3">
   <title>Deleting a Volume Group</title>

   <warning>
    <para>
     Deleting a volume group destroys all of the data in each of its member
     partitions.
    </para>
   </warning>

   <procedure id="brgppd6">
    <step id="brgppd7">
     <para>
      Launch YaST as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="brgppd8">
     <para>
      In YaST, open the <guimenu>Partitioner</guimenu>.
     </para>
    </step>
    <step id="brgppd9">
     <para>
      In the left panel, select Volume Management and expand the list of
      groups.
     </para>
    </step>
    <step id="brgpos4">
     <para>
      Under Volume Management, select the volume group, then click the
      <guimenu>Overview</guimenu> tab.
     </para>
    </step>
    <step id="brgpos5">
     <para>
      At the bottom of the page, click <guimenu>Delete</guimenu>, then click
      <guimenu>Yes</guimenu> to confirm the deletion.
     </para>
    </step>
    <step id="brgpos6">
     <para>
      Click <guimenu>Next</guimenu>, verify that the deleted volume group is
      listed (deletion is indicated by a red colored font), then click
      <guimenu>Finish</guimenu>.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="brgob84">
   <title>Deleting an LVM Partition (Physical Volume)</title>

   <warning>
    <para>
     Deleting a partition destroys all of the data in the partition.
    </para>
   </warning>

   <procedure id="brgobbw">
    <step id="brgobki">
     <para>
      Launch YaST as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="brgobkj">
     <para>
      In YaST, open the <guimenu>Partitioner</guimenu>.
     </para>
    </step>
    <step id="brgobuj">
     <para>
      If the Linux LVM partition is in use as a member of a volume group,
      remove the partition from the volume group, or
      <link linkend="brgpmw3">delete the volume group</link>.
     </para>
    </step>
    <step id="brgobkk">
     <para>
      In the YaST Partitioner under <guimenu>Hard Disks</guimenu>, select
      the device (such as <filename>sdc</filename>).
     </para>
    </step>
    <step id="brgobkl">
     <para>
      On the Partitions page, select a partition that you want to remove,
      click <guimenu>Delete</guimenu>, then click <guimenu>Yes</guimenu> to
      confirm the deletion.
     </para>
    </step>
    <step id="brgobkm">
     <para>
      Click <guimenu>Next</guimenu>, verify that the deleted partition is
      listed (deletion is indicated by a red colored font), then click
      <guimenu>Finish</guimenu>.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="btx9jxu">
   <title>Using LVM Commands</title>

   <para>
    For information about using LVM commands, see the man pages for the
    commands described in <xref linkend="btx8b05" xrefstyle="TableXRef"/>.
    Perform the commands as the <systemitem>root</systemitem> user.
   </para>

   <table id="btx8b05" frame="topbot" rowsep="1" pgwide="0">
    <title>LVM Commands</title>
    <tgroup cols="2">
     <colspec colnum="1" colname="1" colwidth="5001*"/>
     <colspec colnum="2" colname="2" colwidth="5001*"/>
     <thead>
      <row id="btx8b06">
       <entry>
        <para>
         Command
        </para>
       </entry>
       <entry>
        <para>
         Description
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row id="btx8b07">
       <entry>
<screen>
pvcreate <replaceable>&lt;device</replaceable>&gt;
</screen>
       </entry>
       <entry>
        <para>
         Initializes a device (such as <filename>/dev/sdb</filename>) for
         use by LVLM as a physical volume.
        </para>
       </entry>
      </row>
      <row id="btx8yob">
       <entry>
<screen>
pvdisplay &lt;<replaceable>device</replaceable>&gt;
</screen>
       </entry>
       <entry>
        <para>
         Displays information about the LVM physical volume, such as whether
         it is currently being used in a logical volume.
        </para>
       </entry>
      </row>
      <row id="btx8b08">
       <entry>
<screen>
vgcreate -c y &lt;<replaceable>vg_name&gt;</replaceable> &lt;<replaceable>dev1</replaceable>&gt; [<replaceable>dev2</replaceable>...]
</screen>
       </entry>
       <entry>
        <para>
         Creates a clustered volume group with one or more specified
         devices.
        </para>
       </entry>
      </row>
      <row id="btx8b09">
       <entry>
<screen>
vgchange -a [ey | n] &lt;<replaceable>vg_name</replaceable>&gt;
</screen>
       </entry>
       <entry>
        <para>
         Activates (<literal>-a ey</literal>) or deactivates (<literal>-a
         n</literal>) a volume group and its logical volumes for
         input/output.
        </para>
        <important>
         <para>
          Ensure that you use the <literal>ey</literal> option to
          exclusively activate a volume group on a cluster node. This option
          is used by default in the load script.
         </para>
        </important>
       </entry>
      </row>
      <row id="btx8lez">
       <entry>
<screen>
vgremove &lt;<replaceable>vg_name</replaceable>&gt;
</screen>
       </entry>
       <entry>
        <para>
         Removes a volume group. Before using this command, remove the
         logical volumes, then deactivate the volume group.
        </para>
       </entry>
      </row>
      <row id="btx8b0a">
       <entry>
<screen>
vgdisplay &lt;<replaceable>vg_name</replaceable>&gt;
</screen>
       </entry>
       <entry>
        <para>
         Displays information about a specified volume group.
        </para>
        <para>
         To find the total physical extent of a volume group, enter
        </para>
<screen>
vgdisplay vg_name | grep "Total PE"
</screen>
       </entry>
      </row>
      <row id="b15cnuq5">
       <entry>
<screen>
lvcreate -L <replaceable>size</replaceable> -n &lt;<replaceable>lv_name</replaceable>&gt; &lt;<replaceable>vg_name</replaceable>&gt;
</screen>
       </entry>
       <entry>
        <para>
         Creates a logical volume of the specified size.
        </para>
       </entry>
      </row>
      <row id="btx8b0b">
       <entry>
<screen>
lvcreate -L &lt;size&gt; &lt;-T|--thinpool&gt; &lt;<replaceable>vg_name/thin_pool_name</replaceable>&gt;

lvcreate -virtualsize &lt;size&gt; &lt;-T|--thin&gt; 
&lt;<replaceable>vg_name</replaceable>/<replaceable>thin_pool_name</replaceable>&gt; -n &lt;<replaceable>thin_lv_name</replaceable>&gt;

lvcreate -L &lt;size&gt; -T 
&lt;<replaceable>vg_name</replaceable>/<replaceable>thin_pool_name</replaceable>&gt; 
-virtualsize &lt;size&gt; -n &lt;<replaceable>thin_lv_name</replaceable>&gt;


</screen>
       </entry>
       <entry>
        <para>
         Creates a thin logical volume or a thin pool of the specified size.
        </para>
        <para>
         Creates thin pool or thin logical volume or both. Specifying the
         optional argument --size will cause the creation of the thin pool
         logical volume. Specifying the optional argument --virtualsize will
         cause the creation of the thin logical volume from given thin pool
         volume. Specifying both arguments will cause the creation of both
         thin pool and thin volume using this pool.
        </para>
       </entry>
      </row>
      <row id="b15cmns0">
       <entry>
<screen>
lvcreate -s [-L <replaceable>size</replaceable>] -n <replaceable>&lt;snap_volume</replaceable>&gt; <replaceable>&lt;source_volume_path</replaceable>&gt;
</screen>
       </entry>
       <entry>
        <para>
         Creates a snapshot volume for the specified logical volume. If the
         size option (-L, --size) is not included, the snapshot is created
         as a thin snapshot.
        </para>
       </entry>
      </row>
      <row id="btx8b0c">
       <entry>
<screen>
lvremove&lt;<replaceable>/dev/vg_name/lv_name</replaceable>&gt;
</screen>
       </entry>
       <entry>
        <para>
         Removes a logical volume, such as
         <filename>/dev/vg_name/lv_name</filename>.
        </para>
        <para>
         Before using this command, close the logical volume by dismounting
         it with the <command>umount</command> command.
        </para>
       </entry>
      </row>
      <row id="b15cmns1">
       <entry>
<screen>
lvremove <replaceable>snap_volume_path</replaceable>
</screen>
       </entry>
       <entry>
        <para>
         Removes a snapshot volume.
        </para>
       </entry>
      </row>
      <row id="b15cmns2">
       <entry>
<screen>
lvconvert --merge  [-b] [-i &lt;<replaceable>seconds</replaceable>&gt;] [&lt;<replaceable>snap_volume_path</replaceable>&gt;[...&lt;snapN&gt;]|@&lt;<replaceable>volume_tag</replaceable>&gt;]
</screen>
       </entry>
       <entry>
        <para>
         Reverts (rolls back or merges) the snapshot data into the original
         volume.
        </para>
       </entry>
      </row>
      <row id="btx8b0d">
       <entry>
<screen>
vgextend <replaceable>&lt;vg_name&gt;</replaceable>&lt;<replaceable>device</replaceable>&gt;
</screen>
       </entry>
       <entry>
        <para>
         Adds a specified physical volume to an existing volume group
        </para>
       </entry>
      </row>
      <row id="btx8b0e">
       <entry>
<screen>
vgreduce <replaceable>&lt;vg_name</replaceable>&gt; &lt;<replaceable>device</replaceable>&gt;
</screen>
       </entry>
       <entry>
        <para>
         Removes a specified physical volume from an existing volume group.
        </para>
        <important>
         <para>
          Ensure that the physical volume is not currently being used by a
          logical volume. If it is, you must move the data to another
          physical volume by using the <command>pvmove</command> command.
         </para>
        </important>
       </entry>
      </row>
      <row id="btx8b0f">
       <entry>
<screen>
lvextend -L <replaceable>size</replaceable>&lt;<replaceable>/dev/vg_name/lv_name</replaceable>&gt;
</screen>
       </entry>
       <entry>
        <para>
         Extends the size of a specified logical volume. Afterwards, you
         must also expand the file system to take advantage of the newly
         available space.
        </para>
       </entry>
      </row>
      <row id="btx8b0g">
       <entry>
<screen>
lvreduce -L <replaceable>size</replaceable> <replaceable>&lt;/dev/vg_name/lv_name</replaceable>&gt;
</screen>
       </entry>
       <entry>
        <para>
         Reduces the size of a specified logical volume.
        </para>
        <important>
         <para>
          Ensure that you reduce the size of the file system first before
          shrinking the volume, otherwise you risk losing data.
         </para>
        </important>
       </entry>
      </row>
      <row id="b11lxo1l">
       <entry>
<screen>
lvrename <replaceable>&lt;/dev/vg_name/old_lv_name</replaceable>&gt; <replaceable>&lt;/dev/vg_name/new_lv_name</replaceable>&gt;
</screen>
       </entry>
       <entry>
        <para>
         Renames an existing LVM logical volume in a volume group from the
         old volume name to the new volume name. It does not change the
         volume group name.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </sect1>
 </chapter>
 <chapter id="biuymaa" lang="en">
  <title>Resizing File Systems</title>
  <para>
   When your data needs grow for a volume, you might need to increase the
   amount of space allocated to its file system.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="biuynjy" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="biuzt5y" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="biuzt64" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="biuzt6a" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="biuzt6g" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="biuynjy">
   <title>Guidelines for Resizing</title>

   <para>
    Resizing any partition or file system involves some risks that can
    potentially result in losing data.
   </para>

   <warning>
    <para>
     To avoid data loss, ensure that you back up your data before you begin
     any resizing task.
    </para>
   </warning>

   <para>
    Consider the following guidelines when planning to resize a file system.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="biuytzx" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="biuyupz" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="biuyw0v" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="biuytzx">
    <title>File Systems that Support Resizing</title>
    <para>
     The file system must support resizing in order to take advantage of
     increases in available space for the volume. In &productname;, file
     system resizing utilities are available for file systems Ext2, Ext3,
     Ext4, and ReiserFS. The utilities support increasing and decreasing the
     size as follows:
    </para>
    <table id="b8qqe0i" frame="topbot" rowsep="1" pgwide="0">
     <title>File System Support for Resizing</title>
     <tgroup cols="4">
      <colspec colnum="1" colname="1" colwidth="2500*"/>
      <colspec colnum="2" colname="2" colwidth="2500*"/>
      <colspec colnum="3" colname="3" colwidth="2500*"/>
      <colspec colnum="4" colname="4" colwidth="2500*"/>
      <thead>
       <row id="b8qqedo">
        <entry>
         <para>
          File System
         </para>
        </entry>
        <entry>
         <para>
          Utility
         </para>
        </entry>
        <entry>
         <para>
          Increase Size (Grow)
         </para>
        </entry>
        <entry>
         <para>
          Decrease Size (Shrink)
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bxuadqe">
        <entry>
         <para>
          Ext2
         </para>
        </entry>
        <entry>
         <para>
          resize2fs
         </para>
        </entry>
        <entry>
         <para>
          Offline only
         </para>
        </entry>
        <entry>
         <para>
          Offline only
         </para>
        </entry>
       </row>
       <row id="bxuadqf">
        <entry>
         <para>
          Ext3
         </para>
        </entry>
        <entry>
         <para>
          resize2fs
         </para>
        </entry>
        <entry>
         <para>
          Online or offline
         </para>
        </entry>
        <entry>
         <para>
          Offline only
         </para>
        </entry>
       </row>
       <row id="bxuacq3">
        <entry>
         <para>
          Ext4
         </para>
        </entry>
        <entry>
         <para>
          resize2fs
         </para>
        </entry>
        <entry>
         <para>
          Offline only
         </para>
        </entry>
        <entry>
         <para>
          Offline only
         </para>
        </entry>
       </row>
       <row id="b8qqedr">
        <entry>
         <para>
          ReiserFS
         </para>
        </entry>
        <entry>
         <para>
          resize_reiserfs
         </para>
        </entry>
        <entry>
         <para>
          Online or offline
         </para>
        </entry>
        <entry>
         <para>
          Offline only
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </sect2>

   <sect2 id="biuyupz">
    <title>Increasing the Size of a File System</title>
    <para>
     You can grow a file system to the maximum space available on the
     device, or specify an exact size. Ensure that you grow the size of the
     device or logical volume before you attempt to increase the size of the
     file system.
    </para>
    <para>
     When specifying an exact size for the file system, ensure that the new
     size satisfies the following conditions:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The new size must be greater than the size of the existing data;
       otherwise, data loss occurs.
      </para>
     </listitem>
     <listitem>
      <para>
       The new size must be equal to or less than the current device size
       because the file system size cannot extend beyond the space
       available.
      </para>
     </listitem>
    </itemizedlist>
   </sect2>

   <sect2 id="biuyw0v">
    <title>Decreasing the Size of a File System</title>
    <para>
     When decreasing the size of the file system on a device, ensure that
     the new size satisfies the following conditions:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The new size must be greater than the size of the existing data;
       otherwise, data loss occurs.
      </para>
     </listitem>
     <listitem>
      <para>
       The new size must be equal to or less than the current device size
       because the file system size cannot extend beyond the space
       available.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     If you plan to also decrease the size of the logical volume that holds
     the file system, ensure that you decrease the size of the file system
     before you attempt to decrease the size of the device or logical
     volume.
    </para>
   </sect2>
  </sect1>
  <sect1 id="biuzt5y">
   <title>Increasing the Size of an Ext2, Ext3, or Ext4 File System</title>

   <para>
    The size of Ext2, Ext3, and Ext4 file systems can be increased by using
    the <command>resize2fs</command> command when the file system is
    mounted. The size of an Ext3 file system can also be increased by using
    the <command>resize2fs</command> command when the file system is
    unmounted.
   </para>

   <procedure id="biuzt5z">
    <step id="biuzt60">
     <para>
      Open a terminal console, then log in as the
      <systemitem>root</systemitem> user or equivalent.
     </para>
    </step>
    <step id="bxuagy1">
     <para>
      If the file system is Ext2 or Ext4, you must unmount the file system.
      The Ext3 file system can be mounted or unmounted.
     </para>
    </step>
    <step id="biuzt61">
     <para>
      Increase the size of the file system using one of the following
      methods:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        To extend the file system size to the maximum available size of the
        device called <filename>/dev/sda1</filename>, enter
       </para>
<screen>
resize2fs /dev/sda1
</screen>
       <para>
        If a size parameter is not specified, the size defaults to the size
        of the partition.
       </para>
      </listitem>
      <listitem>
       <para>
        To extend the file system to a specific size, enter
       </para>
<screen>
resize2fs /dev/sda1 <replaceable>size</replaceable>
</screen>
       <para>
        The <replaceable>size</replaceable> parameter specifies the
        requested new size of the file system. If no units are specified,
        the unit of the size parameter is the block size of the file system.
        Optionally, the size parameter can be suffixed by one of the
        following the unit designators: s for 512 byte sectors; K for
        kilobytes (1 kilobyte is 1024 bytes); M for megabytes; or G for
        gigabytes.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Wait until the resizing is completed before continuing.
     </para>
    </step>
    <step id="biuzt62">
     <para>
      If the file system is not mounted, mount it now.
     </para>
     <para>
      For example, to mount an Ext2 file system for a device named
      <filename>/dev/sda1</filename> at mount point
      <filename>/home</filename>, enter
     </para>
<screen>
mount -t ext2 /dev/sda1 /home
</screen>
    </step>
    <step id="biuzt63">
     <para>
      Check the effect of the resize on the mounted file system by entering
     </para>
<screen>
df -h
</screen>
     <para>
      The Disk Free (<command>df</command>) command shows the total size of
      the disk, the number of blocks used, and the number of blocks
      available on the file system. The -h option print sizes in
      human-readable format, such as 1K, 234M, or 2G.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="biuzt64">
   <title>Increasing the Size of a Reiser File System</title>

   <para>
    A ReiserFS file system can be increased in size while mounted or
    unmounted.
   </para>

   <procedure id="biuzt65">
    <step id="biuzt66">
     <para>
      Open a terminal console, then log in as the
      <systemitem>root</systemitem> user or equivalent.
     </para>
    </step>
    <step id="biuzt67">
     <para>
      Increase the size of the file system on the device called
      <filename>/dev/sda2</filename>, using one of the following methods:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        To extend the file system size to the maximum available size of the
        device, enter
       </para>
<screen>
resize_reiserfs /dev/sda2
</screen>
       <para>
        When no size is specified, this increases the volume to the full
        size of the partition.
       </para>
      </listitem>
      <listitem>
       <para>
        To extend the file system to a specific size, enter
       </para>
<screen>
resize_reiserfs -s <replaceable>size</replaceable> /dev/sda2
</screen>
       <para>
        Replace <replaceable>size</replaceable> with the desired size in
        bytes. You can also specify units on the value, such as 50000K
        (kilobytes), 250M (megabytes), or 2G (gigabytes). Alternatively, you
        can specify an increase to the current size by prefixing the value
        with a plus (+) sign. For example, the following command increases
        the size of the file system on <filename>/dev/sda2</filename> by 500
        MB:
       </para>
<screen>
resize_reiserfs -s +500M /dev/sda2
</screen>
      </listitem>
     </itemizedlist>
     <para>
      Wait until the resizing is completed before continuing.
     </para>
    </step>
    <step id="biuzt68">
     <para>
      If the file system is not mounted, mount it now.
     </para>
     <para>
      For example, to mount an ReiserFS file system for device
      <filename>/dev/sda2</filename> at mount point
      <filename>/home</filename>, enter
     </para>
<screen>
mount -t reiserfs /dev/sda2 /home
</screen>
    </step>
    <step id="biuzt69">
     <para>
      Check the effect of the resize on the mounted file system by entering
     </para>
<screen>
df -h
</screen>
     <para>
      The Disk Free (<command>df</command>) command shows the total size of
      the disk, the number of blocks used, and the number of blocks
      available on the file system. The -h option print sizes in
      human-readable format, such as 1K, 234M, or 2G.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="biuzt6a">
   <title>Decreasing the Size of an Ext2 or Ext3 File System</title>

   <para>
    You can shrink the size of the Ext2, Ext3, or Ext4 file systems when the
    volume is unmounted.
   </para>

   <procedure id="biuzt6b">
    <step id="biuzt6c">
     <para>
      Open a terminal console, then log in as the
      <systemitem>root</systemitem> user or equivalent.
     </para>
    </step>
    <step id="bt5yjdr">
     <para>
      Unmount the file system.
     </para>
    </step>
    <step id="biuzt6d">
     <para>
      Decrease the size of the file system on the device such as
      <filename>/dev/sda1</filename> by entering
     </para>
<screen>
resize2fs /dev/sda1 &lt;size&gt;
</screen>
     <para>
      Replace <replaceable>size</replaceable> with an integer value in
      kilobytes for the desired size. (A kilobyte is 1024 bytes.)
     </para>
     <para>
      Wait until the resizing is completed before continuing.
     </para>
    </step>
    <step id="biuzt6e">
     <para>
      Mount the file system. For example, to mount an Ext2 file system for a
      device named <filename>/dev/sda1</filename> at mount point
      <filename>/home</filename>, enter
     </para>
<screen>
mount -t ext2 /dev/md0 /home
</screen>
    </step>
    <step id="biuzt6f">
     <para>
      Check the effect of the resize on the mounted file system by entering
     </para>
<screen>
df -h
</screen>
     <para>
      The Disk Free (<command>df</command>) command shows the total size of
      the disk, the number of blocks used, and the number of blocks
      available on the file system. The -h option print sizes in
      human-readable format, such as 1K, 234M, or 2G.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="biuzt6g">
   <title>Decreasing the Size of a Reiser File System</title>

   <para>
    Reiser file systems can be reduced in size only if the volume is
    unmounted.
   </para>

   <procedure id="biuzt6h">
    <step id="biuzt6i">
     <para>
      Open a terminal console, then log in as the
      <systemitem>root</systemitem> user or equivalent.
     </para>
    </step>
    <step id="biuzt6j">
     <para>
      Unmount the device by entering
     </para>
<screen>
umount /mnt/point
</screen>
     <para>
      If the partition you are attempting to decrease in size contains
      system files (such as the root (<filename>/</filename>) volume),
      unmounting is possible only when booting from a removable device.
     </para>
    </step>
    <step id="biuzt6k">
     <para>
      Decrease the size of the file system on a device called
      <filename>/dev/sda1</filename> by entering
     </para>
<screen>
resize_reiserfs -s <replaceable>size</replaceable> /dev/sda2
</screen>
     <para>
      Replace <replaceable>size</replaceable> with the desired size in
      bytes. You can also specify units on the value, such as 50000K
      (kilobytes), 250M (megabytes), or 2G (gigabytes). Alternatively, you
      can specify a decrease to the current size by prefixing the value with
      a minus (-) sign. For example, the following command reduces the size
      of the file system on <filename>/dev/md0</filename> by 500 MB:
     </para>
<screen>
resize_reiserfs -s -500M /dev/sda2
</screen>
     <para>
      Wait until the resizing is completed before continuing.
     </para>
    </step>
    <step id="biuzt6l">
     <para>
      Mount the file system by entering
     </para>
<screen>
mount -t reiserfs /dev/sda2 /mnt/point
</screen>
    </step>
    <step id="biuzt6m">
     <para>
      Check the effect of the resize on the mounted file system by entering
     </para>
<screen>
df -h
</screen>
     <para>
      The Disk Free (<command>df</command>) command shows the total size of
      the disk, the number of blocks used, and the number of blocks
      available on the file system. The -h option print sizes in
      human-readable format, such as 1K, 234M, or 2G.
     </para>
    </step>
   </procedure>
  </sect1>
 </chapter>
 <chapter conformance="sles11,Novell,no,0,80" id="uuid" lang="en" revision="03/15/10">
  <title>Using UUIDs to Mount Devices</title>
  <para>
   This section describes the optional use of UUIDs instead of device names
   to identify file system devices in the boot loader file and the
   <filename>/etc/fstab</filename> file.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="udev" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="understanduuid" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="uuidbootx86" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="uuidbootia64" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="uuidmoreinfo" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="udev">
   <title>Naming Devices with udev</title>

   <para>
    In the Linux 2.6 and later kernel, <command>udev</command> provides a
    userspace solution for the dynamic <filename>/dev</filename> directory,
    with persistent device naming. As part of the hotplug system,
    <command>udev</command> is executed if a device is added to or removed
    from the system.
   </para>

   <para>
    A list of rules is used to match against specific device attributes. The
    <command>udev</command> rules infrastructure (defined in the
    <filename>/etc/udev/rules.d</filename> directory) provides stable names
    for all disk devices, regardless of their order of recognition or the
    connection used for the device. The <command>udev</command> tools
    examine every appropriate block device that the kernel creates to apply
    naming rules based on certain buses, drive types, or file systems. For
    information about how to define your own rules for
    <command>udev</command>, see
    <ulink url="http://reactivated.net/writing_udev_rules.html"><citetitle>Writing
    udev Rules</citetitle></ulink>.
   </para>

   <para>
    Along with the dynamic kernel-provided device node name,
    <command>udev</command> maintains classes of persistent symbolic links
    pointing to the device in the <filename>/dev/disk</filename> directory,
    which is further categorized by the <filename>by-id</filename>,
    <filename>by-label</filename>, <filename>by-path</filename>, and
    <filename>by-uuid</filename> subdirectories.
   </para>

   <note>
    <para>
     Other programs besides <command>udev</command>, such as LVM or
     <command>md</command>, might also generate UUIDs, but they are not
     listed in <filename>/dev/disk</filename>.
    </para>
   </note>
  </sect1>
  <sect1 id="understanduuid">
   <title>Understanding UUIDs</title>

   <para>
    A UUID (Universally Unique Identifier) is a 128-bit number for a file
    system that is unique on both the local system and across other systems.
    It is a randomly generated with system hardware information and time
    stamps as part of its seed. UUIDs are commonly used to uniquely tag
    devices.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="usinguuid" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="findinguuid" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="usinguuid">
    <title>Using UUIDs to Assemble or Activate File System Devices</title>
    <para>
     The UUID is always unique to the partition and does not depend on the
     order in which it appears or where it is mounted. With certain SAN
     devices attached to the server, the system partitions are renamed and
     moved to be the last device. For example, if root
     (<filename>/</filename>) is assigned to <filename>/dev/sda1</filename>
     during the install, it might be assigned to
     <filename>/dev/sdg1</filename> after the SAN is connected. One way to
     avoid this problem is to use the UUID in the boot loader and
     <filename>/etc/fstab</filename> files for the boot device.
    </para>
    <para>
     The device ID assigned by the manufacturer for a drive never changes,
     no matter where the device is mounted, so it can always be found at
     boot. The UUID is a property of the file system and can change if you
     reformat the drive. In a boot loader file, you typically specify the
     location of the device (such as <filename>/dev/sda1</filename>) to
     mount it at system boot. The boot loader can also mount devices by
     their UUIDs and administrator-specified volume labels. However, if you
     use a label and file location, you cannot change the label name when
     the partition is mounted.
    </para>
    <para>
     You can use the UUID as criterion for assembling and activating
     software RAID devices. When a RAID is created, the
     <command>md</command> driver generates a UUID for the device, and
     stores the value in the <filename>md</filename> superblock.
    </para>
   </sect2>

   <sect2 id="findinguuid">
    <title>Finding the UUID for a File System Device</title>
    <para>
     You can find the UUID for any block device in the
     <filename>/dev/disk/by-uuid</filename> directory. For example, a UUID
     looks like this:
    </para>
<screen>
e014e482-1c2d-4d09-84ec-61b3aefde77a 
</screen>
   </sect2>
  </sect1>
  <sect1 id="uuidbootx86">
   <title>Using UUIDs in the Boot Loader and /etc/fstab File (x86)</title>

   <para>
    After the install, you can optionally use the following procedure to
    configure the UUID for the system device in the boot loader and
    <filename>/etc/fstab</filename> files for your x86 system.
   </para>

   <para>
    Before you begin, make a copy of <filename>/etc/default/grub</filename>
    file and the <filename>/etc/fstab</filename> file.
   </para>

   <procedure id="b4pdcrj">
    <step id="b4pdcrk">
     <para>
      Install the SUSE Linux Enterprise Server for x86 with no SAN devices
      connected.
     </para>
    </step>
    <step id="b4pde5j">
     <para>
      After the install, boot the system.
     </para>
    </step>
    <step id="b4pdeci">
     <para>
      Open a terminal console as the <systemitem>root</systemitem> user or
      equivalent.
     </para>
    </step>
    <step id="b4pdejm">
     <para>
      Navigate to the <filename>/dev/disk/by-uuid</filename> directory to
      find the UUID for the device where you installed
      <filename>/boot</filename>, <filename>/root</filename>, and
      <filename>swap</filename>.
     </para>
     <substeps>
      <step id="b4pdgjc">
       <para>
        At the terminal console prompt, enter
       </para>
<screen>
cd /dev/disk/by-uuid
</screen>
      </step>
      <step id="b4pdhde">
       <para>
        List all partitions by entering
       </para>
<screen>
ll
</screen>
      </step>
      <step id="b4pdhy7">
       <para>
        Find the UUID, such as
       </para>
<screen>
e014e482-1c2d-4d09-84ec-61b3aefde77a —&gt; /dev/sda1
</screen>
      </step>
     </substeps>
    </step>
    <step id="b4pdk3y">
     <para>
      Edit the boot loader configuration using the <guimenu>Boot
      Loader</guimenu> option in YaST.
     </para>
     <para>
      For example, change
     </para>
<screen>
kernel /boot/vmlinuz root=/dev/sda1
</screen>
     <para>
      to
     </para>
<screen>
kernel /boot/vmlinuz root=/dev/disk/by-uuid/e014e482-1c2d-4d09-84ec-61b3aefde77a
</screen>
     <important>
      <para>
       If you make a mistake, you can boot the server without the SAN
       connected, and fix the error by using the backup copy of the
       <filename>/boot/grub2/grub.cfg</filename> file as a guide.
      </para>
     </important>
     <para>
      When you use YaST to change the way that the root
      (<filename>/</filename>) device is mounted (such as by UUID or by
      label), the boot loader configuration needs to be saved again to make
      the change effective for the boot loader.
     </para>
    </step>
    <step id="b4pdl03">
     <para>
      As the <systemitem>root</systemitem> user or equivalent, do one of the
      following to place the UUID in the <filename>/etc/fstab</filename>
      file:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Launch YaST as the <systemitem>root</systemitem> user, select
        <menuchoice>
        <guimenu>System</guimenu><guimenu>Partitioner</guimenu></menuchoice>,
        select the device of interest, then modify <guimenu>Fstab
        Options</guimenu>.
       </para>
      </listitem>
      <listitem>
       <para>
        Edit the <filename>/etc/fstab</filename> file to modify the system
        device from the location to the UUID.
       </para>
       <para>
        For example, if the root (<filename>/</filename>) volume has a
        device path of <filename>/dev/sda1</filename> and its UUID is
        <filename>e014e482-1c2d-4d09-84ec-61b3aefde77a</filename>, change
        line entry from
       </para>
<screen>
/dev/sda1   /            reiserfs   acl,user_xattr        1 1
</screen>
       <para>
        to
       </para>
<screen>
UUID=e014e482-1c2d-4d09-84ec-61b3aefde77a   /   reiserfs   acl,user_xattr        1 1 
</screen>
       <important>
        <para>
         Do not leave stray characters or spaces in the file.
        </para>
       </important>
      </listitem>
     </itemizedlist>
    </step>
   </procedure>
  </sect1>
  <sect1 id="uuidbootia64">
   <title>Using UUIDs in the Boot Loader and /etc/fstab File (IA64)</title>

   <para>
    After the install, use the following procedure to configure the UUID for
    the system device in the boot loader and <filename>/etc/fstab</filename>
    files for your IA64 system. IA64 uses the EFI BIOS. Its file system
    configuration file is <filename>/boot/efi/SuSE/elilo.conf</filename>
    instead of <filename>/etc/fstab</filename>.
   </para>

   <para>
    Before you begin, make a copy of the
    <filename>/boot/efi/SuSE/elilo.conf</filename> file.
   </para>

   <procedure id="b4pec1b">
    <step id="b4pec1c">
     <para>
      Install the SUSE Linux Enterprise Server for IA64 with no SAN devices
      connected.
     </para>
    </step>
    <step id="b4pec1d">
     <para>
      After the install, boot the system.
     </para>
    </step>
    <step id="b4pec1e">
     <para>
      Open a terminal console as the <systemitem>root</systemitem> user or
      equivalent.
     </para>
    </step>
    <step id="b4pec1f">
     <para>
      Navigate to the <filename>/dev/disk/by-uuid</filename> directory to
      find the UUID for the device where you installed
      <filename>/boot</filename>, <filename>/root</filename>, and
      <filename>swap</filename>.
     </para>
     <substeps>
      <step id="b4pec1h">
       <para>
        At the terminal console prompt, enter
       </para>
<screen>
cd /dev/disk/by-uuid
</screen>
      </step>
      <step id="b4pec1i">
       <para>
        List all partitions by entering
       </para>
<screen>
ll
</screen>
      </step>
      <step id="b4pec1j">
       <para>
        Find the UUID, such as
       </para>
<screen>
e014e482-1c2d-4d09-84ec-61b3aefde77a —&gt; /dev/sda1
</screen>
      </step>
     </substeps>
    </step>
    <step id="b4pec1k">
     <para>
      Edit the boot loader file, using the Boot Loader option in YaST.
     </para>
     <para>
      For example, change
     </para>
<screen>
root=/dev/sda1
</screen>
     <para>
      to
     </para>
<screen>
root=/dev/disk/by-uuid/e014e482-1c2d-4d09-84ec-61b3aefde77a
</screen>
    </step>
    <step id="b4pec1l">
     <para>
      Edit the <filename>/boot/efi/SuSE/elilo.conf</filename> file to modify
      the system device from the location to the UUID.
     </para>
     <para>
      For example, change
     </para>
<screen>
/dev/sda1   /   reiserfs   acl,user_xattr        1 1
</screen>
     <para>
      to
     </para>
<screen>
UUID=e014e482-1c2d-4d09-84ec-61b3aefde77a   /   reiserfs   acl,user_xattr        1 1
</screen>
     <important>
      <para>
       Do not leave stray characters or spaces in the file.
      </para>
     </important>
    </step>
   </procedure>
  </sect1>
  <sect1 id="uuidmoreinfo">
   <title>Additional Information</title>

   <para>
    For more information about using <command>udev(8)</command> for managing
    devices, see <xref linkend="cha.udev"/>
   </para>

   <para>
    For more information about <command>udev(8)</command> commands, see its
    man page. Enter the following at a terminal console prompt:
   </para>

<screen>
man 8 udev
</screen>
  </sect1>
 </chapter>
 <chapter conformance="sles11,Novell,no,0,80" id="multipathing" lang="en" revision="03/15/10">
  <title>Managing Multipath I/O for Devices</title>
  <para>
   This section describes how to manage failover and path load balancing for
   multiple paths between the servers and block storage devices.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="mpioovw" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="be5rvii" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="mpiotools" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="mpiosysconf" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="mpiostart" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bbillhs" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bbj68de" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bbj5x7z" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="mpionames" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bkj8n9w" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bbi89rh" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="mpioroot" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="mpioraid" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="scandev" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="be48i9g" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="mpiostatus" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="mpioerrormgmt" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="mpiostall" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bpjpirk" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="mpionext" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="mpioovw">
   <title>Understanding Multipath I/O</title>

   <para>
    Multipathing is the ability of a server to communicate with the same
    physical or logical block storage device across multiple physical paths
    between the host bus adapters in the server and the storage controllers
    for the device, typically in Fibre Channel (FC) or iSCSI SAN
    environments. You can also achieve multiple connections with direct
    attached storage when multiple channels are available.
   </para>

   <para>
    Linux multipathing provides connection fault tolerance and can provide
    load balancing across the active connections. When multipathing is
    configured and running, it automatically isolates and identifies device
    connection failures, and reroutes I/O to alternate connections.
   </para>

   <para>
    Typical connection problems involve faulty adapters, cables, or
    controllers. When you configure multipath I/O for a device, the
    multipath driver monitors the active connection between devices. When
    the multipath driver detects I/O errors for an active path, it fails
    over the traffic to the device’s designated secondary path. When the
    preferred path becomes healthy again, control can be returned to the
    preferred path.
   </para>
  </sect1>
  <sect1 id="be5rvii">
   <title>Planning for Multipathing</title>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="mpioovwguide" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b122uvel" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="mpiousingdev" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="mpiousinglvm" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="mpiousingmdadm" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14ff9rs" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bleqcv0" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bok8cn1" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="mpiousingpart" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="be5rs3a" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="be5ruyr" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="mpioovwguide">
    <title>Guidelines for Multipathing</title>
    <para>
     Use the guidelines in this section when planning your multipath I/O
     solution.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="beg2qyg" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="beg2qpf" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="beg2qgh" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="beg2q2x" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="beg2pi0" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="beg2pdq" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="beg2pdr" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="beg2qyg">
     <title>Prerequisites</title>
     <itemizedlist>
      <listitem>
       <para>
        Multipathing is managed at the device level.
       </para>
      </listitem>
      <listitem>
       <para>
        The storage array you use for the multipathed device must support
        multipathing. For more information, see
        <xref linkend="be5ruyr" xrefstyle="SectTitleOnPage"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        You need to configure multipathing only if multiple physical paths
        exist between host bus adapters in the server and host bus
        controllers for the block storage device. You configure multipathing
        for the logical device as seen by the server.
       </para>
      </listitem>
     </itemizedlist>
    </sect3>
    <sect3 id="beg2qpf">
     <title>Vendor-Provided Multipath Solutions</title>
     <para>
      For some storage arrays, the vendor provides its own multipathing
      software to manage multipathing for the array’s physical and logical
      devices. In this case, you should follow the vendor’s instructions
      for configuring multipathing for those devices.
     </para>
    </sect3>
    <sect3 id="beg2qgh">
     <title>Disk Management Tasks</title>
     <para>
      Perform the following disk management tasks before you attempt to
      configure multipathing for a physical or logical device that has
      multiple paths:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Use third-party tools to carve physical disks into smaller logical
        disks.
       </para>
      </listitem>
      <listitem>
       <para>
        Use third-party tools to partition physical or logical disks. If you
        change the partitioning in the running system, the Device Mapper
        Multipath (DM-MP) module does not automatically detect and reflect
        these changes. DM-MPIO must be re-initialized, which usually
        requires a reboot.
       </para>
      </listitem>
      <listitem>
       <para>
        Use third-party SAN array management tools to create and configure
        hardware RAID devices.
       </para>
      </listitem>
      <listitem>
       <para>
        Use third-party SAN array management tools to create logical devices
        such as LUNs. Logical device types that are supported for a given
        array depend on the array vendor.
       </para>
      </listitem>
     </itemizedlist>
    </sect3>
    <sect3 id="beg2q2x">
     <title>Software RAIDs</title>
     <para>
      The Linux software RAID management software runs on top of
      multipathing. For each device that has multiple I/O paths and that you
      plan to use in a software RAID, you must configure the device for
      multipathing before you attempt to create the software RAID device.
      Automatic discovery of multipathed devices is not available. The
      software RAID is not aware of the multipathing management running
      underneath.
     </para>
     <para>
      For information about setting up multipathing for existing software
      RAIDs, see <xref linkend="mpioraid" xrefstyle="SectTitleOnPage"/>.
     </para>
    </sect3>
    <sect3 id="beg2pi0">
     <title>High-Availability Solutions</title>
     <para>
      High-availability solutions for clustering storage resources run on
      top of the multipathing service on each node. Ensure that the
      configuration settings in the <filename>/etc/multipath.conf</filename>
      file on each node are consistent across the cluster.
     </para>
     <para>
      Ensure that multipath devices the same name across all devices by
      doing the following:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Use UUID and alias names to ensure that multipath device names are
        consistent across all nodes in the cluster. Alias names must be
        unique across all nodes. Copy the <filename>/etc/multipath.conf
        </filename>file from the node to the <filename>/etc/</filename>
        directory all of the other nodes in the cluster.
       </para>
      </listitem>
      <listitem>
       <para>
        When using links to multipath-mapped devices, ensure that you
        specify the <filename>dm-uuid*</filename> name or alias name in the
        <filename>/dev/disk/by-id</filename> directory, and not a fixed path
        instance of the device. For information, see
        <xref linkend="mpiousingdev" xrefstyle="SectTitleOnPage"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Set the <literal>user_friendly_names</literal> configuration option
        to no to disable it. A user-friendly name is unique to a node, but a
        device might not be assigned the same user-friendly name on every
        node in the cluster.
       </para>
       <para>
        You can force the system-defined user-friendly names to be
        consistent across all nodes in the cluster by doing the following:
       </para>
       <orderedlist>
        <listitem>
         <para>
          In the <filename>/etc/multipath.conf</filename> file on one node:
         </para>
         <orderedlist>
          <listitem>
           <para>
            Set the <literal>user_friendly_names</literal> configuration
            option to yes to enable it.
           </para>
           <para>
            Multipath uses the
            <filename>/var/lib/multipath/bindings</filename> file to assign
            a persistent and unique name to the device in the form of
            <filename>mpath&lt;<replaceable>n</replaceable>&gt;</filename>
            in the <filename>/dev/mapper </filename>directory.
           </para>
          </listitem>
          <listitem>
           <para>
            (Optional) Set the <literal>bindings_file</literal> option in
            the <literal>defaults</literal> section of the
            <literal>/etc/multipath.conf</literal> file to specify an
            alternate location for the <filename>bindings</filename> file.
           </para>
           <para>
            The default location is
            <filename>/var/lib/multipath/bindings</filename>.
           </para>
          </listitem>
         </orderedlist>
        </listitem>
        <listitem>
         <para>
          Set up all of the multipath devices on the node.
         </para>
        </listitem>
        <listitem>
         <para>
          Copy the <filename>/etc/multipath.conf </filename>file from the
          node to the <filename>/etc/</filename> directory all of the other
          nodes in the cluster.
         </para>
        </listitem>
        <listitem>
         <para>
          Copy the <filename>bindings</filename> file from the node to the
          <filename>bindings_file</filename> path on all of the other nodes
          in the cluster.
         </para>
        </listitem>
       </orderedlist>
      </listitem>
     </itemizedlist>
     <para>
      The Distributed Replicated Block Device (DRBD) high-availability
      solution for mirroring devices across a LAN runs on top of
      multipathing. For each device that has multiple I/O paths and that you
      plan to use in a DRDB solution, you must configure the device for
      multipathing before you configure DRBD.
     </para>
    </sect3>
    <sect3 id="beg2pdq">
     <title>Volume Managers</title>
     <para>
      Volume managers such as LVM2 and Clustered LVM2 run on top of
      multipathing. You must configure multipathing for a device before you
      use LVM2 or cLVM2 to create segment managers and file systems on it.
      For information, see
      <xref linkend="mpiousinglvm" xrefstyle="SectTitleOnPage"/>.
     </para>
    </sect3>
    <sect3 id="beg2pdr">
     <title>Virtualization Environments</title>
     <para>
      When using multipathing in a virtualization environment, the
      multipathing is controlled in the host server environment. Configure
      multipathing for the device before you assign it to a virtual guest
      machine.
     </para>
    </sect3>
   </sect2>

   <sect2 id="b122uvel">
    <title>PRIO Settings in multipath-tools-0.4.9</title>
    <para>
     SLES 11 SP2 upgrades the multipath-tools from 0.4.8 to 0.4.9. Some
     changes in PRIO syntax require that you manually modify the
     <filename>/etc/multipath.conf</filename> file as needed to comply with
     the new syntax.
    </para>
    <para>
     The syntax for the <literal>prio</literal> keyword in the
     <filename>/etc/multipath.conf</filename> file is changed in
     <filename>multipath-tools-0.4.9</filename>. The <literal>prio</literal>
     line specifies the prioritizer. If the prioritizer requires an
     argument, you specify the argument by using the
     <literal>prio_args</literal> keyword on a second line. Previously, the
     prioritizer and its arguments were included on the
     <literal>prio</literal> line.
    </para>
    <para>
     Multipath Tools 0.4.9 and later uses the <literal>prio</literal>
     setting in the <literal>defaults{}</literal> or
     <literal>devices{}</literal> section of the
     <filename>/etc/multipath.conf</filename> file. It silently ignores the
     keyword <literal>prio</literal> when it is specified for an individual
     <literal>multipath</literal> definition in the
     <literal>multipaths{)</literal> section. Multipath Tools 0.4.8 for SLES
     11 SP1 and earlier allows the prio setting in the individual
     <literal>multipath</literal> definition in the
     <literal>multipaths{)</literal> section to override the
     <literal>prio</literal> settings in the <literal>defaults{}</literal>
     or <literal>devices{}</literal> section.
    </para>
   </sect2>

   <sect2 id="mpiousingdev">
    <title>Using WWID, User-Friendly, and Alias Names for Multipathed Devices</title>
    <para role="intro">
     A multipath device can be uniquely identified by its WWID, by a
     user-friendly name, or by an alias that you assign for it. Device node
     names in the form of <filename>/dev/sdn</filename> and
     <filename>/dev/dm-n</filename> can change on reboot and might be
     assigned to different devices each time. A device’s WWID,
     user-friendly name, and alias name persist across reboots, and are the
     preferred way to identify the device.
    </para>
    <para>
     If you want to use the entire LUN directly (for example, if you are
     using the SAN features to partition your storage), you can use the
     <filename>/dev/disk/by-id/xxx</filename> names for
     <command>mkfs</command>, <command>fstab</command>, your application,
     and so on. Partitioned devices have <filename>_part&lt;n&gt;</filename>
     appended to the device name, such as
     <filename>/dev/disk/by-id/xxx_part1</filename>.
    </para>
    <para>
     In the <filename>/dev/disk/by-id</filename> directory, the
     multipath-mapped devices are represented by the device’s
     <filename>dm-uuid*</filename> name or alias name (if you assign an
     alias for it in the <filename>/etc/multipath.conf</filename> file). The
     <filename>scsi-</filename> and <filename>wwn-</filename> device names
     represent physical paths to the devices.
    </para>
    <important>
     <para>
      When using links to multipath-mapped devices, ensure that you specify
      the <filename>dm-uuid*</filename> name or alias name in the
      <filename>/dev/disk/by-id</filename> directory, and not a fixed path
      instance of the device.
     </para>
    </important>
    <para>
     When you define device aliases in the
     <filename>/etc/multipath.conf</filename> file, ensure that you use each
     device’s WWID (such as
     <filename>3600508e0000000009e6baa6f609e7908</filename>) and not its
     WWN, which replaces the first character of a device ID with
     <filename>0x</filename>, such as
     <filename>0x600508e0000000009e6baa6f609e7908</filename>.
    </para>
    <para>
     For information about using user-friendly names and aliases for
     multipathed devices, see
     <xref linkend="mpionames" xrefstyle="HeadingOnPage"/>.
    </para>
   </sect2>

   <sect2 id="mpiousinglvm">
    <title>Using LVM2 on Multipath Devices</title>
    <para>
     Ensure that the configuration file for <filename>lvm.conf</filename>
     points to the multipath-device names instead of fixed path names. This
     should happen automatically if <filename>boot.multipath</filename> is
     enabled and loads before <filename>boot.lvm</filename>.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="b13iv5ro" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b13iv6p5" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b13iv6p6" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="b13iv5ro">
     <title>Adding a Multipath Device Filter in the /etc/lvm/lvm.conf File</title>
     <para>
      By default, LVM2 does not recognize multipathed devices. To make LVM2
      recognize the multipathed devices as possible physical volumes, you
      must modify <filename>/etc/lvm/lvm.conf</filename> to scan multipathed
      devices through the multipath I/O layer.
     </para>
     <para>
      Adding a multipath filter prevents LVM from scanning and using the
      physical paths for raw device nodes that represent individual paths to
      the SAN (/dev/sd*). Ensure that you specify the filter path so that
      LVM scans only the device mapper names for the device
      (<filename>/dev/disk/by-id/dm-uuid-.*-mpath-.*</filename>) after
      multipathing is configured.
     </para>
     <para>
      To modify<filename> /etc/lvm/lvm.conf</filename> for multipath use:
     </para>
     <procedure id="bg89qbw">
      <step id="bg89qbx">
       <para>
        Open the <filename>/etc/lvm/lvm.conf</filename> file in a text
        editor.
       </para>
       <para>
        If <filename>/etc/lvm/lvm.conf</filename> does not exist, you can
        create one based on your current LVM configuration by entering the
        following at a terminal console prompt:
       </para>
<screen>
lvm dumpconfig &gt; /etc/lvm/lvm.conf
</screen>
      </step>
      <step id="bg89qku">
       <para>
        Change the <literal>filter</literal> and <literal>types</literal>
        entries in <filename>/etc/lvm/lvm.conf</filename> as follows:
       </para>
<screen>
filter = [ "a|/dev/disk/by-id/.*|", "r|.*|" ]
types = [ "device-mapper", 1 ]
</screen>
       <para>
        This allows LVM2 to scan only the by-id paths and reject everything
        else.
       </para>
       <para>
        If you are using user-friendly names, specify the filter path so
        that only the Device Mapper names are scanned after multipathing is
        configured. The following filter path accepts only partitions on a
        multipathed device:
       </para>
<screen>
filter = [ "a|/dev/disk/by-id/dm-uuid-.*-mpath-.*|", "r|.*|" ]
</screen>
       <para>
        To accept both raw disks and partitions for Device Mapper names,
        specify the path as follows, with no hyphen (-) before
        <filename>mpath</filename>:
       </para>
<screen>
filter = [ "a|/dev/disk/by-id/dm-uuid-.*mpath-.*|", "r|.*|" ]
</screen>
      </step>
      <step id="bg89rst">
       <para>
        If you are also using LVM2 on non-multipathed devices, make the
        necessary adjustments in the <literal>filter</literal> and
        <literal>types</literal> entries to suit your setup. Otherwise, the
        other LVM devices are not visible with a <command>pvscan</command>
        after you modify the <filename>lvm.conf</filename> file for
        multipathing.
       </para>
       <para>
        You want only those devices that are configured with LVM to be
        included in the LVM cache, so ensure that you are specific about
        which other non-multipathed devices are included by the filter.
       </para>
       <para>
        For example, if your local disk is <filename>/dev/sda</filename> and
        all SAN devices are <filename>/dev/sdb</filename> and above, specify
        the local and multipathing paths in the filter as follows:
       </para>
<screen>
filter = [ "a|/dev/sda.*|", "a|/dev/disk/by-id/.*|", "r|.*|" ]
types = [ "device-mapper", 253 ]
</screen>
      </step>
      <step id="bg89rib">
       <para>
        Save the file.
       </para>
      </step>
      <step id="bg89zhw">
       <para>
        Add dm-multipath to
	<filename>/etc/dracut.conf.d/01-dist.conf</filename> by adding the
	following line:
       </para>
       <screen>force_drivers+="dm-multipath"</screen>
      </step>
      <step id="bg8aqrw">
       <para>
        Make a new <filename>initrd</filename> to ensure that the Device
        Mapper Multipath services are loaded with the changed settings.
        Running <command>dracut</command> is needed only
        if the root (/) device or any parts of it (such as
        <filename>/var</filename>, <filename>/etc</filename>,
        <filename>/log</filename>) are on the SAN and multipath is needed to
        boot.
       </para>
       <para>
        Enter the following at a terminal console prompt:
       </para>
<screen>
dracut -f --add-drivers multipath
</screen>
      </step>
      <step id="bg89rnr">
       <para>
        Reboot the server to apply the changes.
       </para>
      </step>
     </procedure>
    </sect3>
    <sect3 id="b13iv6p5">
     <title>Enabling boot.multipath</title>
     <para>
      Multipath must be loaded before LVM to ensure that multipath maps are
      built correctly. Loading multipath after LVM can result in incomplete
      device maps for a multipath device because LVM locks the device, and
      MPIO cannot create the maps properly.
     </para>
     <para>
      If the system device is a local device that does not use MPIO and LVM,
      you can disable both <filename>boot.multipath</filename> and
      <filename>boot.lvm</filename>. After the server starts, you can
      manually start multipath before you start LVM, then run a
      <command>pvscan</command> command to recognize the LVM objects.
     </para>
    </sect3>
    <sect3 id="b13iv6p6">
     <title>Troubleshooting MPIO Mapping for LVM Devices</title>
     <para>
      Timing is important for starting the LVM process. If LVM starts before
      MPIO maps are done, LVM might use a fixed path for the device instead
      of its multipath. The device works, so you might not be aware that the
      device’s MPIO map is incomplete until that fixed path fails. You can
      help prevent the problem by enabling
      <filename>boot.multipath</filename> and following the instructions in
      <xref linkend="b13iv5ro" xrefstyle="HeadingOnPage"/>.
     </para>
     <para>
      To troubleshoot a mapping problem, you can use
      <command>dmsetup</command> to check that the expected number of paths
      are present for each multipath device. As the
      <systemitem>root</systemitem> user, enter the following at a command
      prompt:
     </para>
<screen>
dmsetup ls --tree
</screen>
     <para>
      In the following sample response, the first device has four paths. The
      second device is a local device with a single path. The third device
      has two paths. The distinction between active and passive paths is not
      reported through this tool.
     </para>
<screen>
  vg910-lv00 (253:23)
    └─ 360a980006465576657346d4b6c593362 (253:10)
      |- (65:96)
      |- (8:128)
      |- (8:240)
      └─ (8:16)
  vg00-lv08 (253:9)
    └─ (8:3)
  system_vg-data_lv (253:1)
    └─36006016088d014007e0d0d2213ecdf11 (253:0)
      ├─ (8:32)
      └─ (8:48)
</screen>
     <para>
      An incorrect mapping typically returns too few paths and does not have
      a major number of 253. For example, the following shows what an
      incorrect mapping looks like for the third device:
     </para>
<screen>
  system_vg-data_lv (8:31)
     └─ (8:32)
</screen>
    </sect3>
   </sect2>

   <sect2 id="mpiousingmdadm">
    <title>Using mdadm with Multipath Devices</title>
    <para>
     The <command>mdadm</command> tool requires that the devices be accessed
     by the ID rather than by the device node path. Therefore, the
     <systemitem>DEVICE</systemitem> entry in
     <filename>/etc/mdadm.conf</filename> file should be set as follows to
     ensure that only device mapper names are scanned after multipathing is
     configured:
    </para>
<screen>
DEVICE /dev/disk/by-id/dm-uuid-.*mpath-.*
</screen>
    <para>
     If you are using user-friendly names or multipath aliases, specify the
     path as follows:
    </para>
<screen>
DEVICE /dev/disk/by-id/dm-name-.*
</screen>
   </sect2>

   <sect2 id="b14ff9rs">
    <title>Using Multipath with NetApp Devices</title>
    <para>
     When using multipath for NetApp devices, we recommend the following
     settings in the <filename>/etc/multipath.conf</filename> file:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Set the default values for the following parameters globally for
       NetApp devices:
      </para>
<screen>
max_fds max

queue_without_daemon no
</screen>
     </listitem>
     <listitem>
      <para>
       Set the default values for the following parameters for NetApp
       devices in the hardware table:
      </para>
<screen>
dev_loss_tmo infinity

fast_io_fail_tmo 5

features "3 queue_if_no_path pg_init_retries 50"
</screen>
     </listitem>
    </itemizedlist>
   </sect2>

   <sect2 id="bleqcv0">
    <title>Using --noflush with Multipath Devices</title>
    <para>
     The <literal>--noflush</literal> option should always be used when
     running on multipath devices.
    </para>
    <para>
     For example, in scripts where you perform a table reload, you use the
     <literal>--noflush</literal> option on resume to ensure that any
     outstanding I/O is not flushed, because you need the multipath topology
     information.
    </para>
<screen>
load
resume --noflush
</screen>
   </sect2>

   <sect2 id="bok8cn1">
    <title>SAN Timeout Settings When the Root Device Is Multipathed</title>
    <remark> Bug 492469 - FC boot lun with device mapper multipath, server died when no path available for short time, comment 42</remark>
    <para>
     A system with root (<filename>/</filename>) on a multipath device might
     stall when all paths have failed and are removed from the system
     because a <literal>dev_loss_tmo</literal> time-out is received from the
     storage subsystem (such as Fibre Channel storage arrays).
    </para>
    <para>
     If the system device is configured with multiple paths and the
     multipath <literal>no_path_retry</literal> setting is active, you
     should modify the storage subsystem’s <literal>dev_loss_tmo</literal>
     setting accordingly to ensure that no devices are removed during an
     all-paths-down scenario. We strongly recommend that you set the
     <literal>dev_loss_tmo</literal> value to be equal to or higher than the
     <literal>no_path_retry</literal> setting from multipath.
    </para>
    <para>
     The recommended setting for the storage subsystem’s
     <literal>dev_los_tmo</literal> is:
    </para>
<screen>
&lt;dev_loss_tmo&gt; = &lt;no_path_retry&gt; * &lt;polling_interval&gt;
</screen>
    <para>
     where the following definitions apply for the multipath values:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>no_path_retry</literal> is the number of retries for
       multipath I/O until the path is considered to be lost, and queuing of
       IO is stopped.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>polling_interval</literal> is the time in seconds between
       path checks.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Each of these multipath values should be set from the
     <filename>/etc/multipath.conf</filename> configuration file. For
     information, see <xref linkend="bbillhs" xrefstyle="SectTitleOnPage"/>.
    </para>
   </sect2>

   <sect2 id="mpiousingpart">
    <title>Partitioning Multipath Devices</title>
    <para>
     Behavior changes for how multipathed devices are partitioned might
     affect your configuration if you are upgrading.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="binawqz" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="binazqd" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="binazxj" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="binawqz">
     <title>SUSE Linux Enterprise Server 11</title>
     <para>
      In SUSE Linux Enterprise Server 11, the default multipath setup relies
      on <command>udev</command> to overwrite the existing symbolic links in
      the <filename>/dev/disk/by-id</filename> directory when multipathing
      is started. Before you start multipathing, the link points to the SCSI
      device by using its <filename>scsi-xxx</filename> name. When
      multipathing is running, the symbolic link points to the device by
      using its <filename>dm-uuid-xxx</filename> name. This ensures that the
      symbolic links in the <filename>/dev/disk/by-id</filename> path
      persistently point to the same device regardless of whether multipath
      is started or not.
     </para>
     <para>
      Ensure that the configuration files for <filename>lvm.conf</filename>
      and <filename>md.conf</filename> point to the multipath-device names.
      This should happen automatically if
      <filename>boot.multipath</filename> is enabled and loads before
      <filename>boot.lvm</filename> and <filename>boot.md</filename>.
      Otherwise, the LVM and MD configuration files might contain fixed
      paths for multipath-devices, and you must correct those paths to use
      the multipath-device names. For LVM2 and cLVM information, see
      <xref linkend="mpiousinglvm" xrefstyle="SectTitleOnPage"/>. For
      software RAID information, see
      <xref linkend="mpioraid" xrefstyle="SectTitleOnPage"/>.
     </para>
    </sect3>
    <sect3 id="binazqd">
     <title>SUSE Linux Enterprise Server 10</title>
     <para>
      In SUSE Linux Enterprise Server 10, the <filename>kpartx</filename>
      software is used in the
<!--taroth 2014-02-27: systemd: the following can stay as it refers to SLE
       10-->
      <filename>/etc/init.d/boot.multipath</filename> to add symbolic links
      to the line <filename>/dev/dm-*</filename> in the
      <filename>multipath.conf</filename> configuration file for any newly
      created partitions without requiring a reboot. This triggers
      <filename>udevd</filename> to fill in the
      <filename>/dev/disk/by-*</filename> symbolic links. The main benefit
      is that you can call <filename>kpartx</filename> with the new
      parameters without rebooting the server.
     </para>
    </sect3>
    <sect3 id="binazxj">
     <title>SUSE Linux Enterprise Server 9</title>
     <para>
      In SUSE Linux Enterprise Server 9, it is not possible to partition
      multipath I/O devices themselves. If the underlying physical device is
      already partitioned, the multipath I/O device reflects those
      partitions and the layer provides
      <filename>/dev/disk/by-id/&lt;name&gt;p1 ... pN</filename> devices so
      you can access the partitions through the multipath I/O layer. As a
      consequence, the devices need to be partitioned prior to enabling
      multipath I/O. If you change the partitioning in the running system,
      DM-MPIO does not automatically detect and reflect these changes. The
      device must be re-initialized, which usually requires a reboot.
     </para>
    </sect3>
   </sect2>

   <sect2 id="be5rs3a">
    <title>Supported Architectures for Multipath I/O</title>
    <para>
     The multipathing drivers and tools support all seven of the supported
     processor architectures: IA32, AMD64/EM64T, IPF/IA64, p-Series (32-bit
     and 64-bit), and z-Series (31-bit and 64-bit).
    </para>
   </sect2>

   <sect2 id="be5ruyr">
    <title>Supported Storage Arrays for Multipathing</title>
    <para>
     The multipathing drivers and tools support most storage arrays. The
     storage array that houses the multipathed device must support
     multipathing in order to use the multipathing drivers and tools. Some
     storage array vendors provide their own multipathing management tools.
     Consult the vendor’s hardware documentation to determine what
     settings are required.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="be5s8ae" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="mpiosuphw" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="be5s6p6" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="be5s8ae">
     <title>Storage Arrays That Are Automatically Detected for Multipathing</title>
     <para>
      The <filename>multipath-tools</filename> package automatically detects
      the following storage arrays:
     </para>
     <simplelist>
      <member>3PARdata VV</member>
      <member>AIX NVDISK</member>
      <member>AIX VDASD</member>
      <member>APPLE Xserve RAID</member>
      <member>COMPELNT Compellent Vol</member>
      <member>COMPAQ/HP HSV101, HSV111, HSV200, HSV210, HSV300, HSV400, HSV 450</member>
      <member>COMPAQ/HP MSA, HSV</member>
      <member>COMPAQ/HP MSA VOLUME</member>
      <member>DataCore SANmelody</member>
      <member>DDN SAN DataDirector</member>
      <member>DEC HSG80</member>
      <member>DELL MD3000</member>
      <member>DELL MD3000i</member>
      <member>DELL MD32xx</member>
      <member>DELL MD32xxi</member>
      <member>DGC</member>
      <member>EMC Clariion</member>
      <member>EMC Invista</member>
      <member>EMC SYMMETRIX</member>
      <member>EUROLOGC FC2502</member>
      <member>FSC CentricStor</member>
      <member>FUJITSU ETERNUS_DX, DXL, DX400, DX8000</member>
      <member>HITACHI DF</member>
      <member>HITACHI/HP OPEN</member>
      <member>HP A6189A</member>
      <member>HP HSVX700</member>
      <member>HP LOGICAL VOLUME</member>
      <member>HP MSA2012fc, MSA 2212fc, MSA2012i</member>
      <member>HP MSA2012sa, MSA2312 fc/i/sa, MCA2324 fc/i/sa, MSA2000s VOLUME</member>
      <member>HP P2000 G3 FC|P2000G3 FC/iSCSI|P2000 G3 SAS|P2000 G3 iSCSI</member>
      <member>IBM 1722-600</member>
      <member>IBM 1724</member>
      <member>IBM 1726</member>
      <member>IBM 1742</member>
      <member>IBM 1745, 1746</member>
      <member>IBM 1750500</member>
      <member>IBM 1814</member>
      <member>IBM 1815</member>
      <member>IBM 1818</member>
      <member>IBM 1820N00</member>
      <member>IBM 2105800</member>
      <member>IBM 2105F20</member>
      <member>IBM 2107900</member>
      <member>IBM 2145</member>
      <member>IBM 2810XIV</member>
      <member>IBM 3303 NVDISK</member>
      <member>IBM 3526</member>
      <member>IBM 3542</member>
      <member>IBM IPR</member>
      <member>IBM Nseries</member>
      <member>IBM ProFibre 4000R</member>
      <member>IBM S/390 DASD ECKD</member>
      <member>IBM S/390 DASD FBA</member>
      <member>Intel Multi-Flex</member>
      <member>LSI/ENGENIO INF-01-00</member>
      <member>NEC DISK ARRAY</member>
      <member>NETAPP LUN</member>
      <member>NEXENTA COMSTAR</member>
      <member>Pillar Axiom</member>
      <member>PIVOT3 RAIGE VOLUME</member>
      <member>SGI IS</member>
      <member>SGI TP9100, TP 9300</member>
      <member>SGI TP9400, TP9500</member>
      <member>STK FLEXLINE 380</member>
      <member>STK OPENstorage D280</member>
      <member>SUN CSM200_R</member>
      <member>SUN LCSM100_[IEFS]</member>
      <member>SUN STK6580, STK6780</member>
      <member>SUN StorEdge 3510, T4</member>
      <member>SUN SUN_6180</member>
     </simplelist>
     <para>
      In general, most other storage arrays should work. When storage arrays
      are automatically detected, the default settings for multipathing
      apply. If you want non-default settings, you must manually create and
      configure the <filename>/etc/multipath.conf</filename> file. For
      information, see
      <xref linkend="bbillhs" xrefstyle="SectTitleOnPage"/>.
     </para>
     <para>
      Testing of the IBM zSeries device with multipathing has shown that the
      <literal>dev_loss_tmo</literal> parameter should be set to 90 seconds,
      and the <literal>fast_io_fail_tmo</literal> parameter should be set to
      5 seconds. If you are using zSeries devices, you must manually create
      and configure the <filename>/etc/multipath.conf</filename> file to
      specify the values. For information, see
      <xref linkend="bkj8n9w" xrefstyle="HeadingOnPage"/>.
     </para>
     <para>
      Hardware that is not automatically detected requires an appropriate
      entry for configuration in the <literal>devices</literal> section of
      the <filename>/etc/multipath.conf</filename> file. In this case, you
      must manually create and configure the configuration file. For
      information, see
      <xref linkend="bbillhs" xrefstyle="SectTitleOnPage"/>.
     </para>
     <para>
      Consider the following caveats:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Not all of the storage arrays that are automatically detected have
        been tested on SUSE Linux Enterprise Server. For information, see
        <xref linkend="mpiosuphw" xrefstyle="HeadingOnPage"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Some storage arrays might require specific hardware handlers. A
        hardware handler is a kernel module that performs hardware-specific
        actions when switching path groups and dealing with I/O errors. For
        information, see
        <xref linkend="be5s6p6" xrefstyle="HeadingOnPage"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        After you modify the <filename>/etc/multipath.conf</filename> file,
        you must run <command>dracut <option>-f</option></command> to
        re-create the INITRD on your system, then reboot in order for the
        changes to take effect.
       </para>
      </listitem>
     </itemizedlist>
    </sect3>
    <sect3 id="mpiosuphw">
     <title>Tested Storage Arrays for Multipathing Support</title>
     <para>
      The following storage arrays have been tested with SUSE Linux
      Enterprise Server:
     </para>
     <simplelist>
      <member>EMC</member>
      <member>Hitachi</member>
      <member>Hewlett-Packard/Compaq</member>
      <member>IBM</member>
      <member>NetApp</member>
      <member>SGI</member>
     </simplelist>
     <para>
      Most other vendor storage arrays should also work. Consult your
      vendor’s documentation for guidance. For a list of the default
      storage arrays recognized by the <filename>multipath-tools</filename>
      package, see <xref linkend="be5s8ae" xrefstyle="HeadingOnPage"/>.
     </para>
    </sect3>
    <sect3 id="be5s6p6">
     <title>Storage Arrays that Require Specific Hardware Handlers</title>
     <para>
      Storage arrays that require special commands on failover from one path
      to the other or that require special nonstandard error handling might
      require more extensive support. Therefore, the Device Mapper Multipath
      service has hooks for hardware handlers. For example, one such handler
      for the EMC CLARiiON CX family of arrays is already provided.
     </para>
     <important>
      <para>
       Consult the hardware vendor’s documentation to determine if its
       hardware handler must be installed for Device Mapper Multipath.
      </para>
     </important>
     <para>
      The <command>multipath -t</command> command shows an internal table of
      storage arrays that require special handling with specific hardware
      handlers. The displayed list is not an exhaustive list of supported
      storage arrays. It lists only those arrays that require special
      handling and that the <filename>multipath-tools</filename> developers
      had access to during the tool development.
     </para>
     <important>
      <para>
       Arrays with true active/active multipath support do not require
       special handling, so they are not listed for the <command>multipath
       -t</command> command.
      </para>
     </important>
     <para>
      A listing in the <command>multipath -t</command> table does not
      necessarily mean that SUSE Linux Enterprise Server was tested on that
      specific hardware. For a list of tested storage arrays, see
      <xref linkend="mpiosuphw" xrefstyle="HeadingOnPage"/>.
     </para>
    </sect3>
   </sect2>
  </sect1>
  <sect1 id="mpiotools">
   <title>Multipath Management Tools</title>

   <para>
    The multipathing support in SUSE Linux Enterprise Server 10 and later is
    based on the Device Mapper Multipath module of the Linux 2.6 kernel and
    the <systemitem>multipath-tools</systemitem> userspace package. You can
    use the Multiple Devices Administration utility (MDADM,
    <command>mdadm</command>) to view the status of multipathed devices.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="mpiotoolsdm" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="mpiotoolsmpt" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="mpiotoolsmdadm" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="beep4ms" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b15jw320" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="mpiotoolsdm">
    <title>Device Mapper Multipath Module</title>
    <para>
     The Device Mapper Multipath (DM-MP) module provides the multipathing
     capability for Linux. DM-MPIO is the preferred solution for
     multipathing on &productname;. It is the only
     multipathing option shipped with the product that is completely
     supported by Novell and SUSE.
    </para>
    <para>
     DM-MPIO features automatic configuration of the multipathing subsystem
     for a large variety of setups. Configurations of up to 8 paths to each
     device are supported. Configurations are supported for active/passive
     (one path active, others passive) or active/active (all paths active
     with round-robin load balancing).
    </para>
    <para>
     The DM-MPIO framework is extensible in two ways:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Using specific hardware handlers. For information, see
       <xref linkend="be5s6p6" xrefstyle="HeadingOnPage"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       Using load-balancing algorithms that are more sophisticated than the
       round-robin algorithm
      </para>
     </listitem>
    </itemizedlist>
    <para>
     The user-space component of DM-MPIO takes care of automatic path
     discovery and grouping, as well as automated path retesting, so that a
     previously failed path is automatically reinstated when it becomes
     healthy again. This minimizes the need for administrator attention in a
     production environment.
    </para>
    <para>
     DM-MPIO protects against failures in the paths to the device, and not
     failures in the device itself. If one of the active paths is lost (for
     example, a network adapter breaks or a fiber-optic cable is removed),
     I/O is redirected to the remaining paths. If the configuration is
     active/passive, then the path fails over to one of the passive paths.
     If you are using the round-robin load-balancing configuration, the
     traffic is balanced across the remaining healthy paths. If all active
     paths fail, inactive secondary paths must be waked up, so failover
     occurs with a delay of approximately 30 seconds.
    </para>
    <para>
     If a disk array has more than one storage processor, ensure that the
     SAN switch has a connection to the storage processor that owns the LUNs
     you want to access. On most disk arrays, all LUNs belong to both
     storage processors, so both connections are active.
    </para>
    <note>
     <para>
      On some disk arrays, the storage array manages the traffic through
      storage processors so that it presents only one storage processor at a
      time. One processor is active and the other one is passive until there
      is a failure. If you are connected to the wrong storage processor (the
      one with the passive path) you might not see the expected LUNs, or you
      might see the LUNs but get errors when you try to access them.
     </para>
    </note>
    <table id="b567ef1" frame="topbot" rowsep="1" pgwide="0">
     <title>Multipath I/O Features of Storage Arrays</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="2857*"/>
      <colspec colnum="2" colname="2" colwidth="7144*"/>
      <thead>
       <row id="b567fx1">
        <entry>
         <para>
          Features of Storage Arrays
         </para>
        </entry>
        <entry>
         <para>
          Description
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b567fx2">
        <entry>
         <para>
          Active/passive controllers
         </para>
        </entry>
        <entry>
         <para>
          One controller is active and serves all LUNs. The second
          controller acts as a standby. The second controller also presents
          the LUNs to the multipath component so that the operating system
          knows about redundant paths. If the primary controller fails, the
          second controller takes over, and it serves all LUNs.
         </para>
         <para>
          In some arrays, the LUNs can be assigned to different controllers.
          A given LUN is assigned to one controller to be its active
          controller. One controller does the disk I/O for any given LUN at
          a time, and the second controller is the standby for that LUN. The
          second controller also presents the paths, but disk I/O is not
          possible. Servers that use that LUN are connected to the LUN’s
          assigned controller. If the primary controller for a set of LUNs
          fails, the second controller takes over, and it serves all LUNs.
         </para>
        </entry>
       </row>
       <row id="b567fx3">
        <entry>
         <para>
          Active/active controllers
         </para>
        </entry>
        <entry>
         <para>
          Both controllers share the load for all LUNs, and can process disk
          I/O for any given LUN. If one controller fails, the second
          controller automatically handles all traffic.
         </para>
        </entry>
       </row>
       <row id="b567fx4">
        <entry>
         <para>
          Load balancing
         </para>
        </entry>
        <entry>
         <para>
          The Device Mapper Multipath driver automatically load balances
          traffic across all active paths.
         </para>
        </entry>
       </row>
       <row id="b567fx6">
        <entry>
         <para>
          Controller failover
         </para>
        </entry>
        <entry>
         <para>
          When the active controller fails over to the passive, or standby,
          controller, the Device Mapper Multipath driver automatically
          activates the paths between the host and the standby, making them
          the primary paths.
         </para>
        </entry>
       </row>
       <row id="b567fx5">
        <entry>
         <para>
          Boot/Root device support
         </para>
        </entry>
        <entry>
         <para>
          Multipathing is supported for the root (<filename>/</filename>)
          device in SUSE Linux Enterprise Server 10 and later. The host
          server must be connected to the currently active controller and
          storage processor for the boot device.
         </para>
         <para>
          Multipathing is supported for the <filename>/boot</filename>
          device in SUSE Linux Enterprise Server 11 and later.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para>
     Device Mapper Multipath detects every path for a multipathed device as
     a separate SCSI device. The SCSI device names take the form
     <filename>/dev/sd<replaceable>N</replaceable></filename>, where
     <filename><replaceable>N</replaceable></filename> is an autogenerated
     letter for the device, beginning with a and issued sequentially as the
     devices are created, such as <filename>/dev/sda</filename>,
     <filename>/dev/sdb</filename>, and so on. If the number of devices
     exceeds 26, the letters are duplicated so that the next device after
     <filename>/dev/sdz</filename> will be named
     <filename>/dev/sdaa</filename>, <filename>/dev/sdab</filename>, and so
     on.
    </para>
    <para>
     If multiple paths are not automatically detected, you can configure
     them manually in the <filename>/etc/multipath.conf</filename> file. The
     <filename>multipath.conf</filename> file does not exist until you
     create and configure it. For information, see
     <xref linkend="bbillhs" xrefstyle="SectTitleOnPage"/>.
    </para>
   </sect2>

   <sect2 id="mpiotoolsmpt">
    <title>Multipath I/O Management Tools</title>
    <para>
     The <command>multipath-tools</command> user-space package takes care of
     automatic path discovery and grouping. It automatically tests the path
     periodically, so that a previously failed path is automatically
     reinstated when it becomes healthy again. This minimizes the need for
     administrator attention in a production environment.
    </para>
    <table id="b59pxng" frame="topbot" rowsep="1" pgwide="0">
     <title>Tools in the multipath-tools Package</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="2857*"/>
      <colspec colnum="2" colname="2" colwidth="7144*"/>
      <thead>
       <row id="b59q186">
        <entry>
         <para>
          Tool
         </para>
        </entry>
        <entry>
         <para>
          Description
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b59q187">
        <entry>
         <para>
          multipath
         </para>
        </entry>
        <entry>
         <para>
          Scans the system for multipathed devices and assembles them.
         </para>
        </entry>
       </row>
       <row id="b59q188">
        <entry>
         <para>
          multipathd
         </para>
        </entry>
        <entry>
         <para>
          Waits for maps events, then executes <command>multipath</command>.
         </para>
        </entry>
       </row>
       <row id="b59q189">
        <entry>
         <para>
          devmap-name
         </para>
        </entry>
        <entry>
         <para>
          Provides a meaningful device name to <command>udev</command> for
          device maps (devmaps).
         </para>
        </entry>
       </row>
       <row id="b59q18a">
        <entry>
         <para>
          kpartx
         </para>
        </entry>
        <entry>
         <para>
          Maps linear devmaps to partitions on the multipathed device, which
          makes it possible to create multipath monitoring for partitions on
          the device.
         </para>
        </entry>
       </row>
       <row id="b15jwqx6">
        <entry>
         <para>
          mpathpersist
         </para>
        </entry>
        <entry>
         <para>
          Manages SCSI persistent reservations on Device Mapper Multipath
          devices.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para>
     The file list for a package can vary for different server
     architectures. For a list of files included in the multipath-tools
     package, go to the
     <ulink url="http://www.suse.com/products/server/?tab=1"><citetitle>SUSE
     Linux Enterprise Server Technical Specifications</citetitle> &gt;
     <citetitle>Package Descriptions </citetitle>Web page</ulink>, find your
     architecture and select <guimenu>Packages Sorted by Name</guimenu>,
     then search on <quote>multipath-tools</quote> to find the package list
     for that architecture.
    </para>
    <para>
     You can also determine the file list for an RPM file by querying the
     package itself: using the <command>rpm -ql</command> or <command>rpm
     -qpl</command> command options.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       To query an installed package, enter
      </para>
<screen>
rpm -ql &lt;<replaceable>package_name</replaceable>&gt;
</screen>
     </listitem>
     <listitem>
      <para>
       To query a package not installed, enter
      </para>
<screen>
rpm -qpl &lt;<replaceable>URL_or_path_to_package</replaceable>&gt;
</screen>
     </listitem>
    </itemizedlist>
    <para>
     To check that the <filename>multipath-tools</filename> package is
     installed, do the following:
    </para>
    <procedure id="bcmre1m">
     <step id="bcmre1n">
      <para>
       Enter the following at a terminal console prompt:
      </para>
<screen>
rpm -q multipath-tools
</screen>
      <para>
       If it is installed, the response repeats the package name and
       provides the version information, such as:
      </para>
<screen>
multipath-tools-04.7-34.23
</screen>
      <para>
       If it is not installed, the response reads:
      </para>
<screen>
package multipath-tools is not installed
</screen>
     </step>
    </procedure>
   </sect2>

   <sect2 id="mpiotoolsmdadm">
    <title>Using MDADM for Multipathed Devices</title>
    <para>
     Udev is the default device handler, and devices are automatically known
     to the system by the Worldwide ID instead of by the device node name.
     This resolves problems in previous releases of MDADM and LVM where the
     configuration files (<filename>mdadm.conf</filename> and
     <filename>lvm.conf)</filename> did not properly recognize multipathed
     devices.
    </para>
    <para>
     As with LVM2, MDADM requires that the devices be accessed by the ID
     rather than by the device node path. Therefore, the
     <systemitem>DEVICE</systemitem> entry in
     <filename>/etc/mdadm.conf</filename> should be set as follows:
    </para>
<screen>
DEVICE /dev/disk/by-id/*
</screen>
    <para>
     If you are using user-friendly names, specify the path as follows so
     that only the device mapper names are scanned after multipathing is
     configured:
    </para>
<screen>
DEVICE /dev/disk/by-id/dm-uuid-.*-mpath-.*
</screen>
    <para>
     To verify that MDADM is installed:
    </para>
    <procedure id="bcmrfp3">
     <step id="bcmrfp4">
      <para>
       Ensure that the <filename>mdadm</filename> package is installed by
       entering the following at a terminal console prompt:
      </para>
<screen>
rpm -q mdadm
</screen>
      <para>
       If it is installed, the response repeats the package name and
       provides the version information. For example:
      </para>
<screen>
mdadm-2.6-0.11
</screen>
      <para>
       If it is not installed, the response reads:
      </para>
<screen>
package mdadm is not installed
</screen>
     </step>
    </procedure>
    <para>
     For information about modifying the <filename>/etc/lvm/lvm.conf
     </filename>file, see
     <xref linkend="mpiousinglvm" xrefstyle="SectTitleOnPage"/>.
    </para>
   </sect2>

   <sect2 id="beep4ms">
    <title>Linux multipath(8) Command</title>
    <para>
     Use the Linux <command>multipath(8)</command> command to configure and
     manage multipathed devices.
    </para>
    <para>
     General syntax for the <command>multipath(8)</command> command:
    </para>
<screen>
multipath [-v verbosity_level] [-b bindings_file] [-d] [-h|-l|-ll|-f|-F|-B|-c|-q|-r|-w|-W] [-p failover|multibus|group_by_serial|group_by_prio|group_by_node_name] [<replaceable>devicename</replaceable>]
</screen>
    <bridgehead id="b15drtkg">Options</bridgehead>
    <variablelist>
     <varlistentry id="b15drtkh">
      <term>-v verbosity_level</term>
      <listitem>
       <para>
        Prints all paths and multipaths.
       </para>
       <variablelist>
        <varlistentry id="b15drtki">
         <term>0</term>
         <listitem>
          <para>
           No output.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry id="b15drtkj">
         <term>1</term>
         <listitem>
          <para>
           Prints only the created or updated multipath names. Used to feed
           other tools like <command>kpartx</command>.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry id="b15drtkk">
         <term>2+</term>
         <listitem>
          <para>
           Print all information: Detected paths, multipaths, and device
           maps.
          </para>
         </listitem>
        </varlistentry>
       </variablelist>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtkl">
      <term>-h</term>
      <listitem>
       <para>
        Print usage text.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtkm">
      <term>-d</term>
      <listitem>
       <para>
        Dry run; do not create or update device maps.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtkn">
      <term>-l</term>
      <listitem>
       <para>
        Show the current multipath topology from information fetched in
        <filename>sysfs</filename> and the device mapper.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtko">
      <term>-ll</term>
      <listitem>
       <para>
        Show the current multipath topology from all available information
        (sysfs, the device mapper, path checkers, and so on).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtkp">
      <term>-f</term>
      <listitem>
       <para>
        Flush a multipath device map specified as a parameter, if unused.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtkq">
      <term>-F</term>
      <listitem>
       <para>
        Flush all unused multipath device maps.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtkr">
      <term>-t</term>
      <listitem>
       <para>
        Print internal hardware table to <filename>stdout</filename>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtks">
      <term>-r</term>
      <listitem>
       <para>
        Force device map reload.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtkt">
      <term>-B</term>
      <listitem>
       <para>
        Treat the bindings file as read only.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtku">
      <term>-b bindings_file</term>
      <listitem>
       <para>
        Set the user_friendly_names bindings file location. The default is
        <filename>/etc/multipath/bindings</filename>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtkv">
      <term>-c</term>
      <listitem>
       <para>
        Check if a block device should be a path in a multipath device.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtkw">
      <term>-q</term>
      <listitem>
       <para>
        Allow device tables with <literal>queue_if_no_path</literal> when
        <filename>multipathd</filename> is not running.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtkx">
      <term>-w</term>
      <listitem>
       <para>
        Remove the WWID for the specified device from the WWIDs file.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drtky">
      <term>-W</term>
      <listitem>
       <para>
        Reset the WWIDs file to only include the current multipath devices.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drv6f">
      <term>-p policy</term>
      <listitem>
       <para>
        Force new maps to use the specified policy. Existing maps are not
        modified.
       </para>
       <variablelist>
        <varlistentry id="b15drv6g">
         <term>failover</term>
         <listitem>
          <para>
           One path per priority group.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry id="b15drv6h">
         <term>multibus</term>
         <listitem>
          <para>
           All paths in one priority group.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry id="b15drv6i">
         <term>group_by_serial</term>
         <listitem>
          <para>
           One priority group per serial.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry id="b15drv6j">
         <term>group_by_prio</term>
         <listitem>
          <para>
           One priority group per priority value. Priorities are determined
           by callout programs specified as a global, per-controller or
           per-multipath option in the configuration file.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry id="b15drv6k">
         <term>group_by_node_name</term>
         <listitem>
          <para>
           One priority group per target node name. Target node names are
           fetched in
           <filename>/sys/class/fc_transport/target*/node_name</filename>.
          </para>
         </listitem>
        </varlistentry>
       </variablelist>
      </listitem>
     </varlistentry>
     <varlistentry id="b15drx1o">
      <term>device_name</term>
      <listitem>
       <para>
        Update only the device map for the specified device. Specify the
        name as the device path such as <filename>/dev/sdb</filename>, or in
        <literal><replaceable>major</replaceable>:<replaceable>minor</replaceable></literal>
        format, or the multipath map name.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <bridgehead id="beep4mt">General Examples</bridgehead>
    <variablelist>
     <varlistentry id="bomamxl">
      <term>multipath</term>
      <listitem>
       <para>
        Configure all multipath devices.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bomalct">
      <term>multipath <replaceable>devicename</replaceable>
      </term>
      <listitem>
       <para>
        Configures a specific multipath device.
       </para>
       <para>
        Replace <replaceable>devicename</replaceable> with the device node
        name such as <filename>/dev/sdb</filename> (as shown by udev in the
        $DEVNAME variable), or in the <literal>major:minor</literal> format.
        The device may alternatively be a multipath map name.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bomamxm">
      <term>multipath -f</term>
      <listitem>
       <para>
        Selectively suppresses a multipath map, and its device-mapped
        partitions.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bomamxn">
      <term>multipath -d</term>
      <listitem>
       <para>
        Dry run. Displays potential multipath devices, but does not create
        any devices and does not update device maps.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bomamxo">
      <term>multipath -v2 -d</term>
      <listitem>
       <para>
        Displays multipath map information for potential multipath devices
        in a dry run. The -v2 option shows only local disks. This verbosity
        level prints the created or updated multipath names only for use to
        feed other tools like kpartx.
       </para>
       <para>
        There is no output if the devices already exists and there are no
        changes. Use <command>multipath -ll</command> to see the status of
        configured multipath devices.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bomaqc3">
      <term>multipath -v2 <replaceable>devicename</replaceable>
      </term>
      <listitem>
       <para>
        Configures a specific potential multipath device and displays
        multipath map information for it. This verbosity level prints only
        the created or updated multipath names for use to feed other tools
        like <command>kpartx</command>.
       </para>
       <para>
        There is no output if the device already exists and there are no
        changes. Use <command>multipath -ll</command> to see the status of
        configured multipath devices.
       </para>
       <para>
        Replace <replaceable>devicename</replaceable> with the device node
        name such as <filename>/dev/sdb</filename> (as shown by
        <command>udev</command> in the $DEVNAME variable), or in the
        <literal>major:minor</literal> format. The device may alternatively
        be a multipath map name.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bomaqc4">
      <term>multipath -v3</term>
      <listitem>
       <para>
        Configures potential multipath devices and displays multipath map
        information for them. This verbosity level prints all detected
        paths, multipaths, and device maps. Both wwid and devnode
        blacklisted devices are displayed.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bomaqc5">
      <term>multipath -v3 <replaceable>devicename</replaceable>
      </term>
      <listitem>
       <para>
        Configures a specific potential multipath device and displays
        information for it. The -v3 option shows the full path list. This
        verbosity level prints all detected paths, multipaths, and device
        maps. Both wwid and devnode blacklisted devices are displayed.
       </para>
       <para>
        Replace <replaceable>devicename</replaceable> with the device node
        name such as <filename>/dev/sdb</filename> (as shown by
        <command>udev</command> in the $DEVNAME variable), or in the
        <literal>major:minor</literal> format. The device may alternatively
        be a multipath map name.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bomaqc6">
      <term>multipath -ll</term>
      <listitem>
       <para>
        Display the status of all multipath devices.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bomasgg">
      <term>multipath -ll <replaceable>devicename</replaceable>
      </term>
      <listitem>
       <para>
        Displays the status of a specified multipath device.
       </para>
       <para>
        Replace <replaceable>devicename</replaceable> with the device node
        name such as <filename>/dev/sdb</filename> (as shown by
        <command>udev</command> in the $DEVNAME variable), or in the
        <literal>major:minor</literal> format. The device may alternatively
        be a multipath map name.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bomaqc7">
      <term>multipath -F</term>
      <listitem>
       <para>
        Flushes all unused multipath device maps. This unresolves the
        multiple paths; it does not delete the devices.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bomasgh">
      <term>multipath -F <replaceable>devicename</replaceable>
      </term>
      <listitem>
       <para>
        Flushes unused multipath device maps for a specified multipath
        device. This unresolves the multiple paths; it does not delete the
        device.
       </para>
       <para>
        Replace <replaceable>devicename</replaceable> with the device node
        name such as <filename>/dev/sdb</filename> (as shown by
        <command>udev</command> in the $DEVNAME variable), or in the
        <literal>major:minor</literal> format. The device may alternatively
        be a multipath map name.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bomasgi">
      <term>multipath -p [ failover | multibus | group_by_serial | group_by_prio | group_by_node_name ]</term>
      <listitem>
       <para>
        Sets the group policy by specifying one of the group policy options
        that are described in
        <xref linkend="beg177b" xrefstyle="TableXRef"/>:
       </para>
       <table id="beg177b" frame="topbot" rowsep="1" pgwide="0">
        <title>Group Policy Options for the multipath -p Command</title>
        <tgroup cols="2">
         <colspec colnum="1" colname="1" colwidth="2684*"/>
         <colspec colnum="2" colname="2" colwidth="7319*"/>
         <thead>
          <row id="beep4mu">
           <entry>
            <para>
             Policy Option
            </para>
           </entry>
           <entry>
            <para>
             Description
            </para>
           </entry>
          </row>
         </thead>
         <tbody>
          <row id="beep4mv">
           <entry>
            <para>
             failover
            </para>
           </entry>
           <entry>
            <para>
             (Default) One path per priority group. You can use only one
             path at a time.
            </para>
           </entry>
          </row>
          <row id="beep4mw">
           <entry>
            <para>
             multibus
            </para>
           </entry>
           <entry>
            <para>
             All paths in one priority group.
            </para>
           </entry>
          </row>
          <row id="beep4mx">
           <entry>
            <para>
             group_by_serial
            </para>
           </entry>
           <entry>
            <para>
             One priority group per detected SCSI serial number (the
             controller node worldwide number).
            </para>
           </entry>
          </row>
          <row id="beep4my">
           <entry>
            <para>
             group_by_prio
            </para>
           </entry>
           <entry>
            <para>
             One priority group per path priority value. Paths with the same
             priority are in the same priority group. Priorities are
             determined by callout programs specified as a global,
             per-controller, or per-multipath option in the
             <filename>/etc/multipath.conf</filename> configuration file.
            </para>
           </entry>
          </row>
          <row id="beep4mz">
           <entry>
            <para>
             group_by_node_name
            </para>
           </entry>
           <entry>
            <para>
             One priority group per target node name. Target node names are
             fetched in the<filename>
             /sys/class/fc_transport/target*/node_name</filename> location.
            </para>
           </entry>
          </row>
         </tbody>
        </tgroup>
       </table>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect2>

   <sect2 id="b15jw320">
    <title>Linux mpathpersist(8) Utility</title>
    <para>
     The <command>mpathpersist(8)</command> utility can be used to manage
     SCSI persistent reservations on Device Mapper Multipath devices.
    </para>
    <para>
     General syntax for the <command>mpathpersist(8)</command> command:
    </para>
<screen>
mpathpersist [options] [device]
</screen>
    <para>
     Use this utility with the service action reservation key
     (<literal>reservation_key</literal> attribute) in the
     <filename>/etc/multipath.conf</filename> file to set persistent
     reservations for SCSI devices. The attribute is not used by default. If
     it is not set, the <command>multipathd</command> daemon does not check
     for persistent reservation for newly discovered paths or reinstated
     paths.
    </para>
<screen>
reservation_key &lt;<replaceable>reservation key</replaceable>&gt;
</screen>
    <para>
     You can add the attribute to the <literal>defaults</literal> section or
     the <literal>multipaths</literal> section. For example:
    </para>
<screen>
multipaths {
        multipath {
                          wwid   XXXXXXXXXXXXXXXX
                         alias      yellow
                         reservation_key  0x123abc
      }
}
</screen>
    <para>
     Set the <literal>reservation_key</literal> parameter for all mpath
     devices applicable for persistent management, then restart the
     <command>multipathd</command> daemon. After it is set up, you can
     specify the reservation key in the mpathpersist commands.
    </para>
    <bridgehead id="b15jw321">Options</bridgehead>
    <variablelist>
     <varlistentry id="b15jwii5">
      <term>-h</term>
      <term>--help</term>
      <listitem>
       <para>
        Outputs the command usage information, then exits.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwii6">
      <term>-d &lt;<replaceable>device</replaceable>&gt;</term>
      <term>--device=&lt;<replaceable>device</replaceable>&gt;</term>
      <listitem>
       <para>
        Query or change the device.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwii7">
      <term>-H</term>
      <term>--hex</term>
      <listitem>
       <para>
        Display the output response in hexadecimal.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwii8">
      <term>-X &lt;tids&gt;</term>
      <term>--transportID=&lt;tids&gt;</term>
      <listitem>
       <para>
        Transport IDs can be mentioned in several forms.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jw322">
      <term>-v &lt;<replaceable>level</replaceable>&gt;</term>
      <term>--verbose &lt;<replaceable>level</replaceable>&gt;</term>
      <listitem>
       <para>
        Specifies the verbosity level.
       </para>
       <variablelist>
        <varlistentry id="b15jw323">
         <term>0</term>
         <listitem>
          <para>
           Critical and error messages.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry id="b15jw324">
         <term>1</term>
         <listitem>
          <para>
           Warning messages.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry id="b15jw325">
         <term>2</term>
         <listitem>
          <para>
           Informational messages.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry id="b15jw326">
         <term>3</term>
         <listitem>
          <para>
           Informational messages with trace enabled.
          </para>
         </listitem>
        </varlistentry>
       </variablelist>
      </listitem>
     </varlistentry>
    </variablelist>
    <bridgehead id="b15jwii9">PR In Options</bridgehead>
    <variablelist>
     <varlistentry id="b15jwiie">
      <term>-i</term>
      <term>--in</term>
      <listitem>
       <para>
        Request PR In command.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwiih">
      <term>-k</term>
      <term>--read-keys</term>
      <listitem>
       <para>
        PR In: Read Keys.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwiim">
      <term>-s</term>
      <term>--read-status</term>
      <listitem>
       <para>
        PR In: Read Full Status.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwiin">
      <term>-r</term>
      <term>--read-reservation</term>
      <listitem>
       <para>
        PR In: Read Reservation.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwiir">
      <term>-c</term>
      <term>--report-capabilities</term>
      <listitem>
       <para>
        PR In: Report Capabilities.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <bridgehead id="b15jwiiz">PR Out Options</bridgehead>
    <variablelist>
     <varlistentry id="b15jwij0">
      <term>-o</term>
      <term>--out</term>
      <listitem>
       <para>
        Request PR Out command.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwij1">
      <term>-c</term>
      <term>--clear</term>
      <listitem>
       <para>
        PR Out: Clear
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwij2">
      <term>-Z</term>
      <term>--param-aptpl</term>
      <listitem>
       <para>
        PR Out parameter <literal>APTPL</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwij3">
      <term>-S SARK</term>
      <term>--param-sark=SARK</term>
      <listitem>
       <para>
        PR Out parameter Service Action Reservation Key (SARK) in
        hexadecimal.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwij4">
      <term>-P</term>
      <term>--preempt</term>
      <listitem>
       <para>
        PR Out: Preempt.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwij5">
      <term>-A</term>
      <term>--preempt-abort</term>
      <listitem>
       <para>
        PR Out: Preempt and Abort.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwij6">
      <term>-T &lt;<replaceable>type</replaceable>&gt;</term>
      <term>--prout-type=&lt;<replaceable>type</replaceable>&gt;</term>
      <listitem>
       <para>
        PR Out command type.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwij7">
      <term>-G</term>
      <term>--register</term>
      <listitem>
       <para>
        PR Out: Register.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwij8">
      <term>-I</term>
      <term>--register-ignore</term>
      <listitem>
       <para>
        PR Out: Register and Ignore.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwij9">
      <term>-L</term>
      <term>--release</term>
      <listitem>
       <para>
        PR Out: Release
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b15jwija">
      <term>-R</term>
      <term>--reserve</term>
      <listitem>
       <para>
        PR Out: Reserve.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <bridgehead id="b15jwmcx">Examples</bridgehead>
    <para>
     Register the Service Action Reservation Key for the
     <filename>/dev/mapper/mpath9</filename> device.
    </para>
<screen>
mpathpersist --out --register --param-sark=123abc --prout-type=5 -d /dev/mapper/mpath9
</screen>
    <para>
     Read the Service Action Reservation Key for the
     <filename>/dev/mapper/mpath9</filename> device.
    </para>
<screen>
mpathpersisst -i -k -d /dev/mapper/mpath9
</screen>
    <para>
     Reserve the Service Action Reservation Key for the
     <filename>/dev/mapper/mpath9</filename> device.
    </para>
<screen>
mpathpersist --out --reserve --param-sark=123abc --prout-type=8 -d /dev/mapper/mpath9
</screen>
    <para>
     Read the reservation status of the
     <filename>/dev/mapper/mpath9</filename> device.
    </para>
<screen>
mpathpersist -i -s -d /dev/mapper/mpath9
</screen>
   </sect2>
  </sect1>
  <sect1 id="mpiosysconf">
   <title>Configuring the System for Multipathing</title>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="mpiosysconfsandevs" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="mpiosysconfpart" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="mpiosysconfsvr" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="mpioboot" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="mpiosysconfsandevs">
    <title>Preparing SAN Devices for Multipathing</title>
    <para>
     Before configuring multipath I/O for your SAN devices, prepare the SAN
     devices, as necessary, by doing the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Configure and zone the SAN with the vendor’s tools.
      </para>
     </listitem>
     <listitem>
      <para>
       Configure permissions for host LUNs on the storage arrays with the
       vendor’s tools.
      </para>
     </listitem>
     <listitem>
      <para>
       Install the Linux HBA driver module. Upon module installation, the
       driver automatically scans the HBA to discover any SAN devices that
       have permissions for the host. It presents them to the host for
       further configuration.
      </para>
      <note>
       <para>
        Ensure that the HBA driver you are using does not have native
        multipathing enabled.
       </para>
      </note>
      <para>
       See the vendor’s specific instructions for more details.
      </para>
     </listitem>
     <listitem>
      <para>
       After the driver module is loaded, discover the device nodes assigned
       to specific array LUNs or partitions.
      </para>
     </listitem>
     <listitem>
      <para>
       If the SAN device will be used as the root device on the server,
       modify the timeout settings for the device as described in
       <xref linkend="bok8cn1" xrefstyle="SectTitleOnPage"/>.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     If the LUNs are not seen by the HBA driver, <command>lsscsi
     </command>can be used to check whether the SCSI devices are seen
     correctly by the operating system. When the LUNs are not seen by the
     HBA driver, check the zoning setup of the SAN. In particular, check
     whether LUN masking is active and whether the LUNs are correctly
     assigned to the server.
    </para>
    <para>
     If the LUNs are seen by the HBA driver, but there are no corresponding
     block devices, additional kernel parameters are needed to change the
     SCSI device scanning behavior, such as to indicate that LUNs are not
     numbered consecutively. For information, see
     <ulink url="http://www.suse.com/support/kb/"><citetitle>TID 3955167:
     Troubleshooting SCSI (LUN) Scanning Issues</citetitle> in the &suse;
     Knowledgebase</ulink>.
    </para>
   </sect2>

   <sect2 id="mpiosysconfpart">
    <title>Partitioning Multipath Devices</title>
    <para>
     Partitioning devices that have multiple paths is not recommended, but
     it is supported. You can use the <command>kpartx</command> tool to
     create partitions on multipath devices without rebooting. You can also
     partition the device before you attempt to configure multipathing by
     using the Partitioner function in YaST, or by using a third-party
     partitioning tool.
    </para>
    <para>
     Multipath devices are device-mapper devices. Modifying device-mapper
     devices with command line tools (such as parted, kpartx, or fdisk)
     works, but it does not necessarily generate the udev events that are
     required to update other layers. After you partition the device-mapper
     device, you should check the multipath map to make sure the
     device-mapper devices were mapped. If they are missing, you can remap
     the multipath devices or reboot the server to pick up all of the new
     partitions in the multipath map.
    </para>
    <para>
     The device-mapper device for a partition on a multipath device is not
     the same as an independent device. When you create an LVM logical
     volume using the whole device, you must specify a device that contains
     no partitions. If you specify a multipath partition as the target
     device for the LVM logical volume, LVM recognizes that the underlying
     physical device is partitioned and the create fails. If you need to
     subdivide a SAN device, you can carve LUNs on the SAN device and
     present each LUN as a separate multipath device to the server.
    </para>
   </sect2>

   <sect2 id="mpiosysconfsvr">
    <title>Configuring the Device Drivers in initrd for Multipathing</title>
    <para>
     The server must be manually configured to automatically load the device
     drivers for the controllers to which the multipath I/O devices are
     connected within the <literal>initrd</literal>. You need to add the
     necessary driver module to the variable <envar>force_drivers</envar>
     in the file <filename>/etc/dracut.conf.d/01-dist.conf</filename>.
    </para>
    <para>
     For example, if your system contains a RAID controller accessed by the
     <filename>cciss</filename> driver and multipathed devices connected to
     a QLogic controller accessed by the driver qla23xx, this entry would
     look like:
    </para>
<screen>force_drivers+="cciss qla23xx"</screen>
    <para>
     After changing <filename>/etc/dracut.conf.d/01-dist.conf</filename>, you
     must re-create the <literal>initrd</literal> on your system with the
     <command>dracut <option>-f</option></command> command, then reboot in
     order for the changes to take effect.
    </para>
    <para>
     In SUSE Linux Enterprise Server 11 SP3 and later, four SCSI hardware
     handlers were added in the SCSI layer that can be used with
     DM-Multipath:
    </para>
    <simplelist>
     <member><filename>scsi_dh_alua</filename>
     </member>
     <member><filename>scsi_dh_rdac</filename>
     </member>
     <member><literal>scsi_dh_hp_sw</literal>
     </member>
     <member><literal>scsi_dh_emc</literal>
     </member>
    </simplelist>
    <para>
     Add the modules to the <filename>initrd</filename> image, then specify
     them in the <filename>/etc/multipath.conf</filename> file as hardware
     handler types <literal>alua</literal>, <literal>rdac</literal>,
     <literal>hp_sw</literal>, and <literal>emc</literal>. For example, add
     one of these lines for a device definition:
    </para>
<screen>
hardware_handler "1 alua"

hardware_handler "1 rdac"

hardware_handler "1 hp_sw"

hardware_handler "1 emc"
</screen>
    <para>
     To include the modules in the <filename>initrd</filename> image:
    </para>
    <procedure id="b15dscl4">
     <step id="b15dscl5">
      <para>
       Add the device handler modules to the <envar>force_drivers</envar>
       variable in <filename>/etc/dracut.conf.d/01-dist.conf</filename>:
      </para>
      <screen>force_drivers+="alua rdac hp_sw emc"</screen>
     </step>
     <step id="b15dscl6">
      <para>
       Create a new <filename>initrd</filename>:
      </para>
      <screen>dracut /boot/initrd-&lt;<replaceable>flavour</replaceable>&gt;-scsi-dh \
<replaceable>KERNELVERSION</replaceable></screen>
     </step>
     <step id="b15dscl7">
      <para>
      <remark condition="clarity">
       2014-09-05 - fs: FIXME: This needs to be adjusted to GRUB2
      </remark>
       Update the boot configuration file
       (<filename>/etc/default/grub</filename>,
       <filename>/boot/efi/SuSE/elilo.conf</filename>) with the newly built
       <filename>initrd</filename>.
      </para>
     </step>
     <step id="b15dscl8">
      <para>
       Restart the server.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="mpioboot">
    <title>Adding multipathd to the Boot Sequence</title>
    <para>
     Use either of the methods in this section to add multipath I/O services
     (<command>multipathd</command>) to the boot sequence.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="b5c8qi6" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b5c8qtj" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="b5c8qi6">
     <title>Using YaST to Add multipathd</title>
     <procedure id="b5997ck">
      <step id="b5997cl">
       <para>
        In YaST, click <guimenu>System</guimenu> &gt;
        <guimenu>&ycc_runlevel;</guimenu>.
       </para>
      </step>
      <step id="b5997cm">
       <para>
        Select <guimenu>multipathd</guimenu>. If the service is listed as
        <literal>inactive</literal>, click <guimenu>Enable/Disable</guimenu>
        to change this.
       </para>
      </step>
      <step id="b5997cn">
       <para>
        Confirm your changes.
       </para>
       <para>
        The changes do not take affect until the server is restarted.
       </para>
      </step>
     </procedure>
    </sect3>
    <sect3 id="b5c8qtj">
     <title>Using the Command Line to Add multipathd</title>
     <procedure id="b5c8qz1">
      <step id="b5c8qz2">
       <para>
        Open a terminal console, then log in as the
        <systemitem>root</systemitem> user or equivalent.
       </para>
      </step>
      <step id="b5c8r4x">
       <para>
        At the terminal console prompt, enter
       </para>
<screen>
systemctl enable multipathd.service
</screen>
      </step>
     </procedure>
    </sect3>
   </sect2>
  </sect1>
  <sect1 id="mpiostart">
   <title>Enabling and Starting Multipath I/O Services</title>

   <para>
    To start multipath services and enable them to start at reboot:
   </para>

   <procedure id="b7jqkxe">
    <step id="b7jqkxg">
     <para>
      Open a terminal console, then log in as the
      <systemitem>root</systemitem> user or equivalent.
     </para>
    </step>
    <step id="b7jqkxh">
     <para>
      At the terminal console prompt, enter
     </para>
<screen>systemctl enable multipathd.service multipath.service  </screen>
<!--taroth 2014-03-19: systemd - according to fcrozat, no more boot.* stuff:
  <screen>
chkconfig multipathd on
</screen>
<screen>
chkconfig boot.multipath on
</screen>-->
    </step>
   </procedure>

   <para>
    If the multipath services does not start automatically on system boot,
    do the following to start them manually:
   </para>

   <procedure id="b5c8s8w">
    <step id="b5c8s8x">
     <para>
      Open a terminal console, then log in as the
      <systemitem>root</systemitem> user or equivalent.
     </para>
    </step>
    <step id="b5c8sla">
     <para>
      Enter
     </para>
<screen>systemctl start multipath.service multipathd.service </screen>
<!--taroth 2014-03-19: systemd - according to fcrozat, no more boot.* stuff:
 <screen>
/etc/init.d/boot.multipath start
</screen>
<screen>
/etc/init.d/multipathd start
</screen>-->
    </step>
   </procedure>
  </sect1>
  <sect1 id="bbillhs">
   <title>Creating or Modifying the /etc/multipath.conf File</title>

   <para>
    The <filename>/etc/multipath.conf</filename> file does not exist unless
    you create it. Default multipath device settings are applied
    automatically when the <command>multipathd</command> daemon runs unless
    you create the multipath configuration file and personalize the
    settings. The
    <filename>/usr/share/doc/packages/multipath-tools/multipath.conf.synthetic</filename>
    file contains a sample <filename>/etc/multipath.conf</filename> file
    that you can use as a guide for multipath settings.
   </para>

   <para>
    Whenever you create or modify the
    <filename>/etc/multipath.conf</filename> file, the changes are not
    automatically applied when you save the file. This allows you time to
    perform a dry run to verify your changes before they are committed. When
    you are satisfied with the revised settings, you can update the
    multipath maps for the running multipathd daemon to use, or the changes
    will be applied the next time that the multipathd daemon is restarted,
    such as on a system restart.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="beenynn" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b122wjjt" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="mpiohwsupconf" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bbj8y2r" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="beenynn">
    <title>Creating the /etc/multipath.conf File</title>
    <para>
     If the <filename>/etc/multipath.conf</filename> file does not exist,
     copy the example to create the file:
    </para>
    <procedure id="bbillht">
     <step id="bbillhu">
      <para>
       In a terminal console, log in as the <systemitem>root</systemitem>
       user.
      </para>
     </step>
     <step id="bbillhv">
      <para>
       Enter the following command (all on one line, of course) to copy the
       template:
      </para>
<screen>
cp /usr/share/doc/packages/multipath-tools/multipath.conf.synthetic /etc/multipath.conf
</screen>
     </step>
     <step id="bcmqvq2">
      <para>
       Use the
       <filename>/usr/share/doc/packages/multipath-tools/multipath.conf.annotated</filename>
       file as a reference to determine how to configure multipathing for
       your system.
      </para>
     </step>
     <step id="bcmqx5z">
      <para>
       Ensure that there is an appropriate <command>device</command> entry
       for your SAN. Most vendors provide documentation on the proper setup
       of the <command>device</command> section.
      </para>
      <para>
       The <filename>/etc/multipath.conf</filename> file requires a
       different <command>device</command> section for different SANs. If
       you are using a storage subsystem that is automatically detected (see
       <xref linkend="mpiosuphw" xrefstyle="HeadingOnPage"/>), the default
       entry for that device can be used; no further configuration of the
       <filename>/etc/multipath.conf</filename> file is required.
      </para>
     </step>
     <step id="bcmqweh">
      <para>
       Save the file.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="b122wjjt">
    <title>Sections in the /etc/multipath.conf File</title>
    <para>
     The <filename>/etc/multipath.conf</filename> file is organized in the
     following sections. See
     <filename>/usr/share/doc/packages/multipath-tools/multipath.conf.annotated</filename>
     for a template with extensive comments for each of the attributes and
     their options.
    </para>
    <variablelist>
     <varlistentry id="b122vffx">
      <term>defaults</term>
      <listitem>
       <para>
        General default settings for multipath I/0. These values are used if
        no values are given in the appropriate device or multipath sections.
        For information, see
        <xref linkend="bbj68de" xrefstyle="SectTitleOnPage"/>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b12donk3">
      <term>blacklist</term>
      <listitem>
       <para>
        Lists the device names to discard as not multipath candidates.
        Devices can be identified by their device node name
        (<literal>devnode</literal>), their WWID (<literal>wwid</literal>),
        or their vendor or product strings (<literal>device</literal>). For
        information, see
        <xref linkend="bbj5x7z" xrefstyle="SectTitleOnPage"/>.
       </para>
       <para>
        You typically ignore non-multipathed devices, such as cciss, fd, hd,
        md, dm, sr, scd, st, ram, raw, loop.
       </para>
       <formalpara id="b122sbge" role="intro">
        <title>Values</title>
        <para/>
       </formalpara>
       <para>
        For an example, see
        <xref linkend="bbj5x7z" xrefstyle="HeadingOnPage"/>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b12doufy">
      <term>blacklist_exceptions</term>
      <listitem>
       <para>
        Lists the device names of devices to be treated as multipath
        candidates even if they are on the blacklist. Devices can be
        identified by their device node name (<literal>devnode</literal>),
        their WWID (<literal>wwid</literal>), or their vendor or product
        strings (<literal>device</literal>). You must specify the excepted
        devices by using the same keyword that you used in the blacklist.
        For example, if you used the devnode keyword for devices in the
        blacklist, you use the devnode keyword to exclude some of the
        devices in the blacklist exceptions. It is not possible to blacklist
        devices by using the <literal>devnode</literal> keyword and to
        exclude some devices of them by using the <literal>wwid</literal>
        keyword.
       </para>
       <formalpara id="b122sbgg" role="intro">
        <title>Values</title>
        <para/>
       </formalpara>
       <para>
        For examples, see
        <xref linkend="bbj5x7z" xrefstyle="HeadingOnPage"/> and the
        <filename>/usr/share/doc/packages/multipath-tools/multipath.conf.annotated</filename>
        file.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b122vffy">
      <term>multipaths</term>
      <listitem>
       <para>
        Specifies settings for individual multipath devices. Except for
        settings that do not support individual settings, these values
        overwrite what is specified in the <literal>defaults</literal> and
        <literal>devices</literal> sections of the configuration file.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b122vh2i">
      <term>devices</term>
      <listitem>
       <para>
        Specifies settings for individual storage controllers. These values
        overwrite values specified in the <filename>defaults</filename>
        section of the configuration file. If you use a storage array that
        is not supported by default, you can create a
        <literal>devices</literal> subsection to specify the default
        settings for it. These values can be overwritten by settings for
        individual multipath devices if the keyword allows it.
       </para>
       <para>
        For information, see the following:
       </para>
       <itemizedlist>
        <listitem>
         <para>
          <xref linkend="mpionames" xrefstyle="SectTitleOnPage"/>
         </para>
        </listitem>
        <listitem>
         <para>
          <xref linkend="bkj8n9w" xrefstyle="SectTitleOnPage"/>
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect2>

   <sect2 id="mpiohwsupconf">
    <title>Verifying the Multipath Setup in the /etc/multipath.conf File</title>
    <para>
     Whenever you create or modify the
     <filename>/etc/multipath.conf</filename> file, the changes are not
     automatically applied when you save the file. You can perform a
     <quote>dry run</quote> of the setup to verify the multipath setup
     before you update the multipath maps.
    </para>
    <para>
     At the server command prompt, enter
    </para>
<screen>
<command>multipath -v2 -d</command>
</screen>
    <para>
     This command scans the devices, then displays what the setup would look
     like if you commit the changes. It is assumed that the
     <filename>multipathd</filename> daemon is already running with the old
     (or default) multipath settings when you modify the
     <filename>/etc/multipath.conf</filename> file and perform the dry run.
     If the changes are acceptable, continue with
     <xref linkend="bbj8y2r" xrefstyle="HeadingOnPage"/>.
    </para>
    <para>
     The output is similar to the following:
    </para>
<screen>
26353900f02796769
[size=127 GB]
[features="0"]
[hwhandler="1    emc"] 
</screen>
<screen>
\_ round-robin 0 [first]
  \_ 1:0:1:2 sdav 66:240  [ready ]
  \_ 0:0:1:2 sdr  65:16   [ready ]
</screen>
<screen>
\_ round-robin 0 
  \_ 1:0:0:2 sdag 66:0    [ready ]
  \_ 0:0:0:2 sdc   8:32   [ready ] 
</screen>
    <para>
     Paths are grouped into priority groups. Only one priority group is in
     active use at a time. To model an active/active configuration, all
     paths end in the same group. To model active/passive configuration, the
     paths that should not be active in parallel are placed in several
     distinct priority groups. This normally happens automatically on device
     discovery.
    </para>
    <para>
     The output shows the order, the scheduling policy used to balance I/O
     within the group, and the paths for each priority group. For each path,
     its physical address (host:bus:target:lun), device node name,
     major:minor number, and state is shown.
    </para>
    <para>
     By using a verbosity level of -v3 in the dry run, you can see all
     detected paths, multipaths, and device maps. Both WWID and device node
     blacklisted devices are displayed.
    </para>
<screen>
multipath -v3 d
</screen>
    <para role="intro">
     The following is an example of -v3 output on a 64-bit SLES 11 SP2
     server with two Qlogic HBA connected to a Xiotech Magnitude 3000 SAN.
     Some multiple entries have been omitted to shorten the example.
    </para>
<screen>
dm-22: device node name blacklisted
&lt; content omitted &gt;
loop7: device node name blacklisted
&lt; content omitted &gt;
md0: device node name blacklisted
&lt; content omitted &gt;
dm-0: device node name blacklisted
sdf: not found in pathvec
sdf: mask = 0x1f
sdf: dev_t = 8:80
sdf: size = 105005056
sdf: subsystem = scsi
sdf: vendor = XIOtech
sdf: product = Magnitude 3D
sdf: rev = 3.00
sdf: h:b:t:l = 1:0:0:2
sdf: tgt_node_name = 0x202100d0b2028da
sdf: serial = 000028DA0014
sdf: getuid= "/lib/udev/scsi_id --whitelisted --device=/dev/%n" (config file default)
sdf: uid = 200d0b2da28001400 (callout)
sdf: prio = const (config file default)
sdf: const prio = 1
&lt; content omitted &gt;
ram15: device node name blacklisted
&lt; content omitted &gt;
===== paths list =====
uuid              hcil    dev dev_t pri dm_st  chk_st  vend/prod/rev
200d0b2da28001400 1:0:0:2 sdf 8:80  1   [undef][undef] XIOtech,Magnitude 3D
200d0b2da28005400 1:0:0:1 sde 8:64  1   [undef][undef] XIOtech,Magnitude 3D
200d0b2da28004d00 1:0:0:0 sdd 8:48  1   [undef][undef] XIOtech,Magnitude 3D
200d0b2da28001400 0:0:0:2 sdc 8:32  1   [undef][undef] XIOtech,Magnitude 3D
200d0b2da28005400 0:0:0:1 sdb 8:16  1   [undef][undef] XIOtech,Magnitude 3D
200d0b2da28004d00 0:0:0:0 sda 8:0   1   [undef][undef] XIOtech,Magnitude 3D
params = 0 0 2 1 round-robin 0 1 1 8:80 1000 round-robin 0 1 1 8:32 1000
status = 2 0 0 0 2 1 A 0 1 0 8:80 A 0 E 0 1 0 8:32 A 0
sdf: mask = 0x4
sdf: path checker = directio (config file default)
directio: starting new request
directio: async io getevents returns 1 (errno=Success)
directio: io finished 4096/0
sdf: state = 2
&lt; content omitted &gt;
</screen>
   </sect2>

   <sect2 id="bbj8y2r">
    <title>Applying the /etc/multipath.conf File Changes to Update the Multipath Maps</title>
    <para>
     Changes to the <filename>/etc/multipath.conf</filename> file cannot
     take effect when <command>multipathd</command> is running. After you
     make changes, save and close the file, then do the following to apply
     the changes and update the multipath maps:
    </para>
    <procedure id="bbj8ync">
     <step id="bbj8ynd">
      <para>
       Stop the <command>multipathd</command> service.
      </para>
     </step>
     <step id="bbj8zk2">
      <para>
       Clear old multipath bindings by entering
      </para>
<screen>
/sbin/multipath -F
</screen>
     </step>
     <step id="bbj91hd">
      <para>
       Create new multipath bindings by entering
      </para>
<screen>
/sbin/multipath -v2 -l
</screen>
     </step>
     <step id="bbj8zu9">
      <para>
       Start the <command>multipathd</command> service.
      </para>
     </step>
     <step id="bi0ay0m">
      <para>
       Run <command>dracut -f</command> to re-create the
       <filename>initrd</filename> image on your system, then reboot in
       order for the changes to take effect.
      </para>
     </step>
    </procedure>
   </sect2>
  </sect1>
  <sect1 id="bbj68de">
   <title>Configuring Default Policies for Polling, Queueing, and Failback</title>

   <para>
    The goal of multipath I/O is to provide connectivity fault tolerance
    between the storage system and the server. The desired default behavior
    depends on whether the server is a standalone server or a node in a
    high-availability cluster.
   </para>

   <para>
    When you configure multipath I/O for a stand-alone server, the
    <literal>no_path_retry</literal> setting protects the server operating
    system from receiving I/O errors as long as possible. It queues messages
    until a multipath failover occurs and provides a healthy connection.
   </para>

   <para>
    When you configure multipath I/O for a node in a high-availability
    cluster, you want multipath to report the I/O failure in order to
    trigger the resource failover instead of waiting for a multipath
    failover to be resolved. In cluster environments, you must modify the
    <literal>no_path_retry </literal>setting so that the cluster node
    receives an I/O error in relation to the cluster verification process
    (recommended to be 50% of the heartbeat tolerance) if the connection is
    lost to the storage system. In addition, you want the multipath I/O fail
    back to be set to manual in order to avoid a ping-pong of resources
    because of path failures.
   </para>

   <para>
    The <filename>/etc/multipath.conf</filename> file should contain a
    <command>defaults</command> section where you can specify default
    behaviors for polling, queueing, and failback. If the field is not
    otherwise specified in a <command>device</command> section, the default
    setting is applied for that SAN configuration.
   </para>

   <para role="intro">
    The following are the compiled in default settings. They will be used
    unless you overwrite these values by creating and configuring a
    personalized <filename>/etc/multipath.conf</filename> file.
   </para>

<screen>
defaults {
  verbosity 2
#  udev_dir is deprecated in SLES 11 SP3
#  udev_dir              /dev
  polling_interval      5
#  path_selector default value is service-time in SLES 11 SP3
#  path_selector         "round-robin 0"
  path selector         "service-time 0"
  path_grouping_policy  failover
#  getuid_callout is deprecated in SLES 11 SP3 and replaced with uid_attribute
#  getuid_callout        "/lib/udev/scsi_id --whitelisted --device=/dev/%n"
#  uid_attribute is new in SLES 11 SP3
  uid_attribute         "ID_SERIAL"
  prio                  "const"
  prio_args             ""
  features              "0"
  path_checker          "directio"
  alias_prefix          "mpath"
  rr_min_io_rq          1
  max_fds               "max"
  rr_weight             "uniform"
  queue_without_daemon  "yes"
  flush_on_last_del     "no"
  user_friendly_names   "no"
  fast_io_fail_tmo      5
  bindings_file         "/etc/multipath/bindings"
  wwids_file            "/etc/multipath/wwids"
  log_checker_err       "always"
  retain_attached_hw_handler  "no"
  detect_prio           "no"
  failback              "manual"
  no_path_retry         "fail"
  }
</screen>

   <para>
    For information about setting the polling, queuing, and failback
    policies, see the following parameters in
    <xref linkend="bbi89rh" xrefstyle="SectTitleOnPage"/>:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      <xref linkend="b122w0sa" xrefstyle="HeadingOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b122sbgo" xrefstyle="HeadingOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b122sbgh" xrefstyle="HeadingOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <para>
    If you modify the settings in the <literal>defaults</literal> section,
    the changes are not applied until you update the multipath maps, or
    until the multipathd daemon is restarted, such as at system restart.
   </para>
  </sect1>
  <sect1 id="bbj5x7z">
   <title>Blacklisting Non-Multipath Devices</title>

   <para>
    The <filename>/etc/multipath.conf</filename> file should contain a
    <command>blacklist</command> section where all non-multipath devices are
    listed. You can blacklist devices by WWID (<literal>wwid</literal>
    keyword), device name (<literal>devnode</literal> keyword), or device
    type (<literal>device</literal> section). You can also use the
    <literal>blacklist_exceptions</literal> section to enable multipath for
    some devices that are blacklisted by the regular expressions used in the
    <literal>blacklist</literal> section.
   </para>

   <para>
    You typically ignore non-multipathed devices, such as cciss, fd, hd, md,
    dm, sr, scd, st, ram, raw, and loop. For example, local IDE hard drives
    and USB drives do not normally have multiple paths. If you want
    <command>multipath</command> to ignore single-path devices, put them in
    the <command>blacklist</command> section.
   </para>

   <note>
    <title>Compatibility</title>
    <para>
     The keyword <literal>devnode_blacklist</literal> has been deprecated
     and replaced with the keyword <literal>blacklist</literal>.
    </para>
    <para>
     With &sls; 12 the glibc-provided regular expressions are used. To match
     an arbitrary string, you must now use <literal>".*"</literal> rather
     than just <literal>"*"</literal>.
    </para>
   </note>

   <para>
    For example, to blacklist local devices and all arrays from the
    <filename>cciss</filename> driver from being managed by multipath, the
    <command>blacklist</command> section looks like this:
   </para>

<screen>
blacklist {
      wwid "26353900f02796769"
      devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st|sda)[0-9]*"
      devnode "^hd[a-z][0-9]*"
      devnode "^cciss!c[0-9]d[0-9].*"
}
</screen>

   <para>
    You can also blacklist only the partitions from a driver instead of the
    entire array. For example, you can use the following regular expression
    to blacklist only partitions from the cciss driver and not the entire
    array:
   </para>

<screen>
blacklist {
      devnode "^cciss!c[0-9]d[0-9]*[p[0-9]*]"
}
</screen>

   <para>
    You can blacklist by specific device types by adding a
    <literal>device</literal> section in the blacklist, and using the
    <literal>vendor</literal> and <literal>product</literal> keywords.
   </para>

<screen>
blacklist {
      device {
           vendor  "DELL"
           product ".*"
       }
}
</screen>

   <para>
    You can use a <literal>blacklist_exceptions</literal> section to enable
    multipath for some devices that were blacklisted by the regular
    expressions used in the <literal>blacklist</literal> section. You add
    exceptions by WWID (<literal>wwid</literal> keyword), device name
    (<literal>devnode</literal> keyword), or device type
    (<literal>device</literal> section). You must specify the exceptions in
    the same way that you blacklisted the corresponding devices. That is,
    <literal>wwid</literal> exceptions apply to a <literal>wwid</literal>
    blacklist, <literal>devnode</literal> exceptions apply to a
    <literal>devnode</literal> blacklist, and device type exceptions apply
    to a device type blacklist.
   </para>

   <para>
    For example, you can enable multipath for a desired device type when you
    have different device types from the same vendor. Blacklist all of the
    vendor’s device types in the <literal>blacklist</literal> section, and
    then enable multipath for the desired device type by adding a
    <literal>device</literal> section in a
    <literal>blacklist_exceptions</literal> section.
   </para>

<screen>
blacklist {
      devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st|sda)[0-9]*"
      device {
           vendor  "DELL"
           product ".*"
       }
}

blacklist_exceptions {
      device {
           vendor  "DELL"
           product "MD3220i"
       }
}
</screen>

   <para>
    You can also use the blacklist_exceptions to enable multipath only for
    specific devices. For example:
   </para>

<screen>
blacklist {
      wwid ".*"
}

blacklist_exceptions {
        wwid "3600d0230000000000e13955cc3751234"
        wwid "3600d0230000000000e13955cc3751235"
}
</screen>

   <para>
    After you modify the <filename>/etc/multipath.conf</filename> file, you
    must run <command>dracut <option>-f</option></command> to re-create the
    <filename>initrd</filename> on your system, then restart the server in
    order for the changes to take effect.
   </para>

   <para>
    After you do this, the local devices should no longer be listed in the
    multipath maps when you issue the <command>multipath -ll</command>
    command.
   </para>
  </sect1>
  <sect1 id="mpionames">
   <title>Configuring User-Friendly Names or Alias Names</title>

   <para role="intro">
    A multipath device can be identified by its WWID, by a user-friendly
    name, or by an alias that you assign for it. Before you begin, review
    the requirements in
    <xref linkend="mpiousingdev" xrefstyle="SectTitleOnPage"/>.
   </para>

   <important>
    <para>
     Because device node names in the form of <filename>/dev/sdn</filename>
     and <filename>/dev/dm-n</filename> can change on reboot, referring to
     multipath devices by their WWID is preferred. You can also use a
     user-friendly name or alias that is mapped to the WWID in order to
     identify the device uniquely across reboots.
    </para>
   </important>

   <para>
    <xref linkend="bq7xokx" xrefstyle="TableXRef"/> describes the types of
    device names that can be used for a device in the
    <filename>/etc/multipath.conf</filename> file. For an example of
    <filename>multipath.conf</filename> settings, see the
    <filename>/usr/share/doc/packages/multipath-tools/multipath.conf.synthetic</filename>
    file.
   </para>

   <table id="bq7xokx" frame="topbot" rowsep="1" pgwide="0">
    <title>Comparison of Multipath Device Name Types</title>
    <tgroup cols="2">
     <colspec colnum="1" colname="1" colwidth="1667*"/>
     <colspec colnum="2" colname="2" colwidth="8334*"/>
     <thead>
      <row id="bq7xoky">
       <entry>
        <para>
         Name Types
        </para>
       </entry>
       <entry>
        <para>
         Description
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row id="bq7xokz">
       <entry>
        <para>
         WWID (default)
        </para>
       </entry>
       <entry>
        <para>
         The serial WWID (Worldwide Identifier) is an identifier for the
         multipath device that is guaranteed to be globally unique and
         unchanging. The default name used in multipathing is the ID of the
         logical unit as found in the <filename>/dev/disk/by-id</filename>
         directory. For example, a device with the WWID of
         <literal>3600508e0000000009e6baa6f609e7908</literal> is listed as
         <filename>/dev/disk/by-id/scsi-3600508e0000000009e6baa6f609e7908</filename>.
        </para>
       </entry>
      </row>
      <row id="bq7xol0">
       <entry>
        <para>
         User-friendly
        </para>
       </entry>
       <entry>
        <para>
         The Device Mapper Multipath device names in the
         <filename>/dev/mapper</filename> directory also reference the ID of
         the logical unit. These multipath device names are user-friendly
         names in the form of
         <filename>/dev/mapper/mpath&lt;<replaceable>n</replaceable>&gt;</filename>,
         such as <filename>/dev/mapper/mpath0</filename>. The names are
         unique and persistent because they use the
         <filename>/var/lib/multipath/bindings</filename> file to track the
         association between the UUID and user-friendly names.
        </para>
       </entry>
      </row>
      <row id="bq7xol1">
       <entry>
        <para>
         Alias
        </para>
       </entry>
       <entry>
        <para>
         An alias name is a globally unique name that the administrator
         provides for a multipath device. Alias names override the WWID and
         the user-friendly <filename>/dev/mapper/mpathN</filename> names.
        </para>
        <para>
         If you are using user_friendly_names, do not set the alias to
         mpathN format. This may conflict with an automatically assigned
         user friendly name, and give you incorrect device node names.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>

   <para>
    The global multipath <literal>user_friendly_names</literal> option in
    the <filename>/etc/multipath.conf</filename> file is used to enable or
    disable the use of user-friendly names for multipath devices. If it is
    set to “no” (the default), multipath uses the WWID as the name of
    the device. If it is set to “yes”, multipath uses the
    <filename>/var/lib/multipath/bindings</filename> file to assign a
    persistent and unique name to the device in the form of
    <filename>mpath&lt;<replaceable>n</replaceable>&gt;</filename> in the
    <filename>/dev/mapper </filename>directory. The <literal>bindings
    file</literal> option in the <literal>/etc/multipath.conf</literal> file
    can be used to specify an alternate location for the
    <filename>bindings</filename> file.
   </para>

   <para>
    The global multipath <literal>alias</literal> option in the
    <filename>/etc/multipath.conf</filename> file is used to explicitly
    assign a name to the device. If an alias name is set up for a multipath
    device, the alias is used instead of the WWID or the user-friendly name.
   </para>

   <para>
    Using the <literal>user_friendly_names</literal> option can be
    problematic in the following situations:
   </para>

   <itemizedlist>
    <listitem>
     <formalpara id="bq7wtcr" role="intro">
      <title>Root Device Is Using Multipath:</title>
      <para>
       If the system root device is using multipath and you use the
       <literal>user_friendly_names</literal> option, the user-friendly
       settings in the <filename>/var/lib/multipath/bindings</filename> file
       are included in the <filename>initrd</filename>. If you later change
       the storage setup, such as by adding or removing devices, there is a
       mismatch between the bindings setting inside the
       <filename>initrd</filename> and the bindings settings in
       <filename>/var/lib/multipath/bindings</filename>.
      </para>
     </formalpara>
     <warning>
      <para>
       A bindings mismatch between <filename>initrd</filename> and
       <filename>/var/lib/multipath/bindings</filename> can lead to a wrong
       assignment of mount points to devices, which can result in file
       system corruption and data loss.
      </para>
     </warning>
     <para>
      To avoid this problem, we recommend that you use the default WWID
      settings for the system root device. You should not use aliases for
      the system root device. Because the device name would differ, using an
      alias causes you to lose the ability to seamlessly switch off
      multipathing via the kernel command line.
     </para>
    </listitem>
    <listitem>
     <formalpara id="bq7uui0">
      <title>Mounting /var from Another Partition:</title>
      <para>
       The default location of the <literal>user_friendly_names</literal>
       configuration file is
       <filename>/var/lib/multipath/bindings</filename>. If the
       <filename>/var</filename> data is not located on the system root
       device but mounted from another partition, the
       <filename>bindings</filename> file is not available when setting up
       multipathing.
      </para>
     </formalpara>
     <para>
      Ensure that the <filename>/var/lib/multipath/bindings</filename> file
      is available on the system root device and multipath can find it. For
      example, this can be done as follows:
     </para>
     <orderedlist>
      <listitem>
       <para>
        Move the <filename>/var/lib/multipath/bindings</filename> file to
        <filename>/etc/multipath/bindings</filename>.
       </para>
      </listitem>
      <listitem>
       <para>
        Set the <literal>bindings_file</literal> option in the
        <literal>defaults</literal> section of
        /<filename>etc/multipath.conf</filename> to this new location. For
        example:
       </para>
<screen>
defaults {
               user_friendly_names yes
               bindings_file "/etc/multipath/bindings"
}
</screen>
      </listitem>
     </orderedlist>
    </listitem>
    <listitem>
     <formalpara id="bq7wpfj" role="intro">
      <title>Multipath Is in the initrd:</title>
      <para>
       Even if the system root device is not on multipath, it is possible
       for multipath to be included in the <filename>initrd</filename>. For
       example, this can happen of the system root device is on LVM. If you
       use the <literal>user_friendly_names</literal> option and multipath
       is in the <filename>initrd</filename>, you should boot with the
       parameter <command>multipath=off</command> to avoid problems.
      </para>
     </formalpara>
     <para>
      This disables multipath only in the <filename>initrd</filename> during
      system boots. After the system boots, the
      <filename>boot.multipath</filename> and
      <filename>multipathd</filename> boot scripts are able to activate
      multipathing.
     </para>
    </listitem>
   </itemizedlist>

   <para>
    To enable user-friendly names or to specify aliases:
   </para>

   <procedure id="b7jqnux">
    <step id="b7jqqgl">
     <para>
      In a terminal console, log in as the <systemitem>root</systemitem>
      user.
     </para>
    </step>
    <step id="b7jqp5o">
     <para>
      Open the <filename>/etc/multipath.conf</filename> file in a text
      editor.
     </para>
    </step>
    <step id="bq7y78q">
     <para role="intro">
      (Optional) Modify the location of the
      <filename>/var/lib/multipath/bindings</filename> file.
     </para>
     <para>
      The alternate path must be available on the system root device where
      multipath can find it.
     </para>
     <substeps>
      <step id="bq7y96x">
       <para>
        Move the <filename>/var/lib/multipath/bindings</filename> file to
        <filename>/etc/multipath/bindings</filename>.
       </para>
      </step>
      <step id="bq7y96y">
       <para>
        Set the <literal>bindings_file</literal> option in the
        <literal>defaults</literal> section of
        /<filename>etc/multipath.conf</filename> to this new location. For
        example:
       </para>
<screen>
defaults {
               user_friendly_names yes
               bindings_file "/etc/multipath/bindings"
}
</screen>
      </step>
     </substeps>
    </step>
    <step id="b7jqp5q">
     <para>
      (Optional, not recommended) Enable user-friendly names:
     </para>
     <substeps>
      <step id="bq7xauk">
       <para>
        Uncomment the <literal>defaults</literal> section and its ending
        bracket.
       </para>
      </step>
      <step id="bq7xb1y">
       <para>
        Uncomment the <literal>user_friendly_names option</literal>, then
        change its value from No to Yes.
       </para>
       <para>
        For example:
       </para>
<screen>
## Use user friendly names, instead of using WWIDs as names.
defaults {
  user_friendly_names yes
}
</screen>
      </step>
     </substeps>
    </step>
    <step id="b7jrp3u">
     <para role="intro">
      (Optional) Specify your own names for devices by using the
      <command>alias</command> option in the <command>multipath</command>
      section.
     </para>
     <para role="intro">
      For example:
     </para>
<screen>
## Use alias names, instead of using WWIDs as names.
multipaths {
       multipath {
               wwid           36006048000028350131253594d303030
               alias             blue1
       }
       multipath {
               wwid           36006048000028350131253594d303041
               alias             blue2
       }
       multipath {
               wwid           36006048000028350131253594d303145
               alias             yellow1
       }
       multipath {
               wwid           36006048000028350131253594d303334
               alias             yellow2
       }
}
</screen>
    </step>
    <step id="b7jqp5r">
     <para>
      Save your changes, then close the file.
     </para>
     <para>
      The changes are not applied until you update the multipath maps, or
      until the multipathd daemon is restarted, such as at system restart.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="bkj8n9w">
   <title>Configuring Default Settings for zSeries Devices</title>

   <para>
    Testing of the IBM zSeries device with multipathing has shown that the
    dev_loss_tmo parameter should be set to 90 seconds, and the
    fast_io_fail_tmo parameter should be set to 5 seconds. If you are using
    zSeries devices, modify the <filename>/etc/multipath.conf</filename>
    file to specify the values as follows:
   </para>

<screen>
defaults {
       dev_loss_tmo 90
       fast_io_fail_tmo 5
}
</screen>

   <para>
    The dev_loss_tmo parameter sets the number of seconds to wait before
    marking a multipath link as bad. When the path fails, any current I/O on
    that failed path fails. The default value varies according to the device
    driver being used. The valid range of values is 0 to 600 seconds. To use
    the driver’s internal timeouts, set the value to zero (0) or to any
    value greater than 600.
   </para>

   <para>
    The fast_io_fail_tmo parameter sets the length of time to wait before
    failing I/O when a link problem is detected. I/O that reaches the driver
    fails. If I/O is in a blocked queue, the I/O does not fail until the
    dev_loss_tmo time elapses and the queue is unblocked.
   </para>

   <para>
    If you modify the <filename>/etc/multipath.conf</filename> file, the
    changes are not applied until you update the multipath maps, or until
    the multipathd daemon is restarted, such as at system restart.
   </para>
  </sect1>
  <sect1 id="bbi89rh">
   <title>Configuring Path Failover Policies and Priorities</title>

   <para>
    In a Linux host, when there are multiple paths to a storage controller,
    each path appears as a separate block device, and results in multiple
    block devices for single LUN. The Device Mapper Multipath service
    detects multiple paths with the same LUN ID, and creates a new multipath
    device with that ID. For example, a host with two HBAs attached to a
    storage controller with two ports via a single unzoned Fibre Channel
    switch sees four block devices: <filename>/dev/sda</filename>,
    <filename>/dev/sdb</filename>, <filename>/dev/sdc</filename>, and
    <filename>/dev/sdd</filename>. The Device Mapper Multipath service
    creates a single block device, <filename>/dev/mpath/mpath1</filename>
    that reroutes I/O through those four underlying block devices.
   </para>

   <para>
    This section describes how to specify policies for failover and
    configure priorities for the paths.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="beep0mu" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bbi89xy" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bbj87iv" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bbi8acn" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bbi8els" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="beep0mu">
    <title>Configuring the Path Failover Policies</title>
    <para>
     Use the <command>multipath</command> command with the -p option to set
     the path failover policy:
    </para>
<screen>
multipath <replaceable>devicename</replaceable> -p <replaceable>policy</replaceable> 
</screen>
    <para role="intro">
     Replace <replaceable>policy</replaceable> with one of the following
     policy options:
    </para>
    <table id="beg1vn7" frame="topbot" rowsep="1" pgwide="0">
     <title>Group Policy Options for the multipath -p Command</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="2381*"/>
      <colspec colnum="2" colname="2" colwidth="7620*"/>
      <thead>
       <row id="beg1vn8">
        <entry>
         <para>
          Policy Option
         </para>
        </entry>
        <entry>
         <para>
          Description
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="beg1vn9">
        <entry>
         <para>
          failover
         </para>
        </entry>
        <entry>
         <para>
          (Default) One path per priority group.
         </para>
        </entry>
       </row>
       <row id="beg1vna">
        <entry>
         <para>
          multibus
         </para>
        </entry>
        <entry>
         <para>
          All paths in one priority group.
         </para>
        </entry>
       </row>
       <row id="beg1vnb">
        <entry>
         <para>
          group_by_serial
         </para>
        </entry>
        <entry>
         <para>
          One priority group per detected serial number.
         </para>
        </entry>
       </row>
       <row id="beg1vnc">
        <entry>
         <para>
          group_by_prio
         </para>
        </entry>
        <entry>
         <para>
          One priority group per path priority value. Priorities are
          determined by callout programs specified as a global,
          per-controller, or per-multipath option in the
          <filename>/etc/multipath.conf</filename> configuration file.
         </para>
        </entry>
       </row>
       <row id="beg1vnd">
        <entry>
         <para>
          group_by_node_name
         </para>
        </entry>
        <entry>
         <para>
          One priority group per target node name. Target node names are
          fetched in the<filename>
          /sys/class/fc_transport/target*/node_name</filename> location.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </sect2>

   <sect2 id="bbi89xy">
    <title>Configuring Failover Priorities</title>
    <para>
     You must manually enter the failover priorities for the device in the
     <filename>/etc/multipath.conf</filename> file. Examples for all
     settings and options can be found in the
     <filename>/usr/share/doc/packages/multipath-tools/multipath.conf.annotated</filename>
     file.
    </para>
    <para>
     If you modify the <filename>/etc/multipath.conf</filename> file, the
     changes are not automatically applied when you save the file. For
     information, see
     <xref linkend="mpiohwsupconf" xrefstyle="SectTitleOnPage"/> and
     <xref linkend="bbj8y2r" xrefstyle="SectTitleOnPage"/>.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="beg263n" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bbi8jjc" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bbi8jjd" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bbi8g9x" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="beg263n">
     <title>Understanding Priority Groups and Attributes</title>
     <para>
      A <emphasis>priority group</emphasis> is a collection of paths that go
      to the same physical LUN. By default, I/O is distributed in a
      round-robin fashion across all paths in the group. The
      <command>multipath</command> command automatically creates priority
      groups for each LUN in the SAN based on the
      <literal>path_grouping_policy</literal> setting for that SAN. The
      <command>multipath</command> command multiplies the number of paths in
      a group by the group’s priority to determine which group is the
      primary. The group with the highest calculated value is the primary.
      When all paths in the primary group are failed, the priority group
      with the next highest value becomes active.
     </para>
     <para>
      A <emphasis>path priority</emphasis> is an integer value assigned to a
      path. The higher the value, the higher the priority is. An external
      program is used to assign priorities for each path. For a given
      device, the paths with the same priorities belong to the same priority
      group.
     </para>
     <para>
      Multipath Tools 0.4.9 for SLES 11 SP2 uses the <literal>prio</literal>
      setting in the <literal>defaults{}</literal> or
      <literal>devices{}</literal> section of the
      <filename>/etc/multipath.conf</filename> file. It silently ignores the
      keyword <literal>prio</literal> when it is specified for an individual
      <literal>multipath</literal> definition in the
      <literal>multipaths{)</literal> section. Multipath Tools 0.4.8 for
      SLES 11 SP1 and earlier allows the prio setting in the individual
      <literal>multipath</literal> definition in the
      <literal>multipaths{)</literal> section to override the
      <literal>prio</literal> settings in the <literal>defaults{}</literal>
      or <literal>devices{}</literal> section.
     </para>
     <para>
      The syntax for the <literal>prio</literal> keyword in the
      <filename>/etc/multipath.conf</filename> file is changed in
      <filename>multipath-tools-0.4.9</filename>. The
      <literal>prio</literal> line specifies the prioritizer. If the
      prioritizer requires an argument, you specify the argument by using
      the <literal>prio_args</literal> keyword on a second line. Previously,
      the prioritizer and its arguments were included on the
      <literal>prio</literal> line.
     </para>
     <bridgehead id="b15gvvaa">PRIO Settings for the Defaults or Devices Sections</bridgehead>
     <variablelist>
      <varlistentry id="b122w59j">
       <term>prio</term>
       <listitem>
        <para>
         Specifies the prioritizer program to call to obtain a path priority
         value. Weights are summed for each path group to determine the next
         path group to use in case of failure.
        </para>
        <para>
         Use the <literal>prio_args</literal> keyword to specify arguments
         if the specified prioritizer requires arguments.
        </para>
        <formalpara id="b122w59k" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <para>
         If no <literal>prio</literal> keyword is specified, all paths are
         equal. The default setting is “const” with a
         <literal>prio_args</literal> setting with no value.
        </para>
<screen>
prio      "const"
prio_args ""
</screen>
        <para>
         Example prioritizer programs include:
        </para>
        <informaltable frame="topbot" rowsep="1" pgwide="0">
         <tgroup cols="2">
          <colspec colnum="1" colname="1" colwidth="2907*"/>
          <colspec colnum="2" colname="2" colwidth="7096*"/>
          <thead>
           <row id="b122xdcu">
            <entry>
             <para>
              Prioritizer Program
             </para>
            </entry>
            <entry>
             <para>
              Description
             </para>
            </entry>
           </row>
          </thead>
          <tbody>
           <row id="b122xdcv">
            <entry>
             <para>
              alua
             </para>
            </entry>
            <entry>
             <para>
              Generates path priorities based on the SCSI-3 ALUA settings.
             </para>
            </entry>
           </row>
           <row id="b122xdcw">
            <entry>
             <para>
              const
             </para>
            </entry>
            <entry>
             <para>
              Generates the same priority for all paths.
             </para>
            </entry>
           </row>
           <row id="b122xdcx">
            <entry>
             <para>
              emc
             </para>
            </entry>
            <entry>
             <para>
              Generates the path priority for EMC arrays.
             </para>
            </entry>
           </row>
           <row id="b122xdcy">
            <entry>
             <para>
              hdc
             </para>
            </entry>
            <entry>
             <para>
              Generates the path priority for Hitachi HDS Modular storage
              arrays.
             </para>
            </entry>
           </row>
           <row id="b122xdcz">
            <entry>
             <para>
              hp_sw
             </para>
            </entry>
            <entry>
             <para>
              Generates the path priority for Compaq/HP controller in
              active/standby mode.
             </para>
            </entry>
           </row>
           <row id="b122xdd0">
            <entry>
             <para>
              ontap
             </para>
            </entry>
            <entry>
             <para>
              Generates the path priority for NetApp arrays.
             </para>
            </entry>
           </row>
           <row id="b122xdd1">
            <entry>
             <para>
              random
             </para>
            </entry>
            <entry>
             <para>
              Generates a random priority for each path.
             </para>
            </entry>
           </row>
           <row id="b122xdd2">
            <entry>
             <para>
              rdac
             </para>
            </entry>
            <entry>
             <para>
              Generates the path priority for LSI/Engenio RDAC controller.
             </para>
            </entry>
           </row>
           <row id="b122xdd3">
            <entry>
             <para>
              weightedpath
             </para>
            </entry>
            <entry>
             <para>
              Generates the path priority based on the weighted values you
              specify in the arguments for <literal>prio_args</literal>,
              such as:
             </para>
<screen>
&lt;hbtl|devname&gt; &lt;<replaceable>regex1</replaceable>&gt; &lt;<replaceable>prio1</replaceable>&gt; &lt;<replaceable>regex2</replaceable>&gt; &lt;<replaceable>prio2</replaceable>&gt;...
</screen>
             <para>
              The <literal>hbtl regex</literal> argument format uses the
              SCSI <literal>H:B:T:L</literal> notation (such as
              <literal>1:0:.:.</literal> and <literal>*:0:0:.</literal>)
              with a weight value, where H, B, T, L are the host, bus,
              target, and LUN IDs for a device. For example:
             </para>
<screen>
prio "weightedpath"
prio_args "hbtl 1:.:.:. 2 4:.:.:. 4"
</screen>
             <para>
              The devname regex argument format uses a device node name with
              a weight value for each device. For example:
             </para>
<screen>
prio "weightedpath"
prio_args "devname sda 50 sde 10 sdc 50 sdf 10"
</screen>
            </entry>
           </row>
          </tbody>
         </tgroup>
        </informaltable>
       </listitem>
      </varlistentry>
      <varlistentry id="b122w59l">
       <term>prio_args</term>
       <listitem>
        <para>
         Specifies the arguments for the specified prioritizer program that
         requires arguments. Most <literal>prio</literal> programs do not
         need arguments.
        </para>
        <formalpara id="b122w59m" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <para>
         There is no default. The value depends on the
         <literal>prio</literal> setting and whether the prioritizer
         requires arguments.
        </para>
<screen>
prio      "const"
prio_args ""
</screen>
       </listitem>
      </varlistentry>
     </variablelist>
     <bridgehead id="b15gvvab">Multipath Attributes</bridgehead>
     <para>
      Multipath attributes are used to control the behavior of multipath I/O
      for devices. You can specify attributes as defaults for all multipath
      devices. You can also specify attributes that apply only to a given
      multipath device by creating an entry for that device in the
      <literal>multipaths</literal> section of the multipath configuration
      file.
     </para>
     <variablelist>
      <varlistentry id="b122sbg9">
       <term>user_friendly_names</term>
       <listitem>
        <para>
         Specifies whether to use world-wide IDs (WWIDs) or to use the
         <filename>/var/lib/multipath/bindings</filename> file to assign a
         persistent and unique alias to the multipath devices in the form of
         <filename>/dev/mapper/mpathN</filename>.
        </para>
        <para>
         This option can be used in the <literal>devices</literal> section
         and the <literal>multipaths</literal> section.
        </para>
        <formalpara id="b122sbga" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <informaltable frame="topbot" rowsep="1" pgwide="0">
         <tgroup cols="2">
          <colspec colnum="1" colname="1" colwidth="2642*"/>
          <colspec colnum="2" colname="2" colwidth="7360*"/>
          <thead>
           <row id="b122skbh">
            <entry>
             <para>
              Value
             </para>
            </entry>
            <entry>
             <para>
              Description
             </para>
            </entry>
           </row>
          </thead>
          <tbody>
           <row id="b122skbi">
            <entry>
             <para>
              no
             </para>
            </entry>
            <entry>
             <para>
              (Default) Use the WWIDs shown in the
              <filename>/dev/disk/by-id/</filename> location.
             </para>
            </entry>
           </row>
           <row id="b122skbj">
            <entry>
             <para>
              yes
             </para>
            </entry>
            <entry>
             <para>
              Autogenerate user-friendly names as aliases for the multipath
              devices instead of the actual ID.
             </para>
            </entry>
           </row>
          </tbody>
         </tgroup>
        </informaltable>
       </listitem>
      </varlistentry>
      <varlistentry id="b122sbgh">
       <term>failback</term>
       <listitem>
        <para>
         Specifies whether to monitor the failed path recovery, and
         indicates the timing for group failback after failed paths return
         to service.
        </para>
        <para>
         When the failed path recovers, the path is added back into the
         multipath enabled path list based on this setting. Multipath
         evaluates the priority groups, and changes the active priority
         group when the priority of the primary path exceeds the secondary
         group.
        </para>
        <formalpara id="b122sbgi" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <informaltable frame="topbot" rowsep="1" pgwide="0">
         <tgroup cols="2">
          <colspec colnum="1" colname="1" colwidth="2642*"/>
          <colspec colnum="2" colname="2" colwidth="7360*"/>
          <thead>
           <row id="b122sm95">
            <entry>
             <para>
              Value
             </para>
            </entry>
            <entry>
             <para>
              Description
             </para>
            </entry>
           </row>
          </thead>
          <tbody>
           <row id="b122sm96">
            <entry>
             <para>
              manual
             </para>
            </entry>
            <entry>
             <para>
              (Default) The failed path is not monitored for recovery. The
              administrator runs the <command>multipath</command> command to
              update enabled paths and priority groups.
             </para>
            </entry>
           </row>
           <row id="b122sm97">
            <entry>
             <para>
              immediate
             </para>
            </entry>
            <entry>
             <para>
              When a path recovers, enable the path immediately.
             </para>
            </entry>
           </row>
           <row id="b122sm98">
            <entry>
             <para>
              n
             </para>
            </entry>
            <entry>
             <para>
              When the path recovers, wait <replaceable>n</replaceable>
              seconds before enabling the path. Specify an integer value
              greater than 0.
             </para>
            </entry>
           </row>
          </tbody>
         </tgroup>
        </informaltable>
        <para>
         We recommend failback setting of “manual” for multipath in
         cluster environments in order to prevent multipath failover
         ping-pong.
        </para>
<screen>
failback "manual"
</screen>
        <important>
         <para>
          Ensure that you verify the failback setting with your storage
          system vendor. Different storage systems can require different
          settings.
         </para>
        </important>
       </listitem>
      </varlistentry>
      <varlistentry id="b122sbgm">
       <term>getuid_callout</term>
       <listitem>
        <para>
         The default program and arguments to call to obtain a unique path
         identifier. Specify the location with an absolute Linux path.
        </para>
        <para>
         This attribute is deprecated in SLES 11 SP3. It is replaced by the
         <link linkend="b15gvth2"><literal>uid_attribute</literal></link>.
        </para>
        <formalpara id="b122sbgn" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <para>
         The default location and arguments are:
        </para>
<screen>
/lib/udev/scsi_id -g -u -s
</screen>
        <para>
         Example:
        </para>
<screen>
getuid_callout "/lib/udev/scsi_id -g -u -d /dev/%n"

getuid_callout "/lib/udev/scsi_id --whitelisted --device=/dev/%n"
</screen>
       </listitem>
      </varlistentry>
      <varlistentry id="b122sbgo">
       <term>no_path_retry</term>
       <listitem>
        <para>
         Specifies the behaviors to use on path failure.
        </para>
        <formalpara id="b122sbgp" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <informaltable frame="topbot" rowsep="1" pgwide="0">
         <tgroup cols="2">
          <colspec colnum="1" colname="1" colwidth="2642*"/>
          <colspec colnum="2" colname="2" colwidth="7360*"/>
          <thead>
           <row id="b122sr39">
            <entry>
             <para>
              Value
             </para>
            </entry>
            <entry>
             <para>
              Description
             </para>
            </entry>
           </row>
          </thead>
          <tbody>
           <row id="b122sr3a">
            <entry>
             <para>
              n
             </para>
            </entry>
            <entry>
             <para>
              Specifies the number of retries until
              <command>multipath</command> stops the queuing and fails the
              path. Specify an integer value greater than 0.
             </para>
             <para>
              In a cluster, you can specify a value of “0” to prevent
              queuing and allow resources to fail over.
             </para>
            </entry>
           </row>
           <row id="b122sr3b">
            <entry>
             <para>
              fail
             </para>
            </entry>
            <entry>
             <para>
              Specifies immediate failure (no queuing).
             </para>
            </entry>
           </row>
           <row id="b122sr3c">
            <entry>
             <para>
              queue
             </para>
            </entry>
            <entry>
             <para>
              Never stop queuing (queue forever until the path comes alive).
             </para>
            </entry>
           </row>
          </tbody>
         </tgroup>
        </informaltable>
        <para>
         We recommend a retry setting of “fail” or “0” in the
         <filename>/etc/multipath.conf</filename> file when working in a
         cluster. This causes the resources to fail over when the connection
         is lost to storage. Otherwise, the messages queue and the resource
         failover cannot occur.
        </para>
<screen>
no_path_retry "fail"
no_path_retry "0"
</screen>
        <important>
         <para>
          Ensure that you verify the retry settings with your storage system
          vendor. Different storage systems can require different settings.
         </para>
        </important>
       </listitem>
      </varlistentry>
      <varlistentry id="b122sbgs">
       <term>path_checker</term>
       <listitem>
        <para role="intro">
         Determines the state of the path.
        </para>
        <formalpara id="b122sbgt" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <informaltable frame="topbot" rowsep="1" pgwide="0">
         <tgroup cols="2">
          <colspec colnum="1" colname="1" colwidth="2642*"/>
          <colspec colnum="2" colname="2" colwidth="7360*"/>
          <thead>
           <row id="b122swb1">
            <entry>
             <para>
              Value
             </para>
            </entry>
            <entry>
             <para>
              Description
             </para>
            </entry>
           </row>
          </thead>
          <tbody>
           <row id="b122swb2">
            <entry>
             <para>
              directio
             </para>
            </entry>
            <entry>
             <para>
              (Default in <filename>multipath-tools</filename> version 0.4.8
              and later) Reads the first sector that has direct I/O. This is
              useful for DASD devices. Logs failure messages in the
              &systemd; journal (see <xref linkend="cha.journalctl"/>).
             </para>
            </entry>
           </row>
           <row id="b122swb3">
            <entry>
             <para>
              readsector0
             </para>
            </entry>
            <entry>
             <para>
              (Default in <filename>multipath-tools</filename> version 0.4.7
              and earlier; deprecated and replaced by
              <literal>directio</literal>.) Reads the first sector of the
              device. Logs failure messages in the &systemd; journal (see
              <xref linkend="cha.journalctl"/>).
             </para>
            </entry>
           </row>
           <row id="b122swb4">
            <entry>
             <para>
              tur
             </para>
            </entry>
            <entry>
             <para>
              Issues a SCSI test unit ready command to the device. This is
              the preferred setting if the LUN supports it. On failure, the
              command does not fill up the &systemd; log journal with
              messages.
             </para>
            </entry>
           </row>
           <row id="b122swb5">
            <entry>
             <para>
              <replaceable>custom_vendor_value</replaceable>
             </para>
            </entry>
            <entry>
             <para>
              Some SAN vendors provide custom path_checker options:
             </para>
             <itemizedlist>
              <listitem>
               <formalpara id="b122w9oz">
                <title>cciss_tur:</title>
                <para>
                 Checks the path state for HP Smart Storage Arrays.
                </para>
               </formalpara>
              </listitem>
              <listitem>
               <formalpara id="b122swb6">
                <title>emc_clariion:</title>
                <para>
                 Queries the EMC Clariion EVPD page 0xC0 to determine the
                 path state.
                </para>
               </formalpara>
              </listitem>
              <listitem>
               <formalpara id="b122swb7">
                <title>hp_sw:</title>
                <para>
                 Checks the path state (Up, Down, or Ghost) for HP storage
                 arrays with Active/Standby firmware.
                </para>
               </formalpara>
              </listitem>
              <listitem>
               <formalpara id="b122swb8">
                <title>rdac:</title>
                <para>
                 Checks the path state for the LSI/Engenio RDAC storage
                 controller.
                </para>
               </formalpara>
              </listitem>
             </itemizedlist>
            </entry>
           </row>
          </tbody>
         </tgroup>
        </informaltable>
       </listitem>
      </varlistentry>
      <varlistentry id="b122sbgq">
       <term>path_grouping_policy</term>
       <listitem>
        <para role="intro">
         Specifies the path grouping policy for a multipath device hosted by
         a given controller.
        </para>
        <formalpara id="b122sbgr" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <informaltable frame="topbot" rowsep="1" pgwide="0">
         <tgroup cols="2">
          <colspec colnum="1" colname="1" colwidth="2642*"/>
          <colspec colnum="2" colname="2" colwidth="7360*"/>
          <thead>
           <row id="b122swaq">
            <entry>
             <para>
              Value
             </para>
            </entry>
            <entry>
             <para>
              Description
             </para>
            </entry>
           </row>
          </thead>
          <tbody>
           <row id="b122swar">
            <entry>
             <para>
              failover
             </para>
            </entry>
            <entry>
             <para>
              (Default) One path is assigned per priority group so that only
              one path at a time is used.
             </para>
            </entry>
           </row>
           <row id="b122swas">
            <entry>
             <para>
              multibus
             </para>
            </entry>
            <entry>
             <para>
              All valid paths are in one priority group. Traffic is
              load-balanced across all active paths in the group.
             </para>
            </entry>
           </row>
           <row id="b122swat">
            <entry>
             <para>
              group_by_prio
             </para>
            </entry>
            <entry>
             <para>
              One priority group exists for each path priority value. Paths
              with the same priority are in the same priority group.
              Priorities are assigned by an external program.
             </para>
            </entry>
           </row>
           <row id="b122swau">
            <entry>
             <para>
              group_by_serial
             </para>
            </entry>
            <entry>
             <para>
              Paths are grouped by the SCSI target serial number (controller
              node WWN).
             </para>
            </entry>
           </row>
           <row id="b122swav">
            <entry>
             <para>
              group_by_node_name
             </para>
            </entry>
            <entry>
             <para>
              One priority group is assigned per target node name. Target
              node names are fetched in
              <filename>/sys/class/fc_transport/target*/node_name</filename>.
             </para>
            </entry>
           </row>
          </tbody>
         </tgroup>
        </informaltable>
       </listitem>
      </varlistentry>
      <varlistentry id="b122sbgu">
       <term>path_selector</term>
       <listitem>
        <para role="intro">
         Specifies the path-selector algorithm to use for load balancing.
        </para>
        <formalpara id="b122sbgv" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <informaltable frame="topbot" rowsep="1" pgwide="0">
         <tgroup cols="2">
          <colspec colnum="1" colname="1" colwidth="2642*"/>
          <colspec colnum="2" colname="2" colwidth="7360*"/>
          <thead>
           <row id="b122t3po">
            <entry>
             <para>
              Value
             </para>
            </entry>
            <entry>
             <para>
              Description
             </para>
            </entry>
           </row>
          </thead>
          <tbody>
           <row id="b122t3pp">
            <entry>
             <para>
              round-robin 0
             </para>
            </entry>
            <entry>
             <para>
              (Default in SLES 11 SP2 and earlier) The load-balancing
              algorithm used to balance traffic across all active paths in a
              priority group.
             </para>
            </entry>
           </row>
           <row id="b122t3pr">
            <entry>
             <para>
              queue-length 0
             </para>
            </entry>
            <entry>
             <para>
              A dynamic load balancer that balances the number of in-flight
              I/O on paths similar to the least-pending option.
             </para>
            </entry>
           </row>
           <row id="b122t3ps">
            <entry>
             <para>
              service-time 0
             </para>
            </entry>
            <entry>
             <para>
              (Default in SLES 11 SP3 and later) A service-time oriented
              load balancer that balances I/O on paths according to the
              latency.
             </para>
            </entry>
           </row>
          </tbody>
         </tgroup>
        </informaltable>
       </listitem>
      </varlistentry>
      <varlistentry id="b122sbgw">
       <term>pg_timeout</term>
       <listitem>
        <para>
         Specifies path group timeout handling.
        </para>
        <formalpara id="b122sbgx" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <para>
         NONE (internal default)
        </para>
       </listitem>
      </varlistentry>
      <varlistentry id="b122w0sa">
       <term>polling_interval</term>
       <listitem>
        <para>
         Specifies the time in seconds between the end of one path checking
         cycle and the beginning of the next path checking cycle.
        </para>
        <formalpara id="b122w0sb" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <para>
         Specify an integer value greater than 0. The default value is 5.
         Ensure that you verify the polling_interval setting with your
         storage system vendor. Different storage systems can require
         different settings.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry id="b122sbgy">
       <term>prio_callout</term>
       <listitem>
        <para>
         Specifies the program and arguments to use to determine the layout
         of the multipath map.
        </para>
        <para>
         Multipath prio_callout programs are located in shared libraries in
         <filename>/lib/libmultipath/lib*</filename>. By using shared
         libraries, the callout programs are loaded into memory on daemon
         startup.
        </para>
        <para>
         When queried by the <command>multipath</command> command, the
         specified mpath_prio_* callout program returns the priority for a
         given path in relation to the entire multipath layout.
        </para>
        <para>
         When it is used with the path_grouping_policy of group_by_prio, all
         paths with the same priority are grouped into one multipath group.
         The group with the highest aggregate priority becomes the active
         group.
        </para>
        <para>
         When all paths in a group fail, the group with the next highest
         aggregate priority becomes active. Additionally, a failover command
         (as determined by the hardware handler) might be send to the
         target.
        </para>
        <para>
         The mpath_prio_* program can also be a custom script created by a
         vendor or administrator for a specified setup.
        </para>
        <itemizedlist>
         <listitem>
          <para>
           A <literal>%n</literal> in the command line expands to the device
           name in the <filename>/dev</filename> directory.
          </para>
         </listitem>
         <listitem>
          <para>
           A <literal>%b</literal> in the command line expands to the device
           number in <replaceable>major:minor</replaceable> format in the
           <filename>/dev</filename> directory.
          </para>
         </listitem>
         <listitem>
          <para>
           A <literal>%d</literal> in the command line expands to the device
           ID in the <filename>/dev/disk/by-id</filename> directory.
          </para>
         </listitem>
        </itemizedlist>
        <para>
         If devices are hot-pluggable, use the <literal>%d</literal> flag
         instead of <literal>%n</literal>. This addresses the short time
         that elapses between the time when devices are available and when
         <command>udev</command> creates the device nodes.
        </para>
        <formalpara id="b122sbgz" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <informaltable frame="topbot" rowsep="1" pgwide="0">
         <tgroup cols="2">
          <colspec colnum="1" colname="1" colwidth="2642*"/>
          <colspec colnum="2" colname="2" colwidth="7360*"/>
          <thead>
           <row id="b122tqh2">
            <entry>
             <para>
              Value
             </para>
            </entry>
            <entry>
             <para>
              Description
             </para>
            </entry>
           </row>
          </thead>
          <tbody>
           <row id="b122tqh3">
            <entry>
             <para>
              (No value)
             </para>
            </entry>
            <entry>
             <para>
              If no <literal>prio_callout</literal> attribute is used, all
              paths are equal. This is the default.
             </para>
            </entry>
           </row>
           <row id="b122tqh4">
            <entry>
             <para>
              /bin/true
             </para>
            </entry>
            <entry>
             <para>
              Specify this value when the group_by_prio is not being used.
             </para>
            </entry>
           </row>
          </tbody>
         </tgroup>
        </informaltable>
        <para>
         The <command>prioritizer</command> programs generate path
         priorities when queried by the <command>multipath</command>
         command. The program names must begin with
         <filename>mpath_prio_</filename> and are named by the device type
         or balancing method used. Current prioritizer programs include the
         following:
        </para>
        <informaltable frame="topbot" rowsep="1" pgwide="0">
         <tgroup cols="2">
          <colspec colnum="1" colname="1" colwidth="3539*"/>
          <colspec colnum="2" colname="2" colwidth="6464*"/>
          <thead>
           <row id="b122tml8">
            <entry>
             <para>
              Prioritizer Program
             </para>
            </entry>
            <entry>
             <para>
              Description
             </para>
            </entry>
           </row>
          </thead>
          <tbody>
           <row id="b122tml9">
            <entry>
             <para>
              mpath_prio_alua %n
             </para>
            </entry>
            <entry>
             <para>
              Generates path priorities based on the SCSI-3 ALUA settings.
             </para>
            </entry>
           </row>
           <row id="b122tmla">
            <entry>
             <para>
              mpath_prio_balance_units
             </para>
            </entry>
            <entry>
             <para>
              Generates the same priority for all paths.
             </para>
            </entry>
           </row>
           <row id="b122tp25">
            <entry>
             <para>
              mpath_prio_emc %n
             </para>
            </entry>
            <entry>
             <para>
              Generates the path priority for EMC arrays.
             </para>
            </entry>
           </row>
           <row id="b122tp26">
            <entry>
             <para>
              mpath_prio_hds_modular %b
             </para>
            </entry>
            <entry>
             <para>
              Generates the path priority for Hitachi HDS Modular storage
              arrays.
             </para>
            </entry>
           </row>
           <row id="b122tp27">
            <entry>
             <para>
              mpath_prio_hp_sw %n
             </para>
            </entry>
            <entry>
             <para>
              Generates the path priority for Compaq/HP controller in
              active/standby mode.
             </para>
            </entry>
           </row>
           <row id="b122tp28">
            <entry>
             <para>
              mpath_prio_netapp %n
             </para>
            </entry>
            <entry>
             <para>
              Generates the path priority for NetApp arrays.
             </para>
            </entry>
           </row>
           <row id="b122tp29">
            <entry>
             <para>
              mpath_prio_random %n
             </para>
            </entry>
            <entry>
             <para>
              Generates a random priority for each path.
             </para>
            </entry>
           </row>
           <row id="b122tp2a">
            <entry>
             <para>
              mpath_prio_rdac %n
             </para>
            </entry>
            <entry>
             <para>
              Generates the path priority for LSI/Engenio RDAC controller.
             </para>
            </entry>
           </row>
           <row id="b122tp2b">
            <entry>
             <para>
              mpath_prio_tpc %n
             </para>
            </entry>
            <entry>
             <para>
              You can optionally use a script created by a vendor or
              administrator that gets the priorities from a file where you
              specify priorities to use for each path.
             </para>
            </entry>
           </row>
           <row id="b122tp2c">
            <entry>
             <para>
              mpath_prio_spec.sh %n
             </para>
            </entry>
            <entry>
             <para>
              Provides the path of a user-created script that generates the
              priorities for multipathing based on information contained in
              a second data file. (This path and filename are provided as an
              example. Specify the location of your script instead.) The
              script can be created by a vendor or administrator. The
              script’s target file identifies each path for all
              multipathed devices and specifies a priority for each path.
              For an example, see
              <xref linkend="bbj87iv" xrefstyle="SectTitleOnPage"/>.
             </para>
            </entry>
           </row>
          </tbody>
         </tgroup>
        </informaltable>
       </listitem>
      </varlistentry>
      <varlistentry id="b122sbh0">
       <term>rr_min_io</term>
       <listitem>
        <para>
         Specifies the number of I/O transactions to route to a path before
         switching to the next path in the same path group, as determined by
         the specified algorithm in the <literal>path_selector</literal>
         setting.
        </para>
        <para>
         The rr_min_io attribute is used only for kernels 2.6.31 and
         earlier. It is obsoleted in SLES 11 SP2 and replaced by the
         <literal>rr_min_io_rq</literal> attribute.
        </para>
        <formalpara id="b122sbh1" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <para>
         Specify an integer value greater than 0. The default value is 1000.
        </para>
<screen>
rr_min_io "1000"
</screen>
       </listitem>
      </varlistentry>
      <varlistentry id="b122sbh2">
       <term>rr_min_io_rq</term>
       <listitem>
        <para role="intro">
         Specifies the number of I/O requests to route to a path before
         switching to the next path in the current path group, using
         request-based device-mapper-multipath.
        </para>
        <para>
         This attribute is available for systems running SLES 11 SP2 and
         later. It replaces the rr_min_io attribute.
        </para>
        <formalpara id="b122sbh3" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <para>
         Specify an integer value greater than 0. The default value is 1.
        </para>
<screen>
rr_min_io_rq "1"
</screen>
       </listitem>
      </varlistentry>
      <varlistentry id="b122sbh4">
       <term>rr_weight</term>
       <listitem>
        <para role="intro">
         Specifies the weighting method to use for paths.
        </para>
        <formalpara id="b122sbh5" role="intro">
         <title>Values</title>
         <para/>
        </formalpara>
        <informaltable frame="topbot" rowsep="1" pgwide="0">
         <tgroup cols="2">
          <colspec colnum="1" colname="1" colwidth="2642*"/>
          <colspec colnum="2" colname="2" colwidth="7360*"/>
          <thead>
           <row id="b122tlbj">
            <entry>
             <para>
              Value
             </para>
            </entry>
            <entry>
             <para>
              Description
             </para>
            </entry>
           </row>
          </thead>
          <tbody>
           <row id="b122tlbk">
            <entry>
             <para>
              uniform
             </para>
            </entry>
            <entry>
             <para>
              (Default) All paths have the same round-robin weights.
             </para>
            </entry>
           </row>
           <row id="b122tlbl">
            <entry>
             <para>
              priorities
             </para>
            </entry>
            <entry>
             <para>
              Each path’s weight is determined by the path’s priority
              times the rr_min_io_rq setting (or the rr_min_io setting for
              kernels 2.6.31 and earlier).
             </para>
            </entry>
           </row>
          </tbody>
         </tgroup>
        </informaltable>
       </listitem>
      </varlistentry>
      <varlistentry id="b15gvth2">
       <term>uid_attribute</term>
       <listitem>
        <para>
         A udev attribute that provides a unique path identifier. The
         default value is <literal>ID_SERIAL</literal>.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </sect3>
    <sect3 id="bbi8jjc">
     <title>Configuring for Round-Robin Load Balancing</title>
     <para>
      All paths are active. I/O is configured for some number of seconds or
      some number of I/O transactions before moving to the next open path in
      the sequence.
     </para>
    </sect3>
    <sect3 id="bbi8jjd">
     <title>Configuring for Single Path Failover</title>
     <para>
      A single path with the highest priority (lowest value setting) is
      active for traffic. Other paths are available for failover, but are
      not used unless failover occurs.
     </para>
    </sect3>
    <sect3 id="bbi8g9x">
     <title>Grouping I/O Paths for Round-Robin Load Balancing</title>
     <para>
      Multiple paths with the same priority fall into the active group. When
      all paths in that group fail, the device fails over to the next
      highest priority group. All paths in the group share the traffic load
      in a round-robin load balancing fashion.
     </para>
    </sect3>
   </sect2>

   <sect2 id="bbj87iv">
    <title>Using a Script to Set Path Priorities</title>
    <para>
     You can create a script that interacts with Device Mapper Multipath
     (DM-MPIO) to provide priorities for paths to the LUN when set as a
     resource for the <command>prio_callout</command> setting.
    </para>
    <para>
     First, set up a text file that lists information about each device and
     the priority values you want to assign to each path. For example, name
     the file <filename>/usr/local/etc/primary-paths</filename>. Enter one
     line for each path in the following format:
    </para>
<screen>
host_wwpn target_wwpn scsi_id priority_value
</screen>
    <para>
     Return a priority value for each path on the device. Ensure that the
     variable FILE_PRIMARY_PATHS resolves to a real file with appropriate
     data (host wwpn, target wwpn, scsi_id and priority value) for each
     device.
    </para>
    <para>
     The contents of the <filename>primary-paths</filename> file for a
     single LUN with eight paths each might look like this:
    </para>
<screen>
0x10000000c95ebeb4 0x200200a0b8122c6e 2:0:0:0 sdb 3600a0b8000122c6d00000000453174fc 50
</screen>
<screen>
0x10000000c95ebeb4 0x200200a0b8122c6e 2:0:0:1 sdc 3600a0b80000fd6320000000045317563 2
</screen>
<screen>
0x10000000c95ebeb4 0x200200a0b8122c6e 2:0:0:2 sdd 3600a0b8000122c6d0000000345317524 50
</screen>
<screen>
0x10000000c95ebeb4 0x200200a0b8122c6e 2:0:0:3 sde 3600a0b80000fd6320000000245317593 2
</screen>
<screen>
0x10000000c95ebeb4 0x200300a0b8122c6e 2:0:1:0 sdi 3600a0b8000122c6d00000000453174fc 5
</screen>
<screen>
0x10000000c95ebeb4 0x200300a0b8122c6e 2:0:1:1 sdj 3600a0b80000fd6320000000045317563 51
</screen>
<screen>
0x10000000c95ebeb4 0x200300a0b8122c6e 2:0:1:2 sdk 3600a0b8000122c6d0000000345317524 5
</screen>
<screen>
0x10000000c95ebeb4 0x200300a0b8122c6e 2:0:1:3 sdl 3600a0b80000fd6320000000245317593 51
</screen>
    <para>
     To continue the example mentioned in
     <xref linkend="b122sbgy" xrefstyle="HeadingOnPage"/>, create a script
     named <filename>/usr/local/sbin/path_prio.sh</filename>. You can use
     any path and filename. The script does the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       On query from multipath, grep the device and its path from the
       <filename>/usr/local/etc/primary-paths</filename> file.
      </para>
     </listitem>
     <listitem>
      <para>
       Return to multipath the priority value in the last column for that
       entry in the file.
      </para>
     </listitem>
    </itemizedlist>
   </sect2>

   <sect2 id="bbi8acn">
    <title>Configuring ALUA (mpath_prio_alua)</title>
    <para>
     The <command>mpath_prio_alua(8)</command> command is used as a priority
     callout for the Linux <command>multipath(8)</command> command. It
     returns a number that is used by DM-MPIO to group SCSI devices with the
     same priority together. This path priority tool is based on ALUA
     (Asynchronous Logical Unit Access).
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="beg2eyu" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="beg2e1m" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bbj46qo" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bbj4fvf" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="beg2eyu">
     <title>Syntax</title>
<screen>
mpath_prio_alua [-d <replaceable>directory</replaceable>] [-h] [-v] [-V] <replaceable>device</replaceable> [<replaceable>device</replaceable>...] 
</screen>
    </sect3>
    <sect3 id="beg2e1m">
     <title>Prerequisite</title>
     <para>
      SCSI devices.
     </para>
    </sect3>
    <sect3 id="bbj46qo">
     <title>Options</title>
     <variablelist>
      <varlistentry id="bbj46v3">
       <term>-d <replaceable>directory</replaceable>
       </term>
       <listitem>
        <para>
         Specifies the Linux directory path where the listed device node
         names can be found. The default directory is
         <filename>/dev</filename>. When you use this option, specify the
         device node name only (such as <filename>sda</filename>) for the
         device or devices you want to manage.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry id="bbj4am6">
       <term>-h</term>
       <listitem>
        <para>
         Displays help for this command, then exits.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry id="bbj4am7">
       <term>-v</term>
       <listitem>
        <para>
         Turns on verbose output to display status in human-readable format.
         Output includes information about which port group the specified
         device is in and its current state.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry id="bbj4am8">
       <term>-V</term>
       <listitem>
        <para>
         Displays the version number of this tool, then exits.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry id="bbj4c17">
       <term><replaceable>device</replaceable> [<replaceable>device</replaceable>...] </term>
       <listitem>
        <para>
         Specifies the SCSI device (or multiple devices) that you want to
         manage. The device must be a SCSI device that supports the Report
         Target Port Groups (<command>sg_rtpg(8)</command>) command. Use one
         of the following formats for the device node name:
        </para>
        <itemizedlist>
         <listitem>
          <para>
           The full Linux directory path, such as
           <filename>/dev/sda</filename>. Do not use with the -d option.
          </para>
         </listitem>
         <listitem>
          <para>
           The device node name only, such as <filename>sda</filename>.
           Specify the directory path by using the -d option.
          </para>
         </listitem>
         <listitem>
          <para>
           The major and minor number of the device separated by a colon (:)
           with no spaces, such as <filename>8:0</filename>. This creates a
           temporary device node in the <filename>/dev</filename> directory
           with a name in the format of
           <filename>tmpdev-&lt;major&gt;:&lt;minor&gt;-&lt;pid&gt;</filename>.
           For example, <filename>/dev/tmpdev-8:0-&lt;pid&gt;</filename>.
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>
     </variablelist>
    </sect3>
    <sect3 id="bbj4fvf">
     <title>Return Values</title>
     <para>
      On success, returns a value of 0 and the priority value for the group.
      <xref linkend="bbilsw8" xrefstyle="TableXRef"/> shows the priority
      values returned by the <command>mpath_prio_alua</command> command.
     </para>
     <table id="bbilsw8" frame="topbot" rowsep="1" pgwide="0">
      <title>ALUA Priorities for Device Mapper Multipath</title>
      <tgroup cols="2">
       <colspec colnum="1" colname="1" colwidth="2381*"/>
       <colspec colnum="2" colname="2" colwidth="7620*"/>
       <thead>
        <row id="bbiltgc">
         <entry>
          <para>
           Priority Value
          </para>
         </entry>
         <entry>
          <para>
           Description
          </para>
         </entry>
        </row>
       </thead>
       <tbody>
        <row id="bbiltgd">
         <entry>
          <para>
           50
          </para>
         </entry>
         <entry>
          <para>
           The device is in the active, optimized group.
          </para>
         </entry>
        </row>
        <row id="bbiltge">
         <entry>
          <para>
           10
          </para>
         </entry>
         <entry>
          <para>
           The device is in an active but non-optimized group.
          </para>
         </entry>
        </row>
        <row id="bbj4j3r">
         <entry>
          <para>
           1
          </para>
         </entry>
         <entry>
          <para>
           The device is in the standby group.
          </para>
         </entry>
        </row>
        <row id="bbiltgf">
         <entry>
          <para>
           0
          </para>
         </entry>
         <entry>
          <para>
           All other groups.
          </para>
         </entry>
        </row>
       </tbody>
      </tgroup>
     </table>
     <para>
      Values are widely spaced because of the way the
      <command>multipath</command> command handles them. It multiplies the
      number of paths in a group with the priority value for the group, then
      selects the group with the highest result. For example, if a
      non-optimized path group has six paths (6 x 10 = 60) and the optimized
      path group has a single path (1 x 50 = 50), the non-optimized group
      has the highest score, so multipath chooses the non-optimized group.
      Traffic to the device uses all six paths in the group in a round-robin
      fashion.
     </para>
     <para>
      On failure, returns a value of 1 to 5 indicating the cause for the
      command’s failure. For information, see the man page for
      <command>mpath_prio_alua</command>.
     </para>
    </sect3>
   </sect2>

   <sect2 id="bbi8els">
    <title>Reporting Target Path Groups</title>
    <para>
     Use the SCSI Report Target Port Groups (<command>sg_rtpg(8)</command>)
     command. For information, see the man page for
     <command>sg_rtpg(8)</command>.
    </para>
   </sect2>
  </sect1>
  <sect1 id="mpioroot">
   <title>Configuring Multipath I/O for the Root Device</title>

   <para>
    Device Mapper Multipath I/O (DM-MPIO) is available and supported for
    <filename>/boot</filename> and <filename>/root</filename> in
    &productname;. In addition, the YaST partitioner in the YaST installer
    supports enabling multipath during the install.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bp27t6u" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bomc64f" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bomc64g" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bp27t6u">
    <title>Enabling Multipath I/O at Install Time</title>
    <para>
     The multipath software must be running at install time if you want to
     install the operating system on a multipath device. The multipathd
     daemon is not automatically active during the system installation. You
     can start it by using the <guimenu>Configure Multipath</guimenu> option
     in the YaST partitioner.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bomcrff" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bomc64e" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bomcrff">
     <title>Enabling Multipath I/O at Install Time on an Active/Active Multipath Storage LUN</title>
     <procedure id="bomcrfg">
      <step id="bomcrfh">
       <para>
        During the install on the YaST Installation Settings page, click on
        <guimenu>Partitioning</guimenu> to open the YaST partitioner.
       </para>
      </step>
      <step id="bomcrfi">
       <para>
        Select <guimenu>Custom Partitioning (for experts)</guimenu>.
       </para>
      </step>
      <step id="bomcrfj">
       <para>
        Select the <guimenu>Hard Disks</guimenu> main icon, click the
        <guimenu>Configure</guimenu> button, then select <guimenu>Configure
        Multipath</guimenu>.
       </para>
      </step>
      <step id="bomcrfk">
       <para role="intro">
        Start multipath.
       </para>
       <para>
        YaST starts to rescan the disks and shows available multipath
        devices (such as
        <filename>/dev/disk/by-id/dm-uuid-mpath-3600a0b80000f4593000012ae4ab0ae65</filename>).
        This is the device that should be used for all further processing.
       </para>
      </step>
      <step id="bomcrfl">
       <para>
        Click <guimenu>Next</guimenu> to continue with the installation.
       </para>
      </step>
     </procedure>
    </sect3>
    <sect3 id="bomc64e">
     <title>Enabling Multipath I/O at Install Time on an Active/Passive Multipath Storage LUN</title>
     <para>
      The multipathd daemon is not automatically active during the system
      installation. You can start it by using the <guimenu>Configure
      Multipath</guimenu> option in the YaST partitioner.
     </para>
     <para>
      To enable multipath I/O at install time for an active/passive
      multipath storage LUN:
     </para>
     <procedure id="bomc6el">
      <step id="bomc6em">
       <para>
        During the install on the YaST Installation Settings page, click on
        <guimenu>Partitioning</guimenu> to open the YaST partitioner.
       </para>
      </step>
      <step id="bomc7xx">
       <para>
        Select <guimenu>Custom Partitioning (for experts)</guimenu>.
       </para>
      </step>
      <step id="bomc83f">
       <para>
        Select the <guimenu>Hard Disks</guimenu> main icon, click the
        <guimenu>Configure</guimenu> button, then select <guimenu>Configure
        Multipath</guimenu>.
       </para>
      </step>
      <step id="bomc8h8">
       <para>
        Start multipath.
       </para>
       <para>
        YaST starts to rescan the disks and shows available multipath
        devices (such as
        <filename>/dev/disk/by-id/dm-uuid-mpath-3600a0b80000f4593000012ae4ab0ae65</filename>).
        This is the device that should be used for all further processing.
        Write down the device path and UUID; you need it later.
       </para>
      </step>
      <step id="bomcatl">
       <para>
        Click <guimenu>Next</guimenu> to continue with the installation.
       </para>
      </step>
      <step id="bomca3n">
       <para>
        After all settings are done and the installation finished, YaST
        starts to write the boot loader information, and displays a
        countdown to restart the system. Stop the counter by clicking the
        <guimenu>Stop</guimenu> button and press CTRL+ALT+F5 to access a
        console.
       </para>
      </step>
      <step id="bomcamo">
       <para>
        <remark condition="needinfo">
	 2014-02-07 - fs: Is this still necessary with GRUB 2 ??
	</remark>
        Use the console to determine if a passive path was entered in the
        <filename>/boot/grub/device.map</filename> file for the
        <filename>hd0</filename> entry.
       </para>
       <para>
        This is necessary because the installation does not distinguish
        between active and passive paths.
       </para>
       <substeps>
        <step id="bomcby2">
         <para role="intro">
          Mount the root device to <filename>/mnt</filename> by entering
         </para>
<screen>
mount /dev/disk/by-id/&lt;<replaceable>UUID</replaceable>&gt;_part2 /mnt
</screen>
         <para role="intro">
          For example, enter
         </para>
<screen>
mount /dev/disk/by-id/dm-uuid-mpath-3600a0b80000f4593000012ae4ab0ae65_part2 /mnt
</screen>
        </step>
        <step id="bomcfm2">
         <para role="intro">
          Mount the boot device to <filename>/mnt/boot</filename> by
          entering
         </para>
<screen>
mount /dev/disk/by-id/&lt;<replaceable>UUID</replaceable>&gt;_part1 /mnt/boot
</screen>
         <para role="intro">
          For example, enter
         </para>
<screen>
mount /dev/disk/by-id/dm-uuid-mpath-3600a0b80000f4593000012ae4ab0ae65_part2 /mnt/boot
</screen>
        </step>
        <step id="bomcg7c">
         <para>
          Open <filename>/mnt/boot/grub/device.map</filename> file by
          entering
         </para>
<screen>
less /mnt/boot/grub/device.map
</screen>
        </step>
        <step id="bomcg05">
         <para>
          In the <filename>/mnt/boot/grub/device.map</filename> file,
          determine if the <filename>hd0</filename> entry points to a
          passive path, then do one of the following:
         </para>
         <itemizedlist>
          <listitem>
           <formalpara id="bomchix">
            <title>Active path:</title>
            <para>
             No action is needed; skip
             <xref linkend="bomc8jz" xrefstyle="StepXRef"/> and continue
             with <xref linkend="bomcboo" xrefstyle="StepXRef"/>.
            </para>
           </formalpara>
          </listitem>
          <listitem>
           <formalpara id="bomciib">
            <title>Passive path:</title>
            <para>
             The configuration must be changed and the boot loader must be
             reinstalled. Continue with
             <xref linkend="bomc8jz" xrefstyle="StepXRef"/>.
            </para>
           </formalpara>
          </listitem>
         </itemizedlist>
        </step>
       </substeps>
      </step>
      <step id="bomc8jz">
       <para role="intro">
        If the hd0 entry points to a passive path, change the configuration
        and reinstall the boot loader:
       </para>
       <substeps>
        <step id="bomcmd6">
         <para role="intro">
          At the console, enter the following commands at the console
          prompt:
         </para>
<screen>
mount -o bind /dev /mnt/dev

mount -o bind /sys /mnt/sys

mount -o bind /proc /mnt/proc

chroot

</screen>
        </step>
        <step id="bomclu7">
         <para role="intro">
          At the console, run <command>multipath -ll</command>, then check
          the output to find the active path.
         </para>
         <para>
          Passive paths are flagged as <literal>ghost</literal>.
         </para>
        </step>
        <step id="bomcmwr">
         <para>
          In the <filename>/mnt/boot/grub/device.map</filename> file, change
          the <literal>hd0</literal> entry to an active path, save the
          changes, and close the file.
         </para>
        </step>
        <step id="bomcndj">
         <para role="intro">
          In case the selection was to boot from MBR,
          <filename>/etc/grub.conf</filename> should look like the
          following:
         </para>
<screen>
setup --stage2=/boot/grub/stage2 (hd0) (hd0,0)
quit
</screen>
        </step>
        <step id="bomcoe1">
         <para role="intro">
          Reinstall the boot loader by entering
         </para>
<screen>
grub &lt; /etc/grub.conf
</screen>
        </step>
        <step id="bomcol7">
         <para role="intro">
          Enter the following commands:
         </para>
<screen>
exit

umount /mnt/*

umount /mnt
</screen>
        </step>
       </substeps>
      </step>
      <step id="bomcboo">
       <para>
        Return to the YaST graphical environment by pressing CTRL+ALT+F7.
       </para>
      </step>
      <step id="bomcjfg">
       <para>
        Click <guimenu>OK</guimenu> to continue with the installation
        reboot.
       </para>
      </step>
     </procedure>
    </sect3>
   </sect2>

   <sect2 id="bomc64f">
    <title>Enabling Multipath I/O for an Existing Root Device</title>
    <procedure id="b7cjqvq">
     <step id="b7cjqvr">
      <para>
       Install Linux with only a single path active, preferably one where
       the <filename>by-id</filename> symbolic links are listed in the
       partitioner.
      </para>
     </step>
     <step id="b7cjrle">
      <para>
       Mount the devices by using the <filename>/dev/disk/by-id</filename>
       path used during the install.
      </para>
     </step>
     <step id="b7cjs32">
      <para>
        Add dm-multipath to
	<filename>/etc/dracut.conf.d/01-dist.conf</filename> by adding the
	following line:
       </para>
       <screen>force_drivers+="dm-multipath"</screen>
     </step>
     <step id="bky5jc7">
      <para>
      <remark condition="clarity">
       2014-09-05 - fs: Check if the following is still true
      </remark>
       For System Z, before running <command>dracut</command>, edit the
       <filename>/etc/zipl.conf</filename> file to change the by-path
       information in <filename>zipl.conf</filename> with the same by-id
       information that was used in <filename>/etc/fstab</filename>.
      </para>
     </step>
     <step id="b7cjsl3">
      <para>
       Run <command>dracut <option>-f</option></command> to update the
       <filename>initrd</filename> image.
      </para>
     </step>
     <step id="bky5j1x">
      <para>
       For System Z, after running <command>dracut</command>, run
       <command>zipl</command>.
      </para>
     </step>
     <step id="b7cjuv4">
      <para>
       Reboot the server.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="bomc64g">
    <title>Disabling Multipath I/O on the Root Device</title>
    <procedure id="b7jqdde">
     <step id="b7jqddf">
      <para>
       Add <literal>multipath=off</literal> to the kernel command line.
      </para>
      <para>
       This affects only the root device. All other devices are not
       affected.
      </para>
     </step>
    </procedure>
   </sect2>
  </sect1>
  <sect1 id="mpioraid">
   <title>Configuring Multipath I/O for an Existing Software RAID</title>

   <para>
    Ideally, you should configure multipathing for devices before you use
    them as components of a software RAID device. If you add multipathing
    after creating any software RAID devices, the DM-MPIO service might be
    starting after the <command>multipath</command> service on reboot, which
    makes multipathing appear not to be available for RAIDs. You can use the
    procedure in this section to get multipathing running for a previously
    existing software RAID.
   </para>

   <para>
    For example, you might need to configure multipathing for devices in a
    software RAID under the following circumstances:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      If you create a new software RAID as part of the Partitioning settings
      during a new install or upgrade.
     </para>
    </listitem>
    <listitem>
     <para>
      If you did not configure the devices for multipathing before using
      them in the software RAID as a member device or spare.
     </para>
    </listitem>
    <listitem>
     <para>
      If you grow your system by adding new HBA adapters to the server or
      expanding the storage subsystem in your SAN.
     </para>
    </listitem>
   </itemizedlist>

   <note>
    <para>
     The following instructions assume the software RAID device is
     <filename>/dev/mapper/mpath0&gt;</filename>, which is its device name
     as recognized by the kernel. It assumes you have enabled
     user-friendly-names in the <filename>/etc/multipath.conf</filename>
     file as described in
     <xref linkend="mpionames" xrefstyle="HeadingOnPage"/>.
    </para>
    <para>
     Ensure that you modify the instructions for the device name of your
     software RAID.
    </para>
   </note>

   <procedure id="b59pkd5">
    <step id="b59qv1e">
     <para role="intro">
      Open a terminal console, then log in as the
      <systemitem>root</systemitem> user or equivalent.
     </para>
     <para>
      Except where otherwise directed, use this console to enter the
      commands in the following steps.
     </para>
    </step>
    <step id="b59qezr">
     <para>
      If any software RAID devices are currently mounted or running, enter
      the following commands for each device to dismount the device and stop
      it.
     </para>
<screen>
umount /dev/mapper/mpath0
</screen>
<screen>
mdadm --misc --stop /dev/mapper/mpath0
</screen>
    </step>
    <step id="b59q95b">
     <para>
      Stop the <command>md</command> service by entering
     </para>
<screen>systemctl stop md.service</screen>
<!--taroth 2014-03-19: systemd - according to fcrozat, no more boot.* stuff:<screen>
/etc/init.d/boot.md stop
</screen>
-->
    </step>
    <step id="b59qeuu">
     <para>
      Start the <filename>multipath</filename> and
      <filename>multipathd</filename> services by entering the following
      commands:
     </para>
<screen>systemctl start multipath.service multipathd.service 
     </screen>
<!--taroth 2014-03-19: systemd - according to fcrozat, no more boot.* stuff:
<screen>
/etc/init.d/boot.multipath start
</screen>
<screen>
/etc/init.s/multipathd start
</screen>-->
    </step>
    <step id="b59qp65">
     <para>
      After the multipathing services are started, verify that the software
      RAID’s component devices are listed in the
      <filename>/dev/disk/by-id</filename> directory. Do one of the
      following:
     </para>
     <itemizedlist>
      <listitem>
       <formalpara id="b59qrp7">
        <title>Devices Are Listed:</title>
        <para>
         The device names should now have symbolic links to their Device
         Mapper Multipath device names, such as
         <filename>/dev/dm-1</filename>.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="b59qrx0">
        <title>Devices Are Not Listed:</title>
        <para>
         Force the multipath service to recognize them by flushing and
         rediscovering the devices.
        </para>
       </formalpara>
       <para>
        To do this, enter the following commands:
       </para>
<screen>
multipath -F
</screen>
<screen>
multipath -v0
</screen>
       <para>
        The devices should now be listed in
        <filename>/dev/disk/by-id</filename>, and have symbolic links to
        their Device Mapper Multipath device names. For example:
       </para>
<screen>
lrwxrwxrwx 1 root root 10 2011-01-06 11:42 dm-uuid-mpath-36006016088d014007e0d0d2213ecdf11 -&gt; ../../dm-1
</screen>
      </listitem>
     </itemizedlist>
    </step>
    <step id="b59qtrw">
     <para>
      Restart the <filename>md</filename> service and the RAID device by
      entering
     </para>
<screen>systemctl start md.service</screen>
<!--taroth 2014-03-19: systemd - according to fcrozat, no more boot.* stuff:
<screen>
/etc/init.d/boot.md start
</screen>
-->
    </step>
    <step id="b59qu7z">
     <para>
      Check the status of the software RAID by entering
     </para>
<screen>
mdadm --detail /dev/mapper/mpath0
</screen>
     <para>
      The RAID’s component devices should match their Device Mapper
      Multipath device names that are listed as the symbolic links of
      devices in the <filename>/dev/disk/by-id</filename> directory.
     </para>
    </step>
    <step id="b59r62x">
     <para>
      Make a new <filename>initrd</filename> to ensure that the Device
      Mapper Multipath services are loaded before the RAID services on
      reboot. Running <command>dracut</command> is needed only if the root
      (/) device or any parts of it (such as <filename>/var</filename>,
      <filename>/etc</filename>, <filename>/log</filename>) are on the SAN
      and multipath is needed to boot.
     </para>
     <para>
      Enter
     </para>
<screen>
dracut -f --add-drivers multipath
</screen>
    </step>
    <step id="b59rekg">
     <para>
      Reboot the server to apply these post-install configuration settings.
     </para>
    </step>
    <step id="b59rf4m">
     <para>
      Verify that the software RAID array comes up properly on top of the
      multipathed devices by checking the RAID status. Enter
     </para>
<screen>
mdadm --detail /dev/mapper/mpath0
</screen>
     <para>
      For example:
     </para>
     <simplelist>
      <member><literal>Number Major Minor RaidDevice State</literal>
      </member>
      <member><literal>0 253 0 0 active sync /dev/dm-0</literal>
      </member>
      <member><literal>1 253 1 1 active sync /dev/dm-1</literal>
      </member>
      <member><literal>2 253 2 2 active sync /dev/dm-2</literal>
      </member>
     </simplelist>
    </step>
   </procedure>
  </sect1>
  <sect1 id="scandev">
   <title>Scanning for New Devices without Rebooting</title>

   <para>
    If your system has already been configured for multipathing and you
    later need to add more storage to the SAN, you can use the
    <command>rescan-scsi-bus.sh</command> script to scan for the new
    devices. By default, this script scans all HBAs with typical LUN ranges.
   </para>

   <warning>
    <para>
     In EMC PowerPath environments, do not use the
     <filename>rescan-scsi-bus.sh</filename> utility provided with the
     operating system or the HBA vendor scripts for scanning the SCSI buses.
     To avoid potential file system corruption, EMC requires that you follow
     the procedure provided in the vendor documentation for EMC PowerPath
     for Linux.
    </para>
   </warning>

   <bridgehead id="boimjzp">Syntax</bridgehead>

<screen>
rescan-scsi-bus.sh [options] [host [host ...]]
</screen>

   <para>
    You can specify hosts on the command line (deprecated), or use the
    <literal>--hosts=LIST</literal> option (recommended).
   </para>

   <bridgehead id="boimk55">Options</bridgehead>

   <para>
    For most storage subsystems, the script can be run successfully without
    options. However, some special cases might need to use one or more of
    the following parameters for the <command>rescan-scsi-bus.sh</command>
    script:
   </para>

   <informaltable frame="topbot" rowsep="1" pgwide="0">
    <tgroup cols="2">
     <colspec colnum="1" colname="1" colwidth="2381*"/>
     <colspec colnum="2" colname="2" colwidth="7620*"/>
     <thead>
      <row id="boiml8l">
       <entry>
        <para>
         Option
        </para>
       </entry>
       <entry>
        <para>
         Description
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row id="boiml8m">
       <entry>
<screen>
-l
</screen>
       </entry>
       <entry>
        <para>
         Activates scanning for LUNs 0-7. [Default: 0]
        </para>
       </entry>
      </row>
      <row id="boiml8n">
       <entry>
<screen>
-L NUM
</screen>
       </entry>
       <entry>
        <para>
         Activates scanning for LUNs 0 to NUM. [Default: 0]
        </para>
       </entry>
      </row>
      <row id="boiml8o">
       <entry>
<screen>
-w
</screen>
       </entry>
       <entry>
        <para>
         Scans for target device IDs 0 to 15. [Default: 0 to 7]
        </para>
       </entry>
      </row>
      <row id="boimoae">
       <entry>
<screen>
-c
</screen>
       </entry>
       <entry>
        <para>
         Enables scanning of channels 0 or 1. [Default: 0]
        </para>
       </entry>
      </row>
      <row id="boimoaf">
       <entry>
<screen>
-r
--remove
</screen>
       </entry>
       <entry>
        <para>
         Enables removing of devices. [Default: Disabled]
        </para>
       </entry>
      </row>
      <row id="boimoag">
       <entry>
<screen>
-i
--issueLip
</screen>
       </entry>
       <entry>
        <para>
         Issues a Fibre Channel LIP reset. [Default: Disabled]
        </para>
       </entry>
      </row>
      <row id="boimoah">
       <entry>
<screen>
--forcerescan
</screen>
       </entry>
       <entry>
        <para>
         Rescans existing devices.
        </para>
       </entry>
      </row>
      <row id="boimoai">
       <entry>
<screen>
--forceremove
</screen>
       </entry>
       <entry>
        <para>
         Removes and re-adds every device.
        </para>
        <warning>
         <para>
          Use with caution, this option is dangerous.
         </para>
        </warning>
       </entry>
      </row>
      <row id="boimoaj">
       <entry>
<screen>
--nooptscan
</screen>
       </entry>
       <entry>
        <para>
         Don’t stop looking for LUNs if 0 is not found.
        </para>
       </entry>
      </row>
      <row id="boj5t9m">
       <entry>
<screen>
--color
</screen>
       </entry>
       <entry>
        <para>
         Use colored prefixes OLD/NEW/DEL.
        </para>
       </entry>
      </row>
      <row id="boj5t9n">
       <entry>
<screen>
--hosts=LIST
</screen>
       </entry>
       <entry>
        <para>
         Scans only hosts in LIST, where LIST is a comma-separated list of
         single values and ranges. No spaces are allowed.
        </para>
<screen>
--hosts=A[-B][,C[-D]]
</screen>
       </entry>
      </row>
      <row id="boj5t9o">
       <entry>
<screen>
--channels=LIST
</screen>
       </entry>
       <entry>
        <para>
         Scans only channels in LIST, where LIST is a comma-separated list
         of single values and ranges. No spaces are allowed. 
        </para>
<screen>
--channels=A[-B][,C[-D]]
</screen>
       </entry>
      </row>
      <row id="boj5t9p">
       <entry>
<screen>
--ids=LIST
</screen>
       </entry>
       <entry>
        <para>
         Scans only target IDs in LIST, where LIST is a comma-separated list
         of single values and ranges. No spaces are allowed. 
        </para>
<screen>
--ids=A[-B][,C[-D]]
</screen>
       </entry>
      </row>
      <row id="boimoak">
       <entry>
<screen>
--luns=LIST
</screen>
       </entry>
       <entry>
        <para>
         Scans only LUNs in LIST, where LIST is a comma-separated list of
         single values and ranges. No spaces are allowed.
        </para>
<screen>
--luns=A[-B][,C[-D]]
</screen>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>

   <bridgehead id="boj5xxj">Procedure</bridgehead>

   <para>
    Use the following procedure to scan the devices and make them available
    to multipathing without rebooting the system.
   </para>

   <procedure id="b59rt6j">
    <step id="b59rt6k">
     <para>
      On the storage subsystem, use the vendor’s tools to allocate the
      device and update its access control settings to allow the Linux
      system access to the new storage. Refer to the vendor’s
      documentation for details.
     </para>
    </step>
    <step id="b59rw69">
     <para>
      Scan all targets for a host to make its new device known to the middle
      layer of the Linux kernel’s SCSI subsystem. At a terminal console
      prompt, enter
     </para>
<screen>
rescan-scsi-bus.sh [options]
</screen>
    </step>
    <step id="b8ru7x3">
     <para>
      Check for scanning progress in the &systemd; journal (see
      <xref
      linkend="cha.journalctl"/>). At a terminal console prompt,
      enter
     </para>
<screen>
journalctl -r
</screen>
     <para>
      This command displays the last lines of the log. For example:
     </para>
<screen>
# journalctl -r
Feb 14 01:03 kernel: SCSI device sde: 81920000
Feb 14 01:03 kernel: SCSI device sdf: 81920000
Feb 14 01:03 multipathd: sde: path checker registered
Feb 14 01:03 multipathd: sdf: path checker registered
Feb 14 01:03 multipathd: mpath4: event checker started
Feb 14 01:03 multipathd: mpath5: event checker started
Feb 14 01:03:multipathd: mpath4: remaining active paths: 1
Feb 14 01:03 multipathd: mpath5: remaining active paths: 1
[...]
</screen>
    </step>
    <step id="b8ruapd">
     <para>
      Repeat <xref linkend="b59rw69" xrefstyle="StepXRef"/> through
      <xref linkend="b8ru7x3" xrefstyle="StepXRef"/> to add paths through
      other HBA adapters on the Linux system that are connected to the new
      device.
     </para>
    </step>
    <step id="b59ryjx">
     <para>
      Run the <command>multipath</command> command to recognize the devices
      for DM-MPIO configuration. At a terminal console prompt, enter
     </para>
<screen>
multipath
</screen>
     <para>
      You can now configure the new device for multipathing.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="be48i9g">
   <title>Scanning for New Partitioned Devices without Rebooting</title>

   <para role="intro">
    Use the example in this section to detect a newly added multipathed LUN
    without rebooting.
   </para>

   <warning>
    <para>
     In EMC PowerPath environments, do not use the
     <filename>rescan-scsi-bus.sh</filename> utility provided with the
     operating system or the HBA vendor scripts for scanning the SCSI buses.
     To avoid potential file system corruption, EMC requires that you follow
     the procedure provided in the vendor documentation for EMC PowerPath
     for Linux.
    </para>
   </warning>

   <procedure id="befqdst">
    <step id="befqdsu">
     <para>
      Open a terminal console, then log in as the
      <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="boj61wx">
     <para>
      Scan all targets for a host to make its new device known to the middle
      layer of the Linux kernel’s SCSI subsystem. At a terminal console
      prompt, enter
     </para>
<screen>
rescan-scsi-bus.sh [options]
</screen>
     <para>
      For syntax and options information for the
      <filename>rescan-scsi-bus-sh</filename> script, see
      <xref linkend="scandev" xrefstyle="SectTitleOnPage"/>.
     </para>
    </step>
    <step id="befqeer">
     <para>
      Verify that the device is seen (such as if the link has a new time
      stamp) by entering
     </para>
<screen>
ls -lrt /dev/dm-*
</screen>
     <para>
      You can also verify the devices in
      <filename>/dev/disk/by-id</filename> by entering
     </para>
<screen>
ls -l /dev/disk/by-id/
</screen>
    </step>
    <step id="befqfh9">
     <para>
      Verify the new device appears in the log by entering
     </para>
<screen>
journalctl -r
</screen>
    </step>
    <step id="befqhvc">
     <para>
      Use a text editor to add a new alias definition for the device in the
      <filename>/etc/multipath.conf</filename> file, such as
      <filename>data_vol3</filename>.
     </para>
     <para>
      For example, if the UUID is
      <filename>36006016088d014006e98a7a94a85db11</filename>, make the
      following changes:
     </para>
<screen>
defaults {
     user_friendly_names   yes
  }
multipaths {  
     multipath {      
          wwid    36006016088d014006e98a7a94a85db11      
          alias  data_vol3      
          }
  }
</screen>
    </step>
    <step id="befqj91">
     <para>
      Create a partition table for the device by entering
     </para>
<screen>
fdisk /dev/disk/by-id/dm-uuid-mpath-&lt;UUID&gt;
</screen>
     <para>
      Replace UUID with the device WWID, such as
      <filename>36006016088d014006e98a7a94a85db11</filename>.
     </para>
    </step>
    <step id="befqjhp">
     <para>
      Trigger udev by entering
     </para>
<screen>
echo 'add' &gt; /sys/block/&lt;<replaceable>dm_device</replaceable>&gt;/uevent
</screen>
     <para>
      For example, to generate the device-mapper devices for the partitions
      on <filename>dm-8</filename>, enter
     </para>
<screen>
echo 'add' &gt; /sys/block/dm-8/uevent
</screen>
    </step>
    <step id="befqkj8">
     <para>
      Create a file system and label for the new partition by entering the
      following commands:
     </para>
<screen>
mke2fs -j /dev/disk/by-id/dm-uuid-mpath-&lt;<replaceable>UUID_partN</replaceable>&gt;
tune2fs -L <replaceable>data_vol3</replaceable> /dev/disk/by-id/dm-uuid-&lt;<replaceable>UUID_partN</replaceable>&gt;
</screen>
     <para>
      Replace <filename>UUID_part1</filename> with the actual UUID and
      partition number, such as 36006016088d014006e98a7a94a85db11_part1.
     </para>
    </step>
    <step id="befqljm">
     <para>
      Restart DM-MPIO to let it read the aliases by entering
     </para>
<screen>
sudo systemctl restart multipathd.service
</screen>
    </step>
    <step id="befqmbi">
     <para role="intro">
      Verify that the device is recognized by <command>multipathd</command>
      by entering
     </para>
<screen>
multipath -ll
</screen>
    </step>
    <step id="befqmno">
     <para>
      Use a text editor to add a mount entry in the
      <filename>/etc/fstab</filename> file.
     </para>
     <para>
      At this point, the alias you created in
      <xref linkend="befqhvc" xrefstyle="StepXRef"/> is not yet in the
      <filename>/dev/disk/by-label</filename> directory. Add the mount entry
      the <filename>/dev/dm-9</filename> path, then change the entry before
      the next time you reboot to
     </para>
<screen>
LABEL=data_vol3
</screen>
    </step>
    <step id="befqr98">
     <para>
      Create a directory to use as the mount point, then mount the device by
      entering
     </para>
<screen>
md <replaceable>/data_vol3</replaceable>
</screen>
<screen>
mount <replaceable>/data_vol3</replaceable>
</screen>
    </step>
   </procedure>
  </sect1>
  <sect1 id="mpiostatus">
   <title>Viewing Multipath I/O Status</title>

   <para>
    Querying the multipath I/O status outputs the current status of the
    multipath maps.
   </para>

   <para>
    The <command>multipath -l</command> option displays the current path
    status as of the last time that the path checker was run. It does not
    run the path checker.
   </para>

   <para>
    The <command>multipath -ll</command> option runs the path checker,
    updates the path information, then displays the current status
    information. This option always the displays the latest information
    about the path status.
   </para>

   <procedure id="b8ruq0c">
    <step id="b8ruq0d">
     <para>
      At a terminal console prompt, enter
     </para>
<screen>
multipath -ll
</screen>
     <para>
      This displays information for each multipathed device. For example:
     </para>
<screen>
3600601607cf30e00184589a37a31d911
[size=127 GB][features="0"][hwhandler="1 emc"]
</screen>
<screen>
\_ round-robin 0 [active][first]
  \_ 1:0:1:2 sdav 66:240  [ready ][active]
  \_ 0:0:1:2 sdr  65:16   [ready ][active]
</screen>
<screen>
\_ round-robin 0 [enabled]
  \_ 1:0:0:2 sdag 66:0    [ready ][active]
  \_ 0:0:0:2 sdc  8:32    [ready ][active]
</screen>
    </step>
   </procedure>

   <para>
    For each device, it shows the device’s ID, size, features, and
    hardware handlers.
   </para>

   <para>
    Paths to the device are automatically grouped into priority groups on
    device discovery. Only one priority group is active at a time. For an
    active/active configuration, all paths are in the same group. For an
    active/passive configuration, the passive paths are placed in separate
    priority groups.
   </para>

   <para>
    The following information is displayed for each group:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      Scheduling policy used to balance I/O within the group, such as
      round-robin
     </para>
    </listitem>
    <listitem>
     <para>
      Whether the group is active, disabled, or enabled
     </para>
    </listitem>
    <listitem>
     <para>
      Whether the group is the first (highest priority) group
     </para>
    </listitem>
    <listitem>
     <para>
      Paths contained within the group
     </para>
    </listitem>
   </itemizedlist>

   <para>
    The following information is displayed for each path:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      The physical address as
      <replaceable>host:bus:target:lun</replaceable>, such as 1:0:1:2
     </para>
    </listitem>
    <listitem>
     <para>
      Device node name, such as <filename>sda</filename>
     </para>
    </listitem>
    <listitem>
     <para>
      Major:minor numbers
     </para>
    </listitem>
    <listitem>
     <para>
      Status of the device
     </para>
    </listitem>
   </itemizedlist>
  </sect1>
  <sect1 id="mpioerrormgmt">
   <title>Managing I/O in Error Situations</title>

   <para>
    You might need to configure multipathing to queue I/O if all paths fail
    concurrently by enabling queue_if_no_path. Otherwise, I/O fails
    immediately if all paths are gone. In certain scenarios, where the
    driver, the HBA, or the fabric experience spurious errors, DM-MPIO
    should be configured to queue all I/O where those errors lead to a loss
    of all paths, and never propagate errors upward.
   </para>

   <para>
    When you use multipathed devices in a cluster, you might choose to
    disable queue_if_no_path. This automatically fails the path instead of
    queuing the I/O, and escalates the I/O error to cause a failover of the
    cluster resources.
   </para>

   <para>
    Because enabling queue_if_no_path leads to I/O being queued indefinitely
    unless a path is reinstated, ensure that <command>multipathd</command>
    is running and works for your scenario. Otherwise, I/O might be stalled
    indefinitely on the affected multipathed device until reboot or until
    you manually return to failover instead of queuing.
   </para>

   <para>
    To test the scenario:
   </para>

   <procedure id="b5c8xoe">
    <step id="b7jqv1b">
     <para>
      In a terminal console, log in as the <systemitem>root</systemitem>
      user.
     </para>
    </step>
    <step id="b7jqw8e">
     <para>
      Activate queuing instead of failover for the device I/O by entering:
     </para>
<screen>
dmsetup message <replaceable>device_ID</replaceable> 0 queue_if_no_path
</screen>
     <para>
      Replace the <replaceable>device_ID</replaceable> with the ID for your
      device. The 0 value represents the sector and is used when sector
      information is not needed.
     </para>
     <para>
      For example, enter:
     </para>
<screen>
dmsetup message 3600601607cf30e00184589a37a31d911 0 queue_if_no_path
</screen>
    </step>
    <step id="b7jqwaz">
     <para>
      Return to failover for the device I/O by entering:
     </para>
<screen>
dmsetup message <replaceable>device_ID</replaceable> 0 fail_if_no_path
</screen>
     <para>
      This command immediately causes all queued I/O to fail.
     </para>
     <para>
      Replace the <replaceable>device_ID</replaceable> with the ID for your
      device. For example, enter:
     </para>
<screen>
dmsetup message 3600601607cf30e00184589a37a31d911 0 fail_if_no_path
</screen>
    </step>
   </procedure>

   <para>
    To set up queuing I/O for scenarios where all paths fail:
   </para>

   <procedure id="b7jqzyt">
    <step id="b7jr0j0">
     <para>
      In a terminal console, log in as the <systemitem>root</systemitem>
      user.
     </para>
    </step>
    <step id="b5c9aza">
     <para>
      Open the <filename>/etc/multipath.conf</filename> file in a text
      editor.
     </para>
    </step>
    <step id="b5c99rh">
     <para>
      Uncomment the defaults section and its ending bracket, then add the
      <literal>default_features</literal> setting, as follows:
     </para>
<screen>
defaults {
  default_features "1 queue_if_no_path"
}
</screen>
    </step>
    <step id="bi0b03b">
     <para>
      After you modify the <filename>/etc/multipath.conf</filename> file, you
      must run <command>dracut <option>-f</option></command> to re-create the
      <filename>initrd</filename> on your system, then reboot in order for the
      changes to take effect.
     </para>
    </step>
    <step id="b7jr4r4">
     <para>
      When you are ready to return over to failover for the device I/O,
      enter:
     </para>
<screen>
dmsetup message <replaceable>mapname</replaceable> 0 fail_if_no_path
</screen>
     <para>
      Replace the <replaceable>mapname</replaceable> with the mapped alias
      name or the device ID for the device. The 0 value represents the
      sector and is used when sector information is not needed.
     </para>
     <para>
      This command immediately causes all queued I/O to fail and propagates
      the error to the calling application.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="mpiostall">
   <title>Resolving Stalled I/O</title>

   <para>
    If all paths fail concurrently and I/O is queued and stalled, do the
    following:
   </para>

   <procedure id="b7jlq1g">
    <step id="b7jlq1h">
     <para>
      Enter the following command at a terminal console prompt:
     </para>
<screen>
dmsetup message <replaceable>mapname</replaceable> 0 fail_if_no_path
</screen>
     <para>
      Replace <literal><replaceable>mapname</replaceable></literal> with the
      correct device ID or mapped alias name for the device. The 0 value
      represents the sector and is used when sector information is not
      needed.
     </para>
     <para>
      This command immediately causes all queued I/O to fail and propagates
      the error to the calling application.
     </para>
    </step>
    <step id="b7jlq74">
     <para>
      Reactivate queueing by entering the following command at a terminal
      console prompt:
     </para>
<screen>
dmsetup message <replaceable>mapname</replaceable> 0 queue_if_no_path
</screen>
    </step>
   </procedure>
  </sect1>
  <sect1 id="bpjpirk">
   <title>Troubleshooting MPIO</title>

   <para>
    This section describes some known issues and possible solutions for
    MPIO.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b11qkzgw" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b122uxzr" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b11qkzgx" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b11qkzgw">
    <title>PRIO Settings for Individual Devices Fail After Upgrading to Multipath 0.4.9</title>
    <para>
     Multipath Tools 0.4.9 for SLES 11 SP2 uses the <literal>prio</literal>
     setting in the <literal>defaults{}</literal> or
     <literal>devices{}</literal> section of the
     <filename>/etc/multipath.conf</filename> file. It silently ignores the
     keyword <literal>prio</literal> when it is specified for an individual
     <literal>multipath</literal> definition in the
     <literal>multipaths{)</literal> section.
    </para>
    <para>
     Multipath Tools 0.4.8 for SLES 11 SP1 and earlier allows the prio
     setting in the individual <literal>multipath</literal> definition in
     the <literal>multipaths{)</literal> section to override the
     <literal>prio</literal> settings in the <literal>defaults{}</literal>
     or <literal>devices{}</literal> section.
    </para>
   </sect2>

   <sect2 id="b122uxzr">
    <title>PRIO Settings with Arguments Fail After Upgrading to multipath-tools-0.4.9</title>
    <para>
     When you upgrade from <filename>multipath-tools-0.4.8</filename> to
     <filename>multipath-tools-0.4.9</filename>, the <literal>prio</literal>
     settings in the <filename>/etc/multipath.conf</filename> file are
     broken for prioritizers that require an argument. In
     multipath-tools-0.4.9, the <literal>prio</literal> keyword is used to
     specify the prioritizer, and the <literal>prio_args</literal> keyword
     is used to specify the argument for prioritizers that require an
     argument. Previously, both the prioritizer and its argument were
     specified on the same <literal>prio</literal> line.
    </para>
    <para>
     For example, in multipath-tools-0.4.8, the following line was used to
     specify a prioritizer and its arguments on the same line.
    </para>
<screen>
prio "weightedpath hbtl [1,3]:.:.+:.+ 260 [0,2]:.:.+:.+ 20"
</screen>
    <para role="intro">
     After upgrading to <command>multipath-tools-0.4.9</command>, the
     command causes an error. The message is similar to the following:
    </para>
<screen>
&lt;Month day hh:mm:ss&gt; | Prioritizer 'weightedpath hbtl [1,3]:.:.+:.+ 260
[0,2]:.:.+:.+ 20' not found in /lib64/multipath
</screen>
    <para>
     To resolve this problem, use a text editor to modify the
     <literal>prio</literal> line in the
     <filename>/etc/multipath.conf</filename> file. Create two lines with
     the prioritizer specified on the <filename>prio</filename> line, and
     the prioritizer argument specified on the
     <filename>prio_args</filename> line below it:
    </para>
<screen>
prio "weightedpath"
prio_args "hbtl [1,3]:.:.+:.+ 260 [0,2]:.:.+:.+ 20"
</screen>
   </sect2>

   <sect2 id="b11qkzgx">
    <title>Technical Information Documents</title>
    <para>
     For information about troubleshooting multipath I/O issues on SUSE
     Linux Enterprise Server, see the following Technical Information
     Documents (TIDs) in the &suse; Knowledgebase:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <ulink url="http://www.suse.com/support/kb/doc.php?id=3231766"><citetitle>Troubleshooting
       SLES Multipathing (MPIO) Problems (TID 3231766)</citetitle></ulink>
      </para>
     </listitem>
     <listitem>
      <para>
       <ulink url="http://www.suse.com/support/kb/doc.php?id=3029706"><citetitle>DM
       MPIO Device Blacklisting Not Honored in multipath.conf (TID
       3029706)</citetitle></ulink>
      </para>
     </listitem>
     <listitem>
      <para>
       <ulink url="http://www.suse.com/support/kb/doc.php?id=3955167"><citetitle>Troubleshooting
       SCSI (LUN) Scanning Issues (TID 3955167)</citetitle></ulink>
      </para>
     </listitem>
     <listitem>
      <para>
       <ulink url="http://www.suse.com/support/kb/doc.php?id=7007498"><citetitle>Using
       LVM on Multipath (DM MPIO) Devices</citetitle></ulink>
      </para>
     </listitem>
    </itemizedlist>
   </sect2>
  </sect1>
  <sect1 id="mpionext">
   <title>What’s Next</title>

   <para>
    If you want to use software RAIDs, create and configure them before you
    create file systems on the devices. For information, see the following:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      <xref linkend="raidyast" xrefstyle="ChapTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="raidmdadm" xrefstyle="ChapTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>
  </sect1>
 </chapter>
 <chapter id="raidyast" lang="en">
  <title>Software RAID Configuration</title>
  <para>
   The purpose of RAID (redundant array of independent disks) is to combine
   several hard disk partitions into one large virtual hard disk to optimize
   performance, data security, or both. Most RAID controllers use the SCSI
   protocol because it can address a larger number of hard disks in a more
   effective way than the IDE protocol and is more suitable for parallel
   processing of commands. There are some RAID controllers that support IDE
   or SATA hard disks. Software RAID provides the advantages of RAID systems
   without the additional cost of hardware RAID controllers. However, this
   requires some CPU time and has memory requirements that make it
   unsuitable for real high performance computers.
  </para>
  <important>
   <para>
    Software RAID is not supported underneath clustered file systems such as
    OCFS2, because RAID does not support concurrent activation. If you want
    RAID for OCFS2, you need the RAID to be handled by the storage
    subsystem.
   </para>
  </important>
  <para>
   SUSE Linux Enterprise offers the option of combining several hard disks
   into one soft RAID system. RAID implies several strategies for combining
   several hard disks in a RAID system, each with different goals,
   advantages, and characteristics. These variations are commonly known as
   <emphasis>RAID levels</emphasis>.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="bi70wwi" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec.yast2.system.raid.yast" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bgchysg" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bgchysh" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="bi70wwi">
   <title>Understanding RAID Levels</title>

   <para>
    This section describes common RAID levels 0, 1, 2, 3, 4, 5, and nested
    RAID levels.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bgchysa" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bgchysb" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bgchysc" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bgchysd" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bgchyse" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bgchysf" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bgchysa">
    <title>RAID&nbsp;0</title>
    <para>
     This level improves the performance of your data access by spreading
     out blocks of each file across multiple disk drives. Actually, this is
     not really a RAID, because it does not provide data backup, but the
     name <emphasis>RAID&nbsp;0</emphasis> for this type of system has
     become the norm. With RAID&nbsp;0, two or more hard disks are pooled
     together. The performance is very good, but the RAID system is
     destroyed and your data lost if even one hard disk fails.
    </para>
   </sect2>

   <sect2 id="bgchysb">
    <title>RAID&nbsp;1</title>
    <para>
     This level provides adequate security for your data, because the data
     is copied to another hard disk 1:1. This is known as <emphasis>hard
     disk mirroring</emphasis>. If a disk is destroyed, a copy of its
     contents is available on another mirrored disk. All disks except one
     could be damaged without endangering your data. However, if damage is
     not detected, damaged data might be mirrored to the correct disk and
     the data is corrupted that way. The writing performance suffers a
     little in the copying process compared to when using single disk access
     (10 to 20 percent slower), but read access is significantly faster in
     comparison to any one of the normal physical hard disks, because the
     data is duplicated so can be scanned in parallel. RAID 1 generally
     provides nearly twice the read transaction rate of single disks and
     almost the same write transaction rate as single disks.
    </para>
   </sect2>

   <sect2 id="bgchysc">
    <title>RAID&nbsp;2 and RAID&nbsp;3</title>
    <para>
     These are not typical RAID implementations. Level&nbsp;2 stripes data
     at the bit level rather than the block level. Level&nbsp;3 provides
     byte-level striping with a dedicated parity disk and cannot service
     simultaneous multiple requests. Both levels are rarely used.
    </para>
   </sect2>

   <sect2 id="bgchysd">
    <title>RAID&nbsp;4</title>
    <para>
     Level&nbsp;4 provides block-level striping like Level&nbsp;0 combined
     with a dedicated parity disk. If a data disk fails, the parity data is
     used to create a replacement disk. However, the parity disk might
     create a bottleneck for write access. Nevertheless, Level&nbsp;4 is
     sometimes used.
    </para>
   </sect2>

   <sect2 id="bgchyse">
    <title>RAID&nbsp;5</title>
    <para>
     RAID&nbsp;5 is an optimized compromise between Level&nbsp;0 and
     Level&nbsp;1 in terms of performance and redundancy. The hard disk
     space equals the number of disks used minus one. The data is
     distributed over the hard disks as with RAID&nbsp;0. <emphasis>Parity
     blocks</emphasis>, created on one of the partitions, are there for
     security reasons. They are linked to each other with XOR, enabling the
     contents to be reconstructed by the corresponding parity block in case
     of system failure. With RAID&nbsp;5, no more than one hard disk can
     fail at the same time. If one hard disk fails, it must be replaced as
     soon as possible to avoid the risk of losing data.
    </para>
   </sect2>

<!-- bg: this is not a common raid level. Merging to SLE_100
   <varlistentry>
    <term>RAID&nbsp;6</term>
    <listitem>
     <para>
      To further increase the reliability of the RAID system, it is
      possible to use RAID&nbsp;6. In this level, even if two disks 
      fail, the array still can be reconstructed. With
      RAID&nbsp;6, at least 4 hard disks are needed to run the array.
      Note, that when running as software raid, this
      configuration needs a considerable amount of CPU time and
      memory.
     </para>
    </listitem>
   </varlistentry> -->

   <sect2 id="bgchysf">
    <title>Nested RAID Levels</title>
    <para>
     Several other RAID levels have been developed, such as RAIDn,
     RAID&nbsp;10, RAID&nbsp;0+1, RAID&nbsp;30, and RAID&nbsp;50. Some of
     them being proprietary implementations created by hardware vendors.
     These levels are not very widespread, and are not explained here.
    </para>
   </sect2>
  </sect1>
  <sect1 id="sec.yast2.system.raid.yast">
   <title>Soft RAID Configuration with YaST</title>

   <para>
    The YaST soft RAID configuration can be reached from the YaST Expert
    Partitioner. This partitioning tool enables you to edit and delete
    existing partitions and create new ones that should be used with soft
    RAID.
   </para>

   <para>
    You can create RAID partitions by first clicking <menuchoice>
    <guimenu>Create</guimenu><guimenu>Do not format</guimenu></menuchoice>
    then selecting <guimenu>0xFD Linux RAID</guimenu> as the partition
    identifier. For RAID&nbsp;0 and RAID&nbsp;1, at least two partitions are
    needed&mdash;for RAID&nbsp;1, usually exactly two and no more. If
    RAID&nbsp;5 is used, at least three partitions are required. It is
    recommended to use only partitions of the same size because each segment
    can contribute only the same amount of space as the smallest sized
    partition. The RAID partitions should be stored on different hard disks
    to decrease the risk of losing data if one is defective (RAID&nbsp;1 and
    5) and to optimize the performance of RAID&nbsp;0. After creating all
    the partitions to use with RAID, click
    <menuchoice><guimenu>RAID</guimenu><guimenu>Create
    RAID</guimenu></menuchoice> to start the RAID configuration.
   </para>

   <para>
    On the next page of the wizard, choose among RAID levels 0, 1, and 5,
    then click <guimenu>Next</guimenu>. The following dialog (see
    <xref linkend="fig.yast2.raid2" xrefstyle="FigureXRef"/>) lists all
    partitions with either the <guimenu>Linux RAID</guimenu> or
    <guimenu>Linux native</guimenu> type. No swap or DOS partitions are
    shown. If a partition is already assigned to a RAID volume, the name of
    the RAID device (for example, <filename>/dev/md0</filename>) is shown in
    the list. Unassigned partitions are indicated with <quote>--</quote>.
   </para>

   <figure pgwide="0" id="fig.yast2.raid2">
    <title>RAID Partitions</title>
<!--RAID partitions-->
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast2_raid2_a.png" width="329pt" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast2_raid2_a.png" width="329pt" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>

   <para>
    To add a previously unassigned partition to the selected RAID volume,
    first select the partition then click <guimenu>Add</guimenu>. At this
    point, the name of the RAID device is displayed next to the selected
    partition. Assign all partitions reserved for RAID. Otherwise, the space
    on the partition remains unused. After assigning all partitions, click
    <guimenu>Next</guimenu> to proceed to the settings dialog where you can
    fine-tune the performance (see
    <xref linkend="fig.yast2.raid3" xrefstyle="FigureXRef"/>).
   </para>

   <figure pgwide="0" id="fig.yast2.raid3">
    <title>File System Settings</title>
<!--File System Settings-->
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast2_raid3_a.png" width="329pt" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast2_raid3_a.png" width="329pt" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>

   <para>
    As with conventional partitioning, set the file system to use as well as
    encryption and the mount point for the RAID volume. After completing the
    configuration with <guimenu>Finish</guimenu>, see the
    <filename>/dev/md0</filename> device and others indicated with
    <emphasis>RAID</emphasis> in the Expert Partitioner.
   </para>
  </sect1>
  <sect1 id="bgchysg">
   <title>Troubleshooting Software RAIDs</title>

   <para>
    Check the <filename>/proc/mdstat</filename> file to find out whether a
    RAID partition has been damaged. In the event of a system failure, shut
    down your Linux system and replace the defective hard disk with a new
    one partitioned the same way. Then restart your system and enter the
    command <command>mdadm /dev/mdX --add /dev/sdX</command>. Replace
    <literal>X</literal> with your particular device identifiers. This
    integrates the hard disk automatically into the RAID system and fully
    reconstructs it.
   </para>

   <para>
    Although you can access all data during the rebuild, you might encounter
    some performance issues until the RAID has been fully rebuilt.
   </para>
  </sect1>
  <sect1 id="bgchysh">
   <title>For More Information</title>

   <para>
    Configuration instructions and more details for soft RAID can be found
    in the HOWTOs at:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      <ulink url="https://raid.wiki.kernel.org/index.php/Linux_Raid"><citetitle>Linux
      RAID wiki</citetitle></ulink>
     </para>
    </listitem>
    <listitem>
     <para>
      <citetitle>The Software RAID HOWTO</citetitle> in the
      <filename>/usr/share/doc/packages/mdadm/Software-RAID.HOWTO.html</filename>
      file
     </para>
    </listitem>
   </itemizedlist>

   <para>
    Linux RAID mailing lists are also available, such as
    <ulink url="http://marc.info/?l=linux-raid"><citetitle/>linux-raid</ulink>.
   </para>
  </sect1>
 </chapter>
 <chapter id="raidroot" lang="en">
  <title>Configuring Software RAID1 for the Root Partition</title>
  <para>
   In &productname;, the Device Mapper RAID tool has been
   integrated into the YaST Partitioner. You can use the partitioner at
   install time to create a software RAID1 for the system device that
   contains your root (<filename>/</filename>) partition. The
   <filename>/boot</filename> partition must be created on a separate device
   than the MD RAID1.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="bi9d2lj" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bi9dppp" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bi9dnnt" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bi9d19d" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="bi9d2lj">
   <title>Prerequisites for Using a Software RAID1 Device for the Root Partition</title>

   <para>
    Ensure that your configuration meets the following requirements:
   </para>

   <itemizedlist mark="box">
    <listitem>
     <para>
      You need two hard drives to create the RAID1 mirror device. The hard
      drives should be similarly sized. The RAID assumes the size of the
      smaller drive. The block storage devices can be any combination of
      local (in or directly attached to the machine), Fibre Channel storage
      subsystems, or iSCSI storage subsystems.
     </para>
    </listitem>
    <listitem>
     <para>
      You need a third device to use for the <filename>/boot
      </filename>partition. The boot device should be a local device.
     </para>
    </listitem>
    <listitem>
     <para>
      If you are using hardware RAID devices, do not attempt to run software
      RAIDs on top of it.
     </para>
    </listitem>
    <listitem>
     <para>
      If you are using iSCSI target devices, you must enable the iSCSI
      initiator support before you create the RAID device.
     </para>
    </listitem>
    <listitem>
     <para>
      If your storage subsystem provides multiple I/O paths between the
      server and its directly attached local devices, Fibre Channel devices,
      or iSCSI devices that you want to use in the software RAID, you must
      enable the multipath support before you create the RAID device.
     </para>
    </listitem>
   </itemizedlist>
  </sect1>
  <sect1 id="bi9dppp">
   <title>Enabling iSCSI Initiator Support at Install Time</title>

   <para>
    If there are iSCSI target devices that you want to use for the root (/)
    partition, you must enable the iSCSI Initiator software to make those
    devices available to you before you create the software RAID1 device.
   </para>

   <procedure id="bi9dppq">
    <step id="bi9dppr">
     <para>
      Proceed with the YaST install of &productname; until you
      reach the Installation Settings page.
     </para>
    </step>
    <step id="bi9dpps">
     <para>
      Click <guimenu>Partitioning</guimenu> to open the Preparing Hard Disk
      page, click <guimenu>Custom Partitioning (for experts)</guimenu>, then
      click <guimenu>Next</guimenu>.
     </para>
    </step>
    <step id="bi9dppt">
     <para>
      On the Expert Partitioner page, expand <guimenu>Hard Disks</guimenu>
      in the <guimenu>System View</guimenu> panel to view the default
      proposal.
     </para>
    </step>
    <step id="bi9dppu">
     <para>
      On the <guimenu>Hard Disks</guimenu> page, select
      <guimenu>Configure</guimenu><guimenu>Configure iSCSI</guimenu>, then
      click <guimenu>Continue</guimenu> when prompted to continue with
      initializing the iSCSI initiator configuration.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="bi9dnnt">
   <title>Enabling Multipath I/O Support at Install Time</title>

   <para>
    If there are multiple I/O paths to the devices that you want to use for
    the root (/) partition, you must enable multipath support before you
    create the software RAID1 device.
   </para>

   <procedure id="bi9dnnu">
    <step id="bi9dnnv">
     <para>
      Proceed with the YaST install of &productname; until you
      reach the Installation Settings page.
     </para>
    </step>
    <step id="bi9dnnw">
     <para>
      Click <guimenu>Partitioning</guimenu> to open the Preparing Hard Disk
      page, click <guimenu>Custom Partitioning (for experts)</guimenu>, then
      click <guimenu>Next</guimenu>.
     </para>
    </step>
    <step id="bi9dnnx">
     <para>
      On the Expert Partitioner page, expand <guimenu>Hard Disks</guimenu>
      in the <guimenu>System View</guimenu> panel to view the default
      proposal.
     </para>
    </step>
    <step id="bi9dnny">
     <para>
      On the <guimenu>Hard Disks</guimenu> page, select
      <guimenu>Configure</guimenu><guimenu>Configure Multipath</guimenu>,
      then click <guimenu>Yes</guimenu> when prompted to activate multipath.
     </para>
     <para>
      This re-scans the devices and resolves the multiple paths so that each
      device is listed only once in the list of hard disks.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="bi9d19d">
   <title>Creating a Software RAID1 Device for the Root (/) Partition</title>

   <procedure id="bi9d207">
    <step id="bi9d208">
     <para>
      Proceed with the YaST install of &productname; until you
      reach the Installation Settings page.
     </para>
    </step>
    <step id="bi9dfe6">
     <para>
      Click <guimenu>Partitioning</guimenu> to open the Preparing Hard Disk
      page, click <guimenu>Custom Partitioning (for experts)</guimenu>, then
      click <guimenu>Next</guimenu>.
     </para>
    </step>
    <step id="bi9dgrg">
     <para>
      On the Expert Partitioner page, expand <guimenu>Hard Disks</guimenu>
      in the <guimenu>System View</guimenu> panel to view the default
      proposal, select the proposed partitions, then click
      <guimenu>Delete</guimenu>.
     </para>
    </step>
    <step id="b13za1ue">
     <para>
      Create a <filename>/boot</filename> partition.
     </para>
     <substeps>
      <step id="b13za3rl">
       <para>
        On the Expert Partitioner page under <guimenu>Hard Disks</guimenu>,
        select the device you want to use for the /boot partition, then
        click <guimenu>Add</guimenu> on the <guimenu>Hard Disk
        Partitions</guimenu> tab.
       </para>
      </step>
      <step id="b13za3rm">
       <para>
        Under <guimenu>New Partition Type</guimenu>, select <guimenu>Primary
        Partition</guimenu>, then click <guimenu>Next</guimenu>.
       </para>
      </step>
      <step id="b13za3rn">
       <para>
        Under <guimenu>New Partition Size</guimenu>, specify the size to
        use, then click <guimenu>Next</guimenu>.
       </para>
      </step>
      <step id="b13za3ro">
       <para>
        Under <guimenu>Format Options</guimenu>, select <guimenu>Format
        partition</guimenu>, then select your preferred file system type
        (such as Ext2 or Ext3) from the drop-down list.
       </para>
      </step>
      <step id="b13za3rp">
       <para>
        Under <guimenu>Mount Options</guimenu>, select <guimenu>Mount
        partition</guimenu>, then select <guimenu>/boot</guimenu> from the
        drop-down list.
       </para>
      </step>
      <step id="b13za3rq">
       <para>
        Click <guimenu>Finish</guimenu>.
       </para>
      </step>
     </substeps>
    </step>
    <step id="bi9drnn">
     <para>
      Create a swap partition.
     </para>
     <substeps>
      <step id="bi9dq0p">
       <para>
        On the Expert Partitioner page under <guimenu>Hard Disks</guimenu>,
        select the device you want to use for the swap partition, then click
        <guimenu>Add</guimenu> on the <guimenu>Hard Disk
        Partitions</guimenu> tab.
       </para>
      </step>
      <step id="bi9drt8">
       <para>
        Under <guimenu>New Partition Type</guimenu>, select <guimenu>Primary
        Partition</guimenu>, then click <guimenu>Next</guimenu>.
       </para>
      </step>
      <step id="bi9drz9">
       <para>
        Under <guimenu>New Partition Size</guimenu>, specify the size to
        use, then click <guimenu>Next</guimenu>.
       </para>
      </step>
      <step id="bi9dt2p">
       <para>
        Under <guimenu>Format Options</guimenu>, select <guimenu>Format
        partition</guimenu>, then select <guimenu>Swap</guimenu> from the
        drop-down list.
       </para>
      </step>
      <step id="bi9dtld">
       <para>
        Under <guimenu>Mount Options</guimenu>, select <guimenu>Mount
        partition</guimenu>, then select <guimenu>swap</guimenu> from the
        drop-down list.
       </para>
      </step>
      <step id="bi9du9n">
       <para>
        Click <guimenu>Finish</guimenu>.
       </para>
      </step>
     </substeps>
    </step>
    <step id="bi9dv06">
     <para>
      Set up the <guimenu>0xFD Linux RAID</guimenu> format for each of the
      devices you want to use for the software RAID1.
     </para>
     <substeps>
      <step id="bi9dx99">
       <para>
        On the Expert Partitioner page under <guimenu>Hard Disks</guimenu>,
        select the device you want to use in the RAID1, then click
        <guimenu>Add</guimenu> on the <guimenu>Hard Disk
        Partitions</guimenu> tab.
       </para>
      </step>
      <step id="bi9dx9a">
       <para>
        Under <guimenu>New Partition Type</guimenu>, select <guimenu>Primary
        Partition</guimenu>, then click <guimenu>Next</guimenu>.
       </para>
      </step>
      <step id="bi9dx9b">
       <para>
        Under <guimenu>New Partition Size</guimenu>, specify to use the
        maximum size, then click <guimenu>Next</guimenu>.
       </para>
      </step>
      <step id="bi9dx9c">
       <para>
        Under <guimenu>Format Options</guimenu>, select <guimenu>Do not
        format partition</guimenu>, then select <guimenu>0xFD Linux
        RAID</guimenu> from the drop-down list.
       </para>
      </step>
      <step id="bi9dx9d">
       <para>
        Under <guimenu>Mount Options</guimenu>, select <guimenu>Do not mount
        partition</guimenu>.
       </para>
      </step>
      <step id="bi9dx9e">
       <para>
        Click <guimenu>Finish</guimenu>.
       </para>
      </step>
      <step id="bi9dy2u">
       <para>
        Repeat <xref linkend="bi9dx99" xrefstyle="StepXRef"/> to
        <xref linkend="bi9dx9e" xrefstyle="StepXRef"/> for each device that
        you plan to use in the software RAID1.
       </para>
      </step>
     </substeps>
    </step>
    <step id="bi9e4kt">
     <para>
      Create the RAID device.
     </para>
     <substeps>
      <step id="bi9djfy">
       <para>
        In the <guimenu>System View</guimenu> panel, select
        <guimenu>RAID</guimenu>, then click <guimenu>Add RAID</guimenu> on
        the RAID page.
       </para>
       <para>
        The devices that you prepared in
        <xref linkend="bi9dv06" xrefstyle="StepXRef"/> are listed in
        <guimenu>Available Devices</guimenu>.
       </para>
       <informalfigure pgwide="0">
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="raid_yast_install_a.png" width="213pt" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="raid_yast_install_a.png" width="213pt" format="PNG"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
      </step>
      <step id="bi9dzpu">
       <para>
        Under <guimenu>RAID Type</guimenu>, select <guimenu>RAID 1
        (Mirroring)</guimenu>.
       </para>
      </step>
      <step id="bi9e26m">
       <para>
        In the <guimenu>Available Devices</guimenu> panel, select the
        devices you want to use for the RAID, then click
        <guimenu>Add</guimenu> to move the devices to the <guimenu>Selected
        Devices</guimenu> panel.
       </para>
       <para>
        Specify two or more devices for a RAID1.
       </para>
       <para>
        To continue the example, two devices are selected for RAID1.
       </para>
       <informalfigure pgwide="0">
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="raid_yast_install2_a.png" width="218pt" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="raid_yast_install2_a.png" width="218pt" format="PNG"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
      </step>
      <step id="bi9e393">
       <para>
        Click <guimenu>Next</guimenu>.
       </para>
      </step>
      <step id="bi9e3nr">
       <para>
        Under <guimenu>RAID Options</guimenu>, select the chunk size from
        the drop-down list.
       </para>
       <para>
        The default chunk size for a RAID1 (Mirroring) is 4 KB.
       </para>
       <para>
        Available chunk sizes are 4 KB, 8 KB, 16 KB, 32 KB, 64 KB, 128 KB,
        256 KB, 512 KB, 1 MB, 2 MB, or 4 MB.
       </para>
      </step>
      <step id="bi9e530">
       <para>
        Under <guimenu>Formatting Options</guimenu>, select <guimenu>Format
        partition</guimenu>, then select the file system type (such as Ext3)
        from the <guimenu>File system</guimenu> drop-down list.
       </para>
      </step>
      <step id="bi9e97s">
       <para>
        Under <guimenu>Mounting Options</guimenu>, select <guimenu>Mount
        partition</guimenu>, then select <filename>/ </filename>from the
        <guimenu>Mount Point</guimenu> drop-down list.
       </para>
      </step>
      <step id="bi9e9uc">
       <para>
        Click <guimenu>Finish</guimenu>.
       </para>
       <para>
        The software RAID device is managed by Device Mapper, and creates a
        device under the <filename>/dev/md0</filename> path.
       </para>
       <informalfigure pgwide="0">
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="raid_yast_install3_a.png" width="213pt" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="raid_yast_install3_a.png" width="213pt" format="PNG"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
      </step>
     </substeps>
    </step>
    <step id="bi9ec68">
     <para>
      On the Expert Partitioner page, click <guimenu>Accept</guimenu>.
     </para>
     <para>
      The new proposal appears under <guimenu>Partitioning</guimenu> on the
      Installation Settings page.
     </para>
     <para>
      For example, the setup for the
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="raid_yast_install4_a.png" width="129pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="raid_yast_install4_a.png" width="129pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="bi9eh7a">
     <para>
      Continue with the install.
     </para>
     <para>
      Whenever you reboot your server, Device Mapper is started at boot time
      so that the software RAID is automatically recognized, and the
      operating system on the root (/) partition can be started.
     </para>
    </step>
   </procedure>
  </sect1>
 </chapter>
 <chapter conformance="sles11,Novell,yes,,80" id="raidmdadm" lang="en" revision="01/23/09">
  <title>Managing Software RAIDs 6 and 10 with mdadm</title>
  <para>
   This section describes how to create software RAID 6 and 10 devices,
   using the Multiple Devices Administration (<command>mdadm(8)</command>)
   tool. You can also use <filename>mdadm</filename> to create RAIDs 0, 1,
   4, and 5. The <command>mdadm</command> tool provides the functionality of
   legacy programs <command>mdtools</command> and
   <command>raidtools</command>.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="raidmdadmr6" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="raidmdadmr10nest" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="raidmdadmr10cpx" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="raidmdadmdegraded" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="raidmdadmr6">
   <title>Creating a RAID 6</title>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="raidmdadmr6ovw" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="raidmdadmr6create" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="raidmdadmr6ovw">
    <title>Understanding RAID 6</title>
    <para>
     RAID 6 is essentially an extension of RAID 5 that allows for additional
     fault tolerance by using a second independent distributed parity scheme
     (dual parity). Even if two of the hard disk drives fail during the data
     recovery process, the system continues to be operational, with no data
     loss.
    </para>
    <para>
     RAID 6 provides for extremely high data fault tolerance by sustaining
     multiple simultaneous drive failures. It handles the loss of any two
     devices without data loss. Accordingly, it requires N+2 drives to store
     N drives worth of data. It requires a minimum of 4 devices.
    </para>
    <para>
     The performance for RAID 6 is slightly lower but comparable to RAID 5
     in normal mode and single disk failure mode. It is very slow in dual
     disk failure mode.
    </para>
    <table id="b8i9zn7" frame="topbot" rowsep="1" pgwide="0">
     <title>Comparison of RAID 5 and RAID 6</title>
     <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="3334*"/>
      <colspec colnum="2" colname="2" colwidth="3334*"/>
      <colspec colnum="3" colname="3" colwidth="3334*"/>
      <thead>
       <row id="b8ihegi">
        <entry>
         <para>
          Feature
         </para>
        </entry>
        <entry>
         <para>
          RAID 5
         </para>
        </entry>
        <entry>
         <para>
          RAID 6
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b8ihegj">
        <entry>
         <para>
          Number of devices
         </para>
        </entry>
        <entry>
         <para>
          N+1, minimum of 3
         </para>
        </entry>
        <entry>
         <para>
          N+2, minimum of 4
         </para>
        </entry>
       </row>
       <row id="b8ihegk">
        <entry>
         <para>
          Parity
         </para>
        </entry>
        <entry>
         <para>
          Distributed, single
         </para>
        </entry>
        <entry>
         <para>
          Distributed, dual
         </para>
        </entry>
       </row>
       <row id="b8ihegl">
        <entry>
         <para>
          Performance
         </para>
        </entry>
        <entry>
         <para>
          Medium impact on write and rebuild
         </para>
        </entry>
        <entry>
         <para>
          More impact on sequential write than RAID 5
         </para>
        </entry>
       </row>
       <row id="b8ihegm">
        <entry>
         <para>
          Fault-tolerance
         </para>
        </entry>
        <entry>
         <para>
          Failure of one component device
         </para>
        </entry>
        <entry>
         <para>
          Failure of two component devices
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </sect2>

   <sect2 id="raidmdadmr6create">
    <title>Creating a RAID 6</title>
    <para>
     The procedure in this section creates a RAID 6 device
     <filename>/dev/md0</filename> with four devices:
     <filename>/dev/sda1</filename>, <filename>/dev/sdb1</filename>,
     <filename>/dev/sdc1</filename>, and <filename>/dev/sdd1</filename>.
     Ensure that you modify the procedure to use your actual device nodes.
    </para>
    <procedure id="b8ihegn">
     <step id="b8ihego">
      <para>
       Open a terminal console, then log in as the
       <systemitem>root</systemitem> user or equivalent.
      </para>
     </step>
     <step id="b8ihegp">
      <para>
       Create a RAID 6 device. At the command prompt, enter
      </para>
<screen>
mdadm --create /dev/md0 --run --level=raid6 --chunk=128 --raid-devices=4 /dev/sdb1 /dev/sdc1 /dev/sdc1 /dev/sdd1
</screen>
      <para>
       The default chunk size is 64 KB.
      </para>
     </step>
     <step id="b8ihegq">
      <para>
       Create a file system on the RAID 6 device
       <filename>/dev/md0</filename>, such as a Reiser file system
       (reiserfs). For example, at the command prompt, enter
      </para>
<screen>
mkfs.reiserfs /dev/md0
</screen>
      <para>
       Modify the command if you want to use a different file system.
      </para>
     </step>
     <step id="b8ihegr">
      <para>
       Edit the <filename>/etc/mdadm.conf</filename> file to add entries for
       the component devices and the RAID device
       <filename>/dev/md0</filename>.
      </para>
     </step>
     <step id="b8ihegs">
      <para>
       Edit the <filename>/etc/fstab</filename> file to add an entry for the
       RAID 6 device <filename>/dev/md0</filename>.
      </para>
     </step>
     <step id="b8ihegt">
      <para>
       Reboot the server.
      </para>
      <para>
       The RAID 6 device is mounted to <filename>/local</filename>.
      </para>
     </step>
     <step id="b8ihegu">
      <para>
       (Optional) Add a hot spare to service the RAID array. For example, at
       the command prompt enter:
      </para>
<screen>
mdadm /dev/md0 -a /dev/sde1
</screen>
     </step>
    </procedure>
   </sect2>
  </sect1>
  <sect1 id="raidmdadmr10nest">
   <title>Creating Nested RAID 10 Devices with mdadm</title>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="raidmdadmr10nestovw" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="raidmdadmr10nest1" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="raidmdadmr10nest2" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="raidmdadmr10nestovw">
    <title>Understanding Nested RAID Devices</title>
    <para>
     A nested RAID device consists of a RAID array that uses another RAID
     array as its basic element, instead of using physical disks. The goal
     of this configuration is to improve the performance and fault tolerance
     of the RAID.
    </para>
    <para>
     Linux supports nesting of RAID 1 (mirroring) and RAID 0 (striping)
     arrays. Generally, this combination is referred to as RAID 10. To
     distinguish the order of the nesting, this document uses the following
     terminology:
    </para>
    <itemizedlist>
     <listitem>
      <formalpara id="b8ghlom">
       <title>RAID 1+0:</title>
       <para>
        RAID 1 (mirror) arrays are built first, then combined to form a RAID
        0 (stripe) array.
       </para>
      </formalpara>
     </listitem>
     <listitem>
      <formalpara id="b8gzg7q">
       <title>RAID 0+1:</title>
       <para>
        RAID 0 (stripe) arrays are built first, then combined to form a RAID
        1 (mirror) array.
       </para>
      </formalpara>
     </listitem>
    </itemizedlist>
    <para>
     The following table describes the advantages and disadvantages of RAID
     10 nesting as 1+0 versus 0+1. It assumes that the storage objects you
     use reside on different disks, each with a dedicated I/O capability.
    </para>
    <table id="b57a7uv" frame="topbot" rowsep="1" pgwide="0">
     <title>Nested RAID Levels</title>
     <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="1191*"/>
      <colspec colnum="2" colname="2" colwidth="1905*"/>
      <colspec colnum="3" colname="3" colwidth="6906*"/>
      <thead>
       <row id="b57a7uw">
        <entry>
         <para>
          RAID Level
         </para>
        </entry>
        <entry>
         <para>
          Description
         </para>
        </entry>
        <entry>
         <para>
          Performance and Fault Tolerance
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b57a7uz">
        <entry>
         <para>
          10 (1+0)
         </para>
        </entry>
        <entry>
         <para>
          RAID 0 (stripe) built with RAID 1 (mirror) arrays
         </para>
        </entry>
        <entry>
         <para>
          RAID 1+0 provides high levels of I/O performance, data redundancy,
          and disk fault tolerance. Because each member device in the RAID 0
          is mirrored individually, multiple disk failures can be tolerated
          and data remains available as long as the disks that fail are in
          different mirrors.
         </para>
         <para>
          You can optionally configure a spare for each underlying mirrored
          array, or configure a spare to serve a spare group that serves all
          mirrors.
         </para>
        </entry>
       </row>
       <row id="b57a7uy">
        <entry>
         <para>
          10 (0+1)
         </para>
        </entry>
        <entry>
         <para>
          RAID 1 (mirror) built with RAID 0 (stripe) arrays
         </para>
        </entry>
        <entry>
         <para>
          RAID 0+1 provides high levels of I/O performance and data
          redundancy, but slightly less fault tolerance than a 1+0. If
          multiple disks fail on one side of the mirror, then the other
          mirror is available. However, if disks are lost concurrently on
          both sides of the mirror, all data is lost.
         </para>
         <para>
          This solution offers less disk fault tolerance than a 1+0
          solution, but if you need to perform maintenance or maintain the
          mirror on a different site, you can take an entire side of the
          mirror offline and still have a fully functional storage device.
          Also, if you lose the connection between the two sites, either
          site operates independently of the other. That is not true if you
          stripe the mirrored segments, because the mirrors are managed at a
          lower level.
         </para>
         <para>
          If a device fails, the mirror on that side fails because RAID 1 is
          not fault-tolerant. Create a new RAID 0 to replace the failed
          side, then resynchronize the mirrors.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </sect2>

   <sect2 id="raidmdadmr10nest1">
    <title>Creating Nested RAID 10 (1+0) with mdadm</title>
    <para>
     A nested RAID 1+0 is built by creating two or more RAID 1 (mirror)
     devices, then using them as component devices in a RAID 0.
    </para>
    <important>
     <para>
      If you need to manage multiple connections to the devices, you must
      configure multipath I/O before configuring the RAID devices. For
      information, see
      <xref linkend="multipathing" xrefstyle="ChapTitleOnPage"/>.
     </para>
    </important>
    <para>
     The procedure in this section uses the device names shown in the
     following table. Ensure that you modify the device names with the names
     of your own devices.
    </para>
    <table id="b6670cx" frame="topbot" rowsep="0" pgwide="0">
     <title>Scenario for Creating a RAID 10 (1+0) by Nesting</title>
     <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="3334*"/>
      <colspec colnum="2" colname="2" colwidth="3334*"/>
      <colspec colnum="3" colname="3" colwidth="3334*"/>
      <thead>
       <row id="b6670cy">
        <entry>
         <para>
          Raw Devices
         </para>
        </entry>
        <entry>
         <para>
          RAID 1 (mirror)
         </para>
        </entry>
        <entry>
         <para>
          RAID 1+0 (striped mirrors)
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b8o7og9">
        <entry>
         <simplelist>
          <member><filename>/dev/sdb1</filename>
          </member>
          <member><filename>/dev/sdc1</filename>
          </member>
         </simplelist>
        </entry>
        <entry>
         <para>
          <filename>/dev/md0</filename>
         </para>
        </entry>
        <entry morerows="1">
         <para/>
         <para>
          <filename>/dev/md2</filename>
         </para>
        </entry>
       </row>
       <row id="b8o7ogb">
        <entry>
         <simplelist>
          <member><filename>/dev/sdd1</filename>
          </member>
          <member><filename>/dev/sde1</filename>
          </member>
         </simplelist>
        </entry>
        <entry>
         <para>
          <filename>/dev/md1</filename>
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <procedure id="b6670d3">
     <step id="b6670d4">
      <para>
       Open a terminal console, then log in as the
       <systemitem>root</systemitem> user or equivalent.
      </para>
     </step>
     <step id="b6670d5">
      <para>
       Create 2 software RAID 1 devices, using two different devices for
       each RAID 1 device. At the command prompt, enter these two commands:
      </para>
<screen>
mdadm --create /dev/md0 --run --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1
</screen>
<screen>
mdadm --create /dev/md1 --run --level=1 --raid-devices=2 /dev/sdd1 /dev/sde1
</screen>
     </step>
     <step id="b6670d6">
      <para>
       Create the nested RAID 1+0 device. At the command prompt, enter the
       following command using the software RAID 1 devices you created in
       <xref linkend="b6670d5" xrefstyle="StepXRef"/>:
      </para>
<screen>
mdadm --create /dev/md2 --run --level=0 --chunk=64 --raid-devices=2 /dev/md0 /dev/md1
</screen>
      <para>
       The default chunk size is 64 KB.
      </para>
     </step>
     <step id="b6670d7">
      <para>
       Create a file system on the RAID 1+0 device
       <filename>/dev/md2</filename>, such as a Reiser file system
       (reiserfs). For example, at the command prompt, enter
      </para>
<screen>
mkfs.reiserfs /dev/md2
</screen>
      <para>
       Modify the command if you want to use a different file system.
      </para>
     </step>
     <step id="b6670d8">
      <para>
       Edit the <filename>/etc/mdadm.conf</filename> file to add entries for
       the component devices and the RAID device
       <filename>/dev/md2</filename>.
      </para>
     </step>
     <step id="b6670d9">
      <para>
       Edit the <filename>/etc/fstab</filename> file to add an entry for the
       RAID 1+0 device <filename>/dev/md2</filename>.
      </para>
     </step>
     <step id="b6670da">
      <para>
       Reboot the server.
      </para>
      <para>
       The RAID 1+0 device is mounted to <filename>/local</filename>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="raidmdadmr10nest2">
    <title>Creating Nested RAID 10 (0+1) with mdadm</title>
    <para>
     A nested RAID 0+1 is built by creating two to four RAID 0 (striping)
     devices, then mirroring them as component devices in a RAID 1.
    </para>
    <important>
     <para>
      If you need to manage multiple connections to the devices, you must
      configure multipath I/O before configuring the RAID devices. For
      information, see
      <xref linkend="multipathing" xrefstyle="ChapTitleOnPage"/>.
     </para>
    </important>
    <para>
     In this configuration, spare devices cannot be specified for the
     underlying RAID 0 devices because RAID 0 cannot tolerate a device loss.
     If a device fails on one side of the mirror, you must create a
     replacement RAID 0 device, than add it into the mirror.
    </para>
    <para>
     The procedure in this section uses the device names shown in the
     following table. Ensure that you modify the device names with the names
     of your own devices.
    </para>
    <table id="b8gzg7s" frame="topbot" rowsep="0" pgwide="0">
     <title>Scenario for Creating a RAID 10 (0+1) by Nesting</title>
     <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="3334*"/>
      <colspec colnum="2" colname="2" colwidth="3334*"/>
      <colspec colnum="3" colname="3" colwidth="3334*"/>
      <thead>
       <row id="b8gzg7t">
        <entry>
         <para>
          Raw Devices
         </para>
        </entry>
        <entry>
         <para>
          RAID 0 (stripe)
         </para>
        </entry>
        <entry>
         <para>
          RAID 0+1 (mirrored stripes)
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bi71dhe">
        <entry>
         <simplelist>
          <member><filename>/dev/sdb1</filename>
          </member>
          <member><filename>/dev/sdc1</filename>
          </member>
         </simplelist>
        </entry>
        <entry>
         <para>
          <filename>/dev/md0</filename>
         </para>
        </entry>
        <entry morerows="1">
         <para/>
         <para>
          <filename>/dev/md2</filename>
         </para>
        </entry>
       </row>
       <row id="bi71dhf">
        <entry>
         <simplelist>
          <member><filename>/dev/sdd1</filename>
          </member>
          <member><filename>/dev/sde1</filename>
          </member>
         </simplelist>
        </entry>
        <entry>
         <para>
          <filename>/dev/md1</filename>
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <procedure id="b8gzg7y">
     <step id="b8gzg7z">
      <para>
       Open a terminal console, then log in as the root user or equivalent.
      </para>
     </step>
     <step id="b8gzg80">
      <para>
       Create two software RAID 0 devices, using two different devices for
       each RAID 0 device. At the command prompt, enter these two commands:
      </para>
<screen>
mdadm --create /dev/md0 --run --level=0 --chunk=64 --raid-devices=2 /dev/sdb1 /dev/sdc1
</screen>
<screen>
mdadm --create /dev/md1 --run --level=0 --chunk=64 --raid-devices=2 /dev/sdd1 /dev/sde1
</screen>
      <para>
       The default chunk size is 64 KB.
      </para>
     </step>
     <step id="b8gzg81">
      <para>
       Create the nested RAID 0+1 device. At the command prompt, enter the
       following command using the software RAID 0 devices you created in
       <xref linkend="b8gzg80" xrefstyle="StepXRef"/>:
      </para>
<screen>
mdadm --create /dev/md2 --run --level=1 --raid-devices=2 /dev/md0 /dev/md1
</screen>
     </step>
     <step id="b8gzg82">
      <para>
       Create a file system on the RAID 0+1 device
       <filename>/dev/md2</filename>, such as a Reiser file system
       (reiserfs). For example, at the command prompt, enter
      </para>
<screen>
mkfs.reiserfs /dev/md2
</screen>
      <para>
       Modify the command if you want to use a different file system.
      </para>
     </step>
     <step id="b8gzg83">
      <para>
       Edit the <filename>/etc/mdadm.conf</filename> file to add entries for
       the component devices and the RAID device
       <filename>/dev/md2</filename>.
      </para>
     </step>
     <step id="b8gzg84">
      <para>
       Edit the <filename>/etc/fstab</filename> file to add an entry for the
       RAID 0+1 device <filename>/dev/md2</filename>.
      </para>
     </step>
     <step id="b8gzg85">
      <para>
       Reboot the server.
      </para>
      <para>
       The RAID 0+1 device is mounted to <filename>/local</filename>.
      </para>
     </step>
    </procedure>
   </sect2>
  </sect1>
  <sect1 id="raidmdadmr10cpx">
   <title>Creating a Complex RAID 10</title>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="raidmdadmr10cpxovw" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="raidmdadmr10cpxcreate" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14drcbo" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="raidmdadmr10cpxovw">
    <title>Understanding the Complex RAID10</title>
    <para>
     In <command>mdadm</command>, the RAID10 level creates a single complex
     software RAID that combines features of both RAID 0 (striping) and RAID
     1 (mirroring). Multiple copies of all data blocks are arranged on
     multiple drives following a striping discipline. Component devices
     should be the same size.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="b8ghyxy" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b7cym9j" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b7cym15" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b7cyneb" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b7cynnk" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="byz81ho" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="b8ghyxy">
     <title>Comparing the Complex RAID10 and Nested RAID 10 (1+0)</title>
     <para role="intro">
      The complex RAID 10 is similar in purpose to a nested RAID 10 (1+0),
      but differs in the following ways:
     </para>
     <table id="b8ghz5p" frame="topbot" rowsep="1" pgwide="0">
      <title>Complex vs. Nested RAID 10</title>
      <tgroup cols="3">
       <colspec colnum="1" colname="1" colwidth="3334*"/>
       <colspec colnum="2" colname="2" colwidth="3334*"/>
       <colspec colnum="3" colname="3" colwidth="3334*"/>
       <thead>
        <row id="b8gzgca">
         <entry>
          <para>
           Feature
          </para>
         </entry>
         <entry>
          <para>
           Complex RAID10
          </para>
         </entry>
         <entry>
          <para>
           Nested RAID 10 (1+0)
          </para>
         </entry>
        </row>
       </thead>
       <tbody>
        <row id="b8gzgcb">
         <entry>
          <para>
           Number of devices
          </para>
         </entry>
         <entry>
          <para>
           Allows an even or odd number of component devices
          </para>
         </entry>
         <entry>
          <para>
           Requires an even number of component devices
          </para>
         </entry>
        </row>
        <row id="b8gzgcc">
         <entry>
          <para>
           Component devices
          </para>
         </entry>
         <entry>
          <para>
           Managed as a single RAID device
          </para>
         </entry>
         <entry>
          <para>
           Manage as a nested RAID device
          </para>
         </entry>
        </row>
        <row id="b8gzgcd">
         <entry>
          <para>
           Striping
          </para>
         </entry>
         <entry>
          <para>
           Striping occurs in the near or far layout on component devices.
          </para>
          <para>
           The far layout provides sequential read throughput that scales by
           number of drives, rather than number of RAID 1 pairs.
          </para>
         </entry>
         <entry>
          <para>
           Striping occurs consecutively across component devices
          </para>
         </entry>
        </row>
        <row id="b8gzgce">
         <entry>
          <para>
           Multiple copies of data
          </para>
         </entry>
         <entry>
          <para>
           Two or more copies, up to the number of devices in the array
          </para>
         </entry>
         <entry>
          <para>
           Copies on each mirrored segment
          </para>
         </entry>
        </row>
        <row id="b8gzgcf">
         <entry>
          <para>
           Hot spare devices
          </para>
         </entry>
         <entry>
          <para>
           A single spare can service all component devices
          </para>
         </entry>
         <entry>
          <para>
           Configure a spare for each underlying mirrored array, or
           configure a spare to serve a spare group that serves all mirrors.
          </para>
         </entry>
        </row>
       </tbody>
      </tgroup>
     </table>
    </sect3>
    <sect3 id="b7cym9j">
     <title>Number of Replicas in the Complex RAID10</title>
     <para>
      When configuring an complex RAID10 array, you must specify the number
      of replicas of each data block that are required. The default number
      of replicas is 2, but the value can be 2 to the number of devices in
      the array.
     </para>
    </sect3>
    <sect3 id="b7cym15">
     <title>Number of Devices in the Complex RAID10</title>
     <para>
      You must use at least as many component devices as the number of
      replicas you specify. However, number of component devices in a RAID10
      array does not need to be a multiple of the number of replicas of each
      data block. The effective storage size is the number of devices
      divided by the number of replicas.
     </para>
     <para>
      For example, if you specify 2 replicas for an array created with 5
      component devices, a copy of each block is stored on two different
      devices. The effective storage size for one copy of all data is 5/2 or
      2.5 times the size of a component device.
     </para>
    </sect3>
    <sect3 id="b7cyneb">
     <title>Near Layout</title>
     <para>
      With the near layout, copies of a block of data are striped near each
      other on different component devices. That is, multiple copies of one
      data block are at similar offsets in different devices. Near is the
      default layout for RAID10. For example, if you use an odd number of
      component devices and two copies of data, some copies are perhaps one
      chunk further into the device.
     </para>
     <para>
      The near layout for the <command>mdadm</command> RAID10 yields read
      and write performance similar to RAID 0 over half the number of
      drives.
     </para>
     <para role="intro">
      Near layout with an even number of disks and two replicas:
     </para>
<screen>
sda1 sdb1 sdc1 sde1
  0&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;1
  2&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;3
  4&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;5
  6&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;7&nbsp;&nbsp;&nbsp;&nbsp;7
  8&nbsp;&nbsp;&nbsp;&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp;9&nbsp;&nbsp;&nbsp;&nbsp;9
</screen>
     <para>
      Near layout with an odd number of disks and two replicas:
     </para>
<screen>
sda1 sdb1 sdc1 sde1 sdf1
  0&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2
  2&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;4
  5&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;7
  7&nbsp;&nbsp;&nbsp;&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp;9&nbsp;&nbsp;&nbsp;&nbsp;9
  10   10   11&nbsp;&nbsp;&nbsp;11&nbsp;&nbsp;&nbsp;12
</screen>
    </sect3>
    <sect3 id="b7cynnk">
     <title>Far Layout</title>
     <para>
      The far layout stripes data over the early part of all drives, then
      stripes a second copy of the data over the later part of all drives,
      making sure that all copies of a block are on different drives. The
      second set of values starts halfway through the component drives.
     </para>
     <para>
      With a far layout, the read performance of the
      <command>mdadm</command> RAID10 is similar to a RAID 0 over the full
      number of drives, but write performance is substantially slower than a
      RAID 0 because there is more seeking of the drive heads. It is best
      used for read-intensive operations such as for read-only file servers.
     </para>
     <para>
      The speed of the raid10 for writing is similar to other mirrored RAID
      types, like raid1 and raid10 using near layout, as the elevator of the
      file system schedules the writes in a more optimal way than raw
      writing. Using raid10 in the far layout well-suited for mirrored
      writing applications.
     </para>
     <para role="intro">
      Far layout with an even number of disks and two replicas:
     </para>
<screen>
sda1 sdb1 sdc1 sde1
  0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;3
  4&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;7       
  . . .
  3&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2
  7&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;6
</screen>
     <para role="intro">
      Far layout with an odd number of disks and two replicas:
     </para>
<screen>
sda1 sdb1 sdc1 sde1 sdf1
  0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;4
  5&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;7&nbsp;&nbsp;&nbsp;&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp;9
  . . .
  4&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;3
  9&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;7&nbsp;&nbsp;&nbsp;&nbsp;8
</screen>
    </sect3>
    <sect3 id="byz81ho">
     <title>Offset Layout</title>
     <para>
      The offset layout duplicates stripes so that the multiple copies of a
      given chunk are laid out on consecutive drives and at consecutive
      offsets. Effectively, each stripe is duplicated and the copies are
      offset by one device. This should give similar read characteristics to
      a far layout if a suitably large chunk size is used, but without as
      much seeking for writes.
     </para>
     <para role="intro">
      Offset layout with an even number of disks and two replicas:
     </para>
<screen>
sda1 sdb1 sdc1 sde1
  0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;3
  3&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;2       
  4    5    6    7
  7&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;6
  8    9   10   11
 11    8    9   10
</screen>
     <para role="intro">
      Offset layout with an odd number of disks and two replicas:
     </para>
<screen>
sda1 sdb1 sdc1 sde1 sdf1
  0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;4
  4    0    1    2    3
  5    6    7    8    9
  9    5    6    7    8
 10   11   12   13   14
 14   10   11   12   13&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</screen>
    </sect3>
   </sect2>

   <sect2 id="raidmdadmr10cpxcreate">
    <title>Creating a Complex RAID 10 with mdadm</title>
    <para>
     The RAID10 option for <command>mdadm</command> creates a RAID 10 device
     without nesting. For information about RAID10, see
     <xref linkend="raidmdadmr10cpxovw" xrefstyle="SectTitleOnPage"/>.
    </para>
    <para>
     The procedure in this section uses the device names shown in the
     following table. Ensure that you modify the device names with the names
     of your own devices.
    </para>
    <table id="b667706" frame="topbot" rowsep="1" pgwide="0">
     <title>Scenario for Creating a RAID 10 Using the mdadm RAID10 Option</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b667707">
        <entry>
         <para>
          Raw Devices
         </para>
        </entry>
        <entry>
         <para>
          RAID10 (near or far striping scheme)
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b667708">
        <entry>
         <para>
          <filename>/dev/sdf1</filename>
         </para>
         <para>
          <filename>/dev/sdg1</filename>
         </para>
         <para>
          <filename>/dev/sdh1</filename>
         </para>
         <para>
          <filename>/dev/sdi1</filename>
         </para>
        </entry>
        <entry>
         <para>
          <filename>/dev/md3</filename>
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <procedure id="b66770c">
     <step id="b667c10">
      <para>
       In YaST, create a 0xFD Linux RAID partition on the devices you want
       to use in the RAID, such as <filename>/dev/sdf1</filename>,
       <filename>/dev/sdg1</filename>, <filename>/dev/sdh1</filename>, and
       <filename>/dev/sdi1</filename>.
      </para>
     </step>
     <step id="b66770d">
      <para>
       Open a terminal console, then log in as the root user or equivalent.
      </para>
     </step>
     <step id="b66770e">
      <para>
       Create a RAID 10 command. At the command prompt, enter (all on the
       same line):
      </para>
<screen>
mdadm --create /dev/md3 --run --level=10 --chunk=4 --raid-devices=4 /dev/sdf1 /dev/sdg1 /dev/sdh1 /dev/sdi1
</screen>
     </step>
     <step id="b66770g">
      <para>
       Create a Reiser file system on the RAID 10 device
       <filename>/dev/md3</filename>. At the command prompt, enter
      </para>
<screen>
mkfs.reiserfs /dev/md3
</screen>
     </step>
     <step id="b66770h">
      <para>
       Edit the <filename>/etc/mdadm.conf</filename> file to add entries for
       the component devices and the RAID device
       <filename>/dev/md3</filename>. For example:
      </para>
<screen>
DEVICE /dev/md3
</screen>
     </step>
     <step id="b66770i">
      <para>
       Edit the <filename>/etc/fstab</filename> file to add an entry for the
       RAID 10 device <filename>/dev/md3</filename>.
      </para>
     </step>
     <step id="b66770j">
      <para>
       Reboot the server.
      </para>
      <para>
       The RAID10 device is mounted to <filename>/raid10</filename>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="b14drcbo">
    <title>Creating a Complex RAID10 with the YaST Partitioner</title>
    <procedure id="b14dtk6s">
     <step id="b14dtk6t">
      <para>
       Launch YaST as the <systemitem>root</systemitem> user, then open the
       Partitioner.
      </para>
     </step>
     <step id="b14dtk6u">
      <para>
       Select <guimenu>Hard Disks</guimenu> to view the available disks,
       such as sdab, sdc, sdd, and sde.
      </para>
     </step>
     <step id="b14dtk6v">
      <para>
       For each disk that you will use in the software RAID, create a RAID
       partition on the device. Each partition should be the same size. For
       a RAID 10 device, you need
      </para>
      <substeps>
       <step id="b14dtk6w">
        <para>
         Under <guimenu>Hard Disks</guimenu>, select the device, then select
         the <guimenu>Partitions</guimenu> tab in the right panel.
        </para>
       </step>
       <step id="b14dtk6x">
        <para>
         Click <guimenu>Add</guimenu> to open the <guimenu>Add
         Partition</guimenu> wizard.
        </para>
       </step>
       <step id="b14dtk6y">
        <para>
         Under <guimenu>New Partition Type</guimenu>, select
         <guimenu>Primary Partition</guimenu>, then click
         <guimenu>Next</guimenu>.
        </para>
       </step>
       <step id="b14dtk6z">
        <para>
         For <guimenu>New Partition Size</guimenu>, specify the desired size
         of the RAID partition on this disk, then click
         <guimenu>Next</guimenu>.
        </para>
       </step>
       <step id="b14dtk70">
        <para>
         Under <guimenu>Formatting Options</guimenu>, select <guimenu>Do not
         format partition</guimenu>, then select <guimenu>0xFD Linux
         RAID</guimenu> from the <guimenu>File system ID</guimenu> drop-down
         list.
        </para>
       </step>
       <step id="b14dtk71">
        <para>
         Under <guimenu>Mounting Options</guimenu>, select <guimenu>Do not
         mount partition</guimenu>, then click <guimenu>Finish</guimenu>.
        </para>
       </step>
       <step id="b14dtk72">
        <para>
         Repeat these steps until you have defined a RAID partition on the
         disks you want to use in the RAID 10 device.
        </para>
       </step>
      </substeps>
     </step>
     <step id="b14dtk73">
      <para>
       Create a RAID 10 device:
      </para>
      <substeps>
       <step id="b14dtk74">
        <para>
         Select <guimenu>RAID</guimenu>, then select <guimenu>Add
         RAID</guimenu> in the right panel to open the <guimenu>Add
         RAID</guimenu> wizard.
        </para>
       </step>
       <step id="b14dtk75">
        <para>
         Under <guimenu>RAID Type</guimenu>, select <guimenu>RAID 10
         (Mirroring and Striping)</guimenu>.
        </para>
        <informalfigure pgwide="0">
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="raid10_a.png" width="302pt" format="PNG"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="raid10_a.png" width="302pt" format="PNG"/>
          </imageobject>
         </mediaobject>
        </informalfigure>
       </step>
       <step id="b14dtk76">
        <para>
         In the <guimenu>Available Devices</guimenu> list, select the
         desired Linux RAID partitions, then click <guimenu>Add</guimenu> to
         move them to the <guimenu>Selected Devices</guimenu> list.
        </para>
       </step>
       <step id="b14dtk77">
        <para>
         (Optional) Click <guimenu>Classify</guimenu>, specify the preferred
         order of the disks in the RAID array.
        </para>
        <para>
         For RAID types where the order of added disks matters, you can
         specify the order in which the devices will be used to ensure that
         one half of the array resides on one disk subsystem and the other
         half of the array resides on a different disk subsystem. For
         example, if one disk subsystem fails, the system keeps running from
         the second disk subsystem.
        </para>
        <substeps>
         <step id="b14dutul">
          <para>
           Select each disk in turn and click one of the <guimenu>Class
           X</guimenu> buttons, where X is the letter you want to assign to
           the disk. Available classes are A, B, C, D and E but for many
           cases fewer classes are needed (e.g. only A and B). Assign all
           available RAID disks this way.
          </para>
          <para>
           You can press the Ctrl or Shift key to select multiple devices.
           You can also right-click a selected device and choose the
           appropriate class from the context menu.
          </para>
         </step>
         <step id="b14dutum">
          <para>
           Specify the order the devices by selecting one of the sorting
           options:
          </para>
          <formalpara id="b14dutun">
           <title>Sorted:</title>
           <para>
            Sorts all devices of class A before all devices of class B and
            so on. For example: <literal>AABBCC</literal>.
           </para>
          </formalpara>
          <formalpara id="b14dutuo">
           <title>Interleaved:</title>
           <para>
            Sorts devices by the first device of class A, then first device
            of class B, then all the following classes with assigned
            devices. Then the second device of class A, the second device of
            class B, and so on follows. All devices without a class are
            sorted to the end of devices list. For example,
            <literal>ABCABC</literal>.
           </para>
          </formalpara>
          <formalpara id="b14dutup">
           <title>Pattern File:</title>
           <para>
            Select an existing file that contains multiple lines, where each
            is a regular expression and a class name (<literal>"sda.*
            A"</literal>). All devices that match the regular expression are
            assigned to the specified class for that line. The regular
            expression is matched against the kernel name
            (<filename>/dev/sda1</filename>), the udev path name
            (<filename>/dev/disk/by-path/pci-0000:00:1f.2-scsi-0:0:0:0-part1</filename>)
            and then the udev ID
            (/dev/disk/by-id/ata-ST3500418AS_9VMN8X8L-part1). The first
            match made determines the class if a device’s name matches
            more then one regular expression.
           </para>
          </formalpara>
         </step>
         <step id="b14dutuq">
          <para>
           At the bottom of the dialog box, click <guimenu>OK</guimenu> to
           confirm the order.
          </para>
          <informalfigure pgwide="0">
           <mediaobject>
            <imageobject role="fo">
             <imagedata fileref="raid10_classify_a.png" width="292pt" format="PNG"/>
            </imageobject>
            <imageobject role="html">
             <imagedata fileref="raid10_classify_a.png" width="292pt" format="PNG"/>
            </imageobject>
           </mediaobject>
          </informalfigure>
         </step>
        </substeps>
       </step>
       <step id="b14dttix">
        <para>
         Click <guimenu>Next</guimenu>.
        </para>
       </step>
       <step id="b14dttiy">
        <para>
         Under <guimenu>RAID Options</guimenu>, specify the C<guimenu>hunk
         Size</guimenu> and <guimenu>Parity Algorithm</guimenu>, then click
         <guimenu>Next</guimenu>.
        </para>
        <para>
         For a RAID 10, the parity options are n (near), f (far), and o
         (offset). The number indicates the number of replicas of each data
         block are required. Two is the default. For information, see
         <xref linkend="raidmdadmr10cpxovw" xrefstyle="SectTitleOnPage"/>.
        </para>
       </step>
       <step id="b14dttiz">
        <para>
         Add a file system and mount options to the RAID device, then click
         <guimenu>Finish</guimenu>.
        </para>
       </step>
      </substeps>
     </step>
     <step id="b14dttj0">
      <para role="intro">
       Select <guimenu>RAID</guimenu>, select the newly created RAID device,
       then click <guimenu>Used Devices</guimenu> to view its partitions.
      </para>
      <informalfigure pgwide="0">
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="raid10_useddevs_a.png" width="302pt" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="raid10_useddevs_a.png" width="302pt" format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step id="b14dtw8j">
      <para>
       Click <guimenu>Next</guimenu>.
      </para>
     </step>
     <step id="b14dtw8k">
      <para>
       Verify the changes to be made, then click <guimenu>Finish</guimenu>
       to create the RAID.
      </para>
     </step>
    </procedure>
   </sect2>
  </sect1>
  <sect1 id="raidmdadmdegraded">
   <title>Creating a Degraded RAID Array</title>

   <para role="intro">
    A degraded array is one in which some devices are missing. Degraded
    arrays are supported only for RAID 1, RAID 4, RAID 5, and RAID 6. These
    RAID types are designed to withstand some missing devices as part of
    their fault-tolerance features. Typically, degraded arrays occur when a
    device fails. It is possible to create a degraded array on purpose.
   </para>

   <informaltable frame="topbot" rowsep="1" pgwide="0">
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="3334*"/>
     <colspec colnum="2" colname="2" colwidth="3334*"/>
     <colspec colnum="3" colname="3" colwidth="3334*"/>
     <thead>
      <row id="b8gzm1p">
       <entry>
        <para>
         RAID Type
        </para>
       </entry>
       <entry>
        <para>
         Allowable Number of Slots Missing
        </para>
       </entry>
       <entry/>
      </row>
     </thead>
     <tbody>
      <row id="b8gzm1q">
       <entry>
        <para>
         RAID 1
        </para>
       </entry>
       <entry>
        <para>
         All but one device
        </para>
       </entry>
       <entry/>
      </row>
      <row id="b8gzm1r">
       <entry>
        <para>
         RAID 4
        </para>
       </entry>
       <entry>
        <para>
         One slot
        </para>
       </entry>
       <entry/>
      </row>
      <row id="b8h0rww">
       <entry>
        <para>
         RAID 5
        </para>
       </entry>
       <entry>
        <para>
         One slot
        </para>
       </entry>
       <entry/>
      </row>
      <row id="b8gzm1s">
       <entry>
        <para>
         RAID 6
        </para>
       </entry>
       <entry>
        <para>
         One or two slots
        </para>
       </entry>
       <entry/>
      </row>
     </tbody>
    </tgroup>
   </informaltable>

   <para>
    To create a degraded array in which some devices are missing, simply
    give the word <literal>missing</literal> in place of a device name. This
    causes <command>mdadm</command> to leave the corresponding slot in the
    array empty.
   </para>

   <para>
    When creating a RAID 5 array, <command>mdadm</command> automatically
    creates a degraded array with an extra spare drive. This is because
    building the spare into a degraded array is generally faster than
    resynchronizing the parity on a non-degraded, but not clean, array. You
    can override this feature with the <literal>--force</literal> option.
   </para>

   <para>
    Creating a degraded array might be useful if you want create a RAID, but
    one of the devices you want to use already has data on it. In that case,
    you create a degraded array with other devices, copy data from the
    in-use device to the RAID that is running in degraded mode, add the
    device into the RAID, then wait while the RAID is rebuilt so that the
    data is now across all devices. An example of this process is given in
    the following procedure:
   </para>

   <procedure id="b8h2aqj">
    <step id="b8h2aqk">
     <para>
      Create a degraded RAID 1 device <filename>/dev/md0</filename>, using
      one single drive <filename>/dev/sd1</filename>, enter the following at
      the command prompt:
     </para>
<screen>
mdadm --create /dev/md0 -l 1 -n 2 /dev/sda1 missing
</screen>
     <para>
      The device should be the same size or larger than the device you plan
      to add to it.
     </para>
    </step>
    <step id="b8h2atq">
     <para>
      If the device you want to add to the mirror contains data that you
      want to move to the RAID array, copy it now to the RAID array while it
      is running in degraded mode.
     </para>
    </step>
    <step id="b8h2avw">
     <para>
      Add a device to the mirror. For example, to add
      <filename>/dev/sdb1</filename> to the RAID, enter the following at the
      command prompt:
     </para>
<screen>
mdadm /dev/md0 -a /dev/sdb1
</screen>
     <para>
      You can add only one device at a time. You must wait for the kernel to
      build the mirror and bring it fully online before you add another
      mirror.
     </para>
    </step>
    <step id="b8h2ayp">
     <para>
      Monitor the build progress by entering the following at the command
      prompt:
     </para>
<screen>
cat /proc/mdstat
</screen>
     <para>
      To see the rebuild progress while being refreshed every second, enter
     </para>
<screen>
watch -n 1 cat /proc/mdstat 
</screen>
    </step>
   </procedure>
  </sect1>
 </chapter>
 <chapter conformance="sles11,Novell,no,,80" id="raidresize" lang="en" revision="02/22/07">
  <title>Resizing Software RAID Arrays with mdadm</title>
  <para>
   This section describes how to increase or reduce the size of a software
   RAID 1, 4, 5, or 6 device with the Multiple Device Administration
   (<command>mdadm(8)</command>) tool.
  </para>
  <warning>
   <para>
    Before starting any of the tasks described in this section, ensure that
    you have a valid backup of all of the data.
   </para>
  </warning>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="resizeunderstand" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="resizeincr" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="resizedecr" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="resizeunderstand">
   <title>Understanding the Resizing Process</title>

   <para>
    Resizing an existing software RAID device involves increasing or
    decreasing the space contributed by each component partition.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b8ppjxz" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b8pr3ol" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b8ppjxz">
    <title>Guidelines for Resizing a Software RAID</title>
    <para>
     The <command>mdadm(8)</command> tool supports resizing only for
     software RAID levels 1, 4, 5, and 6. These RAID levels provide disk
     fault tolerance so that one component partition can be removed at a
     time for resizing. In principle, it is possible to perform a hot resize
     for RAID partitions, but you must take extra care for your data when
     doing so.
    </para>
    <para>
     The file system that resides on the RAID must also be able to be
     resized in order to take advantage of the changes in available space on
     the device. In &productname;, file system resizing
     utilities are available for file systems Ext2, Ext3, and ReiserFS. The
     utilities support increasing and decreasing the size as follows:
    </para>
    <table id="biuzuno" frame="topbot" rowsep="1" pgwide="0">
     <title>File System Support for Resizing</title>
     <tgroup cols="4">
      <colspec colnum="1" colname="1" colwidth="2500*"/>
      <colspec colnum="2" colname="2" colwidth="2500*"/>
      <colspec colnum="3" colname="3" colwidth="2500*"/>
      <colspec colnum="4" colname="4" colwidth="2500*"/>
      <thead>
       <row id="biuzunp">
        <entry>
         <para>
          File System
         </para>
        </entry>
        <entry>
         <para>
          Utility
         </para>
        </entry>
        <entry>
         <para>
          Increase Size
         </para>
        </entry>
        <entry>
         <para>
          Decrease Size
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="biuzunq">
        <entry>
         <para>
          Ext2 or Ext3
         </para>
        </entry>
        <entry>
         <para>
          resize2fs
         </para>
        </entry>
        <entry>
         <para>
          Yes, offline only
         </para>
        </entry>
        <entry>
         <para>
          Yes, offline only
         </para>
        </entry>
       </row>
       <row id="biuzunr">
        <entry>
         <para>
          ReiserFS
         </para>
        </entry>
        <entry>
         <para>
          resize_reiserfs
         </para>
        </entry>
        <entry>
         <para>
          Yes, online or offline
         </para>
        </entry>
        <entry>
         <para>
          Yes, offline only
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para>
     Resizing any partition or file system involves some risks that can
     potentially result in losing data.
    </para>
    <warning>
     <para>
      To avoid data loss, ensure that you back up your data before you begin
      any resizing task.
     </para>
    </warning>
   </sect2>

   <sect2 id="b8pr3ol">
    <title>Overview of Tasks</title>
    <para>
     Resizing the RAID involves the following tasks. The order in which
     these tasks is performed depends on whether you are increasing or
     decreasing its size.
    </para>
    <table id="b8ppko3" frame="topbot" rowsep="1" pgwide="0">
     <title>Tasks Involved in Resizing a RAID</title>
     <tgroup cols="4">
      <colspec colnum="1" colname="1" colwidth="2500*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <colspec colnum="3" colname="3" colwidth="1250*"/>
      <colspec colnum="4" colname="4" colwidth="1250*"/>
      <thead>
       <row id="b8pps5o">
        <entry>
         <para>
          Tasks
         </para>
        </entry>
        <entry>
         <para>
          Description
         </para>
        </entry>
        <entry>
         <para>
          Order If Increasing Size
         </para>
        </entry>
        <entry>
         <para>
          Order If Decreasing Size
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b8pps5p">
        <entry>
         <para>
          Resize each of the component partitions.
         </para>
         <para/>
        </entry>
        <entry>
         <para>
          Increase or decrease the active size of each component partition.
          You remove only one component partition at a time, modify its
          size, then return it to the RAID.
         </para>
        </entry>
        <entry>
         <para>
          1
         </para>
        </entry>
        <entry>
         <para>
          2
         </para>
        </entry>
       </row>
       <row id="b8pps5q">
        <entry>
         <para>
          Resize the software RAID itself.
         </para>
        </entry>
        <entry>
         <para>
          The RAID does not automatically know about the increases or
          decreases you make to the underlying component partitions. You
          must inform it about the new size.
         </para>
        </entry>
        <entry>
         <para>
          2
         </para>
        </entry>
        <entry>
         <para>
          3
         </para>
        </entry>
       </row>
       <row id="b8pps5r">
        <entry>
         <para>
          Resize the file system.
         </para>
        </entry>
        <entry>
         <para>
          You must resize the file system that resides on the RAID. This is
          possible only for file systems that provide tools for resizing,
          such as Ext2, Ext3, and ReiserFS.
         </para>
        </entry>
        <entry>
         <para>
          3
         </para>
        </entry>
        <entry>
         <para>
          1
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para/>
   </sect2>
  </sect1>
  <sect1 id="resizeincr">
   <title>Increasing the Size of a Software RAID</title>

   <para>
    Before you begin, review the guidelines in
    <xref linkend="resizeunderstand" xrefstyle="SectTitleOnPage"/>.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="resizeincrpart" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="resizeincrraid" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="resizeincrfs" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="resizeincrpart">
    <title>Increasing the Size of Component Partitions</title>
    <para>
     Apply the procedure in this section to increase the size of a RAID 1,
     4, 5, or 6. For each component partition in the RAID, remove the
     partition from the RAID, modify its size, return it to the RAID, then
     wait until the RAID stabilizes to continue. While a partition is
     removed, the RAID operates in degraded mode and has no or reduced disk
     fault tolerance. Even for RAIDs that can tolerate multiple concurrent
     disk failures, do not remove more than one component partition at a
     time.
    </para>
    <warning>
     <para>
      If a RAID does not have disk fault tolerance, or it is simply not
      consistent, data loss results if you remove any of its partitions. Be
      very careful when removing partitions, and ensure that you have a
      backup of your data available.
     </para>
    </warning>
    <para>
     The procedure in this section uses the device names shown in the
     following table. Ensure that you modify the names to use the names of
     your own devices.
    </para>
    <table id="b8prrn8" frame="topbot" rowsep="1" pgwide="0">
     <title>Scenario for Increasing the Size of Component Partitions</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b8pry2e">
        <entry>
         <para>
          RAID Device
         </para>
        </entry>
        <entry>
         <para>
          Component Partitions
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b8pry2f">
        <entry>
         <para>
          <filename>/dev/md0</filename>
         </para>
        </entry>
        <entry>
         <para>
          <filename>/dev/sda1</filename>
         </para>
         <para>
          <filename>/dev/sdb1</filename>
         </para>
         <para>
          <filename>/dev/sdc1</filename>
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para>
     To increase the size of the component partitions for the RAID:
    </para>
    <procedure id="b8pp3av">
     <step id="b8progi">
      <para>
       Open a terminal console, then log in as the
       <systemitem>root</systemitem> user or equivalent.
      </para>
     </step>
     <step id="b8pp3aw">
      <para>
       Ensure that the RAID array is consistent and synchronized by entering
      </para>
<screen>
cat /proc/mdstat
</screen>
      <para>
       If your RAID array is still synchronizing according to the output of
       this command, you must wait until synchronization is complete before
       continuing.
      </para>
     </step>
     <step id="b8pp3ax">
      <para>
       Remove one of the component partitions from the RAID array. For
       example, to remove <filename>/dev/sda1</filename>, enter
      </para>
<screen>
mdadm /dev/md0 --fail /dev/sda1 --remove /dev/sda1
</screen>
      <para>
       In order to succeed, both the fail and remove actions must be done.
      </para>
     </step>
     <step id="b8pp3ay">
      <para>
       Increase the size of the partition that you removed in
       <xref linkend="b8pp3ax" xrefstyle="StepXRef"/> by doing one of the
       following:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Increase the size of the partition, using a disk partitioner such
         as <command>fdisk(8)</command>, <command>cfdisk(8)</command>, or
         <command>parted(8)</command>. This option is the usual choice.
        </para>
       </listitem>
       <listitem>
        <para>
         Replace the disk on which the partition resides with a
         higher-capacity device.
        </para>
        <para>
         This option is possible only if no other file systems on the
         original disk are accessed by the system. When the replacement
         device is added back into the RAID, it takes much longer to
         synchronize the data because all of the data that was on the
         original device must be rebuilt.
        </para>
       </listitem>
      </itemizedlist>
     </step>
     <step id="b8pp3az">
      <para>
       Re-add the partition to the RAID array. For example, to add
       <filename>/dev/sda1</filename>, enter
      </para>
<screen>
mdadm -a /dev/md0 /dev/sda1
</screen>
      <para>
       Wait until the RAID is synchronized and consistent before continuing
       with the next partition.
      </para>
     </step>
     <step id="b8ps27a">
      <para>
       Repeat <xref linkend="b8pp3aw" xrefstyle="StepXRef"/> through
       <xref linkend="b8pp3az" xrefstyle="StepXRef"/> for each of the
       remaining component devices in the array. Ensure that you modify the
       commands for the correct component partition.
      </para>
     </step>
     <step id="b8ps2tp">
      <para>
       If you get a message that tells you that the kernel could not re-read
       the partition table for the RAID, you must reboot the computer after
       all partitions have been resized to force an update of the partition
       table.
      </para>
     </step>
     <step id="b8q915s">
      <para>
       Continue with
       <xref linkend="resizeincrraid" xrefstyle="SectTitleOnPage"/>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="resizeincrraid">
    <title>Increasing the Size of the RAID Array</title>
    <para>
     After you have resized each of the component partitions in the RAID
     (see <xref linkend="resizeincrpart" xrefstyle="SectTitleOnPage"/>), the
     RAID array configuration continues to use the original array size until
     you force it to be aware of the newly available space. You can specify
     a size for the RAID or use the maximum available space.
    </para>
    <para>
     The procedure in this section uses the device name
     <filename>/dev/md0</filename> for the RAID device. Ensure that you
     modify the name to use the name of your own device.
    </para>
    <procedure id="b8pp3b0">
     <step id="b8psa0o">
      <para>
       Open a terminal console, then log in as the
       <systemitem>root</systemitem> user or equivalent.
      </para>
     </step>
     <step id="b8pp3b1">
      <para>
       Check the size of the array and the device size known to the array by
       entering
      </para>
<screen>
mdadm -D /dev/md0 | grep -e "Array Size" -e "Device Size"
</screen>
     </step>
     <step id="b8psbwk">
      <para>
       Do one of the following:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Increase the size of the array to the maximum available size by
         entering
        </para>
<screen>
mdadm --grow /dev/md0 -z max
</screen>
       </listitem>
      </itemizedlist>
      <itemizedlist>
       <listitem>
        <para>
         Increase the size of the array to the maximum available size by
         entering
        </para>
<screen>
mdadm --grow /dev/md0 -z max --assume-clean
</screen>
        <para>
         The array makes use of any space that has been added to the
         devices, but this space will not be synchronized. This is
         recommended for RAID1 because the sync is not needed. It can be
         useful for other RAID levels if the space that was added to the
         member devices was pre-zeroed.
        </para>
       </listitem>
      </itemizedlist>
      <itemizedlist>
       <listitem>
        <para>
         Increase the size of the array to a specified value by entering
        </para>
<screen>
mdadm --grow /dev/md0 -z <replaceable>size</replaceable>
</screen>
        <para>
         Replace <replaceable>size</replaceable> with an integer value in
         kilobytes (a kilobyte is 1024 bytes) for the desired size.
        </para>
       </listitem>
      </itemizedlist>
     </step>
     <step id="b8pp3b3">
      <para>
       Recheck the size of your array and the device size known to the array
       by entering
      </para>
<screen>
mdadm -D /dev/md0 | grep -e "Array Size" -e "Device Size"
</screen>
     </step>
     <step id="b8psgk9">
      <para>
       Do one of the following:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         If your array was successfully resized, continue with
         <xref linkend="resizeincrfs" xrefstyle="SectTitleOnPage"/>.
        </para>
       </listitem>
       <listitem>
        <para>
         If your array was not resized as you expected, you must reboot,
         then try this procedure again.
        </para>
       </listitem>
      </itemizedlist>
     </step>
    </procedure>
   </sect2>

   <sect2 id="resizeincrfs">
    <title>Increasing the Size of the File System</title>
    <para>
     After you increase the size of the array (see
     <xref linkend="resizeincrraid" xrefstyle="SectTitleOnPage"/>), you are
     ready to resize the file system.
    </para>
    <para>
     You can increase the size of the file system to the maximum space
     available or specify an exact size. When specifying an exact size for
     the file system, ensure that the new size satisfies the following
     conditions:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The new size must be greater than the size of the existing data;
       otherwise, data loss occurs.
      </para>
     </listitem>
     <listitem>
      <para>
       The new size must be equal to or less than the current RAID size
       because the file system size cannot extend beyond the space
       available.
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="b8uks1t">
     <title>Ext2 or Ext3</title>
     <para>
      Ext2 and Ext3 file systems can be resized when mounted or unmounted
      with the <command>resize2fs</command> command.
     </para>
     <procedure id="b8uksb5">
      <step id="b8uksb6">
       <para>
        Open a terminal console, then log in as the
        <systemitem>root</systemitem> user or equivalent.
       </para>
      </step>
      <step id="b8uksjw">
       <para>
        Increase the size of the file system using one of the following
        methods:
       </para>
       <itemizedlist>
        <listitem>
         <para>
          To extend the file system size to the maximum available size of
          the software RAID device called <filename>/dev/md0</filename>,
          enter
         </para>
<screen>
resize2fs /dev/md0
</screen>
         <para>
          If a size parameter is not specified, the size defaults to the
          size of the partition.
         </para>
        </listitem>
        <listitem>
         <para>
          To extend the file system to a specific size, enter
         </para>
<screen>
resize2fs /dev/md0 <replaceable>size</replaceable>
</screen>
         <para>
          The <replaceable>size</replaceable> parameter specifies the
          requested new size of the file system. If no units are specified,
          the unit of the size parameter is the block size of the file
          system. Optionally, the size parameter can be suffixed by one of
          the following the unit designators: s for 512 byte sectors; K for
          kilobytes (1 kilobyte is 1024 bytes); M for megabytes; or G for
          gigabytes.
         </para>
        </listitem>
       </itemizedlist>
       <para>
        Wait until the resizing is completed before continuing.
       </para>
      </step>
      <step id="b8ule4k">
       <para>
        If the file system is not mounted, mount it now.
       </para>
       <para>
        For example, to mount an Ext2 file system for a RAID named
        <filename>/dev/md0</filename> at mount point
        <filename>/raid</filename>, enter
       </para>
<screen>
mount -t ext2 /dev/md0 /raid
</screen>
      </step>
      <step id="b8ul4ag">
       <para>
        Check the effect of the resize on the mounted file system by
        entering
       </para>
<screen>
df -h
</screen>
       <para>
        The Disk Free (<command>df</command>) command shows the total size
        of the disk, the number of blocks used, and the number of blocks
        available on the file system. The -h option print sizes in
        human-readable format, such as 1K, 234M, or 2G.
       </para>
      </step>
     </procedure>
    </sect3>
    <sect3 id="b8ul8fq">
     <title>ReiserFS</title>
     <para>
      As with Ext2 and Ext3, a ReiserFS file system can be increased in size
      while mounted or unmounted. The resize is done on the block device of
      your RAID array.
     </para>
     <procedure id="b8ul8cv">
      <step id="b8ul8cw">
       <para>
        Open a terminal console, then log in as the
        <systemitem>root</systemitem> user or equivalent.
       </para>
      </step>
      <step id="b8ul8cx">
       <para>
        Increase the size of the file system on the software RAID device
        called <filename>/dev/md0</filename>, using one of the following
        methods:
       </para>
       <itemizedlist>
        <listitem>
         <para>
          To extend the file system size to the maximum available size of
          the device, enter
         </para>
<screen>
resize_reiserfs /dev/md0
</screen>
         <para>
          When no size is specified, this increases the volume to the full
          size of the partition.
         </para>
        </listitem>
        <listitem>
         <para>
          To extend the file system to a specific size, enter
         </para>
<screen>
resize_reiserfs -s <replaceable>size</replaceable> /dev/md0
</screen>
         <para>
          Replace <replaceable>size</replaceable> with the desired size in
          bytes. You can also specify units on the value, such as 50000K
          (kilobytes), 250M (megabytes), or 2G (gigabytes). Alternatively,
          you can specify an increase to the current size by prefixing the
          value with a plus (+) sign. For example, the following command
          increases the size of the file system on
          <filename>/dev/md0</filename> by 500 MB:
         </para>
<screen>
resize_reiserfs -s +500M /dev/md0
</screen>
        </listitem>
       </itemizedlist>
       <para>
        Wait until the resizing is completed before continuing.
       </para>
      </step>
      <step id="b8ulei7">
       <para>
        If the file system is not mounted, mount it now.
       </para>
       <para>
        For example, to mount an ReiserFS file system for a RAID named
        <filename>/dev/md0</filename> at mount point
        <filename>/raid</filename>, enter
       </para>
<screen>
mount -t reiserfs /dev/md0 /raid
</screen>
      </step>
      <step id="b8ul8cy">
       <para>
        Check the effect of the resize on the mounted file system by
        entering
       </para>
<screen>
df -h
</screen>
       <para>
        The Disk Free (<command>df</command>) command shows the total size
        of the disk, the number of blocks used, and the number of blocks
        available on the file system. The -h option print sizes in
        human-readable format, such as 1K, 234M, or 2G.
       </para>
      </step>
     </procedure>
    </sect3>
   </sect2>
  </sect1>
  <sect1 id="resizedecr">
   <title>Decreasing the Size of a Software RAID</title>

   <para>
    Before you begin, review the guidelines in
    <xref linkend="resizeunderstand" xrefstyle="SectTitleOnPage"/>.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="resizedecrfs" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="resizedecrraid" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="resizedecrpart" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="resizedecrfs">
    <title>Decreasing the Size of the File System</title>
    <para>
     When decreasing the size of the file system on a RAID device, ensure
     that the new size satisfies the following conditions:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The new size must be greater than the size of the existing data;
       otherwise, data loss occurs.
      </para>
     </listitem>
     <listitem>
      <para>
       The new size must be equal to or less than the current RAID size
       because the file system size cannot extend beyond the space
       available.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     In SUSE Linux Enterprise Server, Ext2, Ext3, and ReiserFS provide
     utilities for decreasing the size of the file system. Use the
     appropriate procedure below for decreasing the size of your file
     system.
    </para>
    <para>
     The procedures in this section use the device name
     <filename>/dev/md0</filename> for the RAID device. Ensure that you
     modify commands to use the name of your own device.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bznzca1" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bznzca8" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bznzca1">
     <title>Ext2 or Ext3</title>
     <para>
      The Ext2 and Ext3 file systems can be resized when mounted or
      unmounted.
     </para>
     <procedure id="bznzca2">
      <step id="bznzca3">
       <para>
        Open a terminal console, then log in as the
        <systemitem>root</systemitem> user or equivalent.
       </para>
      </step>
      <step id="bznzca4">
       <para>
        Decrease the size of the file system on the RAID by entering
       </para>
<screen>
resize2fs /dev/md0 &lt;size&gt;
</screen>
       <para>
        Replace <replaceable>size</replaceable> with an integer value in
        kilobytes for the desired size. (A kilobyte is 1024 bytes.)
       </para>
       <para>
        Wait until the resizing is completed before continuing.
       </para>
      </step>
      <step id="bznzca5">
       <para>
        If the file system is not mounted, mount it now. For example, to
        mount an Ext2 file system for a RAID named
        <filename>/dev/md0</filename> at mount point
        <filename>/raid</filename>, enter
       </para>
<screen>
mount -t ext2 /dev/md0 /raid
</screen>
      </step>
      <step id="bznzca6">
       <para>
        Check the effect of the resize on the mounted file system by
        entering
       </para>
<screen>
df -h
</screen>
       <para>
        The Disk Free (<command>df</command>) command shows the total size
        of the disk, the number of blocks used, and the number of blocks
        available on the file system. The -h option print sizes in
        human-readable format, such as 1K, 234M, or 2G.
       </para>
      </step>
      <step id="bznzca7">
       <para>
        Continue with
        <xref linkend="resizedecrraid" xrefstyle="SectTitleOnPage"/>.
       </para>
      </step>
     </procedure>
    </sect3>
    <sect3 id="bznzca8">
     <title>ReiserFS</title>
     <para>
      ReiserFS file systems can be decreased in size only if the volume is
      unmounted.
     </para>
     <procedure id="bznzca9">
      <step id="bznzcaa">
       <para>
        Open a terminal console, then log in as the
        <systemitem>root</systemitem> user or equivalent.
       </para>
      </step>
      <step id="bznzcab">
       <para>
        Unmount the device by entering
       </para>
<screen>
umount /mnt/point
</screen>
       <para>
        If the partition you are attempting to decrease in size contains
        system files (such as the root (<filename>/</filename>) volume),
        unmounting is possible only when booting from a removable device.
       </para>
      </step>
      <step id="bznzcac">
       <para>
        Decrease the size of the file system on the software RAID device
        called <filename>/dev/md0</filename> by entering
       </para>
<screen>
resize_reiserfs -s <replaceable>size</replaceable> /dev/md0
</screen>
       <para>
        Replace <replaceable>size</replaceable> with the desired size in
        bytes. You can also specify units on the value, such as 50000K
        (kilobytes), 250M (megabytes), or 2G (gigabytes). Alternatively, you
        can specify a decrease to the current size by prefixing the value
        with a minus (-) sign. For example, the following command reduces
        the size of the file system on <filename>/dev/md0</filename> by 500
        MB:
       </para>
<screen>
resize_reiserfs -s -500M /dev/md0
</screen>
       <para>
        Wait until the resizing is completed before continuing.
       </para>
      </step>
      <step id="bznzcad">
       <para>
        Mount the file system by entering
       </para>
<screen>
mount -t reiserfs /dev/md0 /mnt/point
</screen>
      </step>
      <step id="bznzcae">
       <para>
        Check the effect of the resize on the mounted file system by
        entering
       </para>
<screen>
df -h
</screen>
       <para>
        The Disk Free (<command>df</command>) command shows the total size
        of the disk, the number of blocks used, and the number of blocks
        available on the file system. The -h option print sizes in
        human-readable format, such as 1K, 234M, or 2G.
       </para>
      </step>
      <step id="bznzcaf">
       <para>
        Continue with
        <xref linkend="resizedecrraid" xrefstyle="SectTitleOnPage"/>.
       </para>
      </step>
     </procedure>
    </sect3>
   </sect2>

   <sect2 id="resizedecrraid">
    <title>Decreasing the Size of the RAID Array</title>
    <para>
     After you have resized the file system, the RAID array configuration
     continues to use the original array size until you force it to reduce
     the available space. Use the <command>mdadm --grow</command> mode to
     force the RAID to use a smaller segment size. To do this, you must use
     the -z option to specify the amount of space in kilobytes to use from
     each device in the RAID. This size must be a multiple of the chunk
     size, and it must leave about 128KB of space for the RAID superblock to
     be written to the device.
    </para>
    <para>
     The procedure in this section uses the device name
     <filename>/dev/md0</filename> for the RAID device. Ensure that you
     modify commands to use the name of your own device.
    </para>
    <procedure id="bznzcau">
     <step id="bznzcav">
      <para>
       Open a terminal console, then log in as the
       <systemitem>root</systemitem> user or equivalent.
      </para>
     </step>
     <step id="bznzcaw">
      <para>
       Check the size of the array and the device size known to the array by
       entering
      </para>
<screen>
mdadm -D /dev/md0 | grep -e "Array Size" -e "Device Size"
</screen>
     </step>
     <step id="bznzcax">
      <para>
       Decrease the size of the array’s device size to a specified value
       by entering
      </para>
<screen>
mdadm --grow /dev/md0 -z <replaceable>&lt;size</replaceable>&gt;
</screen>
      <para>
       Replace <replaceable>size</replaceable> with an integer value in
       kilobytes for the desired size. (A kilobyte is 1024 bytes.)
      </para>
      <para>
       For example, the following command sets the segment size for each
       RAID device to about 40 GB where the chunk size is 64 KB. It includes
       128 KB for the RAID superblock.
      </para>
<screen>
mdadm --grow /dev/md2 -z 41943168
</screen>
     </step>
     <step id="bznzcay">
      <para>
       Recheck the size of your array and the device size known to the array
       by entering
      </para>
<screen>
mdadm -D /dev/md0 | grep -e "Array Size" -e "Device Size"
</screen>
     </step>
     <step id="bznzcaz">
      <para>
       Do one of the following:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         If your array was successfully resized, continue with
         <xref linkend="resizedecrpart" xrefstyle="SectTitleOnPage"/>.
        </para>
       </listitem>
       <listitem>
        <para>
         If your array was not resized as you expected, you must reboot,
         then try this procedure again.
        </para>
       </listitem>
      </itemizedlist>
     </step>
    </procedure>
   </sect2>

   <sect2 id="resizedecrpart">
    <title>Decreasing the Size of Component Partitions</title>
    <para>
     After you decrease the segment size that is used on each device in the
     RAID, the remaining space in each component partition is not used by
     the RAID. You can leave partitions at their current size to allow for
     the RAID to grow at a future time, or you can reclaim this now unused
     space.
    </para>
    <para>
     To reclaim the space, you decrease the component partitions one at a
     time. For each component partition, you remove it from the RAID, reduce
     its partition size, return the partition to the RAID, then wait until
     the RAID stabilizes. To allow for metadata, you should specify a
     slightly larger size than the size you specified for the RAID in
     <xref linkend="resizedecrraid" xrefstyle="SectTitleOnPage"/>.
    </para>
    <para>
     While a partition is removed, the RAID operates in degraded mode and
     has no or reduced disk fault tolerance. Even for RAIDs that can
     tolerate multiple concurrent disk failures, you should never remove
     more than one component partition at a time.
    </para>
    <warning>
     <para>
      If a RAID does not have disk fault tolerance, or it is simply not
      consistent, data loss results if you remove any of its partitions. Be
      very careful when removing partitions, and ensure that you have a
      backup of your data available.
     </para>
    </warning>
    <para>
     The procedure in this section uses the device names shown in the
     following table. Ensure that you modify the commands to use the names
     of your own devices.
    </para>
    <table id="b8q9r5c" frame="topbot" rowsep="1" pgwide="0">
     <title>Scenario for Increasing the Size of Component Partitions</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="bznzcai">
        <entry>
         <para>
          RAID Device
         </para>
        </entry>
        <entry>
         <para>
          Component Partitions
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bznzcaj">
        <entry>
         <para>
          <filename>/dev/md0</filename>
         </para>
        </entry>
        <entry>
         <para>
          <filename>/dev/sda1</filename>
         </para>
         <para>
          <filename>/dev/sdb1</filename>
         </para>
         <para>
          <filename>/dev/sdc1</filename>
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para>
     To decrease the size of the component partitions for the RAID:
    </para>
    <procedure id="bznzcak">
     <step id="bznzcal">
      <para>
       Open a terminal console, then log in as the
       <systemitem>root</systemitem> user or equivalent.
      </para>
     </step>
     <step id="bznzcam">
      <para>
       Ensure that the RAID array is consistent and synchronized by entering
      </para>
<screen>
cat /proc/mdstat
</screen>
      <para>
       If your RAID array is still synchronizing according to the output of
       this command, you must wait until synchronization is complete before
       continuing.
      </para>
     </step>
     <step id="bznzcan">
      <para>
       Remove one of the component partitions from the RAID array. For
       example, to remove <filename>/dev/sda1</filename>, enter
      </para>
<screen>
mdadm /dev/md0 --fail /dev/sda1 --remove /dev/sda1
</screen>
      <para>
       In order to succeed, both the fail and remove actions must be done.
      </para>
     </step>
     <step id="bznzcao">
      <para>
       Decrease the size of the partition that you removed in
       <xref linkend="b8pp3ax" xrefstyle="StepXRef"/> to a size that is
       slightly larger than the size you set for the segment size. The size
       should be a multiple of the chunk size and allow 128 KB for the RAID
       superblock. Use a disk partitioner such as <command>fdisk</command>,
       <command>cfdisk</command>, or <command>parted</command> to decrease
       the size of the partition.
      </para>
     </step>
     <step id="bznzcap">
      <para>
       Re-add the partition to the RAID array. For example, to add
       <filename>/dev/sda1</filename>, enter
      </para>
<screen>
mdadm -a /dev/md0 /dev/sda1
</screen>
      <para>
       Wait until the RAID is synchronized and consistent before continuing
       with the next partition.
      </para>
     </step>
     <step id="bznzcaq">
      <para>
       Repeat <xref linkend="b8pp3aw" xrefstyle="StepXRef"/> through
       <xref linkend="b8pp3az" xrefstyle="StepXRef"/> for each of the
       remaining component devices in the array. Ensure that you modify the
       commands for the correct component partition.
      </para>
     </step>
     <step id="bznzcar">
      <para>
       If you get a message that tells you that the kernel could not re-read
       the partition table for the RAID, you must reboot the computer after
       resizing all of its component partitions.
      </para>
     </step>
     <step id="bzo0b0b">
      <para>
       (Optional) Expand the size of the RAID and file system to use the
       maximum amount of space in the now smaller component partitions:
      </para>
      <substeps>
       <step id="bznzcas">
        <para>
         Expand the size of the RAID to use the maximum amount of space that
         is now available in the reduced-size component partitions:
        </para>
<screen>
mdadm --grow /dev/md0 -z max
</screen>
       </step>
       <step id="bzo00hx">
        <para>
         Expand the size of the file system to use all of the available
         space in the newly resized RAID. For information, see
         <xref linkend="resizeincrfs" xrefstyle="SectTitleOnPage"/>.
        </para>
       </step>
      </substeps>
     </step>
    </procedure>
   </sect2>
  </sect1>
 </chapter>
 <chapter id="cha_raids_lee" lang="en">
  <title>Storage Enclosure LED Utilities for MD Software RAIDs</title>
  <para>
   <remark>313626 Update Enclosure LED Utilities</remark>
   Storage enclosure LED Monitoring utility (<command>ledmon(8)</command>)
   and LED Control (<command>ledctl(8)</command>) utility are Linux user
   space applications that use a broad range of interfaces and protocols to
   control storage enclosure LEDs. The primary usage is to visualize the
   status of Linux MD software RAID devices created with the mdadm utility.
   The ledmon daemon monitors the status of the drive array and updates the
   status of the drive LEDs. The ledctl utility allows you to set LED
   patterns for specified devices.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="b14fewb2" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b14fewb3" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b13jg5li" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b13jg5lj" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b13jg5lk" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b14feg6m" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="b14fewb2">
   <title>Supported LED Management Protocols</title>

   <para>
    These LED utilities use the SGPIO (Serial General Purpose Input/Output)
    specification (Small Form Factor (SFF) 8485) and the SCSI Enclosure
    Services (SES) 2 protocol to control LEDs. They implement the
    International Blinking Pattern Interpretation (IBPI) patterns of the
    SFF-8489 specification for SGPIO. The IBPI defines how the SGPIO
    standards are interpreted as states for drives and slots on a backplane
    and how the backplane should visualize the states with LEDs.
   </para>

   <para>
    Some storage enclosures do not adhere strictly to the SFF-8489
    specification. An enclosure processor might accept an IBPI pattern but
    not blink the LEDs according to the SFF-8489 specification, or the
    processor might support only a limited number of the IBPI patterns.
   </para>

   <para>
    LED management (AHCI) and SAF-TE protocols are not supported by the
    <command>ledmon</command> and <command>ledctl</command> utilities.
   </para>
  </sect1>
  <sect1 id="b14fewb3">
   <title>Supported Storage Enclosure Systems</title>

   <para>
    The <command>ledmon</command> and <command>ledctl</command> applications
    have been verified to work with Intel storage controllers such as the
    Intel AHCI controller and Intel SAS controller. Beginning in SUSE Linux
    Enterprise Server 11 SP3, they also support PCIe-SSD (solid state disk)
    enclosure LEDs to control the storage enclosure status (OK, Fail,
    Rebuilding) LEDs of PCIe-SSD devices that are part of an MD software
    RAID volume. The applications might also work with the IBPI-compliant
    storage controllers of other vendors (especially SAS/SCSI controllers);
    however, other vendors’ controllers have not been tested.
   </para>
  </sect1>
  <sect1 id="b13jg5li">
   <title>Storage Enclosure LED Monitor Service (ledmon(8))</title>

   <para>
    The <command>ledmon</command> application is a daemon process that
    constantly monitors the state of MD software RAID devices or the state
    of block devices in a storage enclosure or drive bay. Only a single
    instance of the daemon should be running at a time. The
    <command>ledmon</command> application is part of Intel Enclosure LED
    Utilities.
   </para>

   <para>
    The state is visualized on LEDs associated with each slot in a storage
    array enclosure or a drive bay. The application monitors all software
    RAID devices and visualizes their state. It does not provide a way to
    monitor only selected software RAID volumes.
   </para>

   <para>
    The <command>ledmon</command> application supports two types of LED
    systems: A two-LED system (Activity LED and Status LED) and a three-LED
    system (Activity LED, Locate LED, and Fail LED). This tool has the
    highest priority when accessing the LEDs.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b14fbg00" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14fbfkj" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14fbfkq" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14fbfku" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b14fbg00">
    <title>Syntax</title>
<screen>
ledmon [options]
</screen>
    <para>
     Issue the command as the <systemitem>root</systemitem> user or as user
     with <systemitem>root</systemitem> privileges.
    </para>
   </sect2>

   <sect2 id="b14fbfkj">
    <title>Options</title>
    <variablelist>
     <varlistentry id="b14fbfkk">
      <term>-c</term>
      <term>--confg=path</term>
      <listitem>
       <para>
        Sets a path to local configuration file. If this option is
        specified, the global configuration file and user configuration file
        have no effect.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fbfkl">
      <term>-l</term>
      <term>--log=path</term>
      <listitem>
       <para>
        Sets a path to local log file. If this user-defined file is
        specified, the global log file
        <filename>/var/log/ledmon.log</filename> is not used.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fbfkm">
      <term>-t</term>
      <term>--interval=seconds</term>
      <listitem>
       <para>
        Sets the time interval between scans of <filename>sysfs</filename>.
        The value is given in seconds. The minimum is 5 seconds. The maximum
        is not specified.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fbfkn">
      <term>&lt;--quiet|--error|--warning|--info|--debug|--all&gt;</term>
      <listitem>
       <para>
        Specifies the verbose level. The level options are specified in the
        order of no information to the most information. Use the --quiet
        option for no logging. Use the --all option to log everything. If
        you specify more then one verbose option, the last option in the
        command applies.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fbfko">
      <term>-h</term>
      <term>--help</term>
      <listitem>
       <para>
        Prints the command information to the console, then exits.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fbfkp">
      <term>-v</term>
      <term>--version</term>
      <listitem>
       <para>
        Displays version of <command>ledmon</command> and information about
        the license, then exits.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect2>

   <sect2 id="b14fbfkq">
    <title>Files</title>
    <variablelist>
     <varlistentry id="b14fbfkr">
      <term>/var/log/ledmon.log</term>
      <listitem>
       <para>
        Global log file, used by <command>ledmon</command> application. To
        force logging to a user-defined file, use the <literal>-l</literal>
        option.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fbfks">
      <term>~/.ledctl</term>
      <listitem>
       <para>
        User configuration file, shared between <command>ledmon</command>
        and all <command>ledctl</command> application instances.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fbfkt">
      <term>/etc/ledcfg.conf</term>
      <listitem>
       <para>
        Global configuration file, shared between <command>ledmon</command>
        and all <command>ledctl</command> application instances.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect2>

   <sect2 id="b14fbfku">
    <title>Known Issues</title>
    <para>
     The <command>ledmon</command> daemon does not recognize the PFA
     (Predicted Failure Analysis) state from the SFF-8489 specification.
     Thus, the PFA pattern is not visualized.
    </para>
   </sect2>
  </sect1>
  <sect1 id="b13jg5lj">
   <title>Storage Enclosure LED Control Application (ledctl(8))</title>

   <para>
    The Enclosure LED Application (<command>ledctl(8)</command>) is a user
    space application that controls LEDs associated with each slot in a
    storage enclosure or a drive bay. The <command>ledctl</command>
    application is a part of Intel Enclosure LED Utilities.
   </para>

   <para>
    When you issue the command, the LEDs of the specified devices are set to
    a specified pattern and all other LEDs are turned off. User must have
    root privileges to use this application. Because the
    <command>ledmon</command> application has the highest priority when
    accessing LEDs, some patterns set by ledctl might have no effect if
    <command>ledmon</command> is running (except the Locate pattern).
   </para>

   <para>
    The <command>ledctl</command> application supports two types of LED
    systems: A two-LED system (Activity LED and Status LED) and a three-LED
    system (Activity LED, Fail LED, and Locate LED).
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b14fbijh" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14fbj24" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14fcdo6" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14fck4r" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14fck4s" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14fcmzl" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14fcrjc" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b14fbijh">
    <title>Syntax</title>
<screen>
ledctl [options] <replaceable>pattern_name</replaceable>=list_of_devices 
</screen>
    <para>
     Issue the command as the <systemitem>root</systemitem> user or as user
     with <systemitem>root</systemitem> privileges.
    </para>
   </sect2>

   <sect2 id="b14fbj24">
    <title>Pattern Names</title>
    <para>
     The <command>ledctl</command> application accepts the following names
     for <guimenu>pattern_name</guimenu> argument, according to the SFF-8489
     specification.
    </para>
    <variablelist>
     <varlistentry id="b14fbl2q">
      <term>locate</term>
      <listitem>
       <para>
        Turns on the Locate LED associated with the specified devices or
        empty slots. This state is used to identify a slot or drive.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fbxbj">
      <term>locate_off</term>
      <listitem>
       <para>
        Turns off the Locate LED associated with the specified devices or
        empty slots.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fc0mu">
      <term>normal</term>
      <listitem>
       <para>
        Turns off the Status LED, Failure LED, and Locate LED associated
        with the specified devices.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fc0mv">
      <term>off</term>
      <listitem>
       <para>
        Turns off only the Status LED and Failure LED associated with the
        specified devices.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fc0mw">
      <term>ica</term>
      <term>degraded</term>
      <listitem>
       <para>
        Visualizes the <literal>In a Critical Array</literal> pattern.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fc1ii">
      <term>rebuild</term>
      <term>rebuild_p</term>
      <listitem>
       <para>
        Visualizes the <literal>Rebuild</literal> pattern. This supports
        both of the rebuild states for compatibility and legacy reasons.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fc1ij">
      <term>ifa</term>
      <term>failed_array</term>
      <listitem>
       <para>
        Visualizes the <literal>In a Failed Array</literal> pattern.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fc1ik">
      <term>hotspare</term>
      <listitem>
       <para>
        Visualizes the <literal>Hotspare</literal> pattern.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fc2n0">
      <term>pfa</term>
      <listitem>
       <para>
        Visualizes the <literal>Predicted Failure Analysis</literal>
        pattern.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fc2n1">
      <term>failure</term>
      <term>disk_failed</term>
      <listitem>
       <para>
        Visualizes the <literal>Failure</literal> pattern.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdno">
      <term>ses_abort</term>
      <listitem>
       <para>
        SES-2 R/R ABORT
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdnp">
      <term>ses_rebuild</term>
      <listitem>
       <para>
        SES-2 REBUILD/REMAP
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdnq">
      <term>ses_ifa</term>
      <listitem>
       <para>
        SES-2 IN FAILED ARRAY
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdnr">
      <term>ses_ica</term>
      <listitem>
       <para>
        SES-2 IN CRITICAL ARRAY
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdns">
      <term>ses_cons_check</term>
      <listitem>
       <para>
        SES-2 CONS CHECK
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdnt">
      <term>ses_hotspare</term>
      <listitem>
       <para>
        SES-2 HOTSPARE
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdnu">
      <term>ses_rsvd_dev</term>
      <listitem>
       <para>
        SES-2 RSVD DEVICE
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdnv">
      <term>ses_ok</term>
      <listitem>
       <para>
        SES-2 OK
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdnw">
      <term>ses_ident</term>
      <listitem>
       <para>
        SES-2 IDENT
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdnx">
      <term>ses_rm</term>
      <listitem>
       <para>
        SES-2 REMOVE
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdny">
      <term>ses_insert</term>
      <listitem>
       <para>
        SES-2 INSERT
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdnz">
      <term>ses_missing</term>
      <listitem>
       <para>
        SES-2 MISSING
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdo0">
      <term>ses_dnr</term>
      <listitem>
       <para>
        SES-2 DO NOT REMOVE
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdo1">
      <term>ses_active</term>
      <listitem>
       <para>
        SES-2 ACTIVE
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdo2">
      <term>ses_enable_bb</term>
      <listitem>
       <para>
        SES-2 ENABLE BYP B
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdo3">
      <term>ses_enable_ba</term>
      <listitem>
       <para>
        SES-2 ENABLE BYP A
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdo4">
      <term>ses_devoff</term>
      <listitem>
       <para>
        SES-2 DEVICE OFF
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcdo5">
      <term>ses_fault</term>
      <listitem>
       <para>
        SES-2 FAULT
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect2>

   <sect2 id="b14fcdo6">
    <title>Pattern Translation</title>
    <para>
     When a non-SES-2 pattern is sent to a device in an enclosure, the
     pattern is automatically translated to the SCSI Enclosure Services
     (SES) 2 pattern as shown in
     <xref linkend="b14fcdo7" xrefstyle="TableXRef"/>.
    </para>
    <table id="b14fcdo7"  frame="topbot" rowsep="1" pgwide="0">
     <title>Translation between Non-SES-2 Patterns and SES-2 Patterns</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b14fcdo8">
        <entry>
         <para>
          Non-SES-2 Pattern
         </para>
        </entry>
        <entry>
         <para>
          SES-2 Pattern
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b14fcdo9">
        <entry>
         <para>
          locate
         </para>
        </entry>
        <entry>
         <para>
          ses_ident
         </para>
        </entry>
       </row>
       <row id="b14fcdoa">
        <entry>
         <para>
          locate_off
         </para>
        </entry>
        <entry>
         <para>
          ses_ident
         </para>
        </entry>
       </row>
       <row id="b14fcdob">
        <entry>
         <para>
          normal
         </para>
        </entry>
        <entry>
         <para>
          ses_ok
         </para>
        </entry>
       </row>
       <row id="b14fcdoc">
        <entry>
         <para>
          off
         </para>
        </entry>
        <entry>
         <para>
          ses_ok
         </para>
        </entry>
       </row>
       <row id="b14fcdod">
        <entry>
         <para>
          ica
         </para>
        </entry>
        <entry>
         <para>
          ses_ica
         </para>
        </entry>
       </row>
       <row id="b14fcdoe">
        <entry>
         <para>
          degraded
         </para>
        </entry>
        <entry>
         <para>
          ses_ica
         </para>
        </entry>
       </row>
       <row id="b14fcdof">
        <entry>
         <para>
          rebuild
         </para>
        </entry>
        <entry>
         <para>
          ses_rebuild
         </para>
        </entry>
       </row>
       <row id="b14fcdog">
        <entry>
         <para>
          rebuild_p
         </para>
        </entry>
        <entry>
         <para>
          ses_rebuild
         </para>
        </entry>
       </row>
       <row id="b14fcdoh">
        <entry>
         <para>
          ifa
         </para>
        </entry>
        <entry>
         <para>
          ses_ifa
         </para>
        </entry>
       </row>
       <row id="b14fcdoi">
        <entry>
         <para>
          failed_array
         </para>
        </entry>
        <entry>
         <para>
          ses_ifa
         </para>
        </entry>
       </row>
       <row id="b14fcdoj">
        <entry>
         <para>
          hotspare
         </para>
        </entry>
        <entry>
         <para>
          ses_hotspare
         </para>
        </entry>
       </row>
       <row id="b14fcdok">
        <entry>
         <para>
          pfa
         </para>
        </entry>
        <entry>
         <para>
          ses_rsvd_dev
         </para>
        </entry>
       </row>
       <row id="b14fcdol">
        <entry>
         <para>
          failure
         </para>
        </entry>
        <entry>
         <para>
          ses_fault
         </para>
        </entry>
       </row>
       <row id="b14fcdom">
        <entry>
         <para>
          disk_failed
         </para>
        </entry>
        <entry>
         <para>
          ses_fault
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </sect2>

   <sect2 id="b14fck4r">
    <title>List of Devices</title>
    <para>
     When you issue the <command>ledctl</command> command, the LEDs of the
     specified devices are set to the specified pattern and all other LEDs
     are turned off. The list of devices can be provided in one of two
     formats:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       A list of devices separated by a comma and no spaces
      </para>
     </listitem>
     <listitem>
      <para>
       A list in curly braces with devices separated by a space
      </para>
     </listitem>
    </itemizedlist>
    <para>
     If you specify multiple patterns in the same command, the device list
     for each pattern can use the same or different format. For examples
     that show the two list formats, see
     <xref linkend="b14fcrjc" xrefstyle="SectTitleOnPage"/>.
    </para>
    <para>
     A device is a path to file in the <filename>/dev</filename> directory
     or in the <filename>/sys/block</filename> directory. The path can
     identify a block device, an MD software RAID device, or a container
     device. For a software RAID device or a container device, the reported
     LED state is set for all of the associated block devices.
    </para>
    <para>
     The LEDs of devices listed in list_of_devices are set to the given
     pattern pattern_name and all other LEDs are turned off.
    </para>
   </sect2>

   <sect2 id="b14fck4s">
    <title>Options</title>
    <variablelist>
     <varlistentry id="b14fck4t">
      <term>-c</term>
      <term>--confg=path</term>
      <listitem>
       <para>
        Sets a path to local configuration file. If this option is
        specified, the global configuration file and user configuration file
        have no effect.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fck4u">
      <term>-l</term>
      <term>--log=path</term>
      <listitem>
       <para>
        Sets a path to local log file. If this user-defined file is
        specified, the global log file
        <filename>/var/log/ledmon.log</filename> is not used.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fck4v">
      <term>--quiet</term>
      <listitem>
       <para>
        Turns off all messages sent to <filename>stdout</filename> or
        <filename>stderr</filename> out. The messages are still logged to
        local file and the <filename>syslog</filename> facility.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fck4w">
      <term>-h</term>
      <term>--help</term>
      <listitem>
       <para>
        Prints the command information to the console, then exits.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fck4x">
      <term>-v</term>
      <term>--version</term>
      <listitem>
       <para>
        Displays version of <command>ledctl</command> and information about
        the license, then exits.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect2>

   <sect2 id="b14fcmzl">
    <title>Files</title>
    <variablelist>
     <varlistentry id="b14fcmzm">
      <term>/var/log/ledctl.log</term>
      <listitem>
       <para>
        Global log file, used by all instances of the
        <command>ledctl</command> application. To force logging to a
        user-defined file, use the <literal>-l</literal> option.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcmzn">
      <term>~/.ledctl</term>
      <listitem>
       <para>
        User configuration file, shared between <command>ledmon</command>
        and all <command>ledctl</command> application instances.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="b14fcmzo">
      <term>/etc/ledcfg.conf</term>
      <listitem>
       <para>
        Global configuration file, shared between <command>ledmon</command>
        and all <command>ledctl</command> application instances.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect2>

   <sect2 id="b14fcrjc">
    <title>Examples</title>
    <para>
     To locate a single block device:
    </para>
<screen>
ledctl locate=/dev/sda
</screen>
    <para>
     To turn off the Locate LED off for a single block device:
    </para>
<screen>
ledctl locate_off=/dev/sda
</screen>
    <para>
     To locate disks of an MD software RAID device and to set a rebuild
     pattern for two of its block devices at the same time:
    </para>
<screen>
ledctl locate=/dev/md127 rebuild={ /sys/block/sd[a-b] }
</screen>
    <para>
     To turn off the Status LED and Failure LED for the specified devices:
    </para>
<screen>
ledctl off={ /dev/sda /dev/sdb }
</screen>
    <para>
     To locate three block devices:
    </para>
<screen>
ledctl locate=/dev/sda,/dev/sdb,/dev/sdc

ledctl locate={ /dev/sda /dev/sdb /dev/sdc }
</screen>
   </sect2>
  </sect1>
  <sect1 id="b13jg5lk">
   <title>Enclosure LED Utilities Configuration File (ledctl.conf(5))</title>

   <para>
    The <filename>ledctl.conf</filename> file is the configuration file for
    the Intel Enclosure LED Utilities. The utilities do not use a
    configuration file at the moment. The name and location of file have
    been reserved for feature improvements.
   </para>
  </sect1>
  <sect1 id="b14feg6m">
   <title>Additional Information</title>

   <para>
    See the following resources for details about the LED patterns and
    monitoring tools:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      <ulink url="http://ledmon.sourceforge.net/">LEDMON open source project
      on Sourceforge.net</ulink>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="ftp://ftp.seagate.com/sff/SFF-8485.PDF">SGPIO
      specification SFF-8485</ulink>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="ftp://ftp.seagate.com/sff/SFF-8489.PDF">IBPI specification
      SFF-8489</ulink>
     </para>
    </listitem>
   </itemizedlist>
  </sect1>
 </chapter>
 <chapter id="isns" lang="en">
  <title>iSNS for Linux</title>
  <para>
   Storage area networks (SANs) can contain many disk drives that are
   dispersed across complex networks. This can make device discovery and
   device ownership difficult. iSCSI initiators must be able to identify
   storage resources in the SAN and determine whether they have access to
   them.
  </para>
  <para>
   Internet Storage Name Service (iSNS) is a standards-based service that
   facilitates the automated discovery, management, and configuration of
   iSCSI devices on a TCP/IP network. iSNS provides intelligent storage
   discovery and management services comparable to those found in Fibre
   Channel networks.
  </para>
  <important>
   <para>
    iSNS should be used only in secure internal networks.
   </para>
  </important>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="sec.isns.overview" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec.iscsi.setup" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bgchyh0" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bi0bdwd" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bi0cewc" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bgchyhq" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="sec.isns.overview">
   <title>How iSNS Works</title>

   <para>
    For an iSCSI initiator to discover iSCSI targets, it needs to identify
    which devices in the network are storage resources and what IP addresses
    it needs to access them. A query to an iSNS server returns a list of
    iSCSI targets and the IP addresses that the initiator has permission to
    access.
   </para>

   <para>
    Using iSNS, you create iSNS discovery domains and discovery domain sets.
    You then group or organize iSCSI targets and initiators into discovery
    domains and group the discovery domains into discovery domain sets. By
    dividing storage nodes into domains, you can limit the discovery process
    of each host to the most appropriate subset of targets registered with
    iSNS, which allows the storage network to scale by reducing the number
    of unnecessary discoveries and by limiting the amount of time each host
    spends establishing discovery relationships. This lets you control and
    simplify the number of targets and initiators that must be discovered.
   </para>

   <figure pgwide="0" id="bcna0wb">
    <title>iSNS Discovery Domains and Discovery Domain Sets</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="isns_a.png" width="329pt" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="isns_a.png" width="329pt" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>

   <para>
    Both iSCSI targets and iSCSI initiators use iSNS clients to initiate
    transactions with iSNS servers by using the iSNS protocol. They then
    register device attribute information in a common discovery domain,
    download information about other registered clients, and receive
    asynchronous notification of events that occur in their discovery
    domain.
   </para>

   <para>
    iSNS servers respond to iSNS protocol queries and requests made by iSNS
    clients using the iSNS protocol. iSNS servers initiate iSNS protocol
    state change notifications and store properly authenticated information
    submitted by a registration request in an iSNS database.
   </para>

   <para>
    Some of the benefits provided by iSNS for Linux include:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      Provides an information facility for registration, discovery, and
      management of networked storage assets.
     </para>
    </listitem>
    <listitem>
     <para>
      Integrates with the DNS infrastructure.
     </para>
    </listitem>
<!--   <listitem>
    <para>
     Provides access control for registered targets and initiators
    </para>
   </listitem> -->
    <listitem>
     <para>
      Consolidates registration, discovery, and management of iSCSI storage.
     </para>
    </listitem>
    <listitem>
     <para>
      Simplifies storage management implementations.
     </para>
    </listitem>
    <listitem>
     <para>
      Improves scalability compared to other discovery methods.
     </para>
    </listitem>
   </itemizedlist>

   <para>
    An example of the benefits iSNS provides can be better understood
    through the following scenario:
   </para>

<!-- access control is not supported according to hare. And of
  course, you may always circumvent the iSNS server and discover the
  target directly -->

   <para>
    Suppose you have a company that has 100 iSCSI initiators and 100 iSCSI
    targets. Depending on your configuration, all iSCSI initiators could
    potentially try to discover and connect to any of the 100 iSCSI targets.
    This could create discovery and connection difficulties. By grouping
    initiators and targets into discovery domains, you can prevent iSCSI
    initiators in one department from discovering the iSCSI targets in
    another department. The result is that the iSCSI initiators in a
    specific department only discover those iSCSI targets that are part of
    the department&rsquo;s discovery domain.
   </para>
  </sect1>
  <sect1 id="sec.iscsi.setup">
   <title>Installing iSNS Server for Linux</title>

   <para>
    iSNS Server for Linux is included with SLES 10 SP2 and later, but is not
    installed or configured by default. You must install the iSNS package
    modules (<filename>isns</filename> and <filename>yast2-isns</filename>
    modules) and configure the iSNS service.
   </para>

<!-- this funtionality defined in rfc, and the slave side seems to be
 sort of implemented, the master іs not available right now.
  <para>
   iSNS needs to be installed on only one server on your network. To
   provide an added level of redundancy, you can configure iSNS to be a
   cluster resource that can be failed over or migrated to another server
   on your network.
  </para>
  -->

   <note>
    <para>
     iSNS can be installed on the same server where iSCSI target or iSCSI
     initiator software is installed. Installing both the iSCSI target
     software and iSCSI initiator software on the same server is not
     supported.
    </para>
   </note>

   <para>
    To install iSNS for Linux:
   </para>

   <procedure id="bgchygw">
    <step id="bgchygx">
     <para>
      Start YaST and select <guimenu>Network Services</guimenu><guimenu>iSNS
      Server</guimenu>.
     </para>
    </step>
    <step id="bi0bpeh">
     <para>
      When prompted to install the <filename>isns</filename> package, click
      <guimenu>Install</guimenu>.
     </para>
    </step>
    <step id="bi0bq1e">
     <para>
      Follow the install instructions to provide the &productname;
      installation disks.
     </para>
     <para role="intro">
      When the installation is complete, the iSNS Service configuration
      dialog box opens automatically to the <guimenu>Service</guimenu> tab.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="isns_config_a.png" width="307pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="isns_config_a.png" width="307pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="bi0bxbd">
     <para>
      In <guimenu>Address of iSNS Server</guimenu>, specify the DNS name or
      IP address of the iSNS Server.
     </para>
    </step>
    <step id="bgchygy">
     <para>
      In <guimenu>Service Start</guimenu>, select one of the following:
     </para>
     <itemizedlist>
      <listitem>
       <formalpara id="bi0c15l">
        <title>When Booting:</title>
        <para>
         The iSNS service starts automatically on server startup.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="bi0c69a">
        <title>Manually (Default):</title>
        <para>
         The iSNS service must be started manually by entering <command>sudo
         systemctl start isns.service</command> at the server console of the
         server where you install it.
        </para>
       </formalpara>
      </listitem>
     </itemizedlist>
    </step>
    <step id="bi0c2ei">
     <para>
      Specify the following firewall settings:
     </para>
     <itemizedlist>
      <listitem>
       <formalpara id="bi0c69b">
        <title>Open Port in Firewall:</title>
        <para>
         Select the check box to open the firewall and allow access to the
         service from remote computers. The firewall port is closed by
         default.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="bi0c69c">
        <title>Firewall Details:</title>
        <para>
         If you open the firewall port, the port is open on all network
         interfaces by default. Click <guimenu>Firewall Details</guimenu> to
         select interfaces on which to open the port, select the network
         interfaces to use, then click <guimenu>OK</guimenu>.
        </para>
       </formalpara>
      </listitem>
     </itemizedlist>
    </step>
    <step id="bgchygz">
     <para>
      Click <guimenu>Finish</guimenu> to apply the configuration settings
      and complete the installation.
     </para>
    </step>
    <step id="bi0c7d1">
     <para>
      Continue with <xref linkend="bgchyh0" xrefstyle="SectTitleOnPage"/>.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="bgchyh0">
   <title>Configuring iSNS Discovery Domains</title>

<!-- we do that in the respective iSCSI sections.
  <sect2 id="btzqeya">
   <title>Configuring iSCSI Targets and Initiators to Use iSNS</title>
   <para>
    To configure iSCSI targets and initiators to use iSNS, you must edit
    the iSCSI configuration file on each iSCSI target and initiator server
    and add lines that specify the iSNS server address.
   </para>
   <bridgehead id="btzqhcq">Editing the iSCSI Target Configuration File</bridgehead>
   <para>
    Edit the <filename>/etc/ietd.conf</filename> file and add the following
    line:
   </para>
   <para>
    iSNSServer <replaceable>isns_server_ip_address</replaceable>
   </para>
   <para>
    Replace <replaceable>isns_server_ip_address</replaceable> with the IP
    address of the server where you installed iSNS.
   </para>
   <para>
    A commented-out section with this line might already exist in the
    configuration file. If this is the case, you only need to replace the
    sample IP address with the IP address of your iSNS server.
   </para>
   <bridgehead id="bumds3b">Editing the iSCSI Initiator Configuration File</bridgehead>
   <para>
    Edit the <filename>/etc/iscsi/iscsid.conf</filename> file and add the
    following lines:
   </para>
   <para>
    isns.address = <replaceable>isns_server_ip_address</replaceable>
   </para>
   <para>
    isns.port = 3205
   </para>
   <para>
    Replace <replaceable>isns_server_ip_address</replaceable> with the IP
    address of the server where you installed iSNS.
   </para>
   <para>
    A commented-out section with these lines should already exist in the
    configuration file. If this is the case, you only need to replace the
    sample IP address with the IP address of your iSNS server.
   </para>
  </sect2> -->

   <para>
    In order for iSCSI initiators and targets to use the iSNS service, they
    must belong to a discovery domain.
   </para>

   <important>
    <para>
     The SNS service must be installed and running to configure iSNS
     discovery domains. For information, see
     <xref linkend="bi0bdwd" xrefstyle="SectTitleOnPage"/>.
    </para>
   </important>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="sec.isns.ddcreate" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec.isns.ddscreate" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec.isns.ddadd" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec.isns.ddsadd" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="sec.isns.ddcreate">
    <title>Creating iSNS Discovery Domains</title>
    <para>
     A default discovery domain named <guimenu>default DD</guimenu> is
     automatically created when you install the iSNS service. The existing
     iSCSI targets and initiators that have been configured to use iSNS are
     automatically added to the default discovery domain.
    </para>
    <para>
     To create a new discovery domain:
    </para>
    <procedure id="bgchyh5">
     <step id="bgchyh6">
      <para>
       Start YaST and under <guimenu>Network Services</guimenu>, select
       <guimenu>iSNS Server</guimenu>.
      </para>
     </step>
     <step id="bgchyh7">
      <para>
       Click the <guimenu>Discovery Domains</guimenu> tab.
      </para>
      <para>
       The <guimenu>Discovery Domains</guimenu> area lists all discovery
       domains. You can create new discovery domains, or delete existing
       ones.Deleting a domain removes the members from the domain, but it
       does not delete the iSCSI node members.
      </para>
      <para>
       The <guimenu>Discovery Domain Members</guimenu> area lists all iSCSI
       nodes assigned to a selected discovery domain. Selecting a different
       discovery domain refreshes the list with members from that discovery
       domain. You can add and delete iSCSI nodes from a selected discovery
       domain. Deleting an iSCSI node removes it from the domain, but it
       does not delete the iSCSI node.
      </para>
      <para>
       Creating an iSCSI node allows a node that is not yet registered to be
       added as a member of the discovery domain. When the iSCSI initiator
       or target registers this node, then it becomes part of this domain.
      </para>
      <para role="intro">
       When an iSCSI initiator performs a discovery request, the iSNS
       service returns all iSCSI node targets that are members of the same
       discovery domain.
      </para>
      <informalfigure pgwide="0">
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="isns_discdomains_a.png" width="307pt" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="isns_discdomains_a.png" width="307pt" format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step id="bi0cku8">
      <para>
       Click the <guimenu>Create Discovery Domain</guimenu> button.
      </para>
      <para>
       You can also select an existing discovery domain and click the
       <guimenu>Delete</guimenu> button to remove that discovery domain.
      </para>
     </step>
     <step id="bgchyh8">
      <para>
       Specify the name of the discovery domain you are creating, then click
       <guimenu>OK</guimenu>.
      </para>
     </step>
     <step id="bi71u1v">
      <para>
       Continue with
       <xref linkend="sec.isns.ddscreate" xrefstyle="SectTitleOnPage"/>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="sec.isns.ddscreate">
    <title>Creating iSNS Discovery Domain Sets</title>
    <para>
     Discovery domains must belong to a discovery domain set. You can create
     a discovery domain and add nodes to that discovery domain, but it is
     not active and the iSNS service does not function unless you add the
     discovery domain to a discovery domain set. A default discovery domain
     set named <guimenu>default DDS</guimenu> is automatically created when
     you install iSNS and the default discovery domain is automatically
     added to that domain set.
    </para>
    <para>
     To create a discovery domain set:
    </para>
    <procedure id="bgchyh9">
     <step id="bgchyha">
      <para>
       Start YaST and under <guimenu>Network Services</guimenu>, select
       <guimenu>iSNS Server</guimenu>.
      </para>
     </step>
     <step id="bi0cs2c">
      <para>
       Click the <guimenu>Discovery Domains Sets</guimenu> tab.
      </para>
      <para>
       The <guimenu>Discovery Domain Sets</guimenu> area lists all of the
       discover domain sets. A discovery domain must be a member of a
       discovery domain set in order to be active.
      </para>
      <para>
       In an iSNS database, a discovery domain set contains discovery
       domains, which in turn contains iSCSI node members.
      </para>
      <para>
       The <guimenu>Discovery Domain Set Members</guimenu> area lists all
       discovery domains that are assigned to a selected discovery domain
       set. Selecting a different discovery domain set refreshes the list
       with members from that discovery domain set. You can add and delete
       discovery domains from a selected discovery domain set. Removing a
       discovery domain removes it from the domain set, but it does not
       delete the discovery domain.
      </para>
      <para>
       Adding an discovery domain to a set allows a not yet registered iSNS
       discovery domain to be added as a member of the discovery domain set.
      </para>
      <informalfigure pgwide="0">
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="isns_discdomainsets_a.png" width="422pt" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="isns_discdomainsets_a.png" width="422pt" format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step id="bgchyhb">
      <para>
       Click the <guimenu>Create Discovery Domain Set</guimenu> button.
      </para>
      <para>
       You can also select an existing discovery domain set and click the
       <guimenu>Delete</guimenu> button to remove that discovery domain set.
      </para>
     </step>
     <step id="bgchyhc">
      <para>
       Specify the name of the discovery domain set you are creating, then
       click <guimenu>OK</guimenu>.
      </para>
     </step>
     <step id="bi71u8j">
      <para>
       Continue with
       <xref linkend="sec.isns.ddadd" xrefstyle="SectTitleOnPage"/>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="sec.isns.ddadd">
    <title>Adding iSCSI Nodes to a Discovery Domain</title>
    <procedure id="bgchyhd">
     <step id="bgchyhe">
      <para>
       Start YaST and under <guimenu>Network Services</guimenu>, select
       <guimenu>iSNS Server</guimenu>.
      </para>
     </step>
     <step id="bi0csq8">
      <para>
       Click the <guimenu>iSCSI Nodes</guimenu> tab.
      </para>
      <informalfigure pgwide="0">
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="isns_iscsinodes_a.png" width="422pt" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="isns_iscsinodes_a.png" width="422pt" format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step id="bgchyhf">
      <para>
       Review the list of nodes to ensure that the iSCSI targets and
       initiators that you want to use the iSNS service are listed.
      </para>
      <para>
       If an iSCSI target or initiator is not listed, you might need to
       restart the iSCSI service on the node. You can do this by running the
       <command>sudo systemctl restart open-iscsi.service</command> command
       to restart an initiator or the <command>sudo systemctl restart
       iscsitarget.service</command> command to restart a target.
      </para>
      <para>
       You can select an iSCSI node and click the <guimenu>Delete</guimenu>
       button to remove that node from the iSNS database. This is useful if
       you are no longer using an iSCSI node or have renamed it.
      </para>
      <para>
       The iSCSI node is automatically added to the list (iSNS database)
       again when you restart the iSCSI service or reboot the server unless
       you remove or comment out the iSNS portion of the iSCSI configuration
       file.
      </para>
     </step>
     <step id="bgchyhg">
      <para>
       Click the <guimenu>Discovery Domains</guimenu> tab, select the
       desired discovery domain, then click the <guimenu>Display
       Members</guimenu> button.
      </para>
     </step>
     <step id="bgchyhh">
      <para>
       Click <guimenu>Add existing iSCSI Node</guimenu>, select the node you
       want to add to the domain, then click <guimenu>Add Node</guimenu>.
      </para>
     </step>
     <step id="bgchyhi">
      <para>
       Repeat <xref linkend="bgchyho" xrefstyle="StepXRef"/> for as many
       nodes as you want to add to the discovery domain, then click
       <guimenu>Done</guimenu> when you are finished adding nodes.
      </para>
      <para>
       An iSCSI node can belong to more than one discovery domain.
      </para>
     </step>
     <step id="bi71uie">
      <para>
       Continue with
       <xref linkend="sec.isns.ddsadd" xrefstyle="SectTitleOnPage"/>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="sec.isns.ddsadd">
    <title>Adding Discovery Domains to a Discovery Domain Set</title>
    <procedure id="bgchyhj">
     <step id="bgchyhk">
      <para>
       Start YaST and under <guimenu>Network Services</guimenu>, select
       <guimenu>iSNS Server</guimenu>.
      </para>
     </step>
     <step id="bgchyhl">
      <para>
       Click the <guimenu>Discovery Domains Set</guimenu> tab.
      </para>
     </step>
     <step id="bgchyhm">
      <para>
       Select <guimenu>Create Discovery Domain Set</guimenu> to add a new
       set to the list of discovery domain sets.
      </para>
     </step>
     <step id="bgchyhn">
      <para>
       Choose a discovery domain set to modify.
      </para>
     </step>
     <step id="bgchyho">
      <para>
       Click <guimenu>Add Discovery Domain</guimenu>, select the discovery
       domain you want to add to the discovery domain set, then click
       <guimenu>Add Discovery Domain</guimenu>.
      </para>
     </step>
     <step id="bgchyhp">
      <para>
       Repeat the last step for as many discovery domains as you want to add
       to the discovery domain set, then click <guimenu>Done</guimenu>.
      </para>
      <para>
       A discovery domain can belong to more than one discovery domain set.
      </para>
     </step>
    </procedure>
   </sect2>

<!-- this is done in the iSCSI chapter.
  <sect2 id="bcn9uwp">
   <title>Setting Up the iSCSI Initiator to Use the iSNS Server</title>
   <para>
    Do the following for each iSCSI initiator server that should use the
    iSNS server:
   </para>
   <procedure id="bcn9vbi">
    <step id="bcn9vbj" performance="required">
     <para>
      At the console on the iSCSI initiator server, log in as the
      <systemitem>root</systemitem> user, then open a terminal console.
     </para>
    </step>
    <step id="bcn9vqt" performance="required">
     <para>
      At the terminal console prompt, enter
     </para>
<screen>
scsiadm - -mode discovery - -type isns - -portal <replaceable>isns_server_ip_addr</replaceable>
</screen>
     <para>
      Replace <replaceable>isns_server_ip_addr</replaceable> with the IP
      address of the iSNS server. For example, if the IP address of the
      iSNS server is 10.10.10.200, enter
     </para>
     <screen>
scsiadm - -mode discovery - -type isns - -portal 10.10.10.200
</screen>
    </step>
   </procedure>
  </sect2> -->
  </sect1>
  <sect1 id="bi0bdwd">
   <title>Starting iSNS</title>

   <para>
    iSNS must be started at the server where you install it. Enter the
    following command at a terminal console:
   </para>

<screen>
 <command>sudo systemctl start isns.service</command> 
</screen>

   <para>
    You can also use the <command>stop</command>, <command>status</command>,
    and <command>restart</command> options with iSNS.
   </para>

   <para>
    iSNS can also be configured to start automatically each time the server
    is rebooted:
   </para>

   <procedure id="bgchyh1">
    <step id="bgchyh2">
     <para>
      Start YaST and under <guimenu>Network Services</guimenu>, select
      <guimenu>iSNS Server</guimenu>.
     </para>
    </step>
    <step id="bgchyh3">
     <para>
      With the <guimenu>Service</guimenu> tab selected, specify the IP
      address of your iSNS server, then click <guimenu>Save
      Address</guimenu>.
     </para>
    </step>
    <step id="bgchyh4">
     <para>
      In the <guimenu>Service Start</guimenu> section of the screen, select
      <guimenu>When Booting</guimenu>.
     </para>
     <para>
      You can also choose to start the iSNS server manually. You must then
      use <command>sudo systemctl start isns.service</command> to start the
      service each time the server is restarted.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="bi0cewc">
   <title>Stopping iSNS</title>

   <para>
    iSNS must be stopped at the server where it is running. Enter the
    following command at a terminal console:
   </para>

<screen>
<command>sudo systemctl stop isns.service</command> 
</screen>
  </sect1>
  <sect1 id="bgchyhq">
   <title>For More Information</title>

   <para>
    For information, see the
    <ulink url="http://sourceforge.net/projects/linuxisns/"><citetitle>Linux
    iSNS for iSCSI project</citetitle></ulink>. The electronic mailing list
    for this project is
    <ulink url="http://sourceforge.net/mailarchive/forum.php?forum_name=linuxisns-discussion"><citetitle>Linux
    iSNS - Discussion</citetitle></ulink>.
   </para>

   <para>
    General information about iSNS is available in
    <ulink url="http://www.ietf.org/rfc/rfc4171"><citetitle>RFC 4171:
    Internet Storage Name Service</citetitle></ulink>.
   </para>
  </sect1>
 </chapter>
 <chapter id="cha_inst_system_iscsi" lang="en">
  <title>Mass Storage over IP Networks: iSCSI</title>
  <para>
   One of the central tasks in computer centers and when operating servers
   is providing hard disk capacity for server systems. Fibre Channel is
   often used for this purpose. iSCSI (Internet SCSI) solutions provide a
   lower-cost alternative to Fibre Channel that can leverage commodity
   servers and Ethernet networking equipment. Linux iSCSI provides iSCSI
   initiator and target software for connecting Linux servers to central
   storage systems.
  </para>
  <figure pgwide="0" id="bo1n90f">
   <title>iSCSI SAN with an iSNS Server</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="iscsi_san_a.svg" width="445pt" format="SVG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="iscsi_san_a.png" width="445pt" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
   iSCSI is a storage networking protocol that facilitates data transfers of
   SCSI packets over TCP/IP networks between block storage devices and
   servers. iSCSI target software runs on the target server and defines the
   logical units as iSCSI target devices. iSCSI initiator software runs on
   different servers and connects to the target devices to make the storage
   devices available on that server.
  </para>
  <important>
   <para>
    It is not supported to run iSCSI target software and iSCSI initiator
    software on the same server in a production environment.
   </para>
  </important>
  <para>
   The iSCSI target and initiator servers communicate by sending SCSI
   packets at the IP level in your LAN. When an application running on the
   initiator server starts an inquiry for an iSCSI target device, the
   operating system produces the necessary SCSI commands. The SCSI commands
   are then embedded in IP packets and encrypted as necessary by software
   that is commonly known as the <emphasis>iSCSI initiator</emphasis>. The
   packets are transferred across the internal IP network to the
   corresponding iSCSI remote station, called the <emphasis>iSCSI
   target</emphasis>.
  </para>
  <para>
   Many storage solutions provide access over iSCSI, but it is also possible
   to run a Linux server that provides an iSCSI target. In this case, it is
   important to set up a Linux server that is optimized for file system
   services. The iSCSI target accesses block devices in Linux. Therefore, it
   is possible to use RAID solutions to increase disk space as well as a lot
   of memory to improve data caching. For more information about RAID, also
   see <xref linkend="raidyast" xrefstyle="ChapTitleOnPage"/>.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="bj4rqa5" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec_inst_system_iscsi_target" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec_inst_system_iscsi_initiator" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bwkrqdd" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec_inst_system_iscsi_ts" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec_inst_system_iscsi_initiator_info" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="bj4rqa5">
   <title>Installing iSCSI Target and Initiator</title>

   <para>
    YaST includes entries for iSCSI Target and iSCSI Initiator software, but
    the packages are not installed by default.
   </para>

   <important>
    <para>
     It is not supported to run iSCSI target software and iSCSI initiator
     software on the same server in a production environment.
    </para>
   </important>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bj4rsgl" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bj4rrj9" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bj4rsgl">
    <title>Installing iSCSI Target Software</title>
    <para>
     Install the iSCSI target software on the server where you want to
     create iSCSI target devices.
    </para>
    <procedure id="bj4rsgm">
     <step id="bj4rsgn">
      <para>
       Launch YaST as the <systemitem>root</systemitem> user.
      </para>
     </step>
     <step id="bj4rsgo">
      <para>
       Select <guimenu>Network Services</guimenu><guimenu>iSCSI
       Target.</guimenu>
      </para>
     </step>
     <step id="bj4rsma">
      <para>
       When you are prompted to install the <filename>iscsitarget</filename>
       package, click <guimenu>Install</guimenu>.
      </para>
     </step>
     <step id="bj4rsz7">
      <para>
       Follow the on-screen install instructions, and provide the
       installation media as needed.
      </para>
      <para>
       When the installation is complete, YaST opens to the iSCSI Target
       Overview page with the <guimenu>Service</guimenu> tab selected.
      </para>
     </step>
     <step id="bj58wf8">
      <para>
       Continue with
       <xref linkend="sec_inst_system_iscsi_target" xrefstyle="SectTitleOnPage"/>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="bj4rrj9">
    <title>Installing the iSCSI Initiator Software</title>
    <para>
     Install the iSCSI initiator software on each server where you want to
     access the target devices that you set up on the iSCSI target server.
    </para>
    <procedure id="bj4rqhd">
     <step id="bj4rqhe">
      <para>
       Launch YaST as the <systemitem>root</systemitem> user.
      </para>
     </step>
     <step id="bj4rqsk">
      <para>
       Select <guimenu>Network Services</guimenu><guimenu>iSCSI
       Initiator.</guimenu>
      </para>
     </step>
     <step id="bj4rsth">
      <para>
       When you are prompted to install the <filename>open-iscsi</filename>
       package, click <guimenu>Install</guimenu>.
      </para>
     </step>
     <step id="bj4rt8u">
      <para>
       Follow the on-screen install instructions, and provide the
       installation media as needed.
      </para>
      <para>
       When the installation is complete, YaST opens to the iSCSI Initiator
       Overview page with the <guimenu>Service</guimenu> tab selected.
      </para>
     </step>
     <step id="bj4saer">
      <para>
       Continue with
       <xref linkend="sec_inst_system_iscsi_initiator" xrefstyle="SectTitleOnPage"/>.
      </para>
     </step>
    </procedure>
   </sect2>
  </sect1>
  <sect1 id="sec_inst_system_iscsi_target">
   <title>Setting Up an iSCSI Target</title>

   <para>
    SUSE Linux Enterprise Server comes with an open source iSCSI target
    solution that evolved from the Ardis iSCSI target. A basic setup can be
    done with YaST, but to take full advantage of iSCSI, a manual setup is
    required.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bj5erpx" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_inst_system_iscsi_target_yast" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_inst_system_iscsi_target_manual" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_inst_system_iscsi_target_ietadm" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bj5erpx">
    <title>Preparing the Storage Space</title>
    <para>
     The iSCSI target configuration exports existing block devices to iSCSI
     initiators. You must prepare the storage space you want to use in the
     target devices by setting up unformatted partitions or devices by using
     the Partitioner in YaST, or by partitioning the devices from the
     command line. iSCSI LIO targets can use unformatted partitions with
     Linux, Linux LVM, or Linux RAID file system IDs.
    </para>
    <important>
     <para>
      After you set up a device or partition for use as an iSCSI target, you
      never access it directly via its local path. Do not specify a mount
      point for it when you create it.
     </para>
    </important>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bj5rdlu" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bj5py87" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bj5rdlu">
     <title>Partitioning Devices</title>
     <procedure id="bj5expw">
      <step id="bj5expx">
       <para>
        Launch YaST as the <systemitem>root</systemitem> user.
       </para>
      </step>
      <step id="bj5f8l7">
       <para>
        Select <guimenu>System</guimenu><guimenu>Partitioner</guimenu>.
       </para>
      </step>
      <step id="bj5f902">
       <para>
        Click <guimenu>Yes</guimenu> to continue through the warning about
        using the Partitioner.
       </para>
      </step>
      <step id="bj5f9hl">
       <para>
        Click <guimenu>Add</guimenu> to create a partition, but do not
        format it, and do not mount it.
       </para>
       <substeps>
        <step id="bj5fl5y">
         <para>
          Select <guimenu>Primary Partition</guimenu>, then click
          <guimenu>Next</guimenu>.
         </para>
        </step>
        <step id="bj5fld1">
         <para>
          Specify the amount of space to use, then click
          <guimenu>Next</guimenu>.
         </para>
        </step>
        <step id="bj5flp9">
         <para>
          Select <guimenu>Do not format</guimenu>, then specify the file
          system ID type.
         </para>
         <para>
          iSCSI targets can use unformatted partitions with Linux, Linux
          LVM, or Linux RAID file system IDs.
         </para>
        </step>
        <step id="bj5flz6">
         <para>
          Select <guimenu>Do not mount</guimenu>.
         </para>
        </step>
        <step id="bj5fm1h">
         <para>
          Click <guimenu>Finish</guimenu>.
         </para>
        </step>
       </substeps>
      </step>
      <step id="bj5fjxr">
       <para>
        Repeat <xref linkend="bj5f9hl" xrefstyle="StepXRef"/> for each area
        that you want to use later as an iSCSI LUN.
       </para>
      </step>
      <step id="bj5fm9v">
       <para>
        Click <guimenu>Accept</guimenu> to keep your changes, then close
        YaST.
       </para>
      </step>
     </procedure>
    </sect3>
    <sect3 id="bj5py87">
     <title>Partitioning Devices in a Virtual Environment</title>
     <para>
      You can use a Xen guest server as the iSCSI target server. You must
      assign the storage space you want to use for the iSCSI storage devices
      to the guest virtual machine, then access the space as virtual disks
      within the guest environment. Each virtual disk can be a physical
      block device, such as an entire disk, partition, or volume, or it can
      be a file-backed disk image where the virtual disk is a single image
      file on a larger physical disk on the Xen host server. For the best
      performance, create each virtual disk from a physical disk or a
      partition. After you set up the virtual disks for the guest virtual
      machine, start the guest server, then configure the new blank virtual
      disks as iSCSI target devices by following the same process as for a
      physical server.
     </para>
     <para>
      file-backed disk images are created on the Xen host server, then
      assigned to the Xen guest server. By default, Xen stores file-backed
      disk images in the
      <filename>/var/lib/xen/images/<replaceable>vm_name</replaceable></filename>
      directory, where
      <filename><replaceable>vm_name</replaceable></filename> is the name of
      the virtual machine.
     </para>
     <para>
      For example, if you want to create the disk image
      <filename>/var/lib/xen/images/vm_one/xen-0</filename> with a size of 4
      GB, first ensure that the directory is there, then create the image
      itself.
     </para>
     <procedure id="bj5q7wn">
      <step id="bj5q7wo">
       <para>
        Log in to the host server as the <systemitem>root</systemitem> user.
       </para>
      </step>
      <step id="bj5q4xg">
       <para>
        At a terminal console prompt, enter the following commands
       </para>
<screen>
mkdir -p /var/lib/xen/images/vm_one
dd if=/dev/zero of=/var/lib/xen/images/vm_one/xen-0 seek=1M bs=4096 count=1
</screen>
      </step>
      <step id="bj5q56y">
       <para>
        Assign the file system image to the guest virtual machine in the Xen
        configuration file.
       </para>
      </step>
      <step id="bj5q81p">
       <para>
        Log in as the <systemitem>root</systemitem> user on the guest
        server, then use YaST to set up the virtual block device by using
        the process in <xref linkend="bj5rdlu" xrefstyle="HeadingOnPage"/>.
       </para>
      </step>
     </procedure>
    </sect3>
   </sect2>

   <sect2 id="sec_inst_system_iscsi_target_yast">
    <title>Creating iSCSI Targets with YaST</title>
    <para>
     To configure the iSCSI target, run the <guimenu>iSCSI Target</guimenu>
     module in YaST. The configuration is split into three tabs. In the
     <guimenu>Service</guimenu> tab, select the start mode and the firewall
     settings. If you want to access the iSCSI target from a remote machine,
     select <guimenu>Open Port in Firewall</guimenu>. If an iSNS server
     should manage the discovery and access control, activate <guimenu>iSNS
     Access Control</guimenu> and enter the IP address of your iSNS server.
     You cannot use hostnames or DNS names; you must use the IP address. For
     more about iSNS, read
     <xref linkend="isns" xrefstyle="ChapTitleOnPage"/>.
    </para>
    <para>
     The <guimenu>Global</guimenu> tab provides settings for the iSCSI
     server. The authentication set here is used for the discovery of
     services, not for accessing the targets. If you do not want to restrict
     the access to the discovery, use <guimenu>No Authentication</guimenu>.
    </para>
    <para>
     If authentication is needed, there are two possibilities to consider.
     One is that an initiator must prove that it has the permissions to run
     a discovery on the iSCSI target. This is done with <guimenu>Incoming
     Authentication</guimenu>. The other is that the iSCSI target must prove
     to the initiator that it is the expected target. Therefore, the iSCSI
     target can also provide a user name and password. This is done with
     <guimenu>Outgoing Authentication</guimenu>. Find more information about
     authentication in
     <ulink url="http://www.ietf.org/rfc/rfc3720.txt"><citetitle>RFC
     3720</citetitle></ulink>.
    </para>
    <para>
     The targets are defined in the <guimenu>Targets</guimenu> tab. Use
     <guimenu>Add</guimenu> to create a new iSCSI target. The first dialog
     box asks for information about the device to export.
    </para>
    <variablelist>
     <varlistentry id="bgchy7j">
      <term>Target</term>
      <listitem>
       <para role="intro">
        The <guimenu>Target</guimenu> line has a fixed syntax that looks
        like the following:
       </para>
<screen>
iqn.yyyy-mm.&lt;reversed domain name&gt;:<replaceable>unique_id</replaceable>
</screen>
       <para>
        It always starts with iqn. yyyy-mm is the format of the date when
        this target is activated. Find more about naming conventions in
        <ulink url="http://www.ietf.org/rfc/rfc3722.txt"><citetitle>RFC
        3722</citetitle></ulink>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bgchy7k">
      <term>Identifier</term>
      <listitem>
       <para>
        The <guimenu>Identifier</guimenu> is freely selectable. It should
        follow some scheme to make the whole system more structured.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bgchy7l">
      <term>LUN</term>
      <listitem>
       <para>
        It is possible to assign several LUNs to a target. To do this,
        select a target in the <guimenu>Targets</guimenu> tab, then click
        <guimenu>Edit</guimenu>. Then, add new LUNs to an existing target.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bgchy7m">
      <term>Path</term>
      <listitem>
       <para>
        Add the path to the block device or file system image to export.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     The next menu configures the access restrictions of the target. The
     configuration is very similar to the configuration of the discovery
     authentication. In this case, at least an incoming authentication
     should be setup.
    </para>
    <para>
     <guimenu>Next</guimenu> finishes the configuration of the new target,
     and brings you back to the overview page of the
     <guimenu>Target</guimenu> tab. Activate your changes by clicking
     <guimenu>Finish</guimenu>.
    </para>
    <para>
     To create a target device:
    </para>
    <procedure id="bj5fomq">
     <step id="bj5fomr">
      <para>
       Launch YaST as the <systemitem>root</systemitem> user.
      </para>
     </step>
     <step id="bj5foms">
      <para>
       Select <guimenu>Network Services</guimenu><guimenu>iSCSI
       Target.</guimenu>
      </para>
      <para role="intro">
       YaST opens to the iSCSI Target Overview page with the
       <guimenu>Service</guimenu> tab selected.
      </para>
      <informalfigure pgwide="0">
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="iscsi_target_service_a.png" width="307pt" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="iscsi_target_service_a.png" width="307pt" format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step id="bj5fomv">
      <para>
       In the <guimenu>Service Start </guimenu>area, select one of the
       following:
      </para>
      <itemizedlist>
       <listitem>
        <formalpara id="bj5fomw">
         <title>When booting:</title>
         <para>
          Automatically start the initiator service on subsequent server
          reboots.
         </para>
        </formalpara>
       </listitem>
       <listitem>
        <formalpara id="bj5fomx">
         <title>Manually (default):</title>
         <para>
          Start the service manually.
         </para>
        </formalpara>
       </listitem>
      </itemizedlist>
     </step>
     <step id="bj5fomy">
      <para>
       If you are using iSNS for target advertising, select the
       <guimenu>iSNS Access Control</guimenu> check box, then type the IP
       address.
      </para>
     </step>
     <step id="bj5fydf">
      <para>
       If desired, open the firewall ports to allow access to the server
       from remote computers.
      </para>
      <substeps>
       <step id="bj5fomz">
        <para>
         Select the <guimenu>Open Port in Firewall</guimenu> check box.
        </para>
       </step>
       <step id="bj5fon0">
        <para>
         Specify the network interfaces where you want to open the port by
         clicking <guimenu>Firewall Details</guimenu>, selecting the check
         box next to a network interface to enable it, then clicking
         <guimenu>OK</guimenu> to accept the settings.
        </para>
       </step>
      </substeps>
     </step>
     <step id="bj5fon1">
      <para>
       If authentication is required to connect to target devices you set up
       on this server, select the <guimenu>Global</guimenu> tab, deselect
       <guimenu>No Authentication</guimenu> to enable authentication, then
       specify the necessary credentials for incoming and outgoing
       authentication.
      </para>
      <para role="intro">
       The <guimenu>No Authentication</guimenu> option is enabled by
       default. For a more secure configuration, you can specify
       authentication for incoming, outgoing, or both incoming and outgoing.
       You can also specify multiple sets of credentials for incoming
       authentication by adding pairs of user names and passwords to the
       list under <guimenu>Incoming Authentication</guimenu>.
      </para>
      <informalfigure pgwide="0">
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="iscsi_target_global_a.png" width="307pt" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="iscsi_target_global_a.png" width="307pt" format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step id="bj5fon2">
      <para>
       Configure the iSCSI target devices.
      </para>
      <substeps>
       <step id="bj5fon3">
        <para>
         Select the <guimenu>Targets</guimenu> tab.
        </para>
       </step>
       <step id="bj5fzxo">
        <para>
         If you have not already done so, select and delete the example
         iSCSI target from the list, then confirm the deletion by clicking
         <guimenu>Continue</guimenu>.
        </para>
       </step>
       <step id="bj5fon4">
        <para>
         Click <guimenu>Add</guimenu> to add a new iSCSI target.
        </para>
        <para>
         The iSCSI target automatically presents an unformatted partition or
         block device and completes the Target and Identifier fields.
        </para>
       </step>
       <step id="bj5g2b7">
        <para>
         You can accept this, or browse to select a different space.
        </para>
        <para>
         You can also subdivide the space to create LUNs on the device by
         clicking <guimenu>Add</guimenu> and specifying sectors to allocate
         to that LUN. If you need additional options for these LUNs, select
         <guimenu>Expert Settings</guimenu>.
        </para>
       </step>
       <step id="bj5g2n0">
        <para>
         Click <guimenu>Next</guimenu>
        </para>
       </step>
       <step id="bj5g45h">
        <para>
         Repeat <xref linkend="bj5fon4" xrefstyle="StepXRef"/> to
         <xref linkend="bj5g2n0" xrefstyle="StepXRef"/> for each iSCSI
         target device you want to create.
        </para>
       </step>
       <step id="bojud2n">
        <para>
         (Optional) On the <guimenu>Service</guimenu> tab, click
         <guimenu>Save</guimenu> to export the information about the
         configured iSCSI targets to a file.
        </para>
        <para>
         This makes it easier to later provide this information to consumers
         of the resources.
        </para>
       </step>
       <step id="bj5fon5">
        <para>
         Click <guimenu>Finish</guimenu> to create the devices, then click
         <guimenu>Yes</guimenu> to restart the iSCSI software stack.
        </para>
       </step>
      </substeps>
     </step>
    </procedure>
   </sect2>

   <sect2 id="sec_inst_system_iscsi_target_manual">
    <title>Configuring an iSCSI Target Manually</title>
    <para>
     Configure an iSCSI target in <filename>/etc/ietd.conf</filename>. All
     parameters in this file before the first <emphasis>Target</emphasis>
     declaration are global for the file. Authentication information in this
     portion has a special meaning&mdash;it is not global, but is used for
     the discovery of the iSCSI target.
    </para>
    <para>
     If you have access to an iSNS server, you should first configure the
     file to tell the target about this server. The address of the iSNS
     server must always be given as an IP address. You cannot specify the
     DNS name for the server. The configuration for this functionality looks
     like the following:
    </para>
<screen>
iSNSServer 192.168.1.111
iSNSAccessControl no
</screen>
    <para>
     This configuration makes the iSCSI target register itself with the
     <literal>iSNS</literal> server, which in turn provides the discovery
     for initiators. For more about iSNS, see
     <xref linkend="isns" xrefstyle="ChapTitleOnPage"/>. The access control
     for the iSNS discovery is not supported. Keep
     <literal>iSNSAccessControl no</literal>.
    </para>
    <para>
     All direct iSCSI authentication can be done in two directions. The
     iSCSI target can require the iSCSI initiator to authenticate with the
     <option>IncomingUser</option>, which can be added multiple times. The
     iSCSI initiator can also require the iSCSI target to authenticate. Use
     <option>OutgoingUser</option> for this. Both have the same syntax:
    </para>
<screen>
IncomingUser &lt;username&gt; &lt;password&gt;
OutgoingUser &lt;username&gt; &lt;password&gt;
</screen>
    <para>
     The authentication is followed by one or more target definitions. For
     each target, add a <literal>Target</literal> section. This section
     always starts with a <literal>Target</literal> identifier followed, by
     definitions of logical unit numbers:
    </para>
<screen>
Target iqn.yyyy-mm.&lt;reversed domain name&gt;[:identifier]
          Lun 0 Path=/dev/mapper/system-v3
          Lun 1 Path=/dev/hda4
          Lun 2 Path=/var/lib/xen/images/xen-1,Type=fileio
</screen>
    <para>
     In the <literal>Target</literal> line, <literal>yyyy-mm</literal> is
     the date when this target is activated, and
     <literal>identifier</literal> is freely selectable. Find more about
     naming conventions in
     <ulink url="http://www.ietf.org/rfc/rfc3722.txt"><citetitle>RFC
     3722</citetitle></ulink>. Three different block devices are exported in
     this example. The first block device is a logical volume (see also
     <xref linkend="lvm" xrefstyle="ChapTitleOnPage"/>), the second is an
     IDE partition, and the third is an image available in the local file
     system. All these look like block devices to an iSCSI initiator.
    </para>
    <para>
     Before activating the iSCSI target, add at least one
     <option>IncomingUser</option> after the <option>Lun</option>
     definitions. It does the authentication for the use of this target.
    </para>
    <para>
     To activate all your changes, restart the iscsitarget daemon with
     <command>sudo systemctl restart open-iscsi.service</command>. Check
     your configuration in the <filename>/proc</filename> file system:
    </para>
<screen>
cat /proc/net/iet/volume
tid:1 name:iqn.2006-02.com.example.iserv:systems
        lun:0 state:0 iotype:fileio path:/dev/mapper/system-v3
        lun:1 state:0 iotype:fileio path:/dev/hda4
        lun:2 state:0 iotype:fileio path:/var/lib/xen/images/xen-1
</screen>
    <para>
     There are many more options that control the behavior of the iSCSI
     target. For more information, see the man page of
     <filename>ietd.conf</filename>.
    </para>
    <para>
     Active sessions are also displayed in the <filename>/proc</filename>
     file system. For each connected initiator, an extra entry is added to
     <filename>/proc/net/iet/session</filename>:
    </para>
<screen>
cat /proc/net/iet/session
tid:1 name:iqn.2006-02.com.example.iserv:system-v3
   sid:562949957419520 initiator:iqn.2005-11.de.suse:cn=rome.example.com,01.9ff842f5645
      cid:0 ip:192.168.178.42 state:active hd:none dd:none
   sid:281474980708864 initiator:iqn.2006-02.de.suse:01.6f7259c88b70
      cid:0 ip:192.168.178.72 state:active hd:none dd:none
</screen>
   </sect2>

   <sect2 id="sec_inst_system_iscsi_target_ietadm">
    <title>Configuring Online Targets with ietadm</title>
    <para>
     When changes to the iSCSI target configuration are necessary, you must
     always restart the target to activate changes that are done in the
     configuration file. Unfortunately, all active sessions are interrupted
     in this process. To maintain an undisturbed operation, the changes
     should be done in the main configuration file
     <filename>/etc/ietd.conf</filename>, but also made manually to the
     current configuration with the administration utility ietadm.
    </para>
    <para>
     To create a new iSCSI target with a LUN, first update your
     configuration file. The additional entry could be:
    </para>
<screen>
Target iqn.2006-02.com.example.iserv:system2
          Lun 0 Path=/dev/mapper/system-swap2
          IncomingUser joe secret
</screen>
    <para>
     To set up this configuration manually, proceed as follows:
    </para>
    <procedure id="bgchy7n">
     <step id="bgchy7o">
      <para>
       Create a new target with the command <command>ietadm --op new --tid=2
       --params Name=iqn.2006-02.com.example.iserv:system2</command>.
      </para>
     </step>
     <step id="bgchy7p">
      <para>
       Add a logical unit with <command>ietadm --op new --tid=2 --lun=0
       --params Path=/dev/mapper/system-swap2</command>.
      </para>
     </step>
     <step id="bgchy7q">
      <para>
       Set the user name and password combination on this target with
       <command>ietadm --op new --tid=2 --user
       --params=IncomingUser=joe,Password=secret</command>.
      </para>
     </step>
     <step id="bgchy7r">
      <para>
       Check the configuration with <command>cat
       /proc/net/iet/volume</command>.
      </para>
     </step>
    </procedure>
    <para>
     It is also possible to delete active connections. First, check all
     active connections with the command <command>cat
     /proc/net/iet/session</command>. This might look like:
    </para>
<screen>
cat /proc/net/iet/session
tid:1 name:iqn.2006-03.com.example.iserv:system
        sid:281474980708864 initiator:iqn.1996-04.com.example:01.82725735af5
                cid:0 ip:192.168.178.72 state:active hd:none dd:none
</screen>
    <para>
     To delete the session with the session ID 281474980708864, use the
     command <command>ietadm --op delete --tid=1 --sid=281474980708864
     --cid=0</command>. Be aware that this makes the device inaccessible on
     the client system and processes accessing this device are likely to
     hang.
    </para>
    <para>
     ietadm can also be used to change various configuration parameters.
     Obtain a list of the global variables with <command>ietadm --op show
     --tid=1 --sid=0</command>. The output looks like:
    </para>
<screen>
InitialR2T=Yes
ImmediateData=Yes
MaxConnections=1
MaxRecvDataSegmentLength=8192
MaxXmitDataSegmentLength=8192
MaxBurstLength=262144
FirstBurstLength=65536
DefaultTime2Wait=2
DefaultTime2Retain=20
MaxOutstandingR2T=1
DataPDUInOrder=Yes
DataSequenceInOrder=Yes
ErrorRecoveryLevel=0
HeaderDigest=None
DataDigest=None
OFMarker=No
IFMarker=No
OFMarkInt=Reject
IFMarkInt=Reject
</screen>
    <para>
     All of these parameters can be easily changed. For example, if you want
     to change the maximum number of connections to two, use
    </para>
<screen>
ietadm --op update --tid=1 --params=MaxConnections=2. 
</screen>
    <para>
     In the file <filename>/etc/ietd.conf</filename>, the associated line
     should look like <option>MaxConnections 2</option>.
    </para>
    <warning>
     <para>
      The changes that you make with the <command>ietadm</command> utility
      are not permanent for the system. These changes are lost at the next
      reboot if they are not added to the
      <filename>/etc/ietd.conf</filename> configuration file. Depending on
      the usage of iSCSI in your network, this might lead to severe
      problems.
     </para>
    </warning>
    <para>
     There are several more options available for the
     <command>ietadm</command> utility. Use <command>ietadm -h</command> to
     find an overview. The abbreviations there are target ID (tid), session
     ID (sid), and connection ID (cid). They can also be found in
     <filename>/proc/net/iet/session</filename>.
    </para>
   </sect2>
  </sect1>
  <sect1 id="sec_inst_system_iscsi_initiator">
   <title>Configuring iSCSI Initiator</title>

   <para>
    The iSCSI initiator, also called an iSCSI client, can be used to connect
    to any iSCSI target. This is not restricted to the iSCSI target solution
    explained in
    <xref linkend="sec_inst_system_iscsi_target" xrefstyle="SectTitleOnPage"/>.
    The configuration of iSCSI initiator involves two major steps: the
    discovery of available iSCSI targets and the setup of an iSCSI session.
    Both can be done with YaST.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="sec_inst_system_iscsi_initiator_yast" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_inst_system_iscsi_initiator_manual" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_iscsi_database" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="sec_inst_system_iscsi_initiator_yast">
    <title>Using YaST for the iSCSI Initiator Configuration</title>
    <para>
     The iSCSI Initiator Overview in YaST is divided into three tabs:
    </para>
    <itemizedlist>
     <listitem>
      <formalpara id="bkob6lr">
       <title>Service:</title>
       <para>
        The <guimenu>Service</guimenu> tab can be used to enable the iSCSI
        initiator at boot time. It also offers to set a unique
        <guimenu>Initiator Name</guimenu> and an iSNS server to use for the
        discovery. The default port for iSNS is 3205.
       </para>
      </formalpara>
     </listitem>
     <listitem>
      <formalpara id="bkob6ls">
       <title>Connected Targets:</title>
       <para>
        The <guimenu>Connected Targets</guimenu> tab gives an overview of
        the currently connected iSCSI targets. Like the <guimenu>Discovered
        Targets</guimenu> tab, it also gives the option to add new targets
        to the system.
       </para>
      </formalpara>
      <para>
       On this page, you can select a target device, then toggle the
       start-up setting for each iSCSI target device:
      </para>
      <itemizedlist>
       <listitem>
        <formalpara id="bkob6lt">
         <title>Automatic:</title>
         <para>
          This option is used for iSCSI targets that are to be connected
          when the iSCSI service itself starts up. This is the typical
          configuration.
         </para>
        </formalpara>
       </listitem>
       <listitem>
        <formalpara id="bkob6lu">
         <title>Onboot:</title>
         <para>
          This option is used for iSCSI targets that are to be connected
          during boot; that is, when root (<filename>/</filename>) is on
          iSCSI. As such, the iSCSI target device will be evaluated from the
          initrd on server boots.
         </para>
        </formalpara>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <formalpara id="bkob6lv">
       <title>Discovered Targets:</title>
       <para>
        <guimenu>Discovered Targets</guimenu> provides the possibility of
        manually discovering iSCSI targets in the network.
       </para>
      </formalpara>
     </listitem>
    </itemizedlist>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bkob990" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bkobick" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bkob8pj" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bkob9pj" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bkob990">
     <title>Configuring the iSCSI Initiator</title>
     <procedure id="bj5fon6">
      <step id="bj5fon7">
       <para>
        Launch YaST as the <systemitem>root</systemitem> user.
       </para>
      </step>
      <step id="bj5fon8">
       <para>
        Select <guimenu>Network Services</guimenu><guimenu>iSCSI
        Initiator.</guimenu>
       </para>
       <para role="intro">
        YaST opens to the iSCSI Initiator Overview page with the
        <guimenu>Service</guimenu> tab selected.
       </para>
       <informalfigure pgwide="0">
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="iscsi_init_service_a.png" width="307pt" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="iscsi_init_service_a.png" width="307pt" format="PNG"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
      </step>
      <step id="bj5fonb">
       <para>
        In the <guimenu>Service Start </guimenu>area, select one of the
        following:
       </para>
       <itemizedlist>
        <listitem>
         <formalpara id="bj5fonc">
          <title>When booting:</title>
          <para>
           Automatically start the initiator service on subsequent server
           reboots.
          </para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara id="bj5fond">
          <title>Manually (default):</title>
          <para>
           Start the service manually.
          </para>
         </formalpara>
        </listitem>
       </itemizedlist>
      </step>
      <step id="bj5fone">
       <para>
        Specify or verify the <guimenu>Initiator Name</guimenu>.
       </para>
       <para>
        Specify a well-formed iSCSI qualified name (IQN) for the iSCSI
        initiator on this server. The initiator name must be globally unique
        on your network. The IQN uses the following general format:
       </para>
<screen>
iqn.yyyy-mm.com.mycompany:n1:n2
</screen>
       <para>
        where n1 and n2 are alphanumeric characters. For example:
       </para>
<screen>
iqn.1996-04.de.suse:01:9c83a3e15f64
</screen>
       <para>
        The <guimenu>Initiator Name</guimenu> is automatically completed
        with the corresponding value from the
        <filename>/etc/iscsi/initiatorname.iscsi</filename> file on the
        server.
       </para>
       <para>
        If the server has iBFT (iSCSI Boot Firmware Table) support, the
        <guimenu>Initiator Name</guimenu> is completed with the
        corresponding value in the IBFT, and you are not able to change the
        initiator name in this interface. Use the BIOS Setup to modify it
        instead.The iBFT is a block of information containing various
        parameters useful to the iSCSI boot process, including iSCSI target
        and initiator descriptions for the server.
       </para>
      </step>
      <step id="bj5fonf">
       <para>
        Use either of the following methods to discover iSCSI targets on the
        network.
       </para>
       <itemizedlist>
        <listitem>
         <formalpara id="bj5fong">
          <title>iSNS:</title>
          <para>
           To use iSNS (Internet Storage Name Service) for discovering iSCSI
           targets, continue with
           <xref linkend="bkobick" xrefstyle="HeadingOnPage"/>.
          </para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara id="bj5fonh">
          <title>Discovered Targets:</title>
          <para>
           To discover iSCSI target devices manually, continue with
           <xref linkend="bkob8pj" xrefstyle="HeadingOnPage"/>.
          </para>
         </formalpara>
        </listitem>
       </itemizedlist>
      </step>
     </procedure>
    </sect3>
    <sect3 id="bkobick">
     <title>Discovering iSCSI Targets by Using iSNS</title>
     <para>
      Before you can use this option, you must have already installed and
      configured an iSNS server in your environment. For information, see
      <xref linkend="isns" xrefstyle="ChapTitleOnPage"/>.
     </para>
     <procedure id="bkobiky">
      <step id="bkobikz">
       <para>
        In YaST, select<guimenu> iSCSI Initiator</guimenu>, then select the
        <guimenu>Service</guimenu> tab.
       </para>
      </step>
      <step id="bkobjno">
       <para>
        Specify the IP address of the iSNS server and port.
       </para>
       <para>
        The default port is 3205.
       </para>
      </step>
      <step id="bkobl8n">
       <para>
        On the iSCSI Initiator Overview page, click
        <guimenu>Finish</guimenu> to save and apply your changes.
       </para>
      </step>
     </procedure>
    </sect3>
    <sect3 id="bkob8pj">
     <title>Discovering iSCSI Targets Manually</title>
     <para>
      Repeat the following process for each of the iSCSI target servers that
      you want to access from the server where you are setting up the iSCSI
      initiator.
     </para>
     <procedure id="bgchy7s">
      <step id="bgchy7t">
       <para>
        In YaST, select<guimenu> iSCSI Initiator</guimenu>, then select the
        <guimenu>Discovered Targets</guimenu> tab.
       </para>
      </step>
      <step id="bkobfda">
       <para>
        Click <guimenu>Discovery</guimenu> to open the iSCSI Initiator
        Discovery dialog box.
       </para>
      </step>
      <step id="bgchy7u">
       <para>
        Enter the IP address and change the port if needed.
       </para>
       <para>
        The default port is 3260.
       </para>
      </step>
      <step id="bgchy7v">
       <para>
        If authentication is required, deselect <guimenu>No
        Authentication</guimenu>, then specify the credentials the
        <guimenu>Incoming</guimenu> or <guimenu>Outgoing</guimenu>
        authentication.
       </para>
      </step>
      <step id="bgchy7w">
       <para>
        Click <guimenu>Next</guimenu> to start the discovery and connect to
        the iSCSI target server.
       </para>
      </step>
      <step id="bi728ck">
       <para>
        If credentials are required, after a successful discovery, use
        <guimenu>Login</guimenu> to activate the target.
       </para>
       <para>
        You are prompted for authentication credentials to use the selected
        iSCSI target.
       </para>
      </step>
      <step id="bi727ln">
       <para>
        Click <guimenu>Next</guimenu> to finish the configuration.
       </para>
       <para>
        If everything went well, the target now appears in
        <guimenu>Connected Targets</guimenu>.
       </para>
       <para>
        The virtual iSCSI device is now available.
       </para>
      </step>
      <step id="bkobeht">
       <para>
        On the iSCSI Initiator Overview page, click
        <guimenu>Finish</guimenu> to save and apply your changes.
       </para>
      </step>
      <step id="bi728cl">
       <para>
        You can find the local device path for the iSCSI target device by
        using the <command>lsscsi</command> command:
       </para>
<screen>
lsscsi
[1:0:0:0]   disk    IET      VIRTUAL-DISK     0     /dev/sda
</screen>
      </step>
     </procedure>
    </sect3>
    <sect3 id="bkob9pj">
     <title>Setting the Start-up Preference for iSCSI Target Devices</title>
     <procedure id="bkobg1r">
      <step id="bkobg1s">
       <para>
        In YaST, select<guimenu> iSCSI Initiator</guimenu>, then select the
        <guimenu>Connected Targets</guimenu> tab to view a list of the iSCSI
        target devices that are currently connected to the server.
       </para>
      </step>
      <step id="bkobg1t">
       <para>
        Select the iSCSI target device that you want to manage.
       </para>
      </step>
      <step id="bkobg1u">
       <para>
        Click <guimenu>Toggle Start-Up</guimenu> to modify the setting:
       </para>
       <itemizedlist>
        <listitem>
         <formalpara id="bkobg1v">
          <title>Automatic:</title>
          <para>
           This option is used for iSCSI targets that are to be connected
           when the iSCSI service itself starts up. This is the typical
           configuration.
          </para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara id="bkobg1w">
          <title>Onboot:</title>
          <para>
           This option is used for iSCSI targets that are to be connected
           during boot; that is, when root (<filename>/</filename>) is on
           iSCSI. As such, the iSCSI target device will be evaluated from
           the initrd on server boots.
          </para>
         </formalpara>
        </listitem>
       </itemizedlist>
      </step>
      <step id="bkobg1x">
       <para>
        Click <guimenu>Finish</guimenu> to save and apply your changes.
       </para>
      </step>
     </procedure>
    </sect3>
   </sect2>

<!--  iscsiadm -m discovery - -type=sendtargets - -portal=ip:port
  iscsiadm -m node -r [id]  => display all variables  
  iscsiadm -m node -r bd0ac2 - -op=update - -name=node.session.auth.authmethod - -value=CHAP
  iscsiadm -m node -r bd0ac2 - -login
  
  To use iSCSI, you need the 
  <systemitem class="resource">linux-iscsi</systemitem> package. The connection
  data must be entered in the <filename>/etc/iscsi.conf</filename>
  file. If you have an iSCSI storage
  device, this configuration file might look like this:
 </para> -->

   <sect2 id="sec_inst_system_iscsi_initiator_manual">
    <title>Setting Up the iSCSI Initiator Manually</title>
    <para>
     Both the discovery and the configuration of iSCSI connections require a
     running iscsid. When running the discovery the first time, the internal
     database of the iSCSI initiator is created in the directory
     <filename>/var/lib/open-iscsi</filename>.
    </para>
    <para>
     If your discovery is password protected, provide the authentication
     information to iscsid. Because the internal database does not exist
     when doing the first discovery, it cannot be used at this time.
     Instead, the configuration file <filename>/etc/iscsid.conf</filename>
     must be edited to provide the information. To add your password
     information for the discovery, add the following lines to the end of
     <filename>/etc/iscsid.conf</filename>:
    </para>
<screen>
discovery.sendtargets.auth.authmethod = CHAP
discovery.sendtargets.auth.username = &lt;username&gt;
discovery.sendtargets.auth.password = &lt;password&gt;
</screen>
    <para>
     The discovery stores all received values in an internal persistent
     database. In addition, it displays all detected targets. Run this
     discovery with the following command:
    </para>
<screen>
iscsiadm <option>-m discovery --type=st --portal=&lt;targetip&gt;</option>
</screen>
    <para>
     The output should look like the following:
    </para>
<screen>
10.44.171.99:3260,1 iqn.2006-02.com.example.iserv:systems
</screen>
    <para>
     To discover the available targets on a <literal>iSNS</literal> server,
     use the following command:
    </para>
<screen>
iscsiadm --mode discovery --type isns --portal &lt;targetip&gt; 
</screen>
    <para>
     For each target defined on the iSCSI target, one line appears. For more
     information about the stored data, see
     <xref linkend="sec_iscsi_database" xrefstyle="SectTitleOnPage"/>.
    </para>
    <para>
     The special <option>--login</option> option of
     <command>iscsiadm</command> creates all needed devices:
    </para>
<screen>
iscsiadm -m node -n iqn.2006-02.com.example.iserv:systems --login
</screen>
    <para>
     The newly generated devices show up in the output of
     <command>lsscsi</command> and can now be accessed by mount.
    </para>
   </sect2>

   <sect2 id="sec_iscsi_database">
    <title>The iSCSI Client Databases</title>
    <para>
     All information that was discovered by the iSCSI initiator is stored in
     two database files that reside in
     <filename>/var/lib/open-iscsi</filename>. There is one database for the
     discovery of targets and one for the discovered nodes. When accessing a
     database, you first must select if you want to get your data from the
     discovery or from the node database. Do this with the <option>-m
     discovery</option> and <option>-m node</option> parameters of
     <command>iscsiadm</command>. Using <command>iscsiadm</command> with one
     of these parameters gives an overview of the stored records:
    </para>
<screen>
iscsiadm -m discovery
10.44.171.99:3260,1 iqn.2006-02.com.example.iserv:systems
</screen>
    <para>
     The target name in this example is
     <literal>iqn.2006-02.com.example.iserv:systems</literal>. This name is
     needed for all actions that relate to this special data set. To examine
     the content of the data record with the ID
     <literal>iqn.2006-02.com.example.iserv:systems</literal>, use the
     following command:
    </para>
<screen>
iscsiadm -m node --targetname iqn.2006-02.com.example.iserv:systems
node.name = iqn.2006-02.com.example.iserv:systems
node.transport_name = tcp
node.tpgt = 1
node.active_conn = 1
node.startup = manual
node.session.initial_cmdsn = 0
node.session.reopen_max = 32
node.session.auth.authmethod = CHAP
node.session.auth.username = joe
node.session.auth.password = ********
node.session.auth.username_in = &lt;empty&gt;
node.session.auth.password_in = &lt;empty&gt;
node.session.timeo.replacement_timeout = 0
node.session.err_timeo.abort_timeout = 10
node.session.err_timeo.reset_timeout = 30
node.session.iscsi.InitialR2T = No
node.session.iscsi.ImmediateData = Yes
....
</screen>
    <para>
     To edit the value of one of these variables, use the command
     <command>iscsiadm</command> with the <option>update</option> operation.
     For example, if you want iscsid to log in to the iSCSI target when it
     initializes, set the variable <option>node.startup</option> to the
     value <option>automatic</option>:
    </para>
<screen>
iscsiadm -m node -n iqn.2006-02.com.example.iserv:systems -p ip:port --op=update --name=node.startup --value=automatic
</screen>
    <para>
     Remove obsolete data sets with the <literal>delete</literal> operation
     If the target <literal>iqn.2006-02.com.example.iserv:systems</literal>
     is no longer a valid record, delete this record with the following
     command:
    </para>
<screen>
<command>iscsiadm <option>-m node -n iqn.2006-02.com.example.iserv:systems -p ip:port --op=delete</option></command>
</screen>
    <important>
     <para>
      Use this option with caution because it deletes the record without any
      additional confirmation prompt.
     </para>
    </important>
    <para>
     To get a list of all discovered targets, run the <command>iscsiadm -m
     node</command> command.
    </para>
   </sect2>
  </sect1>
  <sect1 id="bwkrqdd">
   <title>Using iSCSI Disks when Installing</title>

   <para>
    Booting from an iSCSI disk on i386, x86_64, and ppc64 architectures is
    supported, when iSCSI enabled firmware is used.
   </para>

   <para>
    To use iSCSI disks during installation, it is necessary to add the
    following parameter to the boot option line:
   </para>

<screen>
withiscsi=1
</screen>

   <para>
    During installation, an additional screen appears that provides the
    option to attach iSCSI disks to the system and use them in the
    installation process.
   </para>
  </sect1>
  <sect1 id="sec_inst_system_iscsi_ts">
   <title>Troubleshooting iSCSI</title>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="nofail" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bp5cdib" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bpjp8x0" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwk7p6z" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="nofail">
    <title>Hotplug Doesn’t Work for Mounting iSCSI Targets</title>
    <para>
     In SLES 10, you could add the <literal>hotplug</literal> option to your
     device in the <filename>/etc/fstab</filename> file to mount iSCSI
     targets. For example:
    </para>
<screen>
/dev/disk/by-uuid-blah /oracle/db   ext3       hotplug,rw            0 2
</screen>
    <para>
     For SLES 11, the <literal>hotplug</literal> option no longer works. Use
     the <literal>nofail</literal> option instead. For example:
    </para>
<screen>
/dev/sdb1   /mnt/mountpoint   ext3   acl,user,nofail   0 0
</screen>
    <para>
     For information, see
     <ulink url="http://www.suse.com/support/php/search.do?cmd=displayKC&#x0026;docType=kc&#x0026;externalId=7004427"><citetitle>TID
     7004427: /etc/fstab entry does not mount iSCSI device on boot up
     </citetitle></ulink>.
    </para>
   </sect2>

   <sect2 id="bp5cdib">
    <title>Data Packets Dropped for iSCSI Traffic</title>
    <para>
     A firewall might drop packets if it gets to busy. The default for the
     SUSE Firewall is to drop packets after three minutes. If you find that
     iSCSI traffic packets are being dropped, you can consider configuring
     the SUSE Firewall to queue packets instead of dropping them when it
     gets too busy.
    </para>
   </sect2>

   <sect2 id="bpjp8x0">
    <title>Using iSCSI Volumes with LVM</title>
    <para>
     Use the troubleshooting tips in this section when using LVM on iSCSI
     targets.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bpkq3ci" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bpkpzuj" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bpkq3ci">
     <title>Check the iSCSI Initiator Discovery Occurs at Boot</title>
     <para>
      When you set up the iSCSI Initiator, ensure that you enable discovery
      at boot time so that udev can discover the iSCSI devices at boot time
      and set up the devices to be used by LVM.
     </para>
    </sect3>
    <sect3 id="bpkpzuj">
     <title>Check that iSCSI Target Discovery Occurs at Boot</title>
     <para>
      Remember that <filename>udev</filename> provides the default setup for
      devices. Ensure that all of the applications that create devices are
      started at boot time so that <command>udev</command> can recognize and
      assign devices for them at system startup. If the application or
      service is not started until later, <command>udev</command> does not
      create the device automatically as it would at boot time.
     </para>
     <para>
      You can check your settings for LVM2 and iSCSI with
      <guimenu>&ycc_runlevel;</guimenu> or with the
      <command>systemctl</command> commands. The following services should
      be enabled:
     </para>
     <simplelist>
      <member>lvm</member>
      <member>open-iscsi</member>
     </simplelist>
    </sect3>
   </sect2>

   <sect2 id="bwk7p6z">
    <title>iSCSI Targets Are Mounted When the Configuration File Is Set to Manual</title>
    <para>
     When Open-iSCSI starts, it can mount the targets even if the option
     <literal>node.startup</literal> option is set to manual in the
     <filename>/etc/iscsi/iscsid.conf</filename> file if you manually
     modified the configuration file.
    </para>
    <para>
     Check the
     <filename>/etc/iscsi/nodes/&lt;<replaceable>target_name</replaceable>&gt;/&lt;<replaceable>ip_address</replaceable>,<replaceable>port</replaceable>&gt;/default</filename>
     file. It contains a <literal>node.startup</literal> setting that
     overrides the <filename>/etc/iscsi/iscsid.conf</filename> file. Setting
     the mount option to manual by using the YaST interface also sets the
     <literal>node.startup = manual</literal> in the
     <filename>/etc/iscsi/nodes/&lt;<replaceable>target_name</replaceable>&gt;/&lt;<replaceable>ip_address</replaceable>,<replaceable>port</replaceable>&gt;/default
     </filename>files.
    </para>
   </sect2>
  </sect1>
  <sect1 id="sec_inst_system_iscsi_initiator_info">
   <title>Additional Information</title>

   <para>
    The iSCSI protocol has been available for several years. There are many
    reviews comparing iSCSI with SAN solutions, benchmarking performance,
    and there also is documentation describing hardware solutions. Important
    pages for more information about open-iscsi are:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      <ulink url="http://www.open-iscsi.org/"><citetitle>Open-iSCSI
      Project</citetitle></ulink>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="http://www.novell.com/coolsolutions/appnote/15394.html"><citetitle>AppNote:
      iFolder on Open Enterprise Server Linux Cluster using
      iSCSI</citetitle></ulink>
     </para>
    </listitem>
   </itemizedlist>

   <para>
    There is also some online documentation available. See the man pages for
    <command>iscsiadm</command>, <command>iscsid</command>,
    <filename>ietd.conf</filename>, and <command>ietd</command> and the
    example configuration file <filename>/etc/iscsid.conf</filename>.
   </para>
  </sect1>
 </chapter>
 <chapter id="cha_iscsi_lio" lang="en">
  <title>Mass Storage over IP Networks: iSCSI LIO Target Server</title>
  <para>
   <remark>Fate 312231</remark>
   LIO (linux-iscsi.org) is the standard open-source multiprotocol SCSI
   target for Linux. LIO replaced the STGT (SCSI Target) framework as the
   standard unified storage target in Linux with Linux kernel version 2.6.38
   and later. YaST supports the iSCSI LIO Target Server software in SUSE
   Linux Enterprise Server 11 SP3 and later.
  </para>
  <para>
   This section describes how to use YaST to configure an iSCSI LIO Target
   Server and set up iSCSI LIO target devices. You can use any iSCSI
   initiator software to access the target devices.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="sect_iscsi_lio_install" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sect_iscsi_lio_service" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sect_iscsi_lio_discovery" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sect_iscsi_lio_storage" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sect_iscsi_lio_targetgp" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sect_iscsi_lio_edit" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sect_iscsi_lio_delete" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b13g8i2g" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b12iqi8o" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="sect_iscsi_lio_install">
   <title>Installing the iSCSI LIO Target Server Software</title>

   <para>
    Use the YaST Software Management tool to install the iSCSI LIO Target
    Server software on the &productname; server where
    you want to create iSCSI LIO target devices.
   </para>

   <procedure id="b12jepdi">
    <step id="b12jepdj">
     <para>
      Launch YaST as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="b12jepdk">
     <para>
      Select <guimenu>Software</guimenu><guimenu>Software
      Management.</guimenu>
     </para>
    </step>
    <step id="b12jepdl">
     <para>
      Select the <guimenu>Search</guimenu> tab, type <literal>lio</literal>,
      then click <guimenu>Search</guimenu>.
     </para>
    </step>
    <step id="b12jepdm">
     <para>
      Select the iSCSI LIO Target Server packages:
     </para>
     <informaltable  frame="topbot" rowsep="1" pgwide="0">
      <tgroup cols="2">
       <colspec colnum="1" colname="1" colwidth="4091*"/>
       <colspec colnum="2" colname="2" colwidth="5912*"/>
       <thead>
        <row id="b12gs3f1">
         <entry>
          <para>
           iSCSI LIO Target Server Packages
          </para>
         </entry>
         <entry>
          <para>
           Description
          </para>
         </entry>
        </row>
       </thead>
       <tbody>
        <row id="b12gs3f2">
         <entry>
          <para>
           <filename>yast2-iscsi-lio-server</filename>
          </para>
         </entry>
         <entry>
          <para>
           Provides a GUI interface in YaST for the configuration of iSCSI
           LIO target devices.
          </para>
         </entry>
        </row>
        <row id="b12gs3f3">
         <entry>
          <para>
           <filename>lio-utils</filename>
          </para>
         </entry>
         <entry>
          <para>
           Provides APIs for configuring and controlling iSCSI LIO target
           devices that are used by
           <filename>yast2-iscsi-lio-server</filename>.
          </para>
         </entry>
        </row>
        <row id="b12gs3f4">
         <entry>
          <para>
           <filename>lio-mibs</filename>
          </para>
         </entry>
         <entry>
          <para>
           Provides SNMP (Simple Network Management Protocol) monitoring of
           iSCSI LIO target devices by using the dynamic load module
           (<filename>dlmod</filename>) functionality of the Net-SNMP agent.
           It supports SNMP v1, v2c, and v3. The configuration file is
           <filename>/etc/snmp/snmpd.conf</filename>.
          </para>
          <para>
           The <filename>lio-mibs</filename> software uses the
           <filename>perl-SNMP</filename> and <filename>net-snmp</filename>
           packages.
          </para>
          <para>
           For information about Net-SNMP, see the open source
           <ulink url="http://www.net-snmp.org/">Net-SNMP Project</ulink>.
          </para>
         </entry>
        </row>
        <row id="b12gs3f5">
         <entry>
          <para>
           <filename>lio</filename>-utils-debuginfo
          </para>
         </entry>
         <entry>
          <para>
           Provides debug information for the <filename>lio-utils</filename>
           package. You can use this package when developing or debugging
           applications for <filename>lio-utils</filename>.
          </para>
         </entry>
        </row>
       </tbody>
      </tgroup>
     </informaltable>
    </step>
    <step id="b12r0abb">
     <para>
      In the lower right corner of the dialog box, click
      <guimenu>Accept</guimenu> to install the selected packages.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_install_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_install_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b12jepdn">
     <para role="intro">
      When you are prompted to approve the automatic changes, click
      <guimenu>Continue</guimenu> to accept the iSCSI LIO Target Server
      dependencies for the <filename>lio-utils</filename>,
      <filename>perl-SNMP</filename>, and <filename>net-snmp</filename>
      packages.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_install_changes_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_install_changes_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b12jepdo">
     <para role="intro">
      Close and re-launch YaST, then click <guimenu>Network
      Services</guimenu> and verify that the<guimenu> iSCSI LIO
      Target</guimenu> option is available in the menu.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_yast_option_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_yast_option_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b12jepdp">
     <para>
      Continue with
      <xref linkend="sect_iscsi_lio_service" xrefstyle="SectTitleOnPage"/>.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="sect_iscsi_lio_service">
   <title>Starting the iSCSI LIO Target Service</title>

   <para>
    The iSCSI LIO Target service is by default configured to be started
    manually. You can configure the service to start automatically on system
    restart. If you use a firewall on the server and you want the iSCSI LIO
    targets to be available to other computers, you must open a port in the
    firewall for each adapter that you want to use for target access. TCP
    port 3260 is the port number for the iSCSI protocol, as defined by IANA
    (Internet Assigned Numbers Authority).
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b12jbbfh" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b12jbaln" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b12jbalo" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b12jbbfh">
    <title>Configuring iSCSI LIO Startup Preferences</title>
    <para>
     To configure the iSCSI LIO Target Server service settings:
    </para>
    <procedure id="b12jauce">
     <step id="b13n0ujo">
      <para>
       Log in to the iSCSI LIO target server as the
       <systemitem>root</systemitem> user, then launch a terminal console.
      </para>
     </step>
     <step id="b13n0ujp">
      <para>
       Ensure that the <filename>/etc/init.d/target</filename> daemon is
       running. At the command prompt, enter
      </para>
<screen>
 systemctl start target.service
</screen>
      <para>
       The command returns a message to confirm that the daemon is started,
       or that the daemon is already running.
      </para>
     </step>
     <step id="b12jaucf">
      <para>
       Launch YaST as the <systemitem>root</systemitem> user.
      </para>
     </step>
     <step id="b12jaucg">
      <para>
       In the YaST Control Center, select <guimenu>Network
       Services</guimenu>, then select <guimenu>iSCSI LIO Target</guimenu>.
      </para>
      <para>
       You can also search for <quote>lio</quote>, then select
       <guimenu>iSCSI LIO Target</guimenu>.
      </para>
      <informalfigure pgwide="0">
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="lio_yast2_a.png" width="250pt" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="lio_yast2_a.png" width="250pt" format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step id="b12jauch">
      <para role="intro">
       In the iSCSI LIO Target Overview dialog box, select the
       <guimenu>Service</guimenu> tab.
      </para>
      <informalfigure pgwide="0">
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="lio_service_a.png" width="250pt" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="lio_service_a.png" width="250pt" format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step id="b12jauci">
      <para role="intro">
       Under <guimenu>Service Start</guimenu>, specify how you want the
       iSCSI LIO target service to be started:
      </para>
      <itemizedlist>
       <listitem>
        <formalpara id="b12jaucj">
         <title>When Booting:</title>
         <para>
          The service starts automatically on server restart.
         </para>
        </formalpara>
       </listitem>
       <listitem>
        <formalpara id="b12jauck">
         <title>Manually:</title>
         <para>
          (Default) You must start the service manually after a server
          restart. The target devices are not available until you start the
          service.
         </para>
        </formalpara>
       </listitem>
      </itemizedlist>
     </step>
     <step id="b12jaucl">
      <para>
       If you use a firewall on the server and you want the iSCSI LIO
       targets to be available to other computers, open a port in the
       firewall for each adapter interface that you want to use for target
       access.
      </para>
      <para>
       Firewall settings are disabled by default. They are not needed unless
       you deploy a firewall on the server. The default port number is 3260.
       If the port is closed for all of the network interfaces, the iSCSI
       LIO targets are not available to other computers.
      </para>
      <substeps>
       <step id="b12jaucm">
        <para>
         On the <guimenu>Services</guimenu> tab, select the <guimenu>Open
         Port in Firewall</guimenu> check box to enable the firewall
         settings.
        </para>
       </step>
       <step id="b12jaucn">
        <para>
         Click <guimenu>Firewall Details</guimenu> to view or configure the
         network interfaces to use.
        </para>
        <para>
         All available network interfaces are listed, and all are selected
         by default.
        </para>
        <informalfigure pgwide="0">
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="lio_firewall_ports_a.png" width="250pt" format="PNG"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="lio_firewall_ports_a.png" width="250pt" format="PNG"/>
          </imageobject>
         </mediaobject>
        </informalfigure>
       </step>
       <step id="b12jauco">
        <para>
         For each interface, specify whether to open or close a port for it:
        </para>
        <itemizedlist>
         <listitem>
          <formalpara id="b12jaucp">
           <title>Open:</title>
           <para>
            Select the interface’s check box to open the port. You can
            also click <guimenu>Select All</guimenu> to open a port on all
            of the interfaces.
           </para>
          </formalpara>
         </listitem>
         <listitem>
          <formalpara id="b12jaucq">
           <title>Close:</title>
           <para>
            Deselect the interface’s check box to close the port. You can
            also click <guimenu>Select None</guimenu> to close the port on
            all of the interfaces.
           </para>
          </formalpara>
         </listitem>
        </itemizedlist>
       </step>
       <step id="b12jaucr">
        <para>
         Click <guimenu>OK</guimenu> to save and apply your changes.
        </para>
       </step>
       <step id="b12r0it6">
        <para>
         If you are prompted to confirm the settings, click
         <guimenu>Yes</guimenu> to continue, or click <guimenu>No</guimenu>
         to return to the dialog box and make the desired changes.
        </para>
       </step>
      </substeps>
     </step>
     <step id="b12jb97x">
      <para>
       Click <guimenu>Finish</guimenu> to save and apply the iSCSI LIO
       Target service settings.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="b12jbaln">
    <title>Manually Starting iSCSI LIO Target at the Command Line</title>
    <procedure id="b12gboje">
     <step id="b13n0ujq">
      <para>
       Log in as the <systemitem>root</systemitem> user, then launch a
       terminal console.
      </para>
     </step>
     <step id="b13n0ujr">
      <para>
       At the command prompt, enter
      </para>
<screen>
 systemctl start target.service
</screen>
      <para>
       The command returns a message to confirm that the daemon is started,
       or that the daemon is already running.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="b12jbalo">
    <title>Manually Starting iSCSI LIO Target in YaST</title>
    <procedure id="b12gblm7">
     <step id="b12gblm8">
      <para>
       Launch YaST as the <systemitem>root</systemitem> user.
      </para>
     </step>
     <step id="b12gblm9">
      <para>
       In the YaST Control Center, select <guimenu>System</guimenu>, then
       select <guimenu>&ycc_runlevel;</guimenu>.
      </para>
     </step>
     <step id="b12gblma">
      <para>
       In the YaST &ycc_runlevel; dialog box, select
       <guimenu>target</guimenu> (<guimenu>TCM/ConfigFS and
       LIO-Target</guimenu>) in the list of services.
       <remark>taroth 2014-02-27: FIXME systemd - screenshot
        (commented for now) needs update!</remark>
      </para>
<!--  <informalfigure pgwide="0">
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="lio_runlevel_a.png" width="300pt" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="lio_runlevel_a.png" width="300pt" format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>-->
     </step>
     <step id="b12gbmn5">
      <para>
       If the service is listed as <literal>inactive</literal>, click
       <guimenu>Start/Stop</guimenu> to change this.
      </para>
     </step>
     <step id="b12jbalp">
      <para>
       Click <guimenu>OK</guimenu>.
      </para>
     </step>
    </procedure>
   </sect2>
  </sect1>
  <sect1 id="sect_iscsi_lio_discovery">
   <title>Configuring Authentication for Discovery of iSCSI LIO Targets and Clients</title>

   <para>
    The iSCSI LIO Target Server software supports the PPP-CHAP
    (Point-to-Point Protocol Challenge Handshake Authentication Protocol), a
    three-way authentication method defined in the
    <ulink url="http://www.ietf.org/rfc/rfc1994.txt"><citetitle>Internet
    Engineering Task Force (IETF) RFC 1994</citetitle></ulink>. iSCSI LIO
    Target Server uses this authentication method for the discovery of iSCSI
    LIO targets and clients, not for accessing files on the targets. If you
    do not want to restrict the access to the discovery, use <guimenu>No
    Authentication</guimenu>. The <guimenu>No Authentication</guimenu>
    option is enabled by default. If authentication for discovery is
    enabled, its settings apply to all iSCSI LIO target groups.
   </para>

   <important>
    <para>
     We recommend that you use authentication for target and client
     discovery in production environments.
    </para>
   </important>

   <para>
    If authentication is needed for a more secure configuration, you can use
    incoming authentication, outgoing authentication, or both.
    <guimenu>Incoming Authentication</guimenu> requires an iSCSI initiator
    to prove that it has the permissions to run a discovery on the iSCSI LIO
    target. The initiator must provide the incoming user name and password.
    <guimenu>Outgoing Authentication</guimenu> requires the iSCSI LIO target
    to prove to the initiator that it is the expected target. The iSCSI LIO
    target must provide the outgoing user name and password to the iSCSI
    initiator. The user name and password pair can be different for incoming
    and outgoing discovery.
   </para>

   <para>
    To configure authentication preferences for iSCSI LIO targets:
   </para>

   <procedure id="b12g8sjw">
    <step id="b13n0ujs">
     <para>
      Log in to the iSCSI LIO target server as the
      <systemitem>root</systemitem> user, then launch a terminal console.
     </para>
    </step>
    <step id="b13n0ujt">
     <para>
      Ensure that the <filename>/etc/init.d/target</filename> daemon is
      running. At the command prompt, enter
     </para>
<screen>
 systemctl start target.service
</screen>
     <para>
      The command returns a message to confirm that the daemon is started,
      or that the daemon is already running.
     </para>
    </step>
    <step id="b12g8sjx">
     <para>
      Launch YaST as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="b12g8sjy">
     <para>
      In the YaST Control Center, select <guimenu>Network
      Services</guimenu>, then select <guimenu>iSCSI LIO Target</guimenu>.
     </para>
     <para>
      You can also search for <quote>lio</quote>, then select <guimenu>iSCSI
      LIO Target</guimenu>.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_yast2_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_yast2_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b12g9s76">
     <para>
      In the iSCSI LIO Target Overview dialog box, select the
      <guimenu>Global</guimenu> tab to configure the authentication
      settings. Authentication settings are disabled by default.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_global_noauth_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_global_noauth_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b12jco93">
     <para>
      Specify whether to require authentication for iSCSI LIO targets:
     </para>
     <itemizedlist>
      <listitem>
       <formalpara id="b12jcrm7">
        <title>Disable authentication:</title>
        <para>
         (Default) Select the <guimenu>No Authentication</guimenu> check box
         to disable incoming and outgoing authentication for discovery on
         this server. All iSCSI LIO targets on this server can be discovered
         by any iSCSI initiator client on the same network. This server can
         discover any iSCSI initiator client on the same network that does
         not require authentication for discovery. Skip
         <xref linkend="b12r1015" xrefstyle="StepXRef"/> and continue with
         <xref linkend="b12gb2ux" xrefstyle="StepXRef"/>.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="b12jcrm8">
        <title>Enable authentication:</title>
        <para>
         Deselect the <guimenu>No Authentication</guimenu> check box. The
         check boxes for both <guimenu>Incoming Authentication</guimenu> and
         <guimenu>Outgoing Authentication</guimenu> are automatically
         selected. Continue with
         <xref linkend="b12r1015" xrefstyle="StepXRef"/>.
        </para>
       </formalpara>
       <informalfigure pgwide="0">
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="lio_global_in_out_auth_a.png" width="250pt" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="lio_global_in_out_auth_a.png" width="250pt" format="PNG"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
      </listitem>
     </itemizedlist>
    </step>
    <step id="b12r1015">
     <para role="intro">
      Configure the authentication credentials needed for incoming
      discovery, outgoing discovery, or both. The user name and password
      pair can be different for incoming and outgoing discovery.
     </para>
     <substeps>
      <step id="b12jcw2u">
       <para>
        Configure incoming authentication by doing one of the following:
       </para>
       <itemizedlist>
        <listitem>
         <formalpara id="b12r0xyn">
          <title>Disable incoming authentication:</title>
          <para>
           Deselect the <guimenu>Incoming Authentication</guimenu> check
           box. All iSCSI LIO targets on this server can be discovered by
           any iSCSI initiator client on the same network.
          </para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara id="b12r0xyo">
          <title>Enable incoming authentication:</title>
          <para>
           Select the <guimenu>Incoming Authentication</guimenu> check box,
           then specify an existing user name and password pair to use for
           incoming discovery of iSCSI LIO targets.
          </para>
         </formalpara>
        </listitem>
       </itemizedlist>
      </step>
      <step id="b12jcw2x">
       <para>
        Configure outgoing authentication by doing one of the following:
       </para>
       <itemizedlist>
        <listitem>
         <formalpara id="b12r0xyp">
          <title>Disable outgoing authentication:</title>
          <para>
           Deselect the <guimenu>Outgoing Authentication</guimenu> check
           box. This server can discover any iSCSI initiator client on the
           same network that does not require authentication for discovery.
          </para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara id="b12r0xyq">
          <title>Enable outgoing authentication:</title>
          <para>
           Select the <guimenu>Outgoing Authentication</guimenu> check box,
           then specify an existing user name and password pair to use for
           outgoing discovery of iSCSI initiator clients.
          </para>
         </formalpara>
        </listitem>
       </itemizedlist>
      </step>
     </substeps>
    </step>
    <step id="b12gb2ux">
     <para>
      Click <guimenu>Finish</guimenu> to save and apply the settings.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="sect_iscsi_lio_storage">
   <title>Preparing the Storage Space</title>

   <para>
    The iSCSI LIO target configuration exports existing block devices to
    iSCSI initiators. You must prepare the storage space you want to use in
    the target devices by setting up unformatted partitions or devices on
    the server. iSCSI LIO targets can use unformatted partitions with Linux,
    Linux LVM, or Linux RAID file system IDs.
   </para>

   <important>
    <para>
     After you set up a device or partition for use as an iSCSI target, you
     never access it directly via its local path. Do not specify a mount
     point for it when you create it.
    </para>
   </important>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b12jcw6s" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b12jcw75" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b12jcw6s">
    <title>Partitioning Devices</title>
    <procedure id="b12jcw6t">
     <step id="b12jcw6u">
      <para>
       Launch YaST as the <systemitem>root</systemitem> user.
      </para>
     </step>
     <step id="b12jcw6v">
      <para>
       In YaST, select
       <guimenu>System</guimenu><guimenu>Partitioner</guimenu>.
      </para>
     </step>
     <step id="b12jcw6w">
      <para>
       Click <guimenu>Yes</guimenu> to continue through the warning about
       using the Partitioner.
      </para>
     </step>
     <step id="b12jcw6x">
      <para>
       At the bottom of the Partitions page, click <guimenu>Add</guimenu> to
       create a partition, but do not format it, and do not mount it.
      </para>
      <substeps>
       <step id="b12o0qiv">
        <para>
         On the Expert Partitioner page, select <guimenu>Hard
         Disks</guimenu>, then select the leaf node name (such as
         <filename>sdc</filename>) of the disk you want to configure.
        </para>
       </step>
       <step id="b12jcw6y">
        <para>
         Select <guimenu>Primary Partition</guimenu>, then click
         <guimenu>Next</guimenu>.
        </para>
       </step>
       <step id="b12jcw6z">
        <para>
         Specify the amount of space to use, then click
         <guimenu>Next</guimenu>.
        </para>
       </step>
       <step id="b12jcw70">
        <para>
         Under <guimenu>Formatting Options</guimenu>, select <guimenu>Do not
         format</guimenu>, then select the file system ID type from the
         drop-down list.
        </para>
        <para>
         iSCSI LIO targets can use unformatted partitions with Linux (0x83),
         Linux LVM (0x8E), or Linux RAID (0xFD) file system IDs.
        </para>
        <informalfigure pgwide="0">
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="lio_disk_format_a.png" width="250pt" format="PNG"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="lio_disk_format_a.png" width="250pt" format="PNG"/>
          </imageobject>
         </mediaobject>
        </informalfigure>
       </step>
       <step id="b12jcw71">
        <para>
         Under <guimenu>Mounting Options</guimenu>, select <guimenu>Do not
         mount</guimenu>.
        </para>
       </step>
       <step id="b12jcw72">
        <para>
         Click <guimenu>Finish</guimenu>.
        </para>
       </step>
      </substeps>
     </step>
     <step id="b12jcw73">
      <para>
       Repeat <xref linkend="bj5f9hl" xrefstyle="StepXRef"/> to create an
       unformatted partition for each area that you want to use later as an
       iSCSI LIO target.
      </para>
      <informalfigure pgwide="0">
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="lio_disk_format_multi_a.png" width="250pt" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="lio_disk_format_multi_a.png" width="250pt" format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step id="b12jcw74">
      <para>
       Click <guimenu>Next</guimenu><guimenu>Finish</guimenu> to keep your
       changes, then close YaST.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="b12jcw75">
    <title>Partitioning Devices in a Virtual Environment</title>
    <para>
     You can use a virtual machine guest server as a iSCSI LIO Target
     Server. This section describes how to assign partitions to a Xen
     virtual machine. You can also use other virtual environments that are
     supported by SUSE Linux Enterprise Server 11 SP2 or later.
    </para>
    <para>
     In a Xen virtual environment, you must assign the storage space you
     want to use for the iSCSI LIO target devices to the guest virtual
     machine, then access the space as virtual disks within the guest
     environment. Each virtual disk can be a physical block device, such as
     an entire disk, partition, or volume, or it can be a file-backed disk
     image where the virtual disk is a single image file on a larger
     physical disk on the Xen host server. For the best performance, create
     each virtual disk from a physical disk or a partition. After you set up
     the virtual disks for the guest virtual machine, start the guest
     server, then configure the new blank virtual disks as iSCSI target
     devices by following the same process as for a physical server.
    </para>
    <para>
     File-backed disk images are created on the Xen host server, then
     assigned to the Xen guest server. By default, Xen stores file-backed
     disk images in the
     <filename>/var/lib/xen/images/<replaceable>vm_name</replaceable></filename>
     directory, where
     <filename><replaceable>vm_name</replaceable></filename> is the name of
     the virtual machine.
    </para>
    <para>
     For example, if you want to create the disk image
     <filename>/var/lib/xen/images/vm_one/xen-0</filename> with a size of 4
     GB, first ensure that the directory is there, then create the image
     itself.
    </para>
    <procedure id="b12jcw76">
     <step id="b12jcw77">
      <para>
       Log in to the host server as the <systemitem>root</systemitem> user.
      </para>
     </step>
     <step id="b12jcw78">
      <para>
       At a terminal console prompt, enter the following commands:
      </para>
<screen>
mkdir -p /var/lib/xen/images/vm_one
dd if=/dev/zero of=/var/lib/xen/images/vm_one/xen-0 seek=1M bs=4096 count=1
</screen>
     </step>
     <step id="b12jcw79">
      <para>
       Assign the file system image to the guest virtual machine in the Xen
       configuration file.
      </para>
     </step>
     <step id="b12jcw7a">
      <para>
       Log in as the <systemitem>root</systemitem> user on the guest server,
       then use YaST to set up the virtual block device by using the process
       in <xref linkend="b12jcw6s" xrefstyle="HeadingOnPage"/>.
      </para>
     </step>
    </procedure>
   </sect2>
  </sect1>
  <sect1 id="sect_iscsi_lio_targetgp">
   <title>Setting Up an iSCSI LIO Target Group</title>

   <para>
    You can use YaST to configure iSCSI LIO target devices. YaST uses APIs
    provided by the <command>lio-utils</command> software. iSCSI LIO targets
    can use unformatted partitions with Linux, Linux LVM, or Linux RAID file
    system IDs.
   </para>

   <important>
    <para>
     Before you begin, create the unformatted partitions that you want to
     use as iSCSI LIO targets as described in
     <xref linkend="sect_iscsi_lio_storage" xrefstyle="SectTitleOnPage"/>.
    </para>
   </important>

   <procedure id="b12jbfdv">
    <step id="b13n0uju">
     <para>
      Log in to the iSCSI LIO target server as the
      <systemitem>root</systemitem> user, then launch a terminal console.
     </para>
    </step>
    <step id="b13n0ujv">
     <para>
      Ensure that the <filename>/etc/init.d/target</filename> daemon is
      running. At the command prompt, enter
     </para>
<screen>
systemctl start target.service
</screen>
     <para>
      The command returns a message to confirm that the daemon is started,
      or that the daemon is already running.
     </para>
    </step>
    <step id="b12jbfdw">
     <para>
      Launch YaST as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="b12jbfdx">
     <para>
      In the YaST Control Center, select <guimenu>Network
      Services</guimenu>, then select <guimenu>iSCSI LIO Target</guimenu>.
     </para>
     <para>
      You can also search for <quote>lio</quote>, then select <guimenu>iSCSI
      LIO Target</guimenu>.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_yast2_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_yast2_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b12jbfdy">
     <para>
      In the iSCSI LIO Target Overview dialog box, select the
      <guimenu>Targets</guimenu> tab to configure the targets.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_targets_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_targets_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b12jdln1">
     <para role="intro">
      Click <guimenu>Add</guimenu>, then define a new iSCSI LIO target group
      and devices:
     </para>
     <para role="intro">
      The iSCSI LIO Target software automatically completes the
      <guimenu>Target</guimenu>, <guimenu>Identifier</guimenu>,
      <guimenu>Portal Group</guimenu>, <guimenu>IP Address</guimenu>, and
      <guimenu>Port Number</guimenu> fields. <guimenu>Use
      Authentication</guimenu> is selected by default.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_target_add_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_target_add_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
     <substeps>
      <step id="b12jdzge">
       <para>
        If you have multiple network interfaces, use the IP address
        drop-down list to select the IP address of the network interface to
        use for this target group.
       </para>
      </step>
      <step id="b12jdzgf">
       <para>
        Select <guimenu>Use Authentication</guimenu> if you want to require
        client authentication for this target group.
       </para>
       <important>
        <para>
         Requiring authentication is recommended in a production
         environment.
        </para>
       </important>
      </step>
      <step id="b12je5o1">
       <para>
        Click <guimenu>Add</guimenu>, browse to select the device or
        partition, specify a name, then click <guimenu>OK</guimenu>.
       </para>
       <para>
        The LUN number is automatically generated, beginning with 0. A name
        is automatically generated if you leave the field empty.
       </para>
       <informalfigure pgwide="0">
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="lio_target_addnew_a.png" width="91pt" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="lio_target_addnew_a.png" width="91pt" format="PNG"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
      </step>
      <step id="b12jebk1">
       <para>
        (Optional) Repeat <xref linkend="b12jdzge" xrefstyle="StepXRef"/>
        through <xref linkend="b12je5o1" xrefstyle="StepXRef"/> to add more
        targets to this target group.
       </para>
       <informalfigure pgwide="0">
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="lio_target_list_a.png" width="250pt" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="lio_target_list_a.png" width="250pt" format="PNG"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
      </step>
      <step id="b12jeipi">
       <para>
        After all desired targets have been added to the group, click
        <guimenu>Next</guimenu>.
       </para>
      </step>
     </substeps>
    </step>
    <step id="b12jeipj">
     <para>
      On the Modify iSCSI Target Client Setup page, configure information
      for the clients that are permitted to access LUNs in the target group:
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_target_client_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_target_client_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
     <para>
      After you specify at least one client for the target group, the
      <guimenu>Edit LUN</guimenu>, <guimenu>Edit Auth</guimenu>,
      <guimenu>Delete</guimenu>, and <guimenu>Copy</guimenu> buttons are
      enabled. You can use <guimenu>Add</guimenu> or <guimenu>Copy</guimenu>
      to add more clients for the target group.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_client_options_a.png" width="200pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_client_options_a.png" width="200pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
     <itemizedlist>
      <listitem>
       <formalpara id="b12mh6xo">
        <title>Add:</title>
        <para>
         Add a new client entry for the selected iSCSI LIO target group.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="b12mgggu">
        <title>Edit LUN:</title>
        <para>
         Configure which LUNs in the iSCSI LIO target group to map to a
         selected client. You can map each of the allocated targets to a
         preferred client LUN.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="b12mgggv">
        <title>Edit Auth:</title>
        <para>
         Configure the preferred authentication method for a selected
         client. You can specify no authentication, or you can configure
         incoming authentication, outgoing authentication, or both.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="b12mgggw">
        <title>Delete:</title>
        <para>
         Remove a selected client entry from the list of clients allocated
         to the target group.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="b12mgggx">
        <title>Copy:</title>
        <para>
         Add a new client entry with the same LUN mappings and
         authentication settings as a selected client entry. This allows you
         to easily allocate the same shared LUNs, in turn, to each node in a
         cluster.
        </para>
       </formalpara>
      </listitem>
     </itemizedlist>
     <substeps>
      <step id="b12mdr9g">
       <para>
        Click <guimenu>Add</guimenu>, specify the client name, select or
        deselect the <guimenu>Import LUNs from TPG</guimenu> check box, then
        click <guimenu>OK</guimenu> to save the settings.
       </para>
       <informalfigure pgwide="0">
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="lio_client1_new_a.png" width="200pt" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="lio_client1_new_a.png" width="200pt" format="PNG"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
      </step>
      <step id="b12mdr9h">
       <para>
        Select a client entry, click <guimenu>Edit LUN</guimenu>, modify the
        LUN mappings to specify which LUNs in the iSCSI LIO target group to
        allocate to the selected client, then click <guimenu>OK</guimenu> to
        save the changes.
       </para>
       <para>
        If the iSCSI LIO target group consists of multiple LUNs, you can
        allocate one or multiple LUNs to the selected client. By default,
        each of the available LUNs in the group are assigned to a Client
        LUN.
       </para>
       <informalfigure pgwide="0">
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="lio_client3_edit_lun_a.png" width="107pt" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="lio_client3_edit_lun_a.png" width="107pt" format="PNG"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
       <para>
        To modify the LUN allocation, perform one or more of the following
        actions:
       </para>
       <itemizedlist>
        <listitem>
         <formalpara id="b12mhta3">
          <title>Add:</title>
          <para>
           Click <guimenu>Add</guimenu> to create an new <guimenu>Client
           LUN</guimenu> entry, then use the <guimenu>Change</guimenu>
           drop-down list to map a Target LUN to it.
          </para>
         </formalpara>
         <informalfigure pgwide="0">
          <mediaobject>
           <imageobject role="fo">
            <imagedata fileref="lio_client3_edit_lun4_add_a.png" width="107pt" format="PNG"/>
           </imageobject>
           <imageobject role="html">
            <imagedata fileref="lio_client3_edit_lun4_add_a.png" width="107pt" format="PNG"/>
           </imageobject>
          </mediaobject>
         </informalfigure>
        </listitem>
        <listitem>
         <formalpara id="b12mhta4" role="intro">
          <title>Delete:</title>
          <para>
           Select the <guimenu>Client LUN</guimenu> entry, then click
           <guimenu>Delete</guimenu> to remove a Target LUN mapping.
          </para>
         </formalpara>
         <informalfigure pgwide="0">
          <mediaobject>
           <imageobject role="fo">
            <imagedata fileref="lio_client3_edit_lun3_delete_a.png" width="106pt" format="PNG"/>
           </imageobject>
           <imageobject role="html">
            <imagedata fileref="lio_client3_edit_lun3_delete_a.png" width="106pt" format="PNG"/>
           </imageobject>
          </mediaobject>
         </informalfigure>
        </listitem>
        <listitem>
         <formalpara id="b12mhta5">
          <title>Change:</title>
          <para>
           Select the <guimenu>Client LUN</guimenu> entry, then use the
           <guimenu>Change</guimenu> drop-down list to select which Target
           LUN to map to it.
          </para>
         </formalpara>
         <informalfigure pgwide="0">
          <mediaobject>
           <imageobject role="fo">
            <imagedata fileref="lio_client3_edit_lun2_change_a.png" width="108pt" format="PNG"/>
           </imageobject>
           <imageobject role="html">
            <imagedata fileref="lio_client3_edit_lun2_change_a.png" width="108pt" format="PNG"/>
           </imageobject>
          </mediaobject>
         </informalfigure>
        </listitem>
       </itemizedlist>
       <para>
        Typical allocation plans include the following:
       </para>
       <itemizedlist>
        <listitem>
         <para>
          A single server is listed as a client. All of the LUNs in the
          target group are allocated to it.
         </para>
         <para>
          You can use this grouping strategy to logically group the iSCSI
          SAN storage for a given server.
         </para>
         <informalfigure pgwide="0">
          <mediaobject>
           <imageobject role="fo">
            <imagedata fileref="lio_client5_list_a.png" width="250pt" format="PNG"/>
           </imageobject>
           <imageobject role="html">
            <imagedata fileref="lio_client5_list_a.png" width="250pt" format="PNG"/>
           </imageobject>
          </mediaobject>
         </informalfigure>
        </listitem>
        <listitem>
         <para role="intro">
          Multiple independent servers are listed as clients. One or
          multiple target LUNs are allocated to each server. Each LUN is
          allocated to only one server.
         </para>
         <para>
          You can use this grouping strategy to logically group the iSCSI
          SAN storage for a given department or service category in the data
          center.
         </para>
         <informalfigure pgwide="0">
          <mediaobject>
           <imageobject role="fo">
            <imagedata fileref="lio_client5_list3_a.png" width="250pt" format="PNG"/>
           </imageobject>
           <imageobject role="html">
            <imagedata fileref="lio_client5_list3_a.png" width="250pt" format="PNG"/>
           </imageobject>
          </mediaobject>
         </informalfigure>
        </listitem>
        <listitem>
         <para>
          Each node of a cluster is listed as a client. All of the shared
          target LUNs are allocated to each node. All nodes are attached to
          the devices, but for most file systems, the cluster software locks
          a device for access and mounts it on only one node at a time.
          Shared file systems (such as OCFS2) make it possible for multiple
          nodes to concurrently mount the same file structure and to open
          the same files with read and write access.
         </para>
         <para>
          You can use this grouping strategy to logically group the iSCSI
          SAN storage for a given server cluster.
         </para>
         <informalfigure pgwide="0">
          <mediaobject>
           <imageobject role="fo">
            <imagedata fileref="lio_client5_list2_a.png" width="250pt" format="PNG"/>
           </imageobject>
           <imageobject role="html">
            <imagedata fileref="lio_client5_list2_a.png" width="250pt" format="PNG"/>
           </imageobject>
          </mediaobject>
         </informalfigure>
        </listitem>
       </itemizedlist>
      </step>
      <step id="b12mdr9i">
       <para>
        Select a client entry, click <guimenu>Edit Auth</guimenu>, specify
        the authentication settings for the client, then click
        <guimenu>OK</guimenu> to save the settings.
       </para>
       <para>
        You can require <guimenu>No Authentication</guimenu>, or you can
        configure <guimenu>Incoming Authentication</guimenu>,
        <guimenu>Outgoing Authentication</guimenu>, or both. You can specify
        only one user name and password pair for each client. The
        credentials can be different for incoming and outgoing
        authentication for a client. The credentials can be different for
        each client.
       </para>
       <informalfigure pgwide="0">
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="lio_client4_edit_auth2_inout_a.png" width="200pt" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="lio_client4_edit_auth2_inout_a.png" width="200pt" format="PNG"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
      </step>
      <step id="b12mdr9j">
       <para>
        Repeat <xref linkend="b12mdr9g" xrefstyle="StepXRef"/> through
        <xref linkend="b12mdr9i" xrefstyle="StepXRef"/> for each iSCSI
        client that can access this target group.
       </para>
      </step>
      <step id="b12mdr9k">
       <para>
        After the client assignments are configured, click
        <guimenu>Next</guimenu>.
       </para>
      </step>
     </substeps>
    </step>
    <step id="b12jbfdz">
     <para>
      Click <guimenu>Finish</guimenu> to save and apply the settings.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_targets_group_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_targets_group_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   </procedure>
  </sect1>
  <sect1 id="sect_iscsi_lio_edit">
   <title>Modifying an iSCSI LIO Target Group</title>

   <para>
    You can modify an existing iSCSI LIO target group as follows:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      Add or remove target LUN devices from a target group
     </para>
    </listitem>
    <listitem>
     <para>
      Add or remove clients for a target group
     </para>
    </listitem>
    <listitem>
     <para>
      Modify the client LUN-to-target LUN mappings for a client of a target
      group
     </para>
    </listitem>
    <listitem>
     <para>
      Modify the user name and password credentials for a client
      authentication (incoming, outgoing, or both)
     </para>
    </listitem>
   </itemizedlist>

   <para>
    To view or modify the settings for an iSCSI LIO target group:
   </para>

   <procedure id="b12o1qa2">
    <step id="b13n0ujw">
     <para>
      Log in to the iSCSI LIO target server as the
      <systemitem>root</systemitem> user, then launch a terminal console.
     </para>
    </step>
    <step id="b13n0ujx">
     <para>
      Ensure that the <filename>/etc/init.d/target</filename> daemon is
      running. At the command prompt, enter
     </para>
<screen>
 systemctl start target.service
</screen>
     <para>
      The command returns a message to confirm that the daemon is started,
      or that the daemon is already running.
     </para>
    </step>
    <step id="b12o1qa3">
     <para>
      Launch YaST as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="b12o1qa4">
     <para role="intro">
      In the YaST Control Center, select <guimenu>Network
      Services</guimenu>, then select <guimenu>iSCSI LIO Target</guimenu>.
     </para>
     <para role="intro">
      You can also search for <quote>lio</quote>, then select <guimenu>iSCSI
      LIO Target</guimenu>.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_yast2_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_yast2_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b12o1qa5">
     <para role="intro">
      In the iSCSI LIO Target Overview dialog box, select the
      <guimenu>Targets</guimenu> tab to view a list of target groups.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_target_gp_delete_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_target_gp_delete_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b12o1qa6">
     <para role="intro">
      Select the iSCSI LIO target group to be modified, then click
      <guimenu>Edit</guimenu>.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_target_gp_edit_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_target_gp_edit_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b12o1qa7">
     <para>
      On the Modify iSCSI Target LUN Setup page, add LUNs to the target
      group, edit the LUN assignments, or remove target LUNs from the group.
      After all desired changes have been made to the group, click
      <guimenu>Next</guimenu>.
     </para>
     <para>
      For option information, see
      <xref linkend="b12jdln1" xrefstyle="StepXRef"/> in
      <xref linkend="sect_iscsi_lio_targetgp" xrefstyle="SectTitleOnPage"/>.
     </para>
    </step>
    <step id="b12o1qad">
     <para>
      On the Modify iSCSI Target Client Setup page, configure information
      for the clients that are permitted to access LUNs in the target group.
      After all desired changes have been made to the group, click
      <guimenu>Next</guimenu>.
     </para>
     <para>
      For option information, see
      <xref linkend="b12jeipj" xrefstyle="StepXRef"/> in
      <xref linkend="sect_iscsi_lio_targetgp" xrefstyle="SectTitleOnPage"/>.
     </para>
    </step>
    <step id="b12o1qar">
     <para>
      Click <guimenu>Finish</guimenu> to save and apply the settings.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="sect_iscsi_lio_delete">
   <title>Deleting an iSCSI LIO Target Group</title>

   <para>
    Deleting an iSCSI LIO target group removes the definition of the group,
    and the related setup for clients, including LUN mappings and
    authentication credentials. It does not destroy the data on the
    partitions. To give clients access again, you can allocate the target
    LUNs to a different or new target group, and configure the client access
    for them.
   </para>

   <procedure id="b12o1fhc">
    <step id="b13n0ury">
     <para>
      Log in to the iSCSI LIO target server as the
      <systemitem>root</systemitem> user, then launch a terminal console.
     </para>
    </step>
    <step id="b13n0urz">
     <para>
      Ensure that the <filename>/etc/init.d/target</filename> daemon is
      running. At the command prompt, enter
     </para>
<screen>
 systemctl start target.service
</screen>
     <para>
      The command returns a message to confirm that the daemon is started,
      or that the daemon is already running.
     </para>
    </step>
    <step id="b12o1fhd">
     <para>
      Launch YaST as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="b12o1fhe">
     <para>
      In the YaST Control Center, select <guimenu>Network
      Services</guimenu>, then select <guimenu>iSCSI LIO Target</guimenu>.
     </para>
     <para>
      You can also search for <quote>lio</quote>, then select <guimenu>iSCSI
      LIO Target</guimenu>.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_yast2_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_yast2_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b12o1fhf">
     <para>
      In the iSCSI LIO Target Overview dialog box, select the
      <guimenu>Targets</guimenu> tab to view a list of configured target
      groups.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_target_gp_delete_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_target_gp_delete_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b12o1fhg">
     <para role="intro">
      Select the iSCSI LIO target group to be deleted, then click
      <guimenu>Delete</guimenu>.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="lio_target_gp_delete_a.png" width="250pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="lio_target_gp_delete_a.png" width="250pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="b12o1iuu">
     <para>
      When you are prompted, click <guimenu>Continue</guimenu> to confirm
      the deletion, or click <guimenu>Cancel</guimenu> to cancel it.
     </para>
    </step>
    <step id="b12o1fi0">
     <para>
      Click <guimenu>Finish</guimenu> to save and apply the settings.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="b13g8i2g">
   <title>Troubleshooting iSCSI LIO Target Server</title>

   <para>
    This section describes some known issues and possible solutions for
    iSCSI LIO Target Server.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b13g8i2h" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b13g8r6r" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b13g8i2h">
    <title>Portal Error When Setting Up Target LUNs</title>
    <para>
     When adding or editing an iSCSI LIO target group, you get an error:
    </para>
<screen>
Problem setting network portal &lt;ip_address&gt;:3260
</screen>
    <para>
     The <filename>/var/log/YasT2/y2log</filename> log file contains the
     following error:
    </para>
<screen>
find: `/sys/kernel/config/target/iscsi': No such file or directory
</screen>
    <para>
     This problem occurs if the iSCSI LIO Target Server software is not
     currently running. To resolve this issue, exit YaST, manually start
     iSCSI LIO at the command line, then try again.
    </para>
    <procedure id="b13g8i2i">
     <step id="b13g8i2j">
      <para>
       Open a terminal console as the <systemitem>root</systemitem> user.
      </para>
     </step>
     <step id="b13g8i2k">
      <para>
       At the command prompt, enter
      </para>
<screen>
systemctl start target.service
</screen>
     </step>
    </procedure>
    <para role="intro">
     You can also enter the following to check if
     <command>configfs</command>, <command>iscsi_target_mod</command>, and
     <command>target_core_mod</command> are loaded. A sample response is
     shown.
    </para>
<screen>
<command>lsmod | grep iscsi</command>

  iscsi_target_mod      295015  0
  target_core_mod       346745  4
  iscsi_target_mod,target_core_pscsi,target_core_iblock,target_core_file
  configfs               35817  3 iscsi_target_mod,target_core_mod
  scsi_mod              231620  16
  iscsi_target_mod,target_core_pscsi,target_core_mod,sg,sr_mod,mptctl,sd_mod,
  scsi_dh_rdac,scsi_dh_emc,scsi_dh_alua,scsi_dh_hp_sw,scsi_dh,libata,mptspi,
  mptscsih,scsi_transport_spi
</screen>
   </sect2>

   <sect2 id="b13g8r6r">
    <title>iSCSI LIO Targets Are Not Visible from Other Computers</title>
    <para>
     If you use a firewall on the target server, you must open the iSCSI
     port that you are using to allow other computers to see the iSCSI LIO
     targets. For information, see
     <xref linkend="b12jaucl" xrefstyle="StepXRef"/> in
     <xref linkend="b12jbbfh" xrefstyle="SectTitleOnPage"/>.
    </para>
   </sect2>
  </sect1>
  <sect1 id="b12iqi8o">
   <title>iSCSI LIO Target Terminology</title>

   <variablelist>
    <varlistentry id="b12iqi8p">
     <term>backstore</term>
     <listitem>
      <para>
       A physical storage object that provides the actual storage underlying
       an iSCSI endpoint.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12ir33x">
     <term>CDB (command descriptor block</term>
     <listitem>
      <para>
       The standard format for SCSI commands. CDBs are commonly 6, 10, or 12
       bytes long, though they can be 16 bytes or of variable length.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqi8q">
     <term>CHAP (Challenge Handshake Authentication Protocol)</term>
     <listitem>
      <para>
       A point-to-point protocol (PPP) authentication method used to confirm
       the identity of one computer to another. After the Link Control
       Protocol (LCP) connects the two computers, and the CHAP method is
       negotiated, the authenticator sends a random Challenge to the peer.
       The peer issues a cryptographically hashed Response that depends upon
       the Challenge and a secret key. The authenticator verifies the hashed
       Response against its own calculation of the expected hash value, and
       either acknowledges the authentication or terminates the connection.
       CHAP is defined in the <ulink url="http://www.ietf.org">Internet
       Engineering Task Force (IETF)</ulink> RFC 1994.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqi8r">
     <term>CID (connection identifier)</term>
     <listitem>
      <para>
       A 16‐bit number, generated by the initiator, that uniquely
       identifies a connection between two iSCSI devices. This number is
       presented during the login phase.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqi8s">
     <term>endpoint</term>
     <listitem>
      <para>
       The combination of an iSCSI Target Name with an iSCSI TPG (IQN +
       Tag).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqi8t">
     <term>EUI (extended unique identifier)</term>
     <listitem>
      <para>
       A 64‐bit number that uniquely identifies every device in the world.
       The format consists of 24 bits that are unique to a given company,
       and 40 bits assigned by the company to each device it builds.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqi8u">
     <term>initiator</term>
     <listitem>
      <para>
       The originating end of a SCSI session. Typically a controlling device
       such as a computer.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqi8v">
     <term>IPS (Internet Protocol storage)</term>
     <listitem>
      <para>
       The class of protocols or devices that use the IP protocol to move
       data in a storage network. FCIP (Fibre Channel over Internet
       Protocol), iFCP (Internet Fibre Channel Protocol), and iSCSI
       (Internet SCSI) are all examples of IPS protocols.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqi8w">
     <term>IQN (iSCSI qualified name)</term>
     <listitem>
      <para>
       A name format for iSCSI that uniquely identifies every device in the
       world (for example:
       <filename>iqn.5886.com.acme.tapedrive.sn‐a12345678</filename>).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqi8x">
     <term>ISID (initiator session identifier)</term>
     <listitem>
      <para>
       A 48‐bit number, generated by the initiator, that uniquely
       identifies a session between the initiator and the Target. This value
       is created during the login process, and is sent to the target with a
       Login PDU.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqi8y">
     <term>MCS (multiple connections per session)</term>
     <listitem>
      <para>
       A part of the iSCSI specification that allows multiple TCP/IP
       connections between an initiator and a target.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqi8z">
     <term>MPIO (multipath I/O)</term>
     <listitem>
      <para>
       A method by which data can take multiple redundant paths between a
       server and storage.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqu8e">
     <term>network portal</term>
     <listitem>
      <para>
       The combination of an iSCSI Endpoint with an IP address plus a TCP
       (Transmission Control Protocol) port. TCP port 3260 is the port
       number for the iSCSI protocol, as defined by IANA (Internet Assigned
       Numbers Authority).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqu8f">
     <term>SAM (SCSI architectural model)</term>
     <listitem>
      <para>
       A document that describes the behavior of SCSI in general terms,
       allowing for different types of devices communicating over various
       media.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqu8g">
     <term>target</term>
     <listitem>
      <para>
       The receiving end of a SCSI session, typically a device such as a
       disk drive, tape drive, or scanner.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12iqu8h">
     <term>target group (TG)</term>
     <listitem>
      <para>
       A list of SCSI target ports that are all treated the same when
       creating views. Creating a view can help facilitate LUN (logical unit
       number) mapping. Each view entry specifies a target group, host
       group, and a LUN.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12ir33y">
     <term>target port</term>
     <listitem>
      <para>
       The combination of an iSCSI endpoint with one or more LUNs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12ir33z">
     <term>target port group (TPG)</term>
     <listitem>
      <para>
       A list of IP addresses and TCP port numbers that determines which
       interfaces a specific iSCSI target will listen to.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="b12ir340">
     <term>target session identifier (TSID)</term>
     <listitem>
      <para>
       A 16‐bit number, generated by the target, that uniquely identifies
       a session between the initiator and the target. This value is created
       during the login process, and is sent to the initiator with a Login
       Response PDU (protocol data units).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect1>
 </chapter>
 <chapter conformance="sles11,Novell,no,0,80" id="cha_fcoe" lang="en" revision="02/01/12">
  <title>Fibre Channel Storage over Ethernet Networks: FCoE</title>
  <para>
   Many enterprise data centers rely on Ethernet for their LAN and data
   traffic, and on Fibre Channel networks for their storage infrastructure.
   Open Fibre Channel over Ethernet (FCoE) Initiator software allows servers
   with Ethernet adapters to connect to a Fibre Channel storage subsystem
   over an Ethernet network. This connectivity was previously reserved
   exclusively for systems with Fibre Channel adapters over a Fibre Channel
   fabric. The FCoE technology reduces complexity in the data center by
   aiding network convergence. This helps to preserve your existing
   investments in a Fibre Channel storage infrastructure and to simplify
   network management.
  </para>
  <figure pgwide="0" id="bwkfm8v">
   <title>Open Fibre Channel over Ethernet SAN</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="fcoe_san_a.svg" width="445pt" format="SVG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="fcoe_san_a.png" width="445pt" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
   Open-FCoE allows you to run the Fibre Channel protocols on the host,
   instead of on proprietary hardware on the host bus adapter. It is
   targeted for 10 Gbps (gigabit per second) Ethernet adapters, but can work
   on any Ethernet adapter that supports pause frames. The initiator
   software provides a Fibre Channel protocol processing module as well as
   an Ethernet based transport module. The Open-FCoE module acts as a
   low-level driver for SCSI. The Open-FCoE transport uses
   <command>net_device</command> to send and receive packets. Data Center
   Bridging (DCB) drivers provide the quality of service for FCoE.
  </para>
  <para>
   FCoE is an encapsulation protocol that moves the Fibre Channel protocol
   traffic over Ethernet connections without changing the Fibre Channel
   frame. This allows your network security and traffic management
   infrastructure to work the same with FCoE as it does with Fibre Channel.
  </para>
  <para>
   You might choose to deploy Open-FCoE in your enterprise if the following
   conditions exist:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Your enterprise already has a Fibre Channel storage subsystem and
     administrators with Fibre Channel skills and knowledge.
    </para>
   </listitem>
   <listitem>
    <para>
     You are deploying 10 Gbps Ethernet in the network.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   This section describes how to set up FCoE in your network.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="bxurl07" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bycsjf0" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bxyqov2" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bxuri58" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bxuptlv" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bycscw6" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bycscw7" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bxuqb00" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="bxurl07">
   <title>Installing FCoE and the YaST FCoE Client</title>

   <para>
    You can set up FCoE disks in your storage infrastructure by enabling
    FCoE at the switch for the connections to a server. If FCoE disks are
    available when the SUSE Linux Enterprise Server operating system is
    installed, the FCoE Initiator software is automatically installed at
    that time.
   </para>

   <para>
    If the FCoE Initiator software and YaST FCoE Client software are not
    installed, use the following procedure to manually install them on an
    existing system:
   </para>

   <procedure id="bxurln0">
    <step id="byd2977">
     <para>
      Log in to the server as the <systemitem>root</systemitem> user.
     </para>
    </step>
    <step id="bxurln1">
     <para>
      In YaST, select <guimenu>Software</guimenu><guimenu>Software
      Management</guimenu>.
     </para>
    </step>
    <step id="bxurlrs">
     <para>
      Search for and select the following FCoE packages:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        open-fcoe
       </para>
      </listitem>
      <listitem>
       <para>
        yast2-fcoe-client
       </para>
      </listitem>
     </itemizedlist>
     <para role="intro">
      For example, type <literal>fcoe</literal> in the
      <guimenu>Search</guimenu> field, click <guimenu>Search</guimenu> to
      locate the software packages, then select the check box next to each
      software package that you want to install.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="fcoe_install2_a.png" width="241pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="fcoe_install2_a.png" width="241pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="bxurni7">
     <para>
      Click <guimenu>Accept</guimenu>, then click
      <guimenu>Continue</guimenu> to accept the automatic changes.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="bycsjf0">
   <title>Configuring FCoE Interfaces during the Installation</title>

   <para>
    The YaST installation for SUSE Linux Enterprise Server allows you to
    configure FCoE disks during the operating system installation if FCoE is
    enabled at the switch for the connections between the server and the
    Fibre Channel storage infrastructure. Some system BIOS types can
    automatically detect the FCoE disks, and report the disks to the YaST
    Installation software. However, automatic detection of FCoE disks is not
    supported by all BIOS types. To enable automatic detection in this case,
    you can add the <command>withfcoe</command> option to the kernel command
    line when you begin the installation:
   </para>

<screen>
withfcoe=1
</screen>

   <para role="intro">
    When the FCoE disks are detected, the YaST installation offers the
    option to configure FCoE instances at that time. On the Disk Activation
    page, select <guimenu>Configure FCoE Interfaces</guimenu> to access the
    FCoE configuration. For information about configuring the FCoE
    interfaces, see <xref linkend="bxyqov2" xrefstyle="SectTitleOnPage"/>.
   </para>

   <informalfigure pgwide="0">
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="fcoe_inst_disk_activation_a.png" width="253pt" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="fcoe_inst_disk_activation_a.png" width="253pt" format="PNG"/>
     </imageobject>
    </mediaobject>
   </informalfigure>
  </sect1>
  <sect1 id="bxyqov2">
   <title>Managing FCoE Services with YaST</title>

   <para>
    You can use the YaST FCoE Client Configuration option to create,
    configure, and remove FCoE interfaces for the FCoE disks in your Fibre
    Channel storage infrastructure. To use this option, the FCoE Initiator
    service (the <filename>fcoemon</filename> daemon) and the Link Layer
    Discovery Protocol agent daemon (<command>lldpad</command>) must be
    installed and running, and the FCoE connections must be enabled at the
    FCoE-capable switch.
   </para>

   <procedure id="bxyqpcm">
    <step id="bxyqpcn">
     <para>
      Log in as the <systemitem>root</systemitem> user, then launch YaST.
     </para>
    </step>
    <step id="bxyqpl8">
     <para>
      In YaST, select <guimenu>Network Services</guimenu><guimenu>FCoE
      Client Configuration</guimenu>.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="fcoe_yast2_a.png" width="191pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="fcoe_yast2_a.png" width="191pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
     <para>
      The Fibre Channel over Ethernet Configuration dialog box provides
      three tabs:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        <guimenu>Services</guimenu>
       </para>
      </listitem>
      <listitem>
       <para>
        <guimenu>Interfaces</guimenu>
       </para>
      </listitem>
      <listitem>
       <para>
        <guimenu>Configuration</guimenu>
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step id="bycsvr4">
     <para role="intro">
      On the <guimenu>Services</guimenu> tab, view or modify the FCoE
      service and Lldpad (Link Layer Discovery Protocol agent daemon)
      service start time as necessary.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="fcoe_services_a.png" width="201pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="fcoe_services_a.png" width="201pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
     <itemizedlist>
      <listitem>
       <formalpara id="bycssp2" role="intro">
        <title>FCoE Service Start:</title>
        <para>
         Specifies whether to start the Fibre Channel over Ethernet service
         <command>fcoemon</command> daemon at the server boot time or
         manually. The daemon controls the FCoE interfaces and establishes a
         connection with the <command>lldpad</command> daemon. The values
         are <guimenu>When Booting</guimenu> (default) or
         <guimenu>Manually</guimenu>.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="bycsvr5" role="intro">
        <title>Lldpad Service Start:</title>
        <para>
         Specifies whether to start the Link Layer Discovery Protocol agent
         <command>lldpad</command> daemon at the server boot time or
         manually. The <command>lldpad</command> daemon informs the
         <command>fcoemon</command> daemon about the Data Center Bridging
         features and the configuration of the FCoE interfaces. The values
         are <guimenu>When Booting</guimenu> (default) or
         <guimenu>Manually</guimenu>.
        </para>
       </formalpara>
      </listitem>
     </itemizedlist>
     <para>
      If you modify a setting, click <guimenu>OK</guimenu> to save and apply
      the change.
     </para>
    </step>
    <step id="bycsxr4">
     <para>
      On the <guimenu>Interfaces</guimenu> tab, view information about all
      of the detected network adapters on the server, including information
      about VLAN and FCoE configuration. You can also create an FCoE VLAN
      interface, change settings for an existing FCoE interface, or remove
      an FCoE interface.
     </para>
     <formalpara id="byct84b" role="intro">
      <title>View FCoE Information</title>
      <para/>
     </formalpara>
     <para role="intro">
      The <guimenu>FCoE Interfaces</guimenu> table displays the following
      information about each adapter:
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="fcoe_notconfig_interface_a.png" width="201pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="fcoe_notconfig_interface_a.png" width="201pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
     <itemizedlist>
      <listitem>
       <formalpara id="byd3h1o" role="intro">
        <title>Device Name:</title>
        <para>
         Specifies the adapter name such as <filename>eth4</filename>.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="byd3hdt">
        <title>Model:</title>
        <para>
         Specifies the adapter model information.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="byd3l6i" role="intro">
        <title>FCoE VLAN Interface</title>
        <para/>
       </formalpara>
       <itemizedlist>
        <listitem>
         <formalpara id="byd3nbb">
          <title>Interface Name:</title>
          <para>
           If a name is assigned to the interface, such as
           <filename>eth4.200</filename>, FCoE is available on the switch,
           and the FCoE interface is activated for the adapter.
          </para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara id="byd3o01">
          <title>Not Configured:</title>
          <para>
           If the status is <guimenu>not configured</guimenu>, FCoE is
           enabled on the switch, but an FCoE interface has not been
           activated for the adapter. Select the adapter, then click
           <guimenu>Create FCoE VLAN Interface</guimenu> to activate the
           interface on the adapter.
          </para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara id="byd3ohg">
          <title>Not Available:</title>
          <para>
           If the status is <guimenu>not available</guimenu>, FCoE is not
           possible for the adapter because FCoE has not been enabled for
           that connection on the switch.
          </para>
         </formalpara>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <formalpara id="byd3mds">
        <title>FCoE Enable:</title>
        <para>
         Specifies whether FCoE is enabled on the switch for the adapter
         connection. (<guimenu>yes</guimenu> or <guimenu>no</guimenu>)
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="byd3pvk">
        <title>DCB Required:</title>
        <para>
         Specifies whether the adapter requires Data Center Bridging.
         (<guimenu>yes</guimenu> or <guimenu>no</guimenu>)
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="byd3qba">
        <title>Auto VLAN:</title>
        <para>
         Specifies whether automatic VLAN configuration is enabled for the
         adapter. (<guimenu>yes</guimenu> or <guimenu>no</guimenu>)
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="byd3r0o">
        <title>DCB Capable:</title>
        <para>
         Specifies whether the adapter supports Data Center Bridging.
         (<guimenu>yes</guimenu> or <guimenu>no</guimenu>)
        </para>
       </formalpara>
      </listitem>
     </itemizedlist>
     <formalpara id="byct8zr" role="intro">
      <title>Change FCoE Settings</title>
      <para/>
     </formalpara>
     <para role="intro">
      Select an FCoE VLAN interface, then click <guimenu>Change
      Settings</guimenu> at the bottom of the page to open the Change FCoE
      Settings dialog box.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="fcoe_modify_config_a.png" width="201pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="fcoe_modify_config_a.png" width="201pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
     <itemizedlist>
      <listitem>
       <formalpara id="byctmy6">
        <title>FCoE Enable:</title>
        <para>
         Enable or disable the creation of FCoE instances for the adapter.
         Values are <guimenu>yes</guimenu> or <guimenu>no</guimenu>.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="byctn94">
        <title>DCB Required:</title>
        <para>
         Specifies whether Data Center Bridging is required for the adapter.
         Values are <guimenu>yes</guimenu> (default) or
         <guimenu>no</guimenu>. DCB is usually required.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="byctn95">
        <title>Auto VLAN:</title>
        <para>
         Specifies whether the <filename>fcoemon</filename> daemon creates
         the VLAN interfaces automatically. Values are
         <guimenu>yes</guimenu> or <guimenu>no</guimenu>.
        </para>
       </formalpara>
      </listitem>
     </itemizedlist>
     <para>
      If you modify a setting, click <guimenu>Next</guimenu> to save and
      apply the change. The settings are written to the
      <filename>/etc/fcoe/eth<replaceable>X</replaceable></filename> file.
      The <command>fcoemon</command> daemon reads the configuration files
      for each FCoE interface when the daemon is initialized. There is a
      file for every FCoE interface.
     </para>
     <formalpara id="byctarx" role="intro">
      <title>Create FCoE VLAN Interfaces</title>
      <para/>
     </formalpara>
     <para role="intro">
      Select an adapter that has FCoE enabled but is not configured, then
      click <guimenu>Yes</guimenu> to configure the FCoE interface. The
      assigned interface name appears in the list, such as
      <filename>eth5.200</filename>.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="fcoe_create_interface_a.png" width="201pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="fcoe_create_interface_a.png" width="201pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
     <formalpara id="byct8zs" role="intro">
      <title>Remove FCoE Interface</title>
      <para/>
     </formalpara>
     <para role="intro">
      Select the FCoE interface that you want to remove, click
      <guimenu>Remove Interface</guimenu> at the bottom of the page, then
      click <guimenu>Continue</guimenu> to confirm. The FCoE Interface value
      changes to <guimenu>not configured</guimenu>.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="fcoe_remove_interface_a.png" width="201pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="fcoe_remove_interface_a.png" width="201pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step id="bxyqqb9">
     <para role="intro">
      On the <guimenu>Configuration</guimenu> tab, view or modify the
      general settings for the FCoE system service.
     </para>
     <informalfigure pgwide="0">
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="fcoe_configtab_a.png" width="201pt" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="fcoe_configtab_a.png" width="201pt" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
     <itemizedlist>
      <listitem>
       <formalpara id="bycsz4d" role="intro">
        <title>Debug:</title>
        <para>
         Enables or disables debugging messages from the FCoE service script
         and <command>fcoemon</command> daemon. The values are
         <guimenu>Yes</guimenu> or <guimenu>No</guimenu> (default).
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="bycsz4e" role="intro">
        <title>Use syslog:</title>
        <para>
         Specifies whether messages are sent to the system log
         (<filename>/var/log/syslog</filename>). The values are
         <guimenu>Yes</guimenu> (default) or <guimenu>No</guimenu>.
        </para>
       </formalpara>
      </listitem>
     </itemizedlist>
     <para>
      If you modify a setting, click <guimenu>OK</guimenu> to save and apply
      the change. The settings are written to the
      <filename>/etc/fcoe/config</filename> file.
     </para>
    </step>
    <step id="bxyqr8g">
     <para>
      Click <guimenu>OK</guimenu> to save and apply changes.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="bxuri58">
   <title>Configuring FCoE with Commands</title>

   <procedure id="bxupm0j">
    <step id="bxupo5v">
     <para>
      Log in to the server as the <systemitem>root</systemitem> user, then
      open a terminal console.
     </para>
    </step>
    <step id="bxupmnl">
     <para>
      Use YaST to configure the Ethernet network interface card, such as
      <filename>eth2</filename>.
     </para>
    </step>
    <step id="bxuslhx">
     <para>
      Start the Link Layer Discovery Protocol agent daemon
      (<command>lldpad</command>).
     </para>
<screen>
rclldpad start
</screen>
    </step>
    <step id="bxupmpk">
     <para role="intro">
      Enable Data Center Bridging on your Ethernet adapter<filename/>.
     </para>
<screen>
dcbtool sc eth2 dcb on
  Version:       2
  Command:       Set Config
  Feature:       DCB State
  Port:          eth2
  Status:        Successful
</screen>
    </step>
    <step id="bynkhpe">
     <para>
      Enable and set the Priority Flow Control (PFC) settings for Data
      Center Bridging.
     </para>
<screen>
dcbtool sc eth&lt;x&gt; pfc e:1 a:1 w:1
</screen>
     <para>
      Argument setting values are:
     </para>
     <variablelist>
      <varlistentry id="bynkj1s">
       <term>e:&lt;0|1&gt;</term>
       <listitem>
        <para>
         Controls feature enable.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry id="bynkjej">
       <term>a:&lt;0|1&gt;</term>
       <listitem>
        <para>
         Controls whether the feature is advertised via Data Center Bridging
         Exchange protocol to the peer.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry id="bynkjy8">
       <term>w:&lt;0|1&gt;</term>
       <listitem>
        <para>
         Controls whether the feature is willing to change its operational
         configuration based on what is received from the peer.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </step>
    <step id="bxuppth">
     <para>
      Enable the Data Center Bridging to accept the switch’s priority
      setting for FCoE.
     </para>
<screen>
dcbtool sc eth2 app:fcoe e:1
  Version:       2
  Command:       Set Config
  Feature:       Application FCoE
  Port:          eth2
  Status:        Successful
</screen>
    </step>
    <step id="bxupsun">
     <para>
      Copy the default FCoE configuration file to
      <filename>/etc/fcoe/cfg-eth2</filename>.
     </para>
<screen>
cp /etc/fcoe/cfg-ethx /etc/fcoe/cfg-eth2

</screen>
    </step>
    <step id="bxupsnh">
     <para>
      Start the FCoE Initiator service.
     </para>
<screen>
rcfcoe start
  Starting FCoE initiator service
</screen>
    </step>
    <step id="bxuslzn">
     <para>
      Set up the Link Layer Discovery Protocol agent daemon
      (<command>lldpad</command>) and the FCoE Initiator service to start
      when booting.
     </para>
<screen>systemctl enable llpad.service fcoe.service</screen>
<!--taroth 2014-03-19: systemd - according to fcrozat, no more boot.* stuff:<screen>
chkconfig boot.lldpad on

chkconfig boot.fcoe on
</screen>-->
    </step>
   </procedure>
  </sect1>
  <sect1 id="bxuptlv">
   <title>Managing FCoE Instances with the FCoE Administration Tool</title>

   <para>
    The <command>fcoeadm</command> utility is the Fibre Channel over
    Ethernet (FCoE) management tool for the Open-FCoE project. It can be
    used to create, destroy, and reset an FCoE instance on a given network
    interface. The <command>fcoeadm</command> utility sends commands to a
    running <command>fcoemon</command> process via a socket interface. For
    information about <command>fcoemon</command>, see the
    <command>fcoemon(8)</command> man page.
   </para>

   <para>
    The <command>fcoeadm</command> utility allows you to query the FCoE
    instances about the following:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      Interfaces
     </para>
    </listitem>
    <listitem>
     <para>
      Target LUNs
     </para>
    </listitem>
    <listitem>
     <para>
      Port statistics
     </para>
    </listitem>
   </itemizedlist>

   <para>
    The <command>fcoeadm</command> utility is part of the
    <filename>fcoe-utils</filename> package. It is maintained by the
    <ulink url="http://open-fcoe.org/">Open-FCoE project</ulink>.
   </para>

   <variablelist>
    <varlistentry id="bxuq220">
     <term>Syntax</term>
     <listitem>
      <para>
       Fiber Channel over Ethernet Administration version 1.0.12.
      </para>
<screen>
fcoeadm
  [-c|--create] [&lt;ethX&gt;]
  [-d|--destroy] [&lt;ethX&gt;]
  [-r|--reset] [&lt;ethX&gt;]
  [-S|--Scan] [&lt;ethX&gt;]
  [-i|--interface] [&lt;ethX&gt;]
  [-t|--target] [&lt;ethX&gt;]
  [-l|--lun] [&lt;ethX&gt;]
  [-s|--stats &lt;ethX&gt;] [&lt;interval&gt;]
  [-v|--version]
  [-h|--help]
</screen>
     </listitem>
    </varlistentry>
    <varlistentry id="bxus2zv">
     <term>Options</term>
     <listitem>
      <variablelist>
       <varlistentry id="bxus360">
        <term><literal>-c</literal>
        </term>
        <term><literal>--create &lt;eth<replaceable>X&gt;</replaceable></literal>
        </term>
        <listitem>
         <para>
          Creates an FCoE instance based on the specified network interface.
          If an <command>fcoemon</command> configuration file does not exist
          for the Open-FCoE service daemon interface
          (<filename>/etc/fcoe/cfg-ethx</filename>; see
          <command>fcoemon(8)</command> man page), the created FCoE instance
          does not require Data Center Bridging.
         </para>
         <formalpara id="bycrwhb">
          <title>Example:</title>
          <para>
           To create an FCoE instance on <filename>eth2.101</filename>:
          </para>
         </formalpara>
<screen>
fcoeadm -c eth2.101
</screen>
        </listitem>
       </varlistentry>
       <varlistentry id="bxus6nh">
        <term><literal>-d</literal>
        </term>
        <term><literal>--destroy &lt;eth<replaceable>X&gt;</replaceable></literal>
        </term>
        <listitem>
         <para>
          Destroys an FCoE instance on the specified network interface. This
          does not destroy FCoE instances created by
          <command>fipvlan</command>.
         </para>
         <formalpara id="bycrz5k">
          <title>Example:</title>
          <para>
           To destroy an FCoE instance on <filename>eth2.101</filename>:
          </para>
         </formalpara>
<screen>
fcoeadm -d eth2.101
</screen>
        </listitem>
       </varlistentry>
       <varlistentry id="bxusdp0">
        <term><literal>-h</literal>
        </term>
        <term><literal>--help</literal>
        </term>
        <listitem>
         <para>
          Displays the usage message of the <command>fcoeadm</command>
          command.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry id="bxusaym">
        <term><literal>-i</literal>
        </term>
        <term><literal>--interface [&lt;eth<replaceable>X&gt;</replaceable>]</literal>
        </term>
        <listitem>
         <para>
          Shows information about the FCoE instance on the specified network
          interface. If no network interface is specified, it shows
          information for all FCoE instances.
         </para>
         <formalpara id="bycrz5l">
          <title>Examples</title>
          <para/>
         </formalpara>
         <para>
          To show information about all of the adapters and their ports that
          have FCoE instances created:
         </para>
<screen>
fcoeadm -i
</screen>
         <para>
          To show information about all of the FCoE instances on interface
          <filename>eth3</filename>:
         </para>
<screen>
fcoeadm -i eth3
</screen>
        </listitem>
       </varlistentry>
       <varlistentry id="bxusayp">
        <term><literal>-l</literal>
        </term>
        <term><literal>--lun [&lt;eth<replaceable>X&gt;</replaceable>]</literal>
        </term>
        <listitem>
         <para>
          Shows detailed information about the discovered SCSI LUNs
          associated with the FCoE instance on the specified network
          interface. If no network interface is specified, it shows
          information about SCSI LUNs from all FCoE instances.
         </para>
         <formalpara id="bycs78a" role="intro">
          <title>Examples</title>
          <para/>
         </formalpara>
         <para role="intro">
          To show detailed information about all of the LUNs discovered on
          all FCoE connections:
         </para>
<screen>
fcoeadm -l
</screen>
         <para>
          To show detailed information about all of the LUNs discovered on a
          specific connections, such as <filename>eth3.101</filename>:
         </para>
<screen>
fcoeadm -l eth3.101
</screen>
        </listitem>
       </varlistentry>
       <varlistentry id="bxus75n">
        <term><literal>-r</literal>
        </term>
        <term><literal>--reset &lt;eth<replaceable>X&gt;</replaceable></literal>
        </term>
        <listitem>
         <para>
          Resets the FCoE instance on the specified network interface. This
          does not reset FCoE instances created by
          <command>fipvlan</command>.
         </para>
         <formalpara id="bycrz5m">
          <title>Example:</title>
          <para>
           To reset the FCoE instance on <filename>eth2.101</filename>:
          </para>
         </formalpara>
<screen>
fcoeadm -r eth2.101
</screen>
        </listitem>
       </varlistentry>
       <varlistentry id="bxuscy5">
        <term><literal>-s</literal>
        </term>
        <term><literal>--stats &lt;eth<replaceable>X</replaceable>&gt; [interval]</literal>
        </term>
        <listitem>
         <para>
          Shows the statistics (including FC4 statistics) of the FCoE
          instance on the specified network interface. It displays one line
          per given time interval. Specify the interval value in whole
          integers greater than 0. The interval value is the elapsed time of
          the interval in seconds. If an interval is not specified, the
          default interval is 1 second.
         </para>
         <formalpara id="bycs78b">
          <title>Examples:</title>
          <para/>
         </formalpara>
         <para>
          You can show statistics information about a specific
          <filename>eth3</filename> port that has FCoE instances. The
          statistics are displayed one line per time interval. The default
          interval of one second is not specified in the command.
         </para>
<screen>
fcoeadm -s eth3
</screen>
         <para>
          To show statistics information about a specific
          <filename>eth3</filename> port that has FCoE instances, at an
          interval of 3 seconds. The statistics are displayed one line per
          time interval.
         </para>
<screen>
fcoeadm -s eth3 3
</screen>
        </listitem>
       </varlistentry>
       <varlistentry id="bxusayn">
        <term><literal>-S</literal>
        </term>
        <term><literal>--Scan &lt;eth<replaceable>X&gt;</replaceable></literal>
        </term>
        <listitem>
         <para>
          Rescans for new targets and LUN for the specified network
          interface. This does not rescan any NPIV (N_Port ID
          Virtualization) instances created on the same port, and does not
          rescan any FCoE instances created by <command>fipvlan</command>.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry id="bxusayo">
        <term><literal>-t</literal>
        </term>
        <term><literal>--target [&lt;eth<replaceable>X&gt;</replaceable>]</literal>
        </term>
        <listitem>
         <para>
          Shows information about the discovered targets associated with the
          FCoE instance on the specified network interface. If no network
          interface is specified, it shows information about discovered
          targets from all FCoE instances.
         </para>
         <formalpara id="bycs3pt">
          <title>Examples</title>
          <para/>
         </formalpara>
         <para>
          You can show information about all of the discovered targets from
          all of the ports that have FCoE instances. They might be on
          different adapter cards. After each discovered target, any
          associated LUNs are listed.
         </para>
<screen>
fcoeadm -t
</screen>
         <para>
          You can show information about all of the discovered targets from
          a given <filename>eth3</filename> port having FCoE instance. After
          each discovered target, any associated LUNs are listed.
         </para>
<screen>
fcoeadm -t eth3
</screen>
        </listitem>
       </varlistentry>
       <varlistentry id="bxusdp1">
        <term><literal>-v</literal>
        </term>
        <term><literal>--version</literal>
        </term>
        <listitem>
         <para>
          Displays the version of the <command>fcoeadm</command> command.
         </para>
        </listitem>
       </varlistentry>
      </variablelist>
     </listitem>
    </varlistentry>
    <varlistentry id="bxupx3p">
     <term>FCoE Response Examples</term>
     <listitem>
      <variablelist>
       <varlistentry id="bxupyie">
        <term>View FCoE Initiator Status for FC-ID Node/Port Number</term>
        <listitem>
<screen>
fcoeadm -i eth0.201
  Description:      82599EB 10-Gigabit SFI/SFP+ Network Connection 
  Revision:         01
  Manufacturer:     Intel Corporation 
  Serial Number:    001B219B258C 
  Driver:           ixgbe 3.3.8-k2 
  Number of Ports:  1      

      Symbolic Name:     fcoe v0.1 over eth0.201     
      OS Device Name:    host8     
      Node Name:         0x1000001B219B258E     
      Port Name:         0x2000001B219B258E     
      FabricName:        0x2001000573D38141     
      Speed:             10 Gbit     
      Supported Speed:   10 Gbit     
      MaxFrameSize:      2112     
      FC-ID (Port ID):   0x790003     
      State:             Online
</screen>
        </listitem>
       </varlistentry>
       <varlistentry id="bxuq45l">
        <term>View FCoE Targets for FC-ID Node/Port Number</term>
        <listitem>
<screen>
fcoeadm -t eth0.201  
  Interface:        eth0.201 
  Roles:            FCP Target 
  Node Name:        0x200000D0231B5C72 
  Port Name:        0x210000D0231B5C72 
  Target ID:        0 
  MaxFrameSize:     2048 
  OS Device Name:   rport-8:0-7 
  FC-ID (Port ID):  0x79000C 
  State:            Online  

LUN ID  Device Name   Capacity   Block Size  Description 
------  -----------  ----------  ----------  ----------------------------     
    40  /dev/sdqi     792.84 GB      512     IFT DS S24F-R2840-4 (rev 386C)
    72  /dev/sdpk     650.00 GB      512     IFT DS S24F-R2840-4 (rev 386C)
   168  /dev/sdgy       1.30 TB      512     IFT DS S24F-R2840-4 (rev 386C)
</screen>
        </listitem>
       </varlistentry>
      </variablelist>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect1>
  <sect1 id="bycscw6">
   <title>Setting Up Partitions for an FCoE Initiator Disk</title>

   <para role="intro">
    You can use the <command>fdisk(8)</command> command to set up partitions
    for an FCoE initiator disk.
   </para>

<screen>
fdisk /dev/sdc
  Device contains neither a valid DOS partition table, nor Sun, SGI or OSF disklabel.
  Building a new DOS disklabel with disk identifier 0xfc691889.
  Changes will remain in memory only, until you decide to write them.
  After that, of course, the previous content won’t be recoverable.

  Warning: Invalid flag 0x0000 of partition table 4 will be corrected by w(rite)

  Command (n for help): n
  Command action
     e    extended
     p    primary partition (1-4)

  p
     Partition number (1-4): 4
     First cylinder (1-1017, default 1): 
       Using default value 1
     Last cylinder, *cylinders or *size(K,M,G) (1-1017, default 1017):
       Using default value 1017

  Command (n for help): w
     The partition table has been altered!

     Calling loctl() to re-read partition table.
     Syncing disks.
</screen>
  </sect1>
  <sect1 id="bycscw7">
   <title>Creating a File System on an FCoE Initiator Disk</title>

   <para>
    You can use the <command>mkfs(8)</command> command to create a file
    system on an FCoE initiator disk.
   </para>

<screen>
mkfs /dev/sdc
  mke2fs 1.41.9 (22-Aug-2011)
  /dev/sdc is entire device, not just one partition!
  Proceed anyway? (y, n) y
  Filesystem label=
  OS type: Linux
  Block size=4096 (log-2)
  262144 inodes, 1048576 blocks
  52428 blocks (5.00%) reserved for the super user
  First data block=0
  Maximum filesystem blocks=1073741824
  32 block groups
  32768 blocks per group, 32768 fragments per group
  8192 inodes per group
  Superblock backups stored on blocks:
          32768, 98304, 163840, 229376, 294912, 819200, 804736

  Writing inode tables: done
  Writing superblocks and filesystem accounting information: done

  This filesystem will be automatically checked every 27 mounts or
  180 days, whichever comes first. Use tune2fs -c or -i to override.
</screen>
  </sect1>
  <sect1 id="bxuqb00">
   <title>Additional Information</title>

   <para>
    For information, see the follow documentation:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      For information about the Open-FCoE service daemon, see the
      <command>fcoemon(8)</command>man page.
     </para>
    </listitem>
    <listitem>
     <para>
      For information about the Open-FCoE Administration tool, see the
      <command>fcoeadm(8)</command> man page.
     </para>
    </listitem>
    <listitem>
     <para>
      For information about the Data Center Bridging Configuration tool, see
      the <command>dcbtool(8)</command> man page.
     </para>
    </listitem>
    <listitem>
     <para>
      For information about the Link Layer Discovery Protocol agent daemon,
      see the <filename>lldpad(8)</filename> man page.
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="http://www.open-fcoe.org/open-fcoe/wiki/quickstart"><citetitle>Open
      Fibre Channel over Ethernet Quick Start</citetitle></ulink>.
     </para>
    </listitem>
   </itemizedlist>
  </sect1>
 </chapter>
 <chapter id="snapshots" lang="en">
  <title>LVM Volume Snapshots</title>
  <para>
   A Logical Volume Manager (LVM) logical volume snapshot is a copy-on-write
   technology that monitors changes to an existing volume’s data blocks so
   that when a write is made to one of the blocks, the block’s value at
   the snapshot time is copied to a snapshot volume. In this way, a
   point-in-time copy of the data is preserved until the snapshot volume is
   deleted.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="bi7xb8b" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bi7uttc" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="biu259x" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bi7uzm8" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="snapshot_xen_host" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="lvconvert_merge" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="bi7xb8b">
   <title>Understanding Volume Snapshots</title>

   <para>
    A file system snapshot contains metadata about and data blocks from a
    source logical volume that have changed since the snapshot was taken.
    When you access data via the snapshot, you see a point-in-time copy of
    the source logical volume. There is no need to restore data from backup
    media or to overwrite the changed data.
   </para>

   <important>
    <para>
     During the snapshot’s lifetime, the snapshot must be mounted before
     its source logical volume can be mounted.
    </para>
   </important>

   <para>
    LVM volume snapshots allow you to create a backup from a point-in-time
    view of the file system. The snapshot is created instantly and persists
    until you delete it. You can backup the file system from the snapshot
    while the volume itself continues to be available for users. The
    snapshot initially contains some metadata about the snapshot, but no
    actual data from the source logical volume. Snapshot uses copy-on-write
    technology to detect when data changes in an original data block. It
    copies the value it held when the snapshot was taken to a block in the
    snapshot volume, then allows the new data to be stored in the source
    block. As more blocks change from their original value on the source
    logical volume, the snapshot size grows.
   </para>

   <para>
    When you are sizing the snapshot, consider how much data is expected to
    change on the source logical volume and how long you plan to keep the
    snapshot. The amount of space that you allocate for a snapshot volume
    can vary, depending on the size of the source logical volume, how long
    you plan to keep the snapshot, and the number of data blocks that are
    expected to change during the snapshot’s lifetime. The snapshot volume
    cannot be resized after it is created. As a guide, create a snapshot
    volume that is about 10% of the size of the original logical volume. If
    you anticipate that every block in the source logical volume will change
    at least one time before you delete the snapshot, then the snapshot
    volume should be at least as large as the source logical volume plus
    some additional space for metadata about the snapshot volume. Less space
    is required if the data changes infrequently or if the expected lifetime
    is sufficiently brief.
   </para>

   <para>
    In LVM2, snapshots are read/write by default. When you write data
    directly to the snapshot, that block is marked in the exception table as
    used, and never gets copied from the source logical volume. You can
    mount the snapshot volume, and test application changes by writing data
    directly to the snapshot volume. You can easily discard the changes by
    dismounting the snapshot, removing the snapshot, and then re-mounting
    the source logical volume.
   </para>

   <para>
    In a virtual guest environment, you can use the snapshot function for
    LVM logical volumes you create on the server’s disks, as you would on
    a physical server.
   </para>

   <para>
    In a virtual host environment, you can use the snapshot function to back
    up the virtual machine’s storage back-end, or to test changes to a
    virtual machine image, such as for patches or upgrades, without
    modifying the source logical volume. The virtual machine must be using
    an LVM logical volume as its storage back-end, as opposed to using a
    virtual disk file. You can mount the LVM logical volume and use it to
    store the virtual machine image as a file-backed disk, or you can assign
    the LVM logical volume as a physical disk to write to it as a block
    device.
   </para>

   <para>
    Beginning in SLES 11 SP3, an LVM logical volume snapshots can be thinly
    provisioned. Thin provisioning is assumed if you to create a snapshot
    without a specified size. The snapshot is created as a thin volume that
    uses space as needed from a thin pool. A thin snapshot volume has the
    same characteristics as any other thin volume. You can independently
    activate the volume, extend the volume, rename the volume, remove the
    volume, and even snapshot the volume.
   </para>

   <important>
    <para>
     To use thinly provisioned snapshots in a cluster, the source logical
     volume and its snapshots must be managed in a single cluster resource.
     This allows the volume and its snapshots to always be mounted
     exclusively on the same node.
    </para>
   </important>

   <para>
    When you are done with the snapshot, it is important to remove it from
    the system. A snapshot eventually fills up completely as data blocks
    change on the source logical volume. When the snapshot is full, it is
    disabled, which prevents you from remounting the source logical volume.
   </para>

   <para>
    If you create multiple snapshots for a source logical volume, remove the
    snapshots in a last created, first deleted order.
   </para>
  </sect1>
  <sect1 id="bi7uttc">
   <title>Creating Linux Snapshots with LVM</title>

   <para>
    The Logical Volume Manager (LVM) can be used for creating snapshots of
    your file system.
   </para>

   <procedure id="bi7wb4p">
    <step id="bi7wb4q">
     <para>
      Open a terminal console, log in as the <systemitem>root</systemitem>
      user, then enter
     </para>
<screen>
lvcreate -s [-L <replaceable>&lt;size</replaceable>&gt;] -n <replaceable>snap_volume</replaceable> <replaceable>source_volume_path</replaceable>
</screen>
     <para>
      If no size is specified, the snapshot is created as a thin snapshot.
     </para>
     <para>
      For example:
     </para>
<screen>
lvcreate -s -L 1G -n linux01-snap /dev/lvm/linux01
</screen>
     <para>
      The snapshot is created as the
      <filename>/dev/lvm/linux01-snap</filename> volume.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="biu259x">
   <title>Monitoring a Snapshot</title>

   <procedure id="biu25qe">
    <step id="biu25qf">
     <para>
      Open a terminal console, log in as the <systemitem>root</systemitem>
      user, then enter
     </para>
<screen>
lvdisplay <replaceable>snap_volume</replaceable>
</screen>
     <para>
      For example:
     </para>
<screen>
lvdisplay /dev/vg01/linux01-snap

--- Logical volume ---
  LV Name                /dev/lvm/linux01
  VG Name                vg01
  LV UUID                QHVJYh-PR3s-A4SG-s4Aa-MyWN-Ra7a-HL47KL
  LV Write Access        read/write
  LV snapshot status     active destination for /dev/lvm/linux01
  LV Status              available
  # open                 0
  LV Size                80.00 GB
  Current LE             1024
  COW-table size         8.00 GB
  COW-table LE           512
  Allocated to snapshot  30%
  Snapshot chunk size    8.00 KB
  Segments               1
  Allocation             inherit
  Read ahead sectors     0
  Block device           254:5
</screen>
    </step>
   </procedure>
  </sect1>
  <sect1 id="bi7uzm8">
   <title>Deleting Linux Snapshots</title>

   <procedure id="bi7x85s">
    <step id="bi7x85t">
     <para>
      Open a terminal console, log in as the <systemitem>root</systemitem>
      user, then enter
     </para>
<screen>
lvremove <replaceable>snap_volume_path</replaceable>
</screen>
     <para>
      For example:
     </para>
<screen>
lvremove /dev/lvmvg/linux01-snap
</screen>
    </step>
   </procedure>
  </sect1>
  <sect1 id="snapshot_xen_host">
   <title>Using Snapshots for Virtual Machines on a Virtual Host</title>

   <para>
    Using an LVM logical volume for a virtual machine’s back-end storage
    allows flexibility in administering the underlying device, such as
    making it easier to move storage objects, create snapshots, and back up
    data. You can mount the LVM logical volume and use it to store the
    virtual machine image as a file-backed disk, or you can assign the LVM
    logical volume as a physical disk to write to it as a block device. You
    can create a virtual disk image on the LVM logical volume, then snapshot
    the LVM.
   </para>

   <para>
    You can leverage the read/write capability of the snapshot to create
    different instances of a virtual machine, where the changes are made to
    the snapshot for a particular virtual machine instance. You can create a
    virtual disk image on an LVM logical volume, snapshot the source logical
    volume, and modify the snapshot for a particular virtual machine
    instance. You can create another snapshot of the source logical volume,
    and modify it for a different virtual machine instance. The majority of
    the data for the different virtual machine instances resides with the
    image on the source logical volume.
   </para>

   <para>
    You can also leverage the read/write capability of the snapshot to
    preserve the virtual disk image while testing patches or upgrades in the
    guest environment. You create a snapshot of the LVM volume that contains
    the image, and then run the virtual machine on the snapshot location.
    The source logical volume is unchanged, and all changes for that machine
    are written to the snapshot. To return to the source logical volume of
    the virtual machine image, you power off the virtual machine, then
    remove the snapshot from the source logical volume. To start over, you
    re-create the snapshot, mount the snapshot, and restart the virtual
    machine on the snapshot image.
   </para>

   <important>
    <para>
     The following procedure uses a file-backed virtual disk image and the
     Xen hypervisor. You can adapt the procedure in this section for other
     hypervisors that run on the SUSE Linux platform, such as KVM.
    </para>
   </important>

   <para>
    To run a file-backed virtual machine image from the snapshot volume:
   </para>

   <procedure id="b17cn63v">
    <step id="b17cnkv7">
     <para>
      Ensure that the source logical volume that contains the file-backed
      virtual machine image is mounted, such as at mount point
      <filename>/var/lib/xen/images/&lt;<replaceable>image_name</replaceable>&gt;</filename>.
     </para>
    </step>
    <step id="b17cn63w">
     <para>
      Create a snapshot of the LVM logical volume with enough space to store
      the differences that you expect.
     </para>
<screen>
lvcreate -s -L 20G -n myvm-snap /dev/lvmvg/myvm
</screen>
     <para>
      If no size is specified, the snapshot is created as a thin snapshot.
     </para>
    </step>
    <step id="b17cn8re">
     <para>
      Create a mount point where you will mount the snapshot volume.
     </para>
<screen>
mkdir -p /mnt/xen/vm/myvm-snap
</screen>
    </step>
    <step id="b17cn8rf">
     <para role="intro">
      Mount the snapshot volume at the mount point you created.
     </para>
<screen>
mount -t auto /dev/lvmvg/myvm-snap /mnt/xen/vm/myvm-snap
</screen>
    </step>
    <step id="b17cnd0k">
     <para>
      In a text editor, copy the configuration file for the source virtual
      machine, modify the paths to point to the file-backed image file on
      the mounted snapshot volume, and save the file such as
      <filename>/etc/xen/myvm-snap.cfg</filename>.
     </para>
    </step>
    <step id="b17cnd0l">
     <para>
      Start the virtual machine using the mounted snapshot volume of the
      virtual machine.
     </para>
<screen>
xm create -c /etc/xen/myvm-snap.cfg
</screen>
    </step>
    <step id="b17cnd0m">
     <para>
      (Optional) Remove the snapshot, and use the unchanged virtual machine
      image on the source logical volume.
     </para>
<screen>
unmount /mnt/xenvms/myvm-snap
lvremove -f /dev/lvmvg/mylvm-snap
</screen>
    </step>
    <step id="b17cnd0n">
     <para>
      (Optional) Repeat this process as desired.
     </para>
    </step>
   </procedure>
  </sect1>
  <sect1 id="lvconvert_merge">
   <title>Merging a Snapshot with the Source Logical Volume to Revert Changes or Roll Back to a Previous State</title>

   <para>
    Snapshots can be useful if you need to roll back or restore data on a
    volume to a previous state. For example, you might need to revert data
    changes that resulted from an administrator error or a failed or
    undesirable package installation or upgrade.
   </para>

   <para>
    You can use the <command>lvconvert --merge</command> command to revert
    the changes made to an LVM logical volume. The merging begins as
    follows:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      If both the source logical volume and snapshot volume are not open,
      the merge begins immediately.
     </para>
    </listitem>
    <listitem>
     <para>
      If the source logical volume or snapshot volume are open, the merge
      starts the first time either the source logical volume or snapshot
      volume are activated and both are closed.
     </para>
    </listitem>
    <listitem>
     <para>
      If the source logical volume cannot be closed, such as the
      <systemitem>root</systemitem> file system, the merge is deferred until
      the next time the server reboots and the source logical volume is
      activated.
     </para>
    </listitem>
    <listitem>
     <para>
      If the source logical volume contains a virtual machine image, you
      must shut down the virtual machine, deactivate the source logical
      volume and snapshot volume (by dismounting them in that order), and
      then issue the merge command. Because the source logical volume is
      automatically remounted and the snapshot volume is deleted when the
      merge is complete, you should not restart the virtual machine until
      after the merge is complete. After the merge is complete, you use the
      resulting logical volume for the virtual machine.
     </para>
    </listitem>
   </itemizedlist>

   <para>
    After a merge begins, the merge continues automatically after server
    restarts until it is complete. A new snapshot cannot be created for the
    source logical volume while a merge is in progress.
   </para>

   <para>
    While the merge is in progress, reads or writes to the source logical
    volume are transparently redirected to the snapshot that is being
    merged. This allows users to immediately view and access the data as it
    was when the snapshot was created. They do not need to wait for the
    merge to complete.
   </para>

   <para>
    When the merge is complete, the source logical volume contains the same
    data as it did when the snapshot was taken, plus any data changes made
    after the merge began. The resulting logical volume has the source
    logical volume’s name, minor number, and UUID. The source logical
    volume is automatically remounted, and the snapshot volume is removed.
   </para>

   <procedure id="b15cjobl">
    <step id="b15cjobm">
     <para>
      Open a terminal console, log in as the <systemitem>root</systemitem>
      user, then enter
     </para>
<screen>
lvconvert --merge  [-b] [-i &lt;<replaceable>seconds</replaceable>&gt;] [&lt;<replaceable>snap_volume_path</replaceable>&gt;[...&lt;snapN&gt;]|@&lt;<replaceable>volume_tag</replaceable>&gt;]
</screen>
     <para>
      You can specify one or multiple snapshots on the command line. You can
      alternatively tag multiple source logical volumes with the same volume
      tag then specify
      <literal>@&lt;<replaceable>volume_tag</replaceable>&gt;</literal> on
      the command line. The snapshots for the tagged volumes are merged to
      their respective source logical volumes. For information about tagging
      logical volumes, see
      <xref linkend="lvmtagging" xrefstyle="SectTitleOnPage"/>.
     </para>
     <para>
      The options include:
     </para>
     <variablelist>
      <varlistentry id="b15ck6lb">
       <term>-b</term>
       <term>--background</term>
       <listitem>
        <para>
         Run the daemon in the background. This allows multiple specified
         snapshots to be merged concurrently in parallel.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry id="b15ck6lc">
       <term>-i</term>
       <term>--interval &lt;<replaceable>seconds</replaceable>&gt;</term>
       <listitem>
        <para>
         Report progress as a percentage at regular intervals. Specify the
         interval in seconds.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      For more information about this command, see the
      <command>lvconvert(8)</command> man page.
     </para>
     <para>
      For example:
     </para>
<screen>
lvconvert --merge /dev/lvmvg/linux01-snap
</screen>
     <para>
      This command merges <filename>/dev/lvmvg/linux01-snap</filename> into
      its source logical volume.
     </para>
<screen>
lvconvert --merge @mytag
</screen>
     <para>
      If <filename>lvol1</filename>, <filename>lvol2</filename>, and
      <filename>lvol3</filename> are all tagged with
      <literal>mytag</literal>, each snapshot volume is merged serially with
      its respective source logical volume; that is:
      <filename>lvol1</filename>, then <filename>lvol2</filename>, then
      <filename>lvol3</filename>. If the <literal>--background</literal>
      option is specified, the snapshots for the respective tagged logical
      volume are merged concurrently in parallel.
     </para>
    </step>
    <step id="b15clkwi">
     <para>
      (Optional) If both the source logical volume and snapshot volume are
      open and they can be closed, you can manually deactivate and activate
      the source logical volume to get the merge to start immediately.
     </para>
<screen>
umount &lt;<replaceable>original_volume</replaceable>&gt;
lvchange -an &lt;<replaceable>original_volume</replaceable>&gt;
lvchange -ay &lt;<replaceable>original_volume</replaceable>&gt;
mount &lt;<replaceable>original_volume</replaceable>&gt; &lt;<replaceable>mount_point</replaceable>&gt;
</screen>
     <para>
      For example:
     </para>
<screen>
umount /dev/lvmvg/lvol01
lvchange -an /dev/lvmvg/lvol01
lvchange -ay /dev/lvmvg/lvol01
mount /dev/lvmvg/lvol01 /mnt/lvol01
</screen>
    </step>
    <step id="b15clm73">
     <para>
      (Optional) If both the source logical volume and snapshot volume are
      open and the source logical volume cannot be closed, such as the
      <systemitem>root</systemitem> file system, you can restart the server
      and mount the source logical volume to get the merge to start
      immediately after the restart.
     </para>
    </step>
   </procedure>
  </sect1>
 </chapter>
 <chapter id="nfsv4acls" lang="en">
  <title>Managing Access Control Lists over NFSv4</title>
  <para>
   There is no single standard for Access Control Lists (ACLs) in Linux
   beyond the simple user-group-others read, write, and execute
   (<literal>rwx</literal>) flags. One option for finer control are the
   <citetitle>Draft POSIX ACLs</citetitle>, which were never formally
   standardised by POSIX. Another is the NFSv4 ACLs, which were designed to
   be part of the NFSv4 network file system with the goal of making
   something that provided reasonable compatibility between POSIX systems on
   Linux and WIN32 systems on Microsoft Windows.
  </para>
  <para>
   NFSv4 ACLs are not sufficient to correctly implement Draft POSIX ACLs so
   no attempt has been made to map ACL accesses on an NFSv4 client (such as
   using <command>setfacl</command>).
  </para>
  <para>
   When using NFSv4, Draft POSIX ACLs cannot be used even in emulation and
   NFSv4 ACLs need to be used directly; i.e., while
   <command>setfacl</command> can work on NFSv3, it cannot work on NFSv4.+To
   allow NFSv4 ACLs to be used on an NFSv4 file system, SUSE Linux
   Enterprise Server provides the <filename>nfs4-acl-tools</filename>
   package which contains the following:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <filename>nfs4-getfacl</filename>
    </para>
   </listitem>
   <listitem>
    <para>
     <filename>nfs4-setfacl</filename>
    </para>
   </listitem>
   <listitem>
    <para>
     <filename>nfs4-editacl</filename>
    </para>
   </listitem>
  </itemizedlist>
  <para>
   These operate in a generally similar way to <command>getfacl</command>
   and <command>setfacl</command> for examining and modifying NFSv4
   ACLs.These commands are effective only if the file system on the NFS
   server provides full support for NFSv4 ACLs. Any limitation imposed by
   the server will affect programs running on the client in that some
   particular combinations of Access Control Entries (ACEs) might not be
   possible.
  </para>
  <para>
   It is not supported to mount NFS volumes locally on the exporting NFS
   server.
  </para>
  <bridgehead id="bxut1i2">Additional Information</bridgehead>
  <para>
   For information, see
   <ulink url="http://wiki.linux-nfs.org/wiki/index.php/ACLs#Introduction_to_NFSv4_ACLs"><quote>Introduction
   to NFSv4 ACLs</quote> on the Linux-nfs.org Web site</ulink>.
  </para>
 </chapter>
 <chapter id="trbl" lang="en">
  <title>Troubleshooting Storage Issues</title>
  <para>
   This section describes how to work around known issues for devices,
   software RAIDs, multipath I/O, and volumes.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="trblmpioboot" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="trbl_btrfs_volfull" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bpjpklm" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bpjpgil" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bpjphka" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="ts_iscsi_lio" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="trblmpioboot">
   <title>Is DM-MPIO Available for the Boot Partition?</title>

   <para>
    Device Mapper Multipath I/O (DM-MPIO) is supported for the boot
    partition, beginning in SUSE Linux Enterprise Server 10 Support Pack 1.
    For information, see
    <xref linkend="mpioroot" xrefstyle="SectTitleOnPage"/>.
   </para>
  </sect1>
  <sect1 id="trbl_btrfs_volfull">
   <title>Btrfs Error: No space is left on device</title>

   <para>
    The root (<filename>/</filename>) partition using the Btrfs file system
    stops accepting data. You receive the error <quote><literal>No space
    left on device</literal></quote>.
   </para>

   <para>
    See the following sections for information about possible causes and
    prevention of this issue.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b15tk0m4" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b15tk0m7" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b15tk0m4">
    <title>Disk Space Consumed by Snapper Snapshots</title>
    <para>
     If Snapper is running for the Btrfs file system, the <quote><literal>No
     space left on device</literal></quote> problem is typically caused by
     having too much data stored as snapshots on your system.
    </para>
    <para>
     You can remove some snapshots from Snapper, however, the snapshots are
     not deleted immediately and might not free up as much space as you
     need.
    </para>
    <para>
     To delete files from Snapper:
    </para>
    <procedure id="b15q34ft">
     <step id="b15tk0m5">
      <para>
       Log in as the <systemitem>root</systemitem> user, then open a
       terminal console.
      </para>
     </step>
     <step id="b15tk9ch">
      <para>
       Gain back enough space for the system to come up.
      </para>
      <substeps>
       <step id="b15tk0m6">
        <para>
         At the command prompt, enter
        </para>
<screen>
btrfs filesystem show
</screen>
<screen>
Label: none uuid: 40123456-cb2c-4678-8b3d-d014d1c78c78
 Total devices 1 FS bytes used 20.00GB
 devid 1 size 20.00GB used 20.00GB path /dev/sda3
</screen>
       </step>
       <step id="b15q34fu">
        <para>
         Enter
        </para>
<screen>
btrfs fi balance start &lt;<replaceable>/mountpoint</replaceable>&gt; -dusage=5
</screen>
        <para>
         This command attempts to relocate data in empty or near-empty data
         chunks, allowing the space to be reclaimed and reassigned to
         metadata. This can take awhile (many hours for 1 TB) although the
         system is otherwise usable during this time.
        </para>
       </step>
      </substeps>
     </step>
     <step id="b15q34fv">
      <para>
       List the snapshots in Snapper. Enter
      </para>
<screen>
snapper -c root list
</screen>
     </step>
     <step id="b15q34fw">
      <para role="intro">
       Delete one or more snapshots from Snapper. Enter
      </para>
<screen>
snapper -c root delete #
</screen>
      <para>
       Ensure that you delete the oldest snapshots first. The older a
       snapshot is, the more disk space it occupies.
      </para>
     </step>
    </procedure>
    <para>
     To help prevent this problem, you can change the Snapper cleanup
     defaults to be more aggressive in the
     <filename>/etc/snapper/configs/root</filename> configuration file, or
     for other mount points. Snapper provides three algorithms to clean up
     old snapshots. The algorithms are executed in a daily cron-job. The
     cleanup frequency is defined in the Snapper configuration for the mount
     point. Lower the TIMELINE_LIMIT parameters for daily, monthly, and
     yearly to reduce how long and the number of snapshots to be retained.
     For information, see <xref linkend="sec.snapper.config"/>
    </para>
    <para>
     If you use Snapper with Btrfs on the file system disk, it is advisable
     to reserve twice the amount of disk space than the standard storage
     proposal. The YaST Partitioner automatically proposes twice the
     standard disk space in the Btrfs storage proposal for the root file
     system.
    </para>
   </sect2>

   <sect2 id="b15tk0m7">
    <title>Disk Space Consumed by Log, Crash, and Cache Files</title>
    <para>
     If the system disk is filling up with data, you can try deleting files
     from <filename>/var/log</filename>, <filename>/var/crash</filename>,
     and <filename>/var/cache</filename>.
    </para>
    <para>
     The Btrfs <systemitem>root</systemitem> file system subvolumes
     <filename>/var/log</filename>, <filename>/var/crash</filename> and
     <filename>/var/cache</filename> can use all of the available disk space
     during normal operation, and cause a system malfunction. To help avoid
     this situation, &productname; offers Btrfs quota support
     for subvolumes. See the <filename>btrfs(8)</filename> manual page for
     more details.
    </para>
   </sect2>
  </sect1>
  <sect1 id="bpjpklm">
   <title>Issues for Multipath I/O</title>

   <para>
    See <xref linkend="bpjpirk" xrefstyle="SectTitleOnPage"/>.
   </para>
  </sect1>
  <sect1 id="bpjpgil">
   <title>Issues for Software RAIDs</title>

   <para>
    See <xref linkend="bgchysg" xrefstyle="AppTitleOnPage"/>.
   </para>
  </sect1>
  <sect1 id="bpjphka">
   <title>Issues for iSCSI</title>

   <para>
    See
    <xref linkend="sec_inst_system_iscsi_ts" xrefstyle="SectTitleOnPage"/>.
   </para>
  </sect1>
  <sect1 id="ts_iscsi_lio">
   <title>Issues for iSCSI LIO Target</title>

   <para>
    See <xref linkend="b13g8i2g" xrefstyle="SectTitleOnPage"/>.
   </para>
  </sect1>
 </chapter>
 <appendix id="common_legal">
  <title>GNU Licenses</title>
  <para>
   This appendix contains the GNU General Public License Version 2 and the
   GNU Free Documentation License Version 1.2.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="common_gplv2_i" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="gfdl_i" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="common_gplv2_i" role="legal">
   <title>GNU General Public License</title>

   <para>
    Version 2, June 1991
   </para>

   <para>
    Copyright (C) 1989, 1991 Free Software Foundation, Inc. 59 Temple Place
    - Suite 330, Boston, MA 02111-1307, USA
   </para>

   <para>
    Everyone is permitted to copy and distribute verbatim copies of this
    license document, but changing it is not allowed.
   </para>

   <sect2 id="gplv2_preamble">
    <title>Preamble</title>
    <para>
     The licenses for most software are designed to take away your freedom
     to share and change it. By contrast, the GNU General Public License is
     intended to guarantee your freedom to share and change free
     software--to ensure that the software is free for all its users. This
     General Public License applies to most of the Free Software
     Foundation’s software and to any other program whose authors commit
     to using it. (Some other Free Software Foundation software is covered
     by the GNU Library General Public License instead.) You can apply it to
     your programs, too.
    </para>
    <para>
     When we speak of free software, we are referring to freedom, not price.
     Our General Public Licenses are designed to ensure that you have the
     freedom to distribute copies of free software (and charge for this
     service if you wish), that you receive source code or can get it if you
     want it, that you can change the software or use pieces of it in new
     free programs; and that you know you can do these things.
    </para>
    <para>
     To protect your rights, we need to make restrictions that forbid anyone
     to deny you these rights or to ask you to surrender the rights. These
     restrictions translate to certain responsibilities for you if you
     distribute copies of the software, or if you modify it.
    </para>
    <para>
     For example, if you distribute copies of such a program, whether gratis
     or for a fee, you must give the recipients all the rights that you
     have. You must ensure that they, too, receive or can get the source
     code. And you must show them these terms so they know their rights.
    </para>
    <para>
     We protect your rights with two steps: (1) copyright the software, and
     (2) offer you this license which gives you legal permission to copy,
     distribute and/or modify the software.
    </para>
    <para>
     Also, for each author’s protection and ours, we want to make certain
     that everyone understands that there is no warranty for this free
     software. If the software is modified by someone else and passed on, we
     want its recipients to know that what they have is not the original, so
     that any problems introduced by others will not reflect on the original
     authors’ reputations.
    </para>
    <para>
     Finally, any free program is threatened constantly by software patents.
     We wish to avoid the danger that redistributors of a free program will
     individually obtain patent licenses, in effect making the program
     proprietary. To prevent this, we have made it clear that any patent
     must be licensed for everyone’s free use or not licensed at all.
    </para>
    <para>
     The precise terms and conditions for copying, distribution and
     modification follow.
    </para>
   </sect2>


   <sect2 id="gplv2_terms">
    <title>GNU GENERAL PUBLIC LICENSE TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION</title>
    <formalpara id="bw4vaov">
     <title>0.</title>
     <para>
      This License applies to any program or other work which contains a
      notice placed by the copyright holder saying it may be distributed
      under the terms of this General Public License. The
      <quote>Program</quote>, below, refers to any such program or work, and
      a <quote>work based on the Program</quote> means either the Program or
      any derivative work under copyright law: that is to say, a work
      containing the Program or a portion of it, either verbatim or with
      modifications and/or translated into another language. (Hereinafter,
      translation is included without limitation in the term
      <quote>modification</quote>.) Each licensee is addressed as
      <quote>you</quote>.
     </para>
    </formalpara>
    <para>
     Activities other than copying, distribution and modification are not
     covered by this License; they are outside its scope. The act of running
     the Program is not restricted, and the output from the Program is
     covered only if its contents constitute a work based on the Program
     (independent of having been made by running the Program). Whether that
     is true depends on what the Program does.
    </para>
    <formalpara id="bw4vaow">
     <title>1.</title>
     <para>
      You may copy and distribute verbatim copies of the Program’s source
      code as you receive it, in any medium, provided that you conspicuously
      and appropriately publish on each copy an appropriate copyright notice
      and disclaimer of warranty; keep intact all the notices that refer to
      this License and to the absence of any warranty; and give any other
      recipients of the Program a copy of this License along with the
      Program.
     </para>
    </formalpara>
    <para>
     You may charge a fee for the physical act of transferring a copy, and
     you may at your option offer warranty protection in exchange for a fee.
    </para>
    <formalpara id="bw4vaox">
     <title>2.</title>
     <para>
      You may modify your copy or copies of the Program or any portion of
      it, thus forming a work based on the Program, and copy and distribute
      such modifications or work under the terms of Section 1 above,
      provided that you also meet all of these conditions:
     </para>
    </formalpara>
    <formalpara id="bw4vaoy">
     <title>a)</title>
     <para>
      You must cause the modified files to carry prominent notices stating
      that you changed the files and the date of any change.
     </para>
    </formalpara>
    <formalpara id="bw4vaoz">
     <title>b)</title>
     <para>
      You must cause any work that you distribute or publish, that in whole
      or in part contains or is derived from the Program or any part
      thereof, to be licensed as a whole at no charge to all third parties
      under the terms of this License.
     </para>
    </formalpara>
    <formalpara id="bw4vap0">
     <title>c)</title>
     <para>
      If the modified program normally reads commands interactively when
      run, you must cause it, when started running for such interactive use
      in the most ordinary way, to print or display an announcement
      including an appropriate copyright notice and a notice that there is
      no warranty (or else, saying that you provide a warranty) and that
      users may redistribute the program under these conditions, and telling
      the user how to view a copy of this License. (Exception: if the
      Program itself is interactive but does not normally print such an
      announcement, your work based on the Program is not required to print
      an announcement.)
     </para>
    </formalpara>
    <para>
     These requirements apply to the modified work as a whole. If
     identifiable sections of that work are not derived from the Program,
     and can be reasonably considered independent and separate works in
     themselves, then this License, and its terms, do not apply to those
     sections when you distribute them as separate works. But when you
     distribute the same sections as part of a whole which is a work based
     on the Program, the distribution of the whole must be on the terms of
     this License, whose permissions for other licensees extend to the
     entire whole, and thus to each and every part regardless of who wrote
     it.
    </para>
    <para>
     Thus, it is not the intent of this section to claim rights or contest
     your rights to work written entirely by you; rather, the intent is to
     exercise the right to control the distribution of derivative or
     collective works based on the Program.
    </para>
    <para>
     In addition, mere aggregation of another work not based on the Program
     with the Program (or with a work based on the Program) on a volume of a
     storage or distribution medium does not bring the other work under the
     scope of this License.
    </para>
    <formalpara id="bw4vap1">
     <title>3.</title>
     <para>
      You may copy and distribute the Program (or a work based on it, under
      Section 2) in object code or executable form under the terms of
      Sections 1 and 2 above provided that you also do one of the following:
     </para>
    </formalpara>
    <formalpara id="bw4vap2">
     <title>a)</title>
     <para>
      Accompany it with the complete corresponding machine-readable source
      code, which must be distributed under the terms of Sections 1 and 2
      above on a medium customarily used for software interchange; or,
     </para>
    </formalpara>
    <formalpara id="bw4vap3">
     <title>b)</title>
     <para>
      Accompany it with a written offer, valid for at least three years, to
      give any third party, for a charge no more than your cost of
      physically performing source distribution, a complete machine-readable
      copy of the corresponding source code, to be distributed under the
      terms of Sections 1 and 2 above on a medium customarily used for
      software interchange; or,
     </para>
    </formalpara>
    <formalpara id="bw4vap4">
     <title>c)</title>
     <para>
      Accompany it with the information you received as to the offer to
      distribute corresponding source code. (This alternative is allowed
      only for noncommercial distribution and only if you received the
      program in object code or executable form with such an offer, in
      accord with Subsection b above.)
     </para>
    </formalpara>
    <para>
     The source code for a work means the preferred form of the work for
     making modifications to it. For an executable work, complete source
     code means all the source code for all modules it contains, plus any
     associated interface definition files, plus the scripts used to control
     compilation and installation of the executable. However, as a special
     exception, the source code distributed need not include anything that
     is normally distributed (in either source or binary form) with the
     major components (compiler, kernel, and so on) of the operating system
     on which the executable runs, unless that component itself accompanies
     the executable.
    </para>
    <para>
     If distribution of executable or object code is made by offering access
     to copy from a designated place, then offering equivalent access to
     copy the source code from the same place counts as distribution of the
     source code, even though third parties are not compelled to copy the
     source along with the object code.
    </para>
    <formalpara id="bw4vap5">
     <title>4.</title>
     <para>
      You may not copy, modify, sublicense, or distribute the Program except
      as expressly provided under this License. Any attempt otherwise to
      copy, modify, sublicense or distribute the Program is void, and will
      automatically terminate your rights under this License. However,
      parties who have received copies, or rights, from you under this
      License will not have their licenses terminated so long as such
      parties remain in full compliance.
     </para>
    </formalpara>
    <formalpara id="bw4vap6">
     <title>5.</title>
     <para>
      You are not required to accept this License, since you have not signed
      it. However, nothing else grants you permission to modify or
      distribute the Program or its derivative works. These actions are
      prohibited by law if you do not accept this License. Therefore, by
      modifying or distributing the Program (or any work based on the
      Program), you indicate your acceptance of this License to do so, and
      all its terms and conditions for copying, distributing or modifying
      the Program or works based on it.
     </para>
    </formalpara>
    <formalpara id="bw4vap7">
     <title>6.</title>
     <para>
      Each time you redistribute the Program (or any work based on the
      Program), the recipient automatically receives a license from the
      original licensor to copy, distribute or modify the Program subject to
      these terms and conditions. You may not impose any further
      restrictions on the recipients’ exercise of the rights granted
      herein. You are not responsible for enforcing compliance by third
      parties to this License.
     </para>
    </formalpara>
    <formalpara id="bw4vap8">
     <title>7.</title>
     <para>
      If, as a consequence of a court judgment or allegation of patent
      infringement or for any other reason (not limited to patent issues),
      conditions are imposed on you (whether by court order, agreement or
      otherwise) that contradict the conditions of this License, they do not
      excuse you from the conditions of this License. If you cannot
      distribute so as to satisfy simultaneously your obligations under this
      License and any other pertinent obligations, then as a consequence you
      may not distribute the Program at all. For example, if a patent
      license would not permit royalty-free redistribution of the Program by
      all those who receive copies directly or indirectly through you, then
      the only way you could satisfy both it and this License would be to
      refrain entirely from distribution of the Program.
     </para>
    </formalpara>
    <para>
     If any portion of this section is held invalid or unenforceable under
     any particular circumstance, the balance of the section is intended to
     apply and the section as a whole is intended to apply in other
     circumstances.
    </para>
    <para>
     It is not the purpose of this section to induce you to infringe any
     patents or other property right claims or to contest validity of any
     such claims; this section has the sole purpose of protecting the
     integrity of the free software distribution system, which is
     implemented by public license practices. Many people have made generous
     contributions to the wide range of software distributed through that
     system in reliance on consistent application of that system; it is up
     to the author/donor to decide if he or she is willing to distribute
     software through any other system and a licensee cannot impose that
     choice.
    </para>
    <para>
     This section is intended to make thoroughly clear what is believed to
     be a consequence of the rest of this License.
    </para>
    <formalpara id="bw4vap9">
     <title>8.</title>
     <para>
      If the distribution and/or use of the Program is restricted in certain
      countries either by patents or by copyrighted interfaces, the original
      copyright holder who places the Program under this License may add an
      explicit geographical distribution limitation excluding those
      countries, so that distribution is permitted only in or among
      countries not thus excluded. In such case, this License incorporates
      the limitation as if written in the body of this License.
     </para>
    </formalpara>
    <formalpara id="bw4vapa">
     <title>9.</title>
     <para>
      The Free Software Foundation may publish revised and/or new versions
      of the General Public License from time to time. Such new versions
      will be similar in spirit to the present version, but may differ in
      detail to address new problems or concerns.
     </para>
    </formalpara>
    <para>
     Each version is given a distinguishing version number. If the Program
     specifies a version number of this License which applies to it and
     <quote>any later version</quote>, you have the option of following the
     terms and conditions either of that version or of any later version
     published by the Free Software Foundation. If the Program does not
     specify a version number of this License, you may choose any version
     ever published by the Free Software Foundation.
    </para>
    <formalpara id="bw4vapb">
     <title>10.</title>
     <para>
      If you wish to incorporate parts of the Program into other free
      programs whose distribution conditions are different, write to the
      author to ask for permission. For software which is copyrighted by the
      Free Software Foundation, write to the Free Software Foundation; we
      sometimes make exceptions for this. Our decision will be guided by the
      two goals of preserving the free status of all derivatives of our free
      software and of promoting the sharing and reuse of software generally.
     </para>
    </formalpara>
    <sect3 id="gplv2_nowarranty">
     <title>NO WARRANTY</title>
     <formalpara id="bw4vapd">
      <title>11.</title>
      <para>
       BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY
       FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT
       WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER
       PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND,
       EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE
       IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
       PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE
       PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME
       THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
      </para>
     </formalpara>
     <formalpara id="bw4vape">
      <title>12.</title>
      <para>
       IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
       WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR
       REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR
       DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL
       DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM
       (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED
       INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE
       OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH
       HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH
       DAMAGES.
      </para>
     </formalpara>
    </sect3>
    <sect3 id="gplv2_endterms">
     <title>END OF TERMS AND CONDITIONS</title>
     <para/>
    </sect3>
   </sect2>

   <sect2 id="gplv2_apply">
    <title>How to Apply These Terms to Your New Programs</title>
    <para>
     If you develop a new program, and you want it to be of the greatest
     possible use to the public, the best way to achieve this is to make it
     free software which everyone can redistribute and change under these
     terms.
    </para>
    <para>
     To do so, attach the following notices to the program. It is safest to
     attach them to the start of each source file to most effectively convey
     the exclusion of warranty; and each file should have at least the
     <quote>copyright</quote> line and a pointer to where the full notice is
     found.
    </para>
<screen>
one line to give the program’s name and an idea of what it does. 
Copyright (C) yyyy name of author

</screen>
<screen>
This program is free software; you can redistribute it and/or
modify it under the terms of the GNU General Public License
as published by the Free Software Foundation; either version 2
of the License, or (at your option) any later version.

</screen>
<screen>
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details.

</screen>
<screen>
You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307,
USA.

</screen>
    <para>
     Also add information on how to contact you by electronic and paper
     mail.
    </para>
    <para>
     If the program is interactive, make it output a short notice like this
     when it starts in an interactive mode:
    </para>
<screen>
Gnomovision version 69, Copyright (C) year name of author
Gnomovision comes with ABSOLUTELY NO WARRANTY; for details
type `show w’. This is free software, and you are welcome
to redistribute it under certain conditions; type `show c’ 
for details.

</screen>
    <para>
     The hypothetical commands `show w’ and `show c’ should show the
     appropriate parts of the General Public License. Of course, the
     commands you use may be called something other than `show w’ and
     `show c’; they could even be mouse-clicks or menu items--whatever
     suits your program.
    </para>
    <para>
     You should also get your employer (if you work as a programmer) or your
     school, if any, to sign a <quote>copyright disclaimer</quote> for the
     program, if necessary. Here is a sample; alter the names:
    </para>
<screen>
Yoyodyne, Inc., hereby disclaims all copyright
interest in the program `Gnomovision’
(which makes passes at compilers) written 
by James Hacker.

</screen>
<screen>
signature of Ty Coon, 1 April 1989
Ty Coon, President of Vice

</screen>
    <para>
     This General Public License does not permit incorporating your program
     into proprietary programs. If your program is a subroutine library, you
     may consider it more useful to permit linking proprietary applications
     with the library. If this is what you want to do, use the GNU Lesser
     General Public License instead of this License.
    </para>
    <para/>
    <para/>
    <para/>
    <para/>
    <para/>
   </sect2>
  </sect1>
  <sect1 id="gfdl_i" role="legal">
   <title>GNU Free Documentation License</title>

   <para>
    Version 1.2, November 2002
   </para>

   <para>
    Copyright (C) 2000,2001,2002 Free Software Foundation, Inc. 59 Temple
    Place, Suite 330, Boston, MA 02111-1307 USA
   </para>

   <para>
    Everyone is permitted to copy and distribute verbatim copies of this
    license document, but changing it is not allowed.
   </para>

   <sect2 id="gfdl_preamble">
    <title>PREAMBLE</title>
    <para>
     The purpose of this License is to make a manual, textbook, or other
     functional and useful document “free” in the sense of freedom: to
     assure everyone the effective freedom to copy and redistribute it, with
     or without modifying it, either commercially or noncommercially.
     Secondarily, this License preserves for the author and publisher a way
     to get credit for their work, while not being considered responsible
     for modifications made by others.
    </para>
    <para>
     This License is a kind of <quote>copyleft</quote>, which means that
     derivative works of the document must themselves be free in the same
     sense. It complements the GNU General Public License, which is a
     copyleft license designed for free software.
    </para>
    <para>
     We have designed this License in order to use it for manuals for free
     software, because free software needs free documentation: a free
     program should come with manuals providing the same freedoms that the
     software does. But this License is not limited to software manuals; it
     can be used for any textual work, regardless of subject matter or
     whether it is published as a printed book. We recommend this License
     principally for works whose purpose is instruction or reference.
    </para>
   </sect2>

   <sect2 id="gfdl_definitions">
    <title>APPLICABILITY AND DEFINITIONS</title>
    <para>
     This License applies to any manual or other work, in any medium, that
     contains a notice placed by the copyright holder saying it can be
     distributed under the terms of this License. Such a notice grants a
     world-wide, royalty-free license, unlimited in duration, to use that
     work under the conditions stated herein. The <quote>Document</quote>,
     below, refers to any such manual or work. Any member of the public is a
     licensee, and is addressed as <quote>you</quote>. You accept the
     license if you copy, modify or distribute the work in a way requiring
     permission under copyright law.
    </para>
    <para>
     A <quote>Modified Version</quote> of the Document means any work
     containing the Document or a portion of it, either copied verbatim, or
     with modifications and/or translated into another language.
    </para>
    <para>
     A <quote>Secondary Section</quote> is a named appendix or a
     front-matter section of the Document that deals exclusively with the
     relationship of the publishers or authors of the Document to the
     Document’s overall subject (or to related matters) and contains
     nothing that could fall directly within that overall subject. (Thus, if
     the Document is in part a textbook of mathematics, a Secondary Section
     may not explain any mathematics.) The relationship could be a matter of
     historical connection with the subject or with related matters, or of
     legal, commercial, philosophical, ethical or political position
     regarding them.
    </para>
    <para>
     The <quote>Invariant Sections</quote> are certain Secondary Sections
     whose titles are designated, as being those of Invariant Sections, in
     the notice that says that the Document is released under this License.
     If a section does not fit the above definition of Secondary then it is
     not allowed to be designated as Invariant. The Document may contain
     zero Invariant Sections. If the Document does not identify any
     Invariant Sections then there are none.
    </para>
    <para>
     The <quote>Cover Texts</quote> are certain short passages of text that
     are listed, as Front-Cover Texts or Back-Cover Texts, in the notice
     that says that the Document is released under this License. A
     Front-Cover Text may be at most 5 words, and a Back-Cover Text may be
     at most 25 words.
    </para>
    <para>
     A <quote>Transparent</quote> copy of the Document means a
     machine-readable copy, represented in a format whose specification is
     available to the general public, that is suitable for revising the
     document straightforwardly with generic text editors or (for images
     composed of pixels) generic paint programs or (for drawings) some
     widely available drawing editor, and that is suitable for input to text
     formatters or for automatic translation to a variety of formats
     suitable for input to text formatters. A copy made in an otherwise
     Transparent file format whose markup, or absence of markup, has been
     arranged to thwart or discourage subsequent modification by readers is
     not Transparent. An image format is not Transparent if used for any
     substantial amount of text. A copy that is
     not<quote>Transparent</quote> is called <quote>Opaque</quote>.
    </para>
    <para>
     Examples of suitable formats for Transparent copies include plain ASCII
     without markup, Texinfo input format, LaTeX input format, SGML or XML
     using a publicly available DTD, and standard-conforming simple HTML,
     PostScript or PDF designed for human modification. Examples of
     transparent image formats include PNG, XCF and JPG. Opaque formats
     include proprietary formats that can be read and edited only by
     proprietary word processors, SGML or XML for which the DTD and/or
     processing tools are not generally available, and the machine-generated
     HTML, PostScript or PDF produced by some word processors for output
     purposes only.
    </para>
    <para>
     The <quote>Title Page</quote> means, for a printed book, the title page
     itself, plus such following pages as are needed to hold, legibly, the
     material this License requires to appear in the title page. For works
     in formats which do not have any title page as such, <quote>Title
     Page</quote> means the text near the most prominent appearance of the
     work’s title, preceding the beginning of the body of the text.
    </para>
    <para>
     A section <quote>Entitled XYZ</quote> means a named subunit of the
     Document whose title either is precisely XYZ or contains XYZ in
     parentheses following text that translates XYZ in another language.
     (Here XYZ stands for a specific section name mentioned below, such as
     <quote>Acknowledgements</quote>, <quote>Dedications</quote>,
     <quote>Endorsements</quote>, or <quote>History</quote>.) To
     <quote>Preserve the Title</quote> of such a section when you modify the
     Document means that it remains a section <quote>Entitled XYZ</quote>
     according to this definition.
    </para>
    <para>
     The Document may include Warranty Disclaimers next to the notice which
     states that this License applies to the Document. These Warranty
     Disclaimers are considered to be included by reference in this License,
     but only as regards disclaiming warranties: any other implication that
     these Warranty Disclaimers may have is void and has no effect on the
     meaning of this License.
    </para>
   </sect2>

   <sect2 id="gfdl_verbatim">
    <title>VERBATIM COPYING</title>
    <para>
     You may copy and distribute the Document in any medium, either
     commercially or noncommercially, provided that this License, the
     copyright notices, and the license notice saying this License applies
     to the Document are reproduced in all copies, and that you add no other
     conditions whatsoever to those of this License. You may not use
     technical measures to obstruct or control the reading or further
     copying of the copies you make or distribute. However, you may accept
     compensation in exchange for copies. If you distribute a large enough
     number of copies you must also follow the conditions in section 3.
    </para>
    <para>
     You may also lend copies, under the same conditions stated above, and
     you may publicly display copies.
    </para>
   </sect2>

   <sect2 id="gfdl_quantity">
    <title>COPYING IN QUANTITY</title>
    <para>
     If you publish printed copies (or copies in media that commonly have
     printed covers) of the Document, numbering more than 100, and the
     Document’s license notice requires Cover Texts, you must enclose the
     copies in covers that carry, clearly and legibly, all these Cover
     Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on
     the back cover. Both covers must also clearly and legibly identify you
     as the publisher of these copies. The front cover must present the full
     title with all words of the title equally prominent and visible. You
     may add other material on the covers in addition. Copying with changes
     limited to the covers, as long as they preserve the title of the
     Document and satisfy these conditions, can be treated as verbatim
     copying in other respects.
    </para>
    <para>
     If the required texts for either cover are too voluminous to fit
     legibly, you should put the first ones listed (as many as fit
     reasonably) on the actual cover, and continue the rest onto adjacent
     pages.
    </para>
    <para>
     If you publish or distribute Opaque copies of the Document numbering
     more than 100, you must either include a machine-readable Transparent
     copy along with each Opaque copy, or state in or with each Opaque copy
     a computer-network location from which the general network-using public
     has access to download using public-standard network protocols a
     complete Transparent copy of the Document, free of added material. If
     you use the latter option, you must take reasonably prudent steps, when
     you begin distribution of Opaque copies in quantity, to ensure that
     this Transparent copy will remain thus accessible at the stated
     location until at least one year after the last time you distribute an
     Opaque copy (directly or through your agents or retailers) of that
     edition to the public.
    </para>
    <para>
     It is requested, but not required, that you contact the authors of the
     Document well before redistributing any large number of copies, to give
     them a chance to provide you with an updated version of the Document.
    </para>
   </sect2>

   <sect2 id="gfdl_modify">
    <title>MODIFICATIONS</title>
    <para>
     You may copy and distribute a Modified Version of the Document under
     the conditions of sections 2 and 3 above, provided that you release the
     Modified Version under precisely this License, with the Modified
     Version filling the role of the Document, thus licensing distribution
     and modification of the Modified Version to whoever possesses a copy of
     it. In addition, you must do these things in the Modified Version:
    </para>
    <formalpara id="bw4vnmy">
     <title>A.</title>
     <para>
      Use in the Title Page (and on the covers, if any) a title distinct
      from that of the Document, and from those of previous versions (which
      should, if there were any, be listed in the History section of the
      Document). You may use the same title as a previous version if the
      original publisher of that version gives permission.
     </para>
    </formalpara>
    <formalpara id="bw4vnmz">
     <title>B.</title>
     <para>
      List on the Title Page, as authors, one or more persons or entities
      responsible for authorship of the modifications in the Modified
      Version, together with at least five of the principal authors of the
      Document (all of its principal authors, if it has fewer than five),
      unless they release you from this requirement.
     </para>
    </formalpara>
    <formalpara id="bw4vnn0">
     <title>C.</title>
     <para>
      State on the Title page the name of the publisher of the Modified
      Version, as the publisher.
     </para>
    </formalpara>
    <formalpara id="bw4vnn1">
     <title>D.</title>
     <para>
      Preserve all the copyright notices of the Document.
     </para>
    </formalpara>
    <formalpara id="bw4vnn2">
     <title>E.</title>
     <para>
      Add an appropriate copyright notice for your modifications adjacent to
      the other copyright notices.
     </para>
    </formalpara>
    <formalpara id="bw4vnn3">
     <title>F.</title>
     <para>
      Include, immediately after the copyright notices, a license notice
      giving the public permission to use the Modified Version under the
      terms of this License, in the form shown in the Addendum below.
     </para>
    </formalpara>
    <formalpara id="bw4vnn4">
     <title>G.</title>
     <para>
      Preserve in that license notice the full lists of Invariant Sections
      and required Cover Texts given in the Document’s license notice.
     </para>
    </formalpara>
    <formalpara id="bw4vnn5">
     <title>H.</title>
     <para>
      Include an unaltered copy of this License.
     </para>
    </formalpara>
    <formalpara id="bw4vnn6">
     <title>I.</title>
     <para>
      Preserve the section Entitled <quote>History</quote>, Preserve its
      Title, and add to it an item stating at least the title, year, new
      authors, and publisher of the Modified Version as given on the Title
      Page. If there is no section Entitled <quote>History</quote> in the
      Document, create one stating the title, year, authors, and publisher
      of the Document as given on its Title Page, then add an item
      describing the Modified Version as stated in the previous sentence.
     </para>
    </formalpara>
    <formalpara id="bw4vnn7">
     <title>J.</title>
     <para>
      Preserve the network location, if any, given in the Document for
      public access to a Transparent copy of the Document, and likewise the
      network locations given in the Document for previous versions it was
      based on. These may be placed in the <quote>History</quote> section.
      You may omit a network location for a work that was published at least
      four years before the Document itself, or if the original publisher of
      the version it refers to gives permission.
     </para>
    </formalpara>
    <formalpara id="bw4vnn8">
     <title>K.</title>
     <para>
      For any section Entitled <quote>Acknowledgements</quote> or
      <quote>Dedications</quote>, Preserve the Title of the section, and
      preserve in the section all the substance and tone of each of the
      contributor acknowledgements and/or dedications given therein.
     </para>
    </formalpara>
    <formalpara id="bw4vnn9">
     <title>L.</title>
     <para>
      Preserve all the Invariant Sections of the Document, unaltered in
      their text and in their titles. Section numbers or the equivalent are
      not considered part of the section titles.
     </para>
    </formalpara>
    <formalpara id="bw4vnna">
     <title>M.</title>
     <para>
      Delete any section Entitled <quote>Endorsements</quote>. Such a
      section may not be included in the Modified Version.
     </para>
    </formalpara>
    <formalpara id="bw4vnnb">
     <title>N.</title>
     <para>
      Do not retitle any existing section to be Entitled
      <quote>Endorsements</quote> or to conflict in title with any Invariant
      Section.
     </para>
    </formalpara>
    <formalpara id="bw4vnnc">
     <title>O.</title>
     <para>
      Preserve any Warranty Disclaimers.
     </para>
    </formalpara>
    <para>
     If the Modified Version includes new front-matter sections or
     appendices that qualify as Secondary Sections and contain no material
     copied from the Document, you may at your option designate some or all
     of these sections as invariant. To do this, add their titles to the
     list of Invariant Sections in the Modified Version’s license notice.
     These titles must be distinct from any other section titles.
    </para>
    <para>
     You may add a section Entitled <quote>Endorsements</quote>, provided it
     contains nothing but endorsements of your Modified Version by various
     parties--for example, statements of peer review or that the text has
     been approved by an organization as the authoritative definition of a
     standard.
    </para>
    <para>
     You may add a passage of up to five words as a Front-Cover Text, and a
     passage of up to 25 words as a Back-Cover Text, to the end of the list
     of Cover Texts in the Modified Version. Only one passage of Front-Cover
     Text and one of Back-Cover Text may be added by (or through
     arrangements made by) any one entity. If the Document already includes
     a cover text for the same cover, previously added by you or by
     arrangement made by the same entity you are acting on behalf of, you
     may not add another; but you may replace the old one, on explicit
     permission from the previous publisher that added the old one.
    </para>
    <para>
     The author(s) and publisher(s) of the Document do not by this License
     give permission to use their names for publicity for or to assert or
     imply endorsement of any Modified Version.
    </para>
   </sect2>

   <sect2 id="gfdl_combine">
    <title>COMBINING DOCUMENTS</title>
    <para>
     You may combine the Document with other documents released under this
     License, under the terms defined in section 4 above for modified
     versions, provided that you include in the combination all of the
     Invariant Sections of all of the original documents, unmodified, and
     list them all as Invariant Sections of your combined work in its
     license notice, and that you preserve all their Warranty Disclaimers.
    </para>
    <para>
     The combined work need only contain one copy of this License, and
     multiple identical Invariant Sections may be replaced with a single
     copy. If there are multiple Invariant Sections with the same name but
     different contents, make the title of each such section unique by
     adding at the end of it, in parentheses, the name of the original
     author or publisher of that section if known, or else a unique number.
     Make the same adjustment to the section titles in the list of Invariant
     Sections in the license notice of the combined work.
    </para>
    <para>
     In the combination, you must combine any sections Entitled
     <quote>History</quote> in the various original documents, forming one
     section Entitled <quote>History</quote>; likewise combine any sections
     Entitled <quote>Acknowledgements</quote>, and any sections Entitled
     <quote>Dedications</quote>. You must delete all sections Entitled
     <quote>Endorsements</quote>.
    </para>
   </sect2>

   <sect2 id="gfdl_collect">
    <title>COLLECTIONS OF DOCUMENTS</title>
    <para>
     You may make a collection consisting of the Document and other
     documents released under this License, and replace the individual
     copies of this License in the various documents with a single copy that
     is included in the collection, provided that you follow the rules of
     this License for verbatim copying of each of the documents in all other
     respects.
    </para>
    <para>
     You may extract a single document from such a collection, and
     distribute it individually under this License, provided you insert a
     copy of this License into the extracted document, and follow this
     License in all other respects regarding verbatim copying of that
     document.
    </para>
   </sect2>

   <sect2 id="gfdl_aggregate">
    <title>AGGREGATION WITH INDEPENDENT WORKS</title>
    <para>
     A compilation of the Document or its derivatives with other separate
     and independent documents or works, in or on a volume of a storage or
     distribution medium, is called an “aggregate” if the copyright
     resulting from the compilation is not used to limit the legal rights of
     the compilation’s users beyond what the individual works permit. When
     the Document is included in an aggregate, this License does not apply
     to the other works in the aggregate which are not themselves derivative
     works of the Document.
    </para>
    <para>
     If the Cover Text requirement of section 3 is applicable to these
     copies of the Document, then if the Document is less than one half of
     the entire aggregate, the Document’s Cover Texts may be placed on
     covers that bracket the Document within the aggregate, or the
     electronic equivalent of covers if the Document is in electronic form.
     Otherwise they must appear on printed covers that bracket the whole
     aggregate.
    </para>
   </sect2>

   <sect2 id="gfdl_translate">
    <title>TRANSLATION</title>
    <para>
     Translation is considered a kind of modification, so you may distribute
     translations of the Document under the terms of section 4. Replacing
     Invariant Sections with translations requires special permission from
     their copyright holders, but you may include translations of some or
     all Invariant Sections in addition to the original versions of these
     Invariant Sections. You may include a translation of this License, and
     all the license notices in the Document, and any Warranty Disclaimers,
     provided that you also include the original English version of this
     License and the original versions of those notices and disclaimers. In
     case of a disagreement between the translation and the original version
     of this License or a notice or disclaimer, the original version will
     prevail.
    </para>
    <para>
     If a section in the Document is
     Entitled<quote>Acknowledgements</quote>, <quote>Dedications</quote>, or
     <quote>History</quote>, the requirement (section 4) to Preserve its
     Title (section 1) will typically require changing the actual title.
    </para>
   </sect2>

   <sect2 id="gfdl_termination">
    <title>TERMINATION</title>
    <para>
     You may not copy, modify, sublicense, or distribute the Document except
     as expressly provided for under this License. Any other attempt to
     copy, modify, sublicense or distribute the Document is void, and will
     automatically terminate your rights under this License. However,
     parties who have received copies, or rights, from you under this
     License will not have their licenses terminated so long as such parties
     remain in full compliance.
    </para>
   </sect2>

   <sect2 id="gfdl_revisions">
    <title>FUTURE REVISIONS OF THIS LICENSE</title>
    <para>
     The Free Software Foundation may publish new, revised versions of the
     GNU Free Documentation License from time to time. Such new versions
     will be similar in spirit to the present version, but may differ in
     detail to address new problems or concerns. See
     http://www.gnu.org/copyleft/.
    </para>
    <para>
     Each version of the License is given a distinguishing version number.
     If the Document specifies that a particular numbered version of this
     License <quote>or any later version</quote> applies to it, you have the
     option of following the terms and conditions either of that specified
     version or of any later version that has been published (not as a
     draft) by the Free Software Foundation. If the Document does not
     specify a version number of this License, you may choose any version
     ever published (not as a draft) by the Free Software Foundation.
    </para>
   </sect2>

   <sect2 id="gfdl_apply">
    <title>ADDENDUM: How to use this License for your documents</title>
    <para>
     To use this License in a document you have written, include a copy of
     the License in the document and put the following copyright and license
     notices just after the title page:
    </para>
<screen>
Copyright (c) YEAR YOUR NAME.
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover
Texts.
A copy of the license is included in the section entitled “GNU
Free Documentation License”.

</screen>
    <para>
     If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,
     replace the “with...Texts.” line with this:
    </para>
<screen>
with the Invariant Sections being LIST THEIR TITLES, with the
Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.

</screen>
    <para>
     If you have Invariant Sections without Cover Texts, or some other
     combination of the three, merge those two alternatives to suit the
     situation.
    </para>
    <para>
     If your document contains nontrivial examples of program code, we
     recommend releasing these examples in parallel under your choice of
     free software license, such as the GNU General Public License, to
     permit their use in free software.
    </para>
   </sect2>
  </sect1>
 </appendix>
 <appendix id="b653gv6">
  <title>Documentation Updates</title>
  <para>
   This section contains information about documentation content changes
   made to the <citetitle>SUSE Linux Enterprise Server Storage
   Administration Guide</citetitle> since the initial release of SUSE Linux
   Enterprise Server 11.
  </para>
  <para>
   This document was updated on the following dates:
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="b17tlcs5" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b17ir4gy" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b176hzef" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b13jg9cw" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b14jt4a4" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b14dmrrl" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b13z9xd2" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b13g65gi" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b12irb56" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b12mdifb" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b12doybp" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="b10v707p" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bzit5x5" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bw4b1n8" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bvqj59s" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bvc15n0" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="burkc4m" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bt5x8xp" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="brgqw2y" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bq2fgom" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bndh000" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="boj6043" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bmy23qz" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bm18yt7" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bleqdg7" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bkkotcf" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bfmmkat" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="b17tlcs5">
   <title>November 4, 2013</title>

   <para>
    Updates were made to the following section. The changes are explained
    below.
   </para>

   <sect2 id="b17tlcs6">
    <title>Managing Multipath I/O for Devices</title>
    <para>
     Updates were made to the following section. The changes are explained
     below.
    </para>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b17tlcs7">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b17tlcs8">
        <entry>
         <para>
          <xref linkend="mpiosysconfpart" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added issues related to partitioning multipath devices.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="b17ir4gy">
   <title>October 14, 2013</title>

   <para>
    Updates were made to the following section. The changes are explained
    below.
   </para>

   <sect2 id="b17ir4gz">
    <title>iSNS for Linux</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b17ir4h0">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b17ir4h1">
        <entry>
         <para>
          <xref linkend="sec.isns.ddadd" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added the -p ip:port option to the following commands:
         </para>
<screen>
iscsiadm -m node -t iqn.2006-02.com.example.iserv:systems -p ip:port --op=update --name=node.startup --value=automatic

iscsiadm -m node -t iqn.2006-02.com.example.iserv:systems -p ip:port --op=delete
</screen>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="b176hzef">
   <title>October 4, 2013</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b17cmfjk" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b17cmlvf" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b176hzeg" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b17cmfjk">
    <title>Managing Multipath I/O for Devices</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b17cmfjl">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b17cmfjm">
        <entry>
         <para>
          <xref linkend="b12doufy" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added a link to <literal>blacklist_exceptions</literal> examples
          in <xref linkend="bbj5x7z" xrefstyle="SectTitleOnPage"/>.
         </para>
        </entry>
       </row>
       <row id="b17cmfjn">
        <entry>
         <para>
          <xref linkend="bbj5x7z" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added examples for using the
          <literal>blacklist_exceptions</literal> section to enable
          multipath for devices blacklisted by the regular expressions used
          in the <literal>blacklist</literal> section.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="b17cmlvf">
    <title>Volume Snapshots</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b17cmlvg">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b17cmlvh">
        <entry>
         <para>
          <xref linkend="snapshots" xrefstyle="ChapTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          For clarity, changed “original volume” to “source logical
          volume”.
         </para>
        </entry>
       </row>
       <row id="b17cnp7r">
        <entry>
         <para>
          <xref linkend="bi7xb8b" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          In a Xen host environment, you can use the snapshot function to
          back up the virtual machine’s storage back-end or to test
          changes to a virtual machine image such as for patches or
          upgrades.
         </para>
        </entry>
       </row>
       <row id="b17cnp7s">
        <entry>
         <para>
          <xref linkend="snapshot_xen_host" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
       <row id="b17cnp7t">
        <entry>
         <para>
          <xref linkend="lvconvert_merge" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          If the source logical volume contains a virtual machine image, you
          must shut down the virtual machine, deactivate the source logical
          volume and snapshot volume (by dismounting them in that order),
          issue the merge command, and then activate the snapshot volume and
          source logical volume (by mounting them in that order). Because
          the source logical volume is automatically remounted and the
          snapshot volume is deleted when the merge is complete, you should
          not restart the virtual machine until after the merge is complete.
          After the merge is complete, you use the resulting logical volume
          for the virtual machine.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="b176hzeg">
    <title>What’s New for Storage in SLES 11</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b176hzeh">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b176hzei">
        <entry>
         <para>
          <xref linkend="bndgyoe" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added information about how to prepare for a SLES 10 SP4 to SLES
          11 upgrade if the system device is managed by EVMS.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="b13jg9cw">
   <title>June 2013 (SLES 11 SP3)</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b14ffk97" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b13g63v9" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b15coqxp" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14dug5l" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b155zz9h" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b13jg9cx" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b15coqxs" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b15q3ht8" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14ffkub" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b14ffk97">
    <title>LVM Configuration</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b14ffk98">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b14ffk99">
        <entry>
         <para>
          <xref linkend="bgchx8c" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added information about thin provisioning of LVM logical volumes.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="b13g63v9">
    <title>Mass Storage over IP Networks: iSCSI LIO Target Server</title>
    <para>
     <xref linkend="cha_iscsi_lio" xrefstyle="ChapTitleOnPage"/> is new.
    </para>
   </sect2>

   <sect2 id="b15coqxp">
    <title>Managing Multipath I/O for Devices</title>
    <informaltable  frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b15coqxq">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b15coqxr">
        <entry>
         <para>
          <xref linkend="b14ff9rs" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
       <row id="b15gv138">
        <entry>
         <para>
          <xref linkend="be5s8ae" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Updated the list of storage arrays that have a default definition
          in the
          <filename>/usr/share/doc/packages/multipath-tools/multipath.conf.defaults</filename>
          file.
         </para>
        </entry>
       </row>
       <row id="b15gv4eu">
        <entry>
         <para>
          <xref linkend="beep4ms" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The <literal>-r</literal> option is new.
         </para>
        </entry>
       </row>
       <row id="b15jxit2">
        <entry>
         <para>
          <xref linkend="b15jw320" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
       <row id="b15dsfgw">
        <entry>
         <para>
          <xref linkend="mpiosysconfsvr" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added information about the SCSI hardware handlers for
          <literal>alua</literal>, <literal>rdac</literal>,
          <literal>hp-sw</literal>, and <literal>emc</literal>.
         </para>
        </entry>
       </row>
       <row id="b15gv4ev">
        <entry>
         <para>
          <xref linkend="bbj68de" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Updated the default values list to annotate the deprecated
          attributes <literal>udev_dir</literal> and
          <literal>getuid_callout</literal>, the new attribute
          <literal>uid_attribute</literal>, and the changed default values
          for <literal>path_selector</literal> and
          <literal>max_fds</literal>.
         </para>
        </entry>
       </row>
       <row id="b15gvcrm">
        <entry>
         <para>
          <xref linkend="beg263n" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The default for <literal>path_selector</literal> changed from
          <literal>round-robin</literal> to <literal>service-time</literal>.
         </para>
         <para>
          The <literal>getuid_callout</literal> attribute is deprecated and
          replaced by the <literal>uid_attribute</literal> parameter.
         </para>
         <para>
          Added <literal>uid_attribute</literal>.
         </para>
        </entry>
       </row>
       <row id="b15cq0jh">
        <entry>
         <para>
          <xref linkend="scandev" xrefstyle="SectTitleOnPage"/>
         </para>
         <para>
          <xref linkend="be48i9g" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <warning>
          <para>
           In EMC PowerPath environments, do not use the
           <filename>rescan-scsi-bus.sh</filename> utility provided with the
           operating system or the HBA vendor scripts for scanning the SCSI
           buses. To avoid potential file system corruption, EMC requires
           that you follow the procedure provided in the vendor
           documentation for EMC PowerPath for Linux.
          </para>
         </warning>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="b14dug5l">
    <title>Managing Software RAIDs 6 and 10 with mdadm</title>
    <informaltable  frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b14dug5m">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b14dug5n">
        <entry>
         <para>
          <xref linkend="b14drcbo" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="b155zz9h">
    <title>Overview of File Systems in Linux</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b155zz9i">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b15tkrrw">
        <entry>
         <para>
          <xref linkend="b15tkr5k" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The cleanup frequency is defined in the Snapper configuration for
          the mount point. Lower the TIMELINE_LIMIT parameters for daily,
          monthly, and yearly to reduce how long and the number of snapshots
          to be retained.
         </para>
        </entry>
       </row>
       <row id="b155zz9j">
        <entry>
         <para>
          <xref linkend="b15tkr5p" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="b13jg9cx">
    <title>Storage Enclosure LED Utilities for Software RAIDs</title>
    <para>
     <xref linkend="cha_raids_lee" xrefstyle="ChapTitleOnPage"/> is new.
    </para>
   </sect2>

   <sect2 id="b15coqxs">
    <title>Volume Snapshots</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b15coqxt">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b15coqxu">
        <entry>
         <para>
          <xref linkend="bi7uttc" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added information about thin provisioning for LVM logical volume
          snapshots.
         </para>
        </entry>
       </row>
       <row id="b15coqxv">
        <entry>
         <para>
          <xref linkend="lvconvert_merge" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="b15q3ht8">
    <title>Troubleshooting Storage Issues</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b15q3ht9">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b15q3hta">
        <entry>
         <para>
          <xref linkend="trbl_btrfs_volfull" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="b14ffkub">
    <title>What’s New for Storage in SLES 11</title>
    <informaltable  frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b14ffkuc">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b14ffkud">
        <entry>
         <para>
          <xref linkend="new_sles11sp3" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="b14jt4a4">
   <title>March 19, 2013</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b14jt4a5" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b14jt4a5">
    <title>Overview of File Systems in Linux</title>
    <informaltable frame="topbot" rowsep="1" pgwide="1">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b14jt4a6">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b14jt4a7">
        <entry>
         <para>
          <xref linkend="sec_fileystems_major_ext3inodesize" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section was updated to discuss changes to the default
          settings for inode size and bytes-per-inode ratio in SLES 11.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="b14dmrrl">
   <title>March 11, 2013</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b14doxle" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14dmrrm" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b14doxle">
    <title>LVM Configuration</title>
    <informaltable frame="topbot" rowsep="1" pgwide="1">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b14doxlf">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b14doxlg">
        <entry>
         <para>
          <xref linkend="lvm_activate_vgs" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="b14dmrrm">
    <title>Overview of File Systems in Linux</title>
    <informaltable frame="topbot" rowsep="1" pgwide="1">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b14dmrrn">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b14dmrro">
        <entry>
         <para>
          <xref linkend="bwk8gda" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Multiple device support that allows you to grow or shrink the file
          system. The feature is planned to be available in a future release
          of the YaST Partitioner.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="b13z9xd2">
   <title>February 8, 2013</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b13z9xd3" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b13z9xd3">
    <title>Configuring Software RAID1 for the Root Partition</title>
    <para>
     This section has been modified to focus on the software RAID1 type.
     Software RAID0 and RAID5 are not supported. They were previously
     included in error. Additional important changes are noted below.
    </para>
    <informaltable frame="topbot" rowsep="1" pgwide="1">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b13z9xd4">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b13z9xd5">
        <entry>
         <para>
          <xref linkend="bi9d2lj" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          You need a third device to use for the <filename>/boot
          </filename>partition. The devices should be a local device.
         </para>
        </entry>
       </row>
       <row id="b13za2s6">
        <entry>
         <para>
          <xref linkend="b13za1ue" xrefstyle="StepXRef"/> in
          <xref linkend="bi9d19d" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Create the <filename>/boot </filename>partition. The devices
          should be a local device.
         </para>
        </entry>
       </row>
       <row id="b13za83a">
        <entry>
         <para>
          <xref linkend="bi9dzpu" xrefstyle="StepXRef"/> in
          <xref linkend="bi9d19d" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Under <guimenu>RAID Type</guimenu>, select <guimenu>RAID 1
          (Mirroring)</guimenu>.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="b13g65gi">
   <title>January 8, 2013</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b13jh2o0" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b13g65z1" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b13jh2o0">
    <title>LVM Configuration</title>
    <informaltable frame="topbot" rowsep="1" pgwide="1">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b13jh2o1">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b13jh2o2">
        <entry>
         <para>
          <xref linkend="sec_yast2_system_lvm_explanation" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <important>
          <para>
           If you add multipath support after you have configured LVM, you
           must modify the <filename>/etc/lvm/lvm.conf</filename> file to
           scan only the multipath device names in the
           <filename>/dev/disk/by-id</filename> directory as described in
           <xref linkend="mpiousinglvm" xrefstyle="SectTitleOnPage"/>, then
           reboot the server.
          </para>
         </important>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="b13g65z1">
    <title>Managing Multipath I/O for Devices</title>
    <informaltable frame="topbot" rowsep="1" pgwide="1">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b13g65z2">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b13jbspx">
        <entry>
         <para>
          <xref linkend="beg2pi0" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Ensure that the configuration settings in the
          <filename>/etc/multipath.conf</filename> file on each node are
          consistent across the cluster.
         </para>
        </entry>
       </row>
       <row id="b13g65z3">
        <entry>
         <para>
          <xref linkend="mpiousingdev" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          When using links to multipath-mapped devices in the
          <filename>/dev/disk/by-id</filename> directory, ensure that you
          specify the <filename>dm-uuid*</filename> name or alias name, and
          not a fixed path instance of the device.
         </para>
        </entry>
       </row>
       <row id="b13gbii2">
        <entry>
         <para>
          <xref linkend="mpiousinglvm" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          To accept both raw disks and partitions for Device Mapper names,
          specify the path as follows, with no hyphen (-) before
          <filename>mpath</filename>:
         </para>
<screen>
filter = [ "a|/dev/disk/by-id/dm-uuid-.*mpath-.*|", "r|.*|" ]
</screen>
        </entry>
       </row>
       <row id="b13gbxz9">
        <entry>
         <para>
          <xref linkend="mpiousingpart" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Ensure that the configuration files for
          <filename>lvm.conf</filename> and <filename>md.conf</filename>
          point to the multipath-device names. This should happen
          automatically if <filename>boot.multipath</filename> is enabled
          and loads before <filename>boot.lvm</filename> and
          <filename>boot.md</filename>. Otherwise, the LVM and MD
          configuration files might contain fixed paths for
          multipath-devices, and you must correct those paths to use the
          multipath-device names.
         </para>
        </entry>
       </row>
       <row id="b13g705m">
        <entry>
         <para>
          <xref linkend="mpionames" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Before you begin, review the requirements in
          <xref linkend="mpiousingdev" xrefstyle="SectTitleOnPage"/>.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="b12irb56">
   <title>November 14, 2012</title>

   <para>
    Updates were made to the following section. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b12o29bv" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b12o29bv">
    <title>Overview of File Systems in Linux</title>
    <informaltable frame="topbot" rowsep="1" pgwide="1">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b12o29bw">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b12r1r9k">
        <entry>
         <para>
          <xref linkend="sec_filesystems_lfs" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section has been updated to be consistent with
          <ulink url="http://www.suse.com/products/server/technical-information/#FileSystem">File
          System Support and Sizes</ulink> on the
          <ulink url="http://www.suse.com/products/server/technical-information/">SUSE
          Linux Enterprise Server Technical Information Web site</ulink>.
         </para>
        </entry>
       </row>
       <row id="b12o29bx">
        <entry>
         <para>
          <xref linkend="sect_stor_limits" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="b12mdifb">
   <title>October 29, 2012</title>

   <para>
    Corrections for front matter and typographical errata.
   </para>
  </sect1>
  <sect1 id="b12doybp">
   <title>October 19, 2012</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b12doybq" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b12doybu" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b12doybq">
    <title>Managing Multipath I/O for Devices</title>
    <informaltable frame="topbot" rowsep="1" pgwide="1">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b12doybr">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b12doybs">
        <entry>
         <para>
          <xref linkend="bbillhs" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Specific examples for configuring devices were moved to a higher
          organization level.
         </para>
        </entry>
       </row>
       <row id="b12doybt">
        <entry>
         <para>
          <xref linkend="mpiohwsupconf" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          It is assumed that the <filename>multipathd</filename> daemon is
          already running with the old (or default) multipath settings when
          you modify the <filename>/etc/multipath.conf</filename> file and
          perform the dry run.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="b12doybu">
    <title>Overview of File Systems in Linux</title>
    <informaltable frame="topbot" rowsep="1" pgwide="1">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b12doybv">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b12doybw">
        <entry>
         <para>
          <xref linkend="sec_fileystems_major_ext3inodesize" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          To allow space for extended attributes and ACLs for a file on Ext3
          file systems, the default inode size for Ext3 was increased from
          128 bytes on SLES 10 to 256 bytes on SLES11.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="b10v707p">
   <title>September 28, 2012</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="b10v71m2" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b10v71m1" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="b10v71m2">
    <title>Managing Multipath I/O for Devices</title>
    <informaltable frame="topbot" rowsep="1" pgwide="1">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b11v3x0i">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b122v4qy">
        <entry>
         <para>
          <xref linkend="b122uvel" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          In SLES 11 SP2, the <literal>prio</literal> keyword is used to
          specify the prioritizer, and the <literal>prio_args</literal>
          keyword is used to specify its argument if the prioritizer
          requires an argument.
         </para>
         <para>
          Multipath Tools 0.4.9 and later uses the <literal>prio</literal>
          setting in the <literal>defaults{}</literal> or
          <literal>devices{}</literal> section of the
          <filename>/etc/multipath.conf</filename> file. It silently ignores
          the keyword <literal>prio</literal> when it is specified for an
          individual <literal>multipath</literal> definition in the
          <literal>multipaths{)</literal> section.
         </para>
        </entry>
       </row>
       <row id="b122xhwg">
        <entry>
         <para>
          <xref linkend="beg263n" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added information about using <literal>prio</literal> and
          <literal>prio_args</literal> keywords.
         </para>
        </entry>
       </row>
       <row id="b122u3zy">
        <entry>
         <para>
          <xref linkend="beg263n" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          In SLES 11 SP2, the rr_min_io multipath attribute is obsoleted and
          replaced by the rr_min_io_rq attribute.
         </para>
        </entry>
       </row>
       <row id="b11v3x0j">
        <entry>
         <para>
          <xref linkend="b11qkzgw" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
       <row id="b122v4qz">
        <entry>
         <para>
          <xref linkend="b122uxzr" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
       <row id="b122v4r0">
        <entry>
         <para>
          <xref linkend="b11qkzgx" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Fixed broken links.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="b10v71m1">
    <title>Overview of File Systems in Linux</title>
    <informaltable frame="topbot" rowsep="1" pgwide="1">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b10v72jy">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b10v72jz">
        <entry>
         <para>
          <xref linkend="bwk8gda" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The Btrfs tools package is <filename>btrfsprogs</filename>.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="bzit5x5">
   <title>April 12, 2012</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bznyukh" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bznyugr" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bznyukh">
    <title>Managing Multipath I/O for Devices</title>
    <para>
     This section was updated to use the
     <filename>/dev/disk/by-id</filename> directory for device paths in all
     examples.
    </para>
   </sect2>

   <sect2 id="bznyugr">
    <title>Resizing Software RAID Arrays with mdadm</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bznyyes">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bznyyet">
        <entry>
         <para>
          <xref linkend="resizedecr" xrefstyle="HeadingOnPage"/>
         </para>
         <para/>
        </entry>
        <entry>
         <para>
          Revised the order of the procedures so that you reduce the size of
          the RAID before you reduce the individual component partition
          sizes.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="bw4b1n8">
   <title>February 27, 2012 (SLES 11 SP2)</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bw4b52h" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwk7nlj" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwkbgyt" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwkgdv2" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bw4b1n9" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="byz8ra8" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwk7y6n" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwk9zeq" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bxuakwj" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwkf9o0" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bw4b52h">
    <title>Fibre Channel Storage over Ethernet Networks: FCoE</title>
    <para>
     This section is new. Open Fibre Channel over Ethernet (OpenFCoE) is
     supported beginning in SLES 11.
    </para>
   </sect2>

   <sect2 id="bwk7nlj">
    <title>GNU Licenses</title>
    <para>
     This section is new.
    </para>
   </sect2>

   <sect2 id="bwkbgyt">
    <title>LVM Configuration</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bwkbgyu">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bwkbgyv">
        <entry>
         <para>
          <xref linkend="lvmtagging" xrefstyle="HeadingOnPage"/>
         </para>
         <para/>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bwkgdv2">
    <title>Managing Access Control Lists over NFSv4</title>
    <para>
     This section is new.
    </para>
   </sect2>

   <sect2 id="bw4b1n9">
    <title>Managing Multipath I/O for Devices</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bw4b1na">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bw4b1nb">
        <entry>
         <para>
          <xref linkend="bbj68de" xrefstyle="HeadingOnPage"/>
         </para>
         <para/>
        </entry>
        <entry>
         <para>
          The default <literal>getuid</literal> path for SLES 11 is
          /lib/udev/scsi_id.
         </para>
        </entry>
       </row>
       <row id="bwk7js1">
        <entry>
         <para>
          <xref linkend="mpioerrormgmt" xrefstyle="HeadingOnPage"/>
         </para>
         <para>
          <xref linkend="mpiostall" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          In the <command>dmsetup message</command> commands, the 0 value
          represents the sector and is used when sector information is not
          needed.
         </para>
        </entry>
       </row>
       <row id="bxu9zlh">
        <entry>
         <para>
          <xref linkend="beg263n" xrefstyle="HeadingOnPage"/>
         </para>
         <para/>
        </entry>
        <entry>
         <para>
          Recommendations were added for the no_path_retry and failback
          settings when multipath I/O is used in a cluster environment.
         </para>
        </entry>
       </row>
       <row id="bxu9zli">
        <entry>
         <para>
          <xref linkend="beg263n" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The path-selector option names and settings were corrected:
         </para>
         <simplelist>
          <member>round-robin 0</member>
          <member>least-pending 0</member>
          <member>service-time 0</member>
          <member>queue-length 0</member>
         </simplelist>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="byz8ra8">
    <title>Managing Software RAIDs 6 and 10 with mdadm</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0" >
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="byz8ra9">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="byz8raa">
        <entry>
         <para>
          <xref linkend="byz81ho" xrefstyle="HeadingOnPage"/>
         </para>
         <para/>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bwk7y6n">
    <title>Mass Storage over IP Networks: iSCSI</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bwk7ylk">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bwkrs5c">
        <entry>
         <para>
          <xref linkend="bwkrqdd" xrefstyle="HeadingOnPage"/>
         </para>
         <para/>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
       <row id="bwk7yll">
        <entry>
         <para>
          <xref linkend="bwk7p6z" xrefstyle="HeadingOnPage"/>
         </para>
         <para/>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bwk9zeq">
    <title>Overview of File Systems on Linux</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bwk9zer">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bwk9zes">
        <entry>
         <para>
          <xref linkend="bwk8gda" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
         <para>
          With SUSE Linux Enterprise 11 SP2, the Btrfs file system is
          supported as root file system, that is, the file system for the
          operating system, across all architectures of SUSE Linux
          Enterprise 11 SP2.
         </para>
        </entry>
       </row>
       <row id="bwkbczk">
        <entry>
         <para>
          <xref linkend="sec_filesystems_major_reiser" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <important>
          <para>
           The ReiserFS file system is fully supported for the lifetime of
           SUSE Linux Enterprise Server 11 specifically for migration
           purposes. SUSE plans to remove support for creating new ReiserFS
           file systems starting with SUSE Linux Enterprise Server 12.
          </para>
         </important>
        </entry>
       </row>
       <row id="bwks4dw">
        <entry>
         <para>
          <xref linkend="bwkryet" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
       <row id="bwkbc4f">
        <entry>
         <para>
          <xref linkend="sec_filesystems_lfs" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The values in this section were updated to current standards.
         </para>
        </entry>
       </row>
       <row id="bwkbkhq">
        <entry>
         <para>
          <xref linkend="bwkbhpd" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bxuakwj">
    <title>Resizing File Systems</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bxuakwk">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bxuakwl">
        <entry>
         <para>
          <xref linkend="biuynjy" xrefstyle="SectTitleOnPage"/>
         </para>
         <para>
          <xref linkend="biuzt6a" xrefstyle="SectTitleOnPage"/>
         </para>
         <para/>
        </entry>
        <entry>
         <para>
          The resize2fs command allows only the Ext3 file system to be
          resized if mounted. The size of an Ext3 volume can be increased or
          decreased when the volume is mounted or unmounted. The Ext2/4 file
          systems must be unmounted for increasing or decreasing the volume
          size.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bwkf9o0">
    <title>Resizing Software RAID Arrays with mdadm</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bwkfa7i">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bwkfa7j">
        <entry>
         <para>
          <xref linkend="resizeincrraid" xrefstyle="HeadingOnPage"/>
         </para>
         <para/>
        </entry>
        <entry>
         <para>
          The <command>--assume-clean</command> option is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="bvqj59s">
   <title>July 12, 2011</title>

   <para>
    Updates were made to the following section. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bvqj7sc" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bvqj7sc">
    <title>Managing Multipath I/O for Devices</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bvqj7sd">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bvqj7se">
        <entry>
         <para>
          <xref linkend="mpiousinglvm" xrefstyle="SectTitleOnPage"/>
         </para>
         <para>
          <xref linkend="mpioraid" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Running <command>dracut</command> is needed only if the root (/)
          device or any parts of it (such as <filename>/var</filename>,
          <filename>/etc</filename>, <filename>/log</filename>) are on the
          SAN and multipath is needed to boot.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="bvc15n0">
   <title>June 14, 2011</title>

   <para>
    Updates were made to the following section. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bvc17t5" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bvc17t8" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bvc17t5">
    <title>Managing Multipath I/O for Devices</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bvc17t6">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bvc17t7">
        <entry>
         <para>
          <xref linkend="beg263n" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The default setting for path_grouping_policy changed from multibus
          to failover in SLES 11.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bvc17t8">
    <title>What’s New for Storage in SLES 11</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bvc17t9">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bvc17ta">
        <entry>
         <para>
          <xref linkend="bvc111p" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="burkc4m">
   <title>May 5, 2011</title>

   <para>
    This release fixes broken links and removes obsolete references.
   </para>
  </sect1>
  <sect1 id="bt5x8xp">
   <title>January 2011</title>

   <para>
    Updates were made to the following section. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bt5yoiu" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bt5wm3n" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bt5ylzd" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bt5yoiu">
    <title>LVM Configuration</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bt5yoiv">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bt5yoiw">
        <entry>
         <para>
          <xref linkend="bgchx8a" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          LVM2 does not restrict the number of physical extents. Having a
          large number of extents has no impact on I/O performance to the
          logical volume, but it slows down the LVM tools.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bt5wm3n">
    <title>Managing Multipath I/O for Devices</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bt5wpqq">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bt5wpqr">
        <entry>
         <para>
          Tuning the Failover for Specific Host Bus Adapters
         </para>
        </entry>
        <entry>
         <para>
          This section was removed. For HBA failover guidance, refer to your
          vendor documentation.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bt5ylzd">
    <title>Resizing File Systems</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bt5ylze">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bt5ylzf">
        <entry>
         <para>
          <xref linkend="resizedecrfs" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Decreasing the size of the file system is supported when the file
          system is unmounted.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="brgqw2y">
   <title>September 16, 2010</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="brgqysi" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="brgqysi">
    <title>LVM Configuration</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="brgqysj">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="brgqysk">
        <entry>
         <para>
          <xref linkend="bi706ct" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The discussion and procedure were expanded to explain how to
          configure a partition that uses the entire disk.
         </para>
         <para>
          The procedure was modified to use the Hard Disk partitioning
          feature in the YaST Partitioner.
         </para>
        </entry>
       </row>
       <row id="brgqysl">
        <entry>
         <para>
          All LVM Management sections
         </para>
        </entry>
        <entry>
         <para>
          Procedures throughout the chapter were modified to use Volume
          Management in the YaST Partitioner.
         </para>
        </entry>
       </row>
       <row id="brgqysm">
        <entry>
         <para>
          <xref linkend="brgpqe2" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
       <row id="brgqysn">
        <entry>
         <para>
          <xref linkend="brgqpfy" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
       <row id="brgqyso">
        <entry>
         <para>
          <xref linkend="brgpmw3" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
       <row id="brgqysp">
        <entry>
         <para>
          <xref linkend="brgob84" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="bq2fgom">
   <title>June 21, 2010</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bq2fjfm" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bq7udwm" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bq2fh2u" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bq2gisd" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bq2fjfm">
    <title>LVM Configuration</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bq2fjfn">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bq2fjfo">
        <entry>
         <para>
          <xref linkend="bi706ct" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Details were added to the procedure.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bq7udwm">
    <title>Managing Multipath I/O</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bq7udwn">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bq7udwo">
        <entry>
         <para>
          <xref linkend="mpionames" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Using user-friendly names for the root device can result in data
          loss. Added alternatives from
          <ulink url="http://www.suse.com/support/kb/doc.php?id=7001133">
          <citetitle>TID 7001133: Recommendations for the usage of
          user_friendly_names in multipath
          configurations</citetitle></ulink>.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bq2fh2u">
    <title>Managing Software RAIDs 6 and 10 with mdadm</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bq2fi7n">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bq2fi7o">
        <entry>
         <para>
          <xref linkend="b7cynnk" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Errata in the example were corrected.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bq2gisd">
    <title>Mass Storage on IP NetWork: iSCSI</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bq2gise">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bq2gisf">
        <entry>
         <para>
          <xref linkend="nofail" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="bndh000">
   <title>May 2010 (SLES 11 SP1)</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bo1piss" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bok8zbg" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bpjchpc" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bndh0qn" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bo1piss">
    <title>Managing Multipath I/O for Devices</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bo1pist">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bo1pisu">
        <entry>
         <para>
          <xref linkend="mpiousinglvm" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The example in <xref linkend="bg89rst" xrefstyle="StepOnPage"/>
          was corrected.
         </para>
        </entry>
       </row>
       <row id="bombwdc">
        <entry>
         <para>
          <xref linkend="bok8cn1" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
       <row id="bothcg7">
        <entry>
         <para>
          <xref linkend="mpiotoolsmpt" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The file list for a package can vary for different server
          architectures. For a list of files included in the multipath-tools
          package, go to the
          <ulink url="http://www.suse.com/products/server/technical-information/"><guimenu>SUSE
          Linux Enterprise Server Technical
          Specifications</guimenu><guimenu>Package Descriptions</guimenu>
          Web page</ulink>, find your architecture and select
          <guimenu>Packages Sorted by Name</guimenu>, then search on
          “multipath-tools” to find the package list for that
          architecture.
         </para>
        </entry>
       </row>
       <row id="bomc4i7">
        <entry>
         <para>
          <xref linkend="mpiosysconfsandevs" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          If the SAN device will be used as the root device on the server,
          modify the timeout settings for the device as described in
          <xref linkend="bok8cn1" xrefstyle="SectTitleOnPage"/>.
         </para>
        </entry>
       </row>
       <row id="bpjp7nk">
        <entry>
         <para>
          <xref linkend="mpiohwsupconf" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added example output for -v3 verbosity.
         </para>
        </entry>
       </row>
       <row id="bomcuqx">
        <entry>
         <para>
          <xref linkend="bomcrff" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
       <row id="bomcuqy">
        <entry>
         <para>
          <xref linkend="bomc64e" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bok8zbg">
    <title>Mass Storage over IP Networks: iSCSI</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bok8zbh">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bok8zbi">
        <entry>
         <para>
          <xref linkend="bojud2n" xrefstyle="StepXRef"/> in
          <xref linkend="sec_inst_system_iscsi_target_yast" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          In the <guimenu>YaST</guimenu><guimenu>Network
          Services</guimenu><guimenu>iSCSI Target</guimenu> function, the
          <guimenu>Save</guimenu> option allows you to export the iSCSI
          target information, which makes it easier to provide this
          information to consumers of the resources.
         </para>
        </entry>
       </row>
       <row id="bp5cfck">
        <entry>
         <para>
          <xref linkend="sec_inst_system_iscsi_ts" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bpjchpc">
    <title>Software RAID Configuration</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bpjchpd">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bpjchpe">
        <entry>
         <para>
          <xref linkend="bgchysh" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The Software RAID HOW-TO has been deprecated. Use the
          <ulink url="https://raid.wiki.kernel.org/index.php/Linux_Raid"><citetitle>Linux
          RAID </citetitle>wiki</ulink> instead.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bndh0qn">
    <title>What’s New</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bndh0qo">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bndh0qp">
        <entry>
         <para>
          <xref linkend="bndgyod" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
<screen>

</screen>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="boj6043">
   <title>February 23, 2010</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="boj6044" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="boja57r" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="boj6044">
    <title>Configuring Software RAID for the Root Partition</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="boj6045">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="boj62jo">
        <entry>
         <para>
          <xref linkend="bi9d2lj" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Corrected an error in the RAID 0 definition..
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="boja57r">
    <title>Managing Multipath I/O</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="boja57s">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="boja57t">
        <entry>
         <para>
          <xref linkend="scandev" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added information about using the
          <filename>rescan-scsi-bus.sh</filename> script to scan for devices
          without rebooting.
         </para>
        </entry>
       </row>
       <row id="boja57u">
        <entry>
         <para>
          <xref linkend="be48i9g" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added information about using the
          <filename>rescan-scsi-bus.sh</filename> script to scan for devices
          without rebooting.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="bmy23qz">
   <title>December 1, 2009</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bmy26at" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bmy2h0k" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bmy26aw" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bmy26at">
    <title>Managing Multipath I/O for Devices</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bmy26au">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bn4z2ku">
        <entry>
         <para>
          <xref linkend="mpiousinglvm" xrefstyle="SectTitleOnPage"/>
         </para>
         <para>
          <xref linkend="mpioraid" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The -f mpath option changed to -f multipath:
         </para>
         <para>
          mkinitrd -f multipath
         </para>
        </entry>
       </row>
       <row id="bmy26av">
        <entry>
         <para>
          <link linkend="b122sbgy">prio_callout</link> in
          <xref linkend="beg263n" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Multipath prio_callout programs are located in shared libraries in
          <filename>/lib/libmultipath/lib*</filename>. By using shared
          libraries, the callout programs are loaded into memory on daemon
          startup.
         </para>
<screen>

</screen>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bmy2h0k">
    <title>Resizing File Systems</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bmy2h0l">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bmy2h0m">
        <entry>
         <para>
          <xref linkend="biuytzx" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The resize2fs utility supports online or offline resizing for the
          ext3 file system.
         </para>
<screen>

</screen>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bmy26aw">
    <title>What’s New</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bmy26ax">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bmy26ay">
        <entry>
         <para>
          <xref linkend="bmy1edt" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
<screen>

</screen>
        </entry>
       </row>
       <row id="bn4z2kv">
        <entry>
         <para>
          <xref linkend="bn4yxd1" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
<screen>

</screen>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="bm18yt7">
   <title>October 20, 2009</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bm18yt8" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bm1n6qa" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bmqj45v" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bm18yt8">
    <title>LVM Configuration</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bm18yt9">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bm18yta">
        <entry>
         <para>
          <xref linkend="sec_yast2_system_lvm_explanation" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          In the YaST Control Center, select <menuchoice>
          <guimenu>System</guimenu><guimenu>Partitioner</guimenu></menuchoice>.
         </para>
<screen>

</screen>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bm1n6qa">
    <title>Managing Multipath I/O for Devices</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bm1n6qb">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bm1n6qc">
        <entry>
         <para>
          <xref linkend="bbj5x7z" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          The keyword devnode_blacklist has been deprecated and replaced
          with the keyword blacklist.
         </para>
        </entry>
       </row>
       <row id="bmqiea8">
        <entry>
         <para>
          <xref linkend="bbj68de" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Changed getuid_callout to getuid.
         </para>
<screen>

</screen>
        </entry>
       </row>
       <row id="bmqicwf">
        <entry>
         <para>
          <xref linkend="beg263n" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Changed getuid_callout to getuid.
         </para>
<screen>

</screen>
        </entry>
       </row>
       <row id="bmqj3nw">
        <entry>
         <para>
          <xref linkend="beg263n" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added descriptions for path_selector of least-pending,
          length-load-balancing, and service-time options.
         </para>
<screen>

</screen>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bmqj45v">
    <title>What’s New</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bmqj45w">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bmqj45x">
        <entry>
         <para>
          <xref linkend="bmqiv07" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
<screen>

</screen>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="bleqdg7">
   <title>August 3, 2009</title>

   <para>
    Updates were made to the following section. The change is explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bleqegm" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bleqegm">
    <title>Managing Multipath I/O</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bleqegn">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bleqego">
        <entry>
         <para>
          <xref linkend="bleqcv0" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
<screen>

</screen>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="bkkotcf">
   <title>June 22, 2009</title>

   <para>
    Updates were made to the following sections. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bkobnh2" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bky5or6" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bkkout0" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bkobnh2">
    <title>Managing Multipath I/O</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bkobnh3">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bky5lwj">
        <entry>
         <para>
          <xref linkend="mpioroot" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Added <xref linkend="bky5jc7" xrefstyle="StepOnPage"/> and
          <xref linkend="bky5j1x" xrefstyle="StepOnPage"/> for System Z.
         </para>
<screen>

</screen>
        </entry>
       </row>
       <row id="bkobnh4">
        <entry>
         <para>
          <xref linkend="be48i9g" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Corrected the syntax for the command lines in Step 2.
         </para>
<screen>

</screen>
        </entry>
       </row>
       <row id="bky51ut">
        <entry>
         <para>
          <xref linkend="be48i9g" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          <xref linkend="befqjhp" xrefstyle="StepOnPage"/> replaces old Step
          7 and Step 8.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bky5or6">
    <title>Managing Software RAIDs 6 and 10 with mdadm</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bky5or7">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bky5or8">
        <entry>
         <para>
          <xref linkend="raidmdadmdegraded" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          To see the rebuild progress while being refreshed every second,
          enter
         </para>
<screen>
watch -n 1 cat /proc/mdstat 
</screen>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>

   <sect2 id="bkkout0">
    <title>Mass Storage over IP Networks: iSCSI</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bkkout1">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bkkout2">
        <entry>
         <para>
          <xref linkend="sec_inst_system_iscsi_initiator_yast" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Re-organized material for clarity.
         </para>
         <para>
          Added information about how to use the settings for the Start-up
          option for iSCSI target devices:
         </para>
         <itemizedlist>
          <listitem>
           <formalpara id="bkobopt">
            <title>Automatic:</title>
            <para>
             This option is used for iSCSI targets that are to be connected
             when the iSCSI service itself starts up. This is the typical
             configuration.
            </para>
           </formalpara>
          </listitem>
          <listitem>
           <formalpara id="bkobopu">
            <title>Onboot:</title>
            <para>
             This option is used for iSCSI targets that are to be connected
             during boot; that is, when root (<filename>/</filename>) is on
             iSCSI. As such, the iSCSI target device will be evaluated from
             the initrd on server boots.
            </para>
           </formalpara>
          </listitem>
         </itemizedlist>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
  <sect1 id="bfmmkat">
   <title>May 21, 2009</title>

   <para>
    Updates were made to the following section. The changes are explained
    below.
   </para>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bfmml01" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bfmml01">
    <title>Managing Multipath I/O</title>
    <informaltable frame="topbot" rowsep="1" pgwide="0">
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="4048*"/>
      <colspec colnum="2" colname="2" colwidth="5953*"/>
      <thead>
       <row id="bfmml02">
        <entry>
         <para>
          Location
         </para>
        </entry>
        <entry>
         <para>
          Change
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bkk3nxd">
        <entry>
         <para>
          <xref linkend="be5s8ae" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Testing of the IBM zSeries device with multipathing has shown that
          the dev_loss_tmo parameter should be set to 90 seconds, and the
          fast_io_fail_tmo parameter should be set to 5 seconds. If you are
          using zSeries devices, you must manually create and configure the
          <filename>/etc/multipath.conf</filename> file to specify the
          values. For information, see
          <xref linkend="bkj8n9w" xrefstyle="HeadingOnPage"/>.
         </para>
        </entry>
       </row>
       <row id="bfmml04">
        <entry>
         <para>
          <xref linkend="mpiotoolsdm" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          Multipathing is supported for the <filename>/boot</filename>
          device in SUSE Linux Enterprise Server 11 and later.
         </para>
        </entry>
       </row>
       <row id="bkj949q">
        <entry>
         <para>
          <xref linkend="bkj8n9w" xrefstyle="HeadingOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          This section is new.
         </para>
        </entry>
       </row>
       <row id="bfmml03">
        <entry>
         <para>
          <xref linkend="mpioroot" xrefstyle="SectTitleOnPage"/>
         </para>
        </entry>
        <entry>
         <para>
          DM-MP is now available and supported for
          <filename>/boot</filename> and <filename>/root</filename> in SUSE
          Linux Enterprise Server 11.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </sect2>
  </sect1>
 </appendix>
</book>
