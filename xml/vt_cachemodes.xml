<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-cachemodes">
 <title>Disk cache modes</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para/>
 <sect1 xml:id="what-is-disk-cache">
  <title>What is a disk cache?</title>

  <para>
   A disk cache is a memory used to speed up the process of storing and
   accessing data from the hard disk. Physical hard disks have their cache
   integrated as a standard feature. For virtual disks, the cache uses
   &vmhost;'s memory and you can fine-tune its behavior, for example, by
   setting its type.
  </para>
 </sect1>
 <sect1 xml:id="how-it-works-disk-cache">
  <title>How does a disk cache work?</title>

  <para>
   Normally, a disk cache stores the most recent and frequently used programs
   and data. When a user or program requests data, the operating system first
   checks the disk cache. If the data is there, the operating system quickly
   delivers the data to the program instead of re-reading the data from the
   disk.
  </para>

  <figure xml:id="fig-caching">
   <title>Caching mechanism</title>
   <mediaobject>
    <imageobject role="html">
     <imagedata fileref="virt-disk-cache.png"/>
    </imageobject>
    <imageobject role="fo">
     <imagedata fileref="virt-disk-cache.png" width="85%"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="benefits-disk-cache">
  <title>Benefits of disk caching</title>

  <para>
   Caching of virtual disk devices affects the overall performance of guest
   machines. You can improve the performance by optimizing the combination of
   cache mode, disk image format, and storage subsystem.
  </para>
 </sect1>
 <sect1 xml:id="cachemodes-descr">
  <title>Virtual disk cache modes</title>

  <para>
   If you do not specify a cache mode, <literal>writeback</literal> is used by
   default. Each guest disk can use one of the following cache modes:
  </para>

  <variablelist>
   <varlistentry xml:id="cache-writeback">
    <term>writeback</term>
    <listitem>
     <para>
      <literal>writeback</literal> uses the host page cache. Writes are
      reported to the guest as completed when they are placed in the host
      cache. Cache management handles commitment to the storage device. The
      guest's virtual storage adapter is informed of the
      <emphasis>writeback</emphasis> cache and therefore expected to send flush
      commands as needed to manage data integrity.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="cache-writethrough">
    <term>writethrough</term>
    <listitem>
     <para>
      Writes are reported as completed only when the data has been committed to
      the storage device. The guest's virtual storage adapter is informed that
      there is no <emphasis>writeback</emphasis> cache, so the guest does not
      need to send flush commands to manage data integrity.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="cache-none">
    <term>none</term>
    <listitem>
     <para>
      The host cache is bypassed, and reads and writes happen directly between
      the hypervisor and the storage device. Because the actual storage device
      may report a write as completed when the data is still placed in its
      write queue only, the guest's virtual storage adapter is informed that
      there is a <emphasis>writeback</emphasis> cache. This mode is equivalent
      to direct access to the host's disk.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="cache-unsafe">
    <term>unsafe</term>
    <listitem>
     <para>
      Similar to the <emphasis>writeback</emphasis> mode, except all flush
      commands from the guests are ignored. Using this mode implies that the
      user prioritizes performance gain over the risk of data loss in case of a
      host failure. This mode can be useful during guest installation, but not
      for production workloads.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="cache-directsync">
    <term>directsync</term>
    <listitem>
     <para>
      Writes are reported as completed only when the data has been committed to
      the storage device and the host cache is bypassed. Similar to
      <emphasis>writethrough</emphasis>, this mode can be useful for guests
      that do not send flushes when needed.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="qemu-cachemodes-data-integrity">
  <title>Cache modes and data integrity</title>

  <variablelist>
   <varlistentry>
    <term>writethrough, none, directsync</term>
    <listitem>
     <para>
      These modes are considered to be safest when the guest operating system
      uses flushes as needed. For unsafe or unstable guests, use
      <emphasis>writethough</emphasis> or <emphasis>directsync</emphasis>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>writeback</term>
    <listitem>
     <para>
      This mode informs the guest of the presence of a write cache, and it
      relies on the guest to send flush commands as needed to maintain data
      integrity within its disk image. This mode exposes the guest to data loss
      in the case of a host failure caused by a gap between the moment a write
      is reported as completed and the time the write being committed to the
      storage device.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>unsafe</term>
    <listitem>
     <para>
      This mode is similar to <emphasis>writeback</emphasis> caching, except
      the guest flush commands are ignored. This means a higher risk of data
      loss caused by host failure.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec-cache-mode-live-migration">
  <title>Cache modes and live migration</title>

  <para>
   The caching of storage data restricts the configurations that support live
   migration. Currently, only <literal>raw</literal> and
   <literal>qcow2</literal> image formats can be used for live migration. If a
   clustered file system is used, all cache modes support live migration.
   Otherwise the only cache mode that supports live migration on read/write
   shared storage is <literal>none</literal>.
  </para>

  <para>
   The &libvirt; management layer includes checks for migration compatibility
   based on several factors. If the guest storage is hosted on a clustered file
   system, is read-only, or is marked shareable, then the cache mode is ignored
   when determining if migration can be allowed. Otherwise &libvirt; will not
   allow migration unless the cache mode is set to <literal>none</literal>.
   However, this restriction can be overridden with the <quote>--unsafe</quote>
   option to the migration APIs, which is also supported by
   <command>virsh</command>. For example
  </para>

<screen>&prompt.user;virsh migrate --live --unsafe</screen>

  <tip>
   <para>
    The cache mode <literal>none</literal> is required for the AIO mode setting
    <literal>native</literal>. If another cache mode is used, the AIO mode is
    silently switched back to the default <literal>threads</literal>.
   </para>
  </tip>
 </sect1>
</chapter>
