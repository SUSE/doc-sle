<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="qemu_host_installation.xml" version="5.0" xml:id="cha-qemu-host">
  <title>设置 KVM VM 主机服务器</title>
  <info>
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
      <dm:bugtracker/>
      <dm:translation>yes</dm:translation>
    </dm:docmanager>
  </info>
  <para>
    本节介绍如何设置并使用 <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise Server</phrase></phrase> <phrase role="productnumber"><phrase os="sles;sled">15 SP5</phrase></phrase> 作为基于 QEMU-KVM 的虚拟机主机。
  </para>
  <tip>
    <title>资源</title>
    <para>
      虚拟 Guest 系统所需的硬件资源与将其安装在物理机上时所需的资源相同。您打算在主机系统上运行的 Guest 越多，需要添加到 VM 主机服务器的硬件资源（CPU、磁盘、内存和网络）就越多。
    </para>
  </tip>
  <sect1 xml:id="kvm-host-cpu">
    <title>CPU 的虚拟化支持</title>

    <para>
      要运行 KVM，您的 CPU 必须支持虚拟化，并且需要在 BIOS 中启用虚拟化。<filename>/proc/cpuinfo</filename> 文件包含有关 CPU 功能的信息。
    </para>

    <para os="sles;sled">
      要确定您的系统是否支持虚拟化，请参见<xref linkend="sec-kvm-requires-hardware"/>。
    </para>
  </sect1>
  <sect1 xml:id="kvm-host-soft">
    <title>所需的软件</title>

    <para>
      需在 KVM 主机上安装多个软件包。要安装所有必要的软件包，请执行以下操作：
    </para>

    <procedure>
      <step>
        <para>
          校验是否已安装 <package>yast2-vm</package> 软件包。此软件包是 YaST 的配置工具，可以简化虚拟化超级管理程序的安装过程。
        </para>
      </step>
      <step>
        <para>
          运行 <menuchoice><guimenu>YaST</guimenu><guimenu>虚拟化</guimenu><guimenu>安装超级管理程序和工具</guimenu></menuchoice>。
        </para>
        <figure os="sles;sled">
          <title>安装 KVM 超级管理程序和工具</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="yast2_hypervisors.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="yast2_hypervisors.png" width="75%"/>
            </imageobject>
          </mediaobject>
        </figure>
        
      </step>
      <step>
        <para>
          选择 <guimenu>KVM 服务器</guimenu>，最好也选择 <guimenu>KVM 工具</guimenu>，然后单击<guimenu>接受</guimenu>确认。
        </para>
      </step>
      <step>
        <para>
          在安装过程中，您可以选择性地让 YaST 自动为您创建<guimenu>网桥</guimenu>。如果您不打算另外为虚拟 Guest 使用一块物理网卡，那么将 Guest 计算机连接到网络的标准方式就是使用网桥。
        </para>
        <figure>
          <title>网桥</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="yast2_netbridge.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="yast2_netbridge.png" width="75%"/>
            </imageobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          安装所有所需的软件包（并激活新网络设置）后，尝试装载适用于您的 CPU 类型的 KVM 内核模块 — <systemitem>kvm_intel</systemitem> 或 <systemitem>kvm_amd</systemitem>：
        </para>
<screen><prompt role="root"># </prompt>modprobe kvm_intel</screen>
        <para>
          检查该模块是否已装载到内存中：
        </para>
<screen><prompt>&gt; </prompt>lsmod | grep kvm
kvm_intel              64835  6
kvm                   411041  1 kvm_intel</screen>
        <para>
          现在，KVM 主机即可为 KVM VM Guest 提供服务。有关更多信息，请参见<xref linkend="cha-qemu-running"/>。
        </para>
      </step>
    </procedure>
  </sect1>
  <sect1 xml:id="kvm-host-virtio">
    <title>特定于 KVM 主机的功能</title>

    <para>
      您可以让基于 KVM 的 VM Guest 充分使用 VM 主机服务器硬件的特定功能（<emphasis>半虚拟化</emphasis>），以此提高这些 Guest 的性能。本节将介绍可以通过哪些方法来使 Guest 直接访问物理主机的硬件（无需模拟层），以充分利用这些硬件。
    </para>

    <tip>
      <para>
        本节中的示例假设读者基本了解 <command>qemu-system-<replaceable>ARCH</replaceable></command> 命令行选项。有关详细信息，请参见<xref linkend="cha-qemu-running"/>。
      </para>
    </tip>

    <sect2 xml:id="kvm-virtio-scsi">
      <title>使用具有 <systemitem>virtio-scsi</systemitem> 的主机储存设备</title>
      <para>
        <systemitem>virtio-scsi</systemitem> 是 KVM 的高级储存堆栈。它取代了以前用于 SCSI 设备直通的 <systemitem>virtio-blk</systemitem> 堆栈。与 <systemitem>virtio-blk</systemitem> 相比，它具有多项优势：
      </para>
      <variablelist>
        <varlistentry>
          <term>提高了可缩放性</term>
          <listitem>
            <para>
              KVM Guest 的 PCI 控制器数量有限，导致挂接的设备数量也受到限制。<systemitem>virtio-scsi</systemitem> 解决了这个限制，因为它可以将多个储存设备组合到单个控制器上。<systemitem>virtio-scsi</systemitem> 控制器上的每个设备以逻辑单元 (<emphasis>LUN</emphasis>) 表示。
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>标准命令集</term>
          <listitem>
            <para>
              <systemitem>virtio-blk</systemitem> 使用 <systemitem>virtio-blk</systemitem> 驱动程序和虚拟机监视器均需知道的一小组命令，因此引入新命令需要同时更新该驱动程序和监视器。
            </para>
            <para>
              相比之下，<systemitem>virtio-scsi</systemitem> 并不定义命令，而是遵循行业标准 SCSI 规范为这些命令定义一个传输协议。此方法与光纤通道、ATAPI 和 USB 设备等其他技术共享。
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>设备命名</term>
          <listitem>
            <para>
              <systemitem>virtio-blk</systemitem> 设备在 Guest 中显示为 <filename>/dev/vd<replaceable>X</replaceable></filename>，这与物理系统中的设备名称不同，可能导致迁移时出现问题。
            </para>
            <para>
              <systemitem>virtio-scsi</systemitem> 会确保设备名称与物理系统上的名称相同，这样便可轻松重新定位虚拟机。
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>SCSI 设备直通</term>
          <listitem>
            <para>
              对于由主机上整个 LUN 提供支持的虚拟磁盘，最好让 Guest 直接向 LUN 发送 SCSI 命令（直通）。此功能在 <systemitem>virtio-blk</systemitem> 中受到限制，因为 Guest 需使用 virtio-blk 协议而不是 SCSI 命令直通，此外，此功能并不适用于 Windows Guest。<systemitem>virtio-scsi</systemitem> 原生消除了这些限制。
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
      <sect3>
        <title><systemitem>virtio-scsi</systemitem> 的用法</title>
        <para>
          KVM 支持对 <systemitem>virtio-scsi-pci</systemitem> 设备使用 SCSI 直通功能：
        </para>
<screen><prompt role="root"># </prompt>qemu-system-x86_64 [...] \
-device virtio-scsi-pci,id=scsi</screen>
      </sect3>
    </sect2>

    <sect2 xml:id="kvm-qemu-vnet">
      <title>使用 <systemitem>vhost-net</systemitem> 实现加速网络</title>
      <para>
        <systemitem>vhost-net</systemitem> 模块用于加速 KVM 的半虚拟化网络驱动程序。它可提供更低的延迟和更高的网络吞吐量。通过以下示例命令行启动 Guest 即可使用 <literal>vhost-net</literal> 驱动程序：
      </para>
<screen><prompt role="root"># </prompt>qemu-system-x86_64 [...] \
-netdev tap,id=guest0,vhost=on,script=no \
-net nic,model=virtio,netdev=guest0,macaddr=00:16:35:AF:94:4B</screen>
      <para>
        <literal>guest0</literal> 是 vhost 驱动的设备的标识字符串。
      </para>
    </sect2>

    <sect2 xml:id="kvm-qemu-multiqueue">
      <title>使用多队列 virtio-net 提升网络性能</title>
      <para>
        QEMU 提供了使用<emphasis>多队列</emphasis>提升网络性能的方式，来应对 VM Guest 中的虚拟 CPU 数量增加的情况。多队列 virtio-net 允许 VM Guest 的虚拟 CPU 并行传输包，因此可以提升网络性能。VM 主机服务器和 VM Guest 端都需要提供多队列支持。
      </para>
      <tip>
        <title>性能优势</title>
        <para>
          多队列 virtio-net 解决方案在以下情况下最有利：
        </para>
        <itemizedlist mark="bullet" spacing="normal">
          <listitem>
            <para>
              网络流量包较大。
            </para>
          </listitem>
          <listitem>
            <para>
              VM Guest 上存在许多同时保持活动状态的连接，这些连接主要是在 Guest 系统之间、Guest 与主机之间，或者 Guest 与外部系统之间建立的。
            </para>
          </listitem>
          <listitem>
            <para>
              活动队列数量等于 VM Guest 中虚拟 CPU 的数量。
            </para>
          </listitem>
        </itemizedlist>
      </tip>
      <note>
        <para>
          尽管多队列 virtio-net 可以增加总网络吞吐量，但由于使用了虚拟 CPU 的计算资源，因此也会增加 CPU 消耗量。
        </para>
      </note>
      <procedure xml:id="kvm-qemu-mq-enable">
        <title>如何启用多队列 virtio-net</title>
        <para>
          以下过程列出了使用 <command>qemu-system-ARCH</command> 启用多队列功能的重要步骤。假设在 VM 主机服务器上设置了一个具有多队列功能（自内核版本 3.8 开始支持此功能）的 tap 网络设备。
        </para>
        <step>
          <para>
            在 <command>qemu-system-ARCH</command> 中，为该 tap 设备启用多队列：
          </para>
<screen>-netdev tap,vhost=on,queues=<replaceable>2*N</replaceable></screen>
          <para>
            其中 <literal>N</literal> 表示队列对的数量。
          </para>
        </step>
        <step>
          <para>
            在 <command>qemu-system-ARCH</command> 中，为 virtio-net-pci 设备启用多队列并指定 MSI-X（消息信号式中断）矢量：
          </para>
<screen>-device virtio-net-pci,mq=on,vectors=<replaceable>2*N+2</replaceable></screen>
          <para>
            在用于计算 MSI-X 矢量数量的公式中：N 个矢量用于 TX（传输）队列，N 个矢量用于 RX（接收）队列，一个矢量用于配置目的，一个矢量用于可能的 VQ（矢量量化）控制。
          </para>
        </step>
        <step>
          <para>
            在 VM Guest 中的相关网络接口（在本示例中为 <literal>eth0</literal>）上启用多队列：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> ethtool -L eth0 combined 2*N</screen>
        </step>
      </procedure>
      <para>
        最终的 <command>qemu-system-ARCH</command> 命令行类似于以下示例：
      </para>
<screen>qemu-system-x86_64 [...] -netdev tap,id=guest0,queues=8,vhost=on \
-device virtio-net-pci,netdev=guest0,mq=on,vectors=10</screen>
      <para>
        请注意，对于命令行中的两个选项，需指定相同的网络设备 <literal>id</literal> (<literal>guest0</literal>)。
      </para>
      <para>
        在运行中的 VM Guest 内部，以 <systemitem class="username">root</systemitem> 特权指定以下命令：
      </para>
<screen><prompt>&gt; </prompt><command>sudo</command> ethtool -L eth0 combined 8</screen>
      <para>
        现在，Guest 系统网络将使用 <command>qemu-system-ARCH</command> 超级管理程序的多队列支持。
      </para>
    </sect2>

    <sect2 xml:id="kvm-vfio">
      <title>VFIO：对设备进行安全的直接访问</title>
      <para>
        将 PCI 设备直接指派到 VM Guest（PCI 直通）可以避免在性能关键型路径中进行任何模拟，从而避免性能问题。VFIO 取代了传统的 KVM PCI 直通设备指派。要使用此功能，VM 主机服务器配置必须符合<xref linkend="ann-vt-io-require"/>中所述的要求。
      </para>
      <para>
        要通过 VFIO 将 PCI 设备指派到 VM Guest，需要确定该设备属于哪个 IOMMU 组。<xref linkend="gloss-vt-acronym-iommu"/>（用于将支持直接内存访问的 I/O 总线连接到主内存的输入/输出内存管理单元）API 支持组表示法。组是可与系统中的所有其他设备相互隔离的一组设备。因此，组是 <xref linkend="vt-io-vfio"/> 使用的所有权单元。
      </para>
      <procedure>
        <title>通过 VFIO 将 PCI 设备指派到 VM Guest</title>
        <step>
          <para>
            标识要指派到 Guest 的主机 PCI 设备。
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> lspci -nn
[...]
00:10.0 Ethernet controller [0200]: Intel Corporation 82576 \
Virtual Function [8086:10ca] (rev 01)
[...]</screen>
          <para>
            记下设备 ID（在本例中为 <literal>00:10.0</literal>）和供应商 ID (<literal>8086:10ca</literal>)。
          </para>
        </step>
        <step>
          <para>
            确定此设备的 IOMMU 组：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> readlink /sys/bus/pci/devices/0000\:00\:10.0/iommu_group
../../../kernel/iommu_groups/20</screen>
          <para>
            此设备的 IOMMU 组为 <literal>20</literal>。现在，您可以检查该设备是否属于同一个 IOMMU 组：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> ls -l /sys/bus/pci/devices/0000\:01\:10.0/iommu_group/devices/
[...] 0000:00:1e.0 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0
[...] 0000:01:10.0 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0/0000:01:10.0
[...] 0000:01:10.1 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0/0000:01:10.1</screen>
        </step>
        <step>
          <para>
            从设备驱动程序取消绑定设备：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> echo "0000:01:10.0" &gt; /sys/bus/pci/devices/0000\:01\:10.0/driver/unbind</screen>
        </step>
        <step>
          <para>
            使用步骤 1 中记下的供应商 ID 将设备绑定到 vfio-pci 驱动程序：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> echo "8086 153a" &gt; /sys/bus/pci/drivers/vfio-pci/new_id</screen>
          <para>
            随即会创建一个新设备 <filename>/dev/vfio/<replaceable>IOMMU_GROUP</replaceable></filename>，在本例中为 <filename>/dev/vfio/20</filename>。
          </para>
        </step>
        <step>
          <para>
            更改新建设备的所有权：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> chown qemu.qemu /dev/vfio/<replaceable>DEVICE</replaceable></screen>
        </step>
        <step>
          <para>
            现在，运行为其指派了 PCI 设备的 VM Guest。
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> qemu-system-<replaceable>ARCH</replaceable> [...] -device
     vfio-pci,host=00:10.0,id=<replaceable>ID</replaceable></screen>
        </step>
      </procedure>
      <important>
        <title>不支持热插拔</title>
        <para>
          从 <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise Server</phrase></phrase> <phrase role="productnumber"><phrase os="sles;sled">15 SP5</phrase></phrase> 开始，不支持通过 VFIO 热插拔传递给 VM Guest 的 PCI 设备。
        </para>
      </important>
      
      <para>
        <filename>/usr/src/linux/Documentation/vfio.txt</filename> 文件中提供了有关 <xref linkend="vt-io-vfio"/> 驱动程序的更详细信息（需要安装软件包 <systemitem>kernel-source</systemitem>）。
      </para>
    </sect2>

    <sect2 xml:id="kvm-qemu-virtfs">
      <title>VirtFS：在主机与 Guest 之间共享目录</title>
      <para>
        VM Guest 通常在单独的计算空间中运行 — 它们各自都有自己的内存范围、专用的 CPU 和文件系统空间。能够共享 VM 主机服务器文件系统的某些部分，就能通过简化相互数据交换来提高虚拟化环境的灵活性。网络文件系统（例如 CIFS 和 NFS）是共享目录的传统方式，但由于它们不是专为虚拟化目的而设计，因此会受制于重大的性能和功能问题。
      </para>
      <para>
        KVM 引入了一种经过优化的新方法，称为 <emphasis>VirtFS</emphasis>（有时称为<quote>文件系统直通</quote>）。VirtFS 使用半虚拟文件系统驱动程序，可以避免将 Guest 应用程序文件系统操作转换为块设备操作，然后又将块设备操作转换为主机文件系统操作。
      </para>
      <para>
        VirtFS 通常可用于以下情况：
      </para>
      <itemizedlist mark="bullet" spacing="normal">
        <listitem>
          <para>
            要从多个 Guest 访问共享目录，或者提供 Guest 到 Guest 的文件系统访问。
          </para>
        </listitem>
        <listitem>
          <para>
            在 Guest 引导过程中，要将虚拟磁盘替换为 Guest 的 RAM 磁盘所要连接到的根文件系统。
          </para>
        </listitem>
        <listitem>
          <para>
            要从云环境中的单个主机文件系统为不同的客户提供储存服务。
          </para>
        </listitem>
      </itemizedlist>
      <sect3 xml:id="kvm-qemu-virtfs-implement">
        <title>实施</title>
        <para>
          在 QEMU 中，可以通过定义两种类型的服务来简化 VirtFS 的实现：
        </para>
        <itemizedlist mark="bullet" spacing="normal">
          <listitem>
            <para>
              用于在主机与 Guest 之间传输协议消息和数据的 <literal>virtio-9p-pci</literal> 设备。
            </para>
          </listitem>
          <listitem>
            <para>
              用于定义导出文件系统属性（例如文件系统类型和安全模型）的 <literal>fsdev</literal> 设备。
            </para>
          </listitem>
        </itemizedlist>
        <example xml:id="ex-qemu-virtfs-host">
          <title>使用 VirtFS 导出主机的文件系统</title>
<screen><prompt>&gt; </prompt><command>sudo</command> qemu-system-x86_64 [...] \
-fsdev local,id=exp1<co xml:id="co-virtfs-host-id"/>,path=/tmp/<co xml:id="co-virtfs-host-path"/>,security_model=mapped<co xml:id="co-virtfs-host-sec-model"/> \
-device virtio-9p-pci,fsdev=exp1<co xml:id="co-virtfs-host-fsdev"/>,mount_tag=v_tmp<co xml:id="co-virtfs-host-mnt-tag"/></screen>
          <calloutlist>
            <callout arearefs="co-virtfs-host-id">
              <para>
                要导出的文件系统的标识。
              </para>
            </callout>
            <callout arearefs="co-virtfs-host-path">
              <para>
                要导出的主机上的文件系统路径。
              </para>
            </callout>
            <callout arearefs="co-virtfs-host-sec-model">
              <para>
                要使用的安全模型 — <literal>mapped</literal> 可使 Guest 文件系统模式和权限与主机相互隔离，而 <literal>none</literal> 会调用<quote>直通</quote>安全模型，其中，对 Guest 文件进行的权限更改也会反映在主机上。
              </para>
            </callout>
            <callout arearefs="co-virtfs-host-fsdev">
              <para>
                前面使用 <literal>-fsdev
                id=</literal> 定义的已导出文件系统 ID。
              </para>
            </callout>
            <callout arearefs="co-virtfs-host-mnt-tag">
              <para>
                稍后要用来在 Guest 上挂载所导出文件系统的挂载标记。
              </para>
            </callout>
          </calloutlist>
          <para>
            可按如下所示在 Guest 上挂载此类导出的文件系统：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> mount -t 9p -o trans=virtio v_tmp /mnt</screen>
          <para>
            其中，<literal>v_tmp</literal> 是前面使用 <literal>-device mount_tag=</literal> 定义的挂载标记，<literal>/mnt</literal> 是要将导出的文件系统挂载到的挂载点。
          </para>
        </example>
      </sect3>
    </sect2>

    <sect2 xml:id="kvm-qemu-ksm">
      <title>KSM：在 Guest 之间共享内存页</title>
      <para>
        内核同页合并 (<xref linkend="gloss-vt-acronym-ksm"/>) 是 Linux 内核的一项功能，可将多个运行中进程的相同内存页合并到一个内存区域中。由于 KVM Guest 在 Linux 中以进程的形式运行，<xref linkend="gloss-vt-acronym-ksm"/> 为超级管理程序提供了内存过量使用功能，以提高内存的使用效率。因此，如果您需要在内存有限的主机上运行多个虚拟机，<xref linkend="gloss-vt-acronym-ksm"/> 可能有所帮助。
      </para>
      <para>
        <xref linkend="gloss-vt-acronym-ksm"/> 将其状态信息存储在 <filename>/sys/kernel/mm/ksm</filename> 目录下的文件中：
      </para>
<screen><prompt>&gt; </prompt>ls -1 /sys/kernel/mm/ksm
full_scans
merge_across_nodes
pages_shared
pages_sharing
pages_to_scan
pages_unshared
pages_volatile
run
sleep_millisecs</screen>
      <para>
        有关 <filename>/sys/kernel/mm/ksm/*</filename> 文件含义的详细信息，请参见 <filename>/usr/src/linux/Documentation/vm/ksm.txt</filename>（软件包 <systemitem>kernel-source</systemitem>）。
      </para>
      <para>
        要使用 <xref linkend="gloss-vt-acronym-ksm"/>，请执行以下操作。
      </para>
      <procedure>
        <step>
          <para>
            尽管 <phrase role="productname"><phrase os="sles">SLES</phrase></phrase> 在内核中包含了 <xref linkend="gloss-vt-acronym-ksm"/> 支持，但其默认处于禁用状态。要启用该支持，请运行以下命令：
          </para>
<screen><prompt role="root"># </prompt>echo 1 &gt; /sys/kernel/mm/ksm/run</screen>
        </step>
        <step>
          <para>
            现在，请在 KVM 中运行多个 VM Guest，并检查 <filename>pages_sharing</filename> 和 <filename>pages_shared</filename> 文件的内容，例如：
          </para>
<screen><prompt>&gt; </prompt>while [ 1 ]; do cat /sys/kernel/mm/ksm/pages_shared; sleep 1; done
13522
13523
13519
13518
13520
13520
13528</screen>
        </step>
      </procedure>
    </sect2>
  </sect1>
</chapter>
