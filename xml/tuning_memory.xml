<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.tuning.memory">
<!-- fs 2015-05-28
     No longer supported: http://bugzilla.suse.com/show_bug.cgi?id=874971

 <sect1 id="cha.tuning.memory.numa">
  <title>Non-Uniform Memory Access (NUMA)</title>

  <para>
   Another increasingly important role of the VM is to provide good NUMA
   allocation strategies. NUMA stands for non-uniform memory access, and
   most of today's multi-socket servers are NUMA machines. NUMA is a
   secondary concern to managing swapping and caches in terms of
   performance, and there are lots of documents about improving NUMA memory
   allocations. One particular parameter interacts with page reclaim:
  </para>

  <variablelist>
   <varlistentry>
    <term><filename>/proc/sys/vm/zone_reclaim_mode</filename>
    </term>
    <listitem>
     <para>
      This parameter controls whether memory reclaim is performed on a local
      NUMA node even if there is plenty of memory free on other nodes. This
      parameter is automatically turned on on machines with more pronounced
      NUMA characteristics.
     </para>
     <para>
      If the VM caches are not being allowed to fill all of memory on a NUMA
      machine, it could be because of zone_reclaim_mode being set. Setting to 0
      will disable this behavior.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
-->
 <title>Tuning the Memory Management Subsystem</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
          </dm:bugtracker>
      </dm:docmanager>
    </info>
    <para>
  To understand and tune the memory management behavior of the
  kernel, it is important to first have an overview of how it works and
  cooperates with other subsystems.
 </para>
 <para>
  <remark>sknorr, 2014-08-21: Using VM for "virtual memory manager" is really
   dangerous - everyone knows that VM means "virtual machine" and half of
   the people reading this chapter might skip this prargraph and be completely
   confused. At least use VMM or shorten "memory management subsystem" to
   "memory subsystem", the latter of which I think should be short enough.
  </remark>
  The memory management subsystem, also called the virtual memory manager,
  will subsequently be called <quote>VM</quote>. The role of the VM
  is to manage the allocation of physical memory (RAM) for the entire kernel
  and user programs. It is also responsible for providing a virtual memory
  environment for user processes (managed via POSIX APIs with Linux
  extensions). Finally, the VM is responsible for freeing up RAM when there
  is a shortage, either by trimming caches or swapping out
  <quote>anonymous</quote> memory.
 </para>
 <para>
  The most important thing to understand when examining and tuning VM is how
  its caches are managed. The basic goal of the VM's caches is to minimize
  the cost of I/O as generated by swapping and file system operations
  (including network file systems). This is achieved by avoiding I/O
  completely, or by submitting I/O in better patterns.
 </para>
 <para>
  Free memory will be used and filled up by these caches as required. The
  more memory is available for caches and anonymous memory, the more
  effectively caches and swapping will operate. However, if a memory
  shortage is encountered, caches will be trimmed or memory will be swapped
  out.
 </para>
 <para>
  For a particular workload, the first thing that can be done to improve
  performance is to increase memory and reduce the frequency that memory
  must be trimmed or swapped. The second thing is to change the way caches
  are managed by changing kernel parameters.
 </para>
 <para>
  Finally, the workload itself should be examined and tuned as well. If an
  application is allowed to run more processes or threads, effectiveness of
  VM caches can be reduced, if each process is operating in its own area of
  the file system. Memory overheads are also increased. If applications
  allocate their own buffers or caches, larger caches will mean that less
  memory is available for VM caches. However, more processes and threads can
  mean more opportunity to overlap and pipeline I/O, and may take better
  advantage of multiple cores. Experimentation will be required for the best
  results.
 </para>
 <sect1 xml:id="cha.tuning.memory.usage">
  <title>Memory Usage</title>

  <para>
   Memory allocations in general can be characterized as
   <quote>pinned</quote> (also known as <quote>unreclaimable</quote>),
   <quote>reclaimable</quote> or <quote>swappable</quote>.
  </para>

  <sect2 xml:id="cha.tuning.memory.usage.anonymous">
   <title>Anonymous Memory</title>
   <para>
    Anonymous memory tends to be program heap and stack memory (for example,
    <literal>&gt;malloc()</literal>). It is reclaimable, except in special
    cases such as <literal>mlock</literal> or if there is no available swap
    space. Anonymous memory must be written to swap before it can be
    reclaimed. Swap I/O (both swapping in and swapping out pages) tends to
    be less efficient than pagecache I/O, because of allocation and access
    patterns.
   </para>
  </sect2>

  <sect2 xml:id="cha.tuning.memory.usage.pagecache">
   <title>Pagecache</title>
   <para>
    A cache of file data. When a file is read from disk or network, the
    contents are stored in pagecache. No disk or network access is required,
    if the contents are up-to-date in pagecache. tmpfs and shared memory
    segments count toward pagecache.
   </para>
   <para>
    When a file is written to, the new data is stored in pagecache before
    being written back to a disk or the network (making it a write-back
    cache). When a page has new data not written back yet, it is called
    <quote>dirty</quote>. Pages not classified as dirty are
    <quote>clean</quote>. Clean pagecache pages can be reclaimed if there is
    a memory shortage by simply freeing them. Dirty pages must first be made
    clean before being reclaimed.
   </para>
  </sect2>

  <sect2 xml:id="cha.tuning.memory.usage.buffercache">
   <title>Buffercache</title>
   <para>
    This is a type of pagecache for block devices (for example, /dev/sda). A
    file system typically uses the buffercache when accessing its on-disk
    metadata structures such as inode tables, allocation bitmaps, and so
    forth. Buffercache can be reclaimed similarly to pagecache.
   </para>
  </sect2>

  <sect2 xml:id="cha.tuning.memory.usage.bufferheads">
   <title>Buffer Heads</title>
   <para>
    Buffer heads are small auxiliary structures that tend to be allocated
    upon pagecache access. They can generally be reclaimed easily when the
    pagecache or buffercache pages are clean.
   </para>
  </sect2>

  <sect2 xml:id="cha.tuning.memory.usage.writeback">
   <title>Writeback</title>
   <para>
    As applications write to files, the pagecache becomes dirty
    and the buffercache may become dirty. When the amount of
    dirty memory reaches a specified number of pages in bytes
    (<emphasis>vm.dirty_background_bytes</emphasis>), or when the
    amount of dirty memory reaches a specific ratio to total memory
    (<emphasis>vm.dirty_background_ratio</emphasis>), or when the pages
    have been dirty for longer than a specified amount of time
    (<emphasis>vm.dirty_expire_centisecs)</emphasis>), the kernel begins
    writeback of pages starting with files that had the pages dirtied first.
    The background bytes and ratios are mutually exclusive and setting one
    will overwrite the other. Flusher threads perform writeback in the
    background and allow applications to continue running. If the I/O
    cannot keep up with applications dirtying pagecache, and dirty data
    reaches a critical setting (<emphasis>vm.dirty_bytes</emphasis> or
    <emphasis>vm.dirty_ratio</emphasis>), then applications begin to be
    throttled to prevent dirty data exceeding this threshold.
   </para>
  </sect2>

  <sect2 xml:id="cha.tuning.memory.usage.readahead">
   <title>Readahead</title>
   <para>
    The VM monitors file access patterns and may attempt to perform
    readahead. Readahead reads pages into the pagecache from the file system
    that have not been requested yet. It is done to allow fewer,
    larger I/O requests to be submitted (more efficient). And for I/O to be
    pipelined (I/O performed at the same time as the application is
    running).
   </para>
  </sect2>

  <sect2 xml:id="cha.tuning.memory.usage.vfs">
   <title>VFS caches</title>
   <para/>
   <sect3 xml:id="cha.tuning.memory.usage.vfs.inode">
    <title>Inode Cache</title>
    <para>
     This is an in-memory cache of the inode structures for each file
     system. These contain attributes such as the file size, permissions and
     ownership, and pointers to the file data.
    </para>
   </sect3>
   <sect3 xml:id="cha.tuning.memory.usage.vfs.dir_entry">
    <title>Directory Entry Cache</title>
    <para>
     This is an in-memory cache of the directory entries in the system.
     These contain a name (the name of a file), the inode which it refers
     to, and children entries. This cache is used when traversing the
     directory structure and accessing a file by name.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="cha.tuning.memory.optimize">
  <title>Reducing Memory Usage</title>

  <para/>

  <sect2 xml:id="cha.tuning.memory.optimize.malloc">
   <title>Reducing malloc (Anonymous) Usage</title>
   <para>
    Applications running on &productname; &productnumber; can allocate
    more memory compared to &productname; 10. This is because of
    <systemitem class="resource">glibc</systemitem> changing its default
    behavior while allocating user space memory. See
    <link xlink:href="http://www.gnu.org/s/libc/manual/html_node/Malloc-Tunable-Parameters.html"/>
    for explanation of these parameters.
   </para>
   <para>
    To restore a &productname; 10-like behavior, M_MMAP_THRESHOLD should
    be set to 128*1024. This can be done with mallopt() call from the
    application, or via setting MALLOC_MMAP_THRESHOLD environment variable
    before running the application.
   </para>
  </sect2>

  <sect2 xml:id="cha.tuning.memory.optimize.overhead">
   <title>Reducing Kernel Memory Overheads</title>
   <para>
    Kernel memory that is reclaimable (caches, described above) will be
    trimmed automatically during memory shortages. Most other kernel memory
    cannot be easily reduced but is a property of the workload given to the
    kernel.
   </para>
   <para>
    Reducing the requirements of the user space workload will reduce the
    kernel memory usage (fewer processes, fewer open files and sockets,
    etc.)
   </para>
  </sect2>

  <sect2 xml:id="cha.tuning.memory.optimize.cgoups">
   <title>Memory Controller (Memory Cgroups)</title>
   <para>
    If the memory cgroups feature is not needed, it can be switched off by
    passing cgroup_disable=memory on the kernel command line, reducing
    memory consumption of the kernel a bit. There is also a slight
    performance benefit as there is a small amount of accounting overhead
    when memory cgroups are available even if none are configured.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="cha.tuning.memory.vm">
  <title>Virtual Memory Manager (VM) Tunable Parameters</title>

  <para>
   When tuning the VM it should be understood that some  changes will
   take time to affect the workload and take full effect. If the workload
   changes throughout the day, it may behave very differently at different
   times. A change that increases throughput under some conditions may
   decrease it under other conditions.
  </para>

  <sect2 xml:id="cha.tuning.memory.vm.reclaim">
   <title>Reclaim Ratios</title>
   <variablelist>
    <varlistentry>
     <term><filename>/proc/sys/vm/swappiness</filename>
     </term>
     <listitem>
      <para>
       This control is used to define how aggressively the kernel swaps out
       anonymous memory relative to pagecache and other caches. Increasing
       the value increases the amount of swapping. The default value is
       <literal>60</literal>.
      </para>
      <para>
       Swap I/O tends to be much less efficient than other I/O. However,
       some pagecache pages will be accessed much more frequently than less
       used anonymous memory. The right balance should be found here.
      </para>
      <para>
       If swap activity is observed during slowdowns, it may be worth
       reducing this parameter. If there is a lot of I/O activity and the
       amount of pagecache in the system is rather small, or if there are
       large dormant applications running, increasing this value might
       improve performance.
      </para>
      <para>
       Note that the more data is swapped out, the longer the system will
       take to swap data back in when it is needed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/vfs_cache_pressure</filename>
     </term>
     <listitem>
      <para>
       This variable controls the tendency of the kernel to reclaim the
       memory which is used for caching of VFS caches, versus pagecache and
       swap. Increasing this value increases the rate at which VFS caches
       are reclaimed.
      </para>
      <para>
       It is difficult to know when this should be changed, other than by
       experimentation. The <command>slabtop</command> command (part of the
       package <systemitem class="resource">procps</systemitem>) shows top
       memory objects used by the kernel. The vfs caches are the "dentry"
       and the "*_inode_cache" objects. If these are consuming a large
       amount of memory in relation to pagecache, it may be worth trying to
       increase pressure. Could also help to reduce swapping. The default
       value is <literal>100</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/min_free_kbytes</filename>
     </term>
     <listitem>
      <para>
       This controls the amount of memory that is kept free for use by
       special reserves including <quote>atomic</quote> allocations (those
       which cannot wait for reclaim). This should not normally be lowered
       unless the system is being very carefully tuned for memory usage
       (normally useful for embedded rather than server applications). If
       <quote>page allocation failure</quote> messages and stack traces are
       frequently seen in logs, min_free_kbytes could be increased until the
       errors disappear. There is no need for concern, if these messages are
       very infrequent. The default value depends on the amount of RAM.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/watermark_scale_factor</filename></term>
     <listitem>
      <para>
      Broadly speaking, free memory has high, low and min watermarks. When
      the low watermark is reached then <command>kswapd</command> wakes to
      reclaim memory in the background. It stays awake until free memory
      reaches the high watermark. Applications will stall and reclaim
      memory when the low watermark is reached.
      </para>
      <para>
      The <literal>watermark_scale_factor</literal> defines the amount
      of memory left in a node/system before kswapd is woken up and how
      much memory needs to be free before kswapd goes back to sleep.
      The unit is in fractions of 10,000. The default value of 10 means
      the distances between watermarks are 0.1% of the available memory
      in the node/system. The maximum value is 1000, or 10% of memory.
      </para>
      <para>
      Workloads that frequently stall in direct reclaim, accounted by
      <literal>allocstall</literal> in <filename>/proc/vmstat</filename>,
      may benefit from altering this parameter. Similarly, if
      <command>kswapd</command> is sleeping prematurely, as accounted for by
      <literal>kswapd_low_wmark_hit_quickly</literal>, then it may indicate
      that the number of pages kept free to avoid stalls is too low.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cha.tuning.memory.vm.writeback">
   <title>Writeback Parameters</title>
   <para>
    One important change in writeback behavior since &productname; 10 is
    that modification to file-backed mmap() memory is accounted immediately
    as dirty memory (and subject to writeback). Whereas previously it would
    only be subject to writeback after it was unmapped, upon an msync()
    system call, or under heavy memory pressure.
   </para>
   <para>
    Some applications do not expect mmap modifications to be subject to such
    writeback behavior, and performance can be reduced. Berkeley DB (and
    applications using it) is one known example that can cause problems.
    Increasing writeback ratios and times can improve this type of slowdown.
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>/proc/sys/vm/dirty_background_ratio</filename>
     </term>
     <listitem>
      <para>
       This is the percentage of the total amount of free and reclaimable
       memory. When the amount of dirty pagecache exceeds this percentage,
       writeback threads start writing back dirty memory. The default value
       is <literal>10</literal> (%).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/dirty_background_bytes</filename>
     </term>
     <listitem>
      <para>
       This contains the amount of dirty memory at which
       the background kernel flusher threads will start writeback.
       <filename>dirty_background_bytes</filename> is the counterpart of
       <filename>dirty_background_ratio</filename>. If one of them is set,
       the other one will automatically be read as <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/dirty_ratio</filename>
     </term>
     <listitem>
      <para>
       Similar percentage value as for
       <filename>dirty_background_ratio</filename>. When this is exceeded,
       applications that want to write to the pagecache are blocked and
       wait for kernel background flusher threads to reduce amount of dirty
       memory. The default value is <literal>20</literal> (%).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/dirty_bytes</filename>
     </term>
     <listitem>
      <para>
       This file controls the same tunable as <filename>dirty_ratio</filename>
       however the amount of dirty memory is in bytes as opposed to a
       percentage of reclaimable memory. Since both
       <filename>dirty_ratio</filename> and <filename>dirty_bytes</filename>
       control the same tunable, if one of them is set, the other one will
       automatically be read as <literal>0</literal>. The minimum value allowed
       for <filename>dirty_bytes</filename> is two pages (in bytes); any value
       lower than this limit will be ignored and the old configuration will be
       retained.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/proc/sys/vm/dirty_expire_centisecs</filename>
     </term>
     <listitem>
      <para>
       Data which has been dirty in-memory for longer than this interval
       will be written out next time a flusher thread wakes up. Expiration
       is measured based on the modification time of a file's inode.
       Therefore, multiple dirtied pages from the same file will all be
       written when the interval is exceeded.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    <filename>dirty_background_ratio</filename> and
    <filename>dirty_ratio</filename> together determine the pagecache
    writeback behavior. If these values are increased, more dirty memory is
    kept in the system for a longer time. With more dirty memory allowed in
    the system, the chance to improve throughput by avoiding writeback I/O
    and to submitting more optimal I/O patterns increases. However, more
    dirty memory can either harm latency when memory needs to be reclaimed
    or at points of data integrity (<quote>synchronization points</quote>) when it
    needs to be written back to disk.
   </para>
  </sect2>

  <sect2 xml:id="cha.tuning.memory.vm.writetiming">
   <title>Timing Differences of I/O Writes between &sle; 12 and &sle; 11</title>
   <para>
    The system is required to limit what percentage of the system's memory
    contains file-backed data that needs writing to disk. This guarantees
    that the system can always allocate the necessary data structures to
    complete I/O. The maximum amount of memory that may be dirty and
    requires writing at any given time is controlled by
    <literal>vm.dirty_ratio</literal>
    (<filename>/proc/sys/vm/dirty_ratio</filename>). The defaults are:
   </para>
<screen>SLE-11-SP3:     vm.dirty_ratio = 40
SLE-12:         vm.dirty_ratio = 20</screen>
   <para>
    The primary advantage of using the lower ratio in &sle; 12 is that
    page reclamation and allocation in low memory situations completes
    faster as there is a higher probability that old clean pages will be
    quickly found and discarded. The secondary advantage is that if all
    data on the system must be synchronized, then the time to complete the
    operation on &sle; 12 will be lower than &sle; 11 SP3 by default.
    Most workloads will not notice this change as data is synchronized with
    <literal>fsync()</literal> by the application or data is not dirtied
    quickly enough to hit the limits.
   </para>
   <para>
    There are exceptions and if your application is affected by this, it
    will manifest as an unexpected stall during writes. To prove it is
    affected by dirty data rate limiting then monitor
    <literal>/proc/<replaceable>PID_OF_APPLICATION</replaceable>/stack</literal>
    and it will be observed that the application spends significant time in
    <literal>balance_dirty_pages_ratelimited</literal>. If this is observed
    and it is a problem, then increase the value of
    <literal>vm.dirty_ratio</literal> to 40 to restore the &sle; 11 SP3
    behavior.
   </para>
   <para>
    It is important to note that the overall I/O throughput is the same
    regardless of the setting. The only difference is the timing of when
    the I/O is queued.
   </para>
   <para>
    This is an example of using <command>dd</command> to asynchronously
    write 30% of memory to disk which would happen to be affected by the
    change in <literal>vm.dirty_ratio</literal>:
   </para>
<screen>&prompt.root;MEMTOTAL_MBYTES=`free -m | grep Mem: | awk '{print $2}'`
&prompt.root;sysctl vm.dirty_ratio=40
&prompt.root;dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100))
2507145216 bytes (2.5 GB) copied, 8.00153 s, 313 MB/s
&prompt.root;sysctl vm.dirty_ratio=20
dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100))
2507145216 bytes (2.5 GB) copied, 10.1593 s, 247 MB/s</screen>
   <para>
    Note that the parameter affects the time it takes for the command to
    complete and the apparent write speed of the device. With
    <literal>dirty_ratio=40</literal>, more of the data is cached and
    written to disk in the background by the kernel. It is very important
    to note that the speed of I/O is identical in both cases. To
    demonstrate, this is the result when <command>dd</command> synchronizes
    the data before exiting:
   </para>
<screen>&prompt.root;sysctl vm.dirty_ratio=40
&prompt.root;dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync
2507145216 bytes (2.5 GB) copied, 21.0663 s, 119 MB/s
&prompt.root;sysctl vm.dirty_ratio=20
&prompt.root;dd if=/dev/zero of=zerofile ibs=1048576 count=$((MEMTOTAL_MBYTES*30/100)) conv=fdatasync
2507145216 bytes (2.5 GB) copied, 21.7286 s, 115 MB/s</screen>
   <para>
    Note that <literal>dirty_ratio</literal> had almost no impact here and
    is within the natural variability of a command. Hence,
    <literal>dirty_ratio</literal> does not directly impact I/O performance
    but it may affect the apparent performance of a workload that writes
    data asynchronously without synchronizing.
   </para>
  </sect2>

  <sect2 xml:id="cha.tuning.memory.vm.readahead">
   <title>Readahead Parameters</title>
   <variablelist>
    <varlistentry>
     <term><filename>/sys/block/<replaceable>&lt;bdev&gt;</replaceable>/queue/read_ahead_kb</filename>
     </term>
     <listitem>
      <para>
       If one or more processes are sequentially reading a file, the kernel
       reads some data in advance (ahead) to reduce the amount of
       time that processes need to wait for data to be available. The actual
       amount of data being read in advance is computed dynamically, based
       on how much "sequential" the I/O seems to be. This parameter sets the
       maximum amount of data that the kernel reads ahead for a single file.
       If you observe that large sequential reads from a file are not fast
       enough, you can try increasing this value. Increasing it too far may
       result in readahead thrashing where pagecache used for readahead is
       reclaimed before it can be used, or slowdowns because of a large
       amount of useless I/O. The default value is <literal>512</literal>
       (KB).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cha.tuning.memory.vm.thp">
   <title>Transparent Huge Page Parameters</title>
    <para>
     Transparent Huge Pages (THP) provide a way to dynamically allocate huge
     pages either on-demand by the process or deferring the allocation until
     later via the <command>khugepaged</command> kernel thread. This method
     is distinct from the use of <literal>hugetlbfs</literal> to manually
     manage their allocation and use. Workloads with contiguous memory
     access patterns can benefit greatly from THP. A 1000 fold decrease
     in page faults can be observed when running synthetic workloads with
     contiguous memory access patterns.
   </para>
   <para>
     There are cases when THP may be undesirable. Workloads with sparse
     memory access patterns may perform poorly with THP due to excessive
     memory usage. For example, 2 MB of memory may be used at fault time
     instead of 4 KB for each fault and ultimately lead to premature page
     reclaim. On releases older then &sle; 12 SP2, it was possible for an
     application to stall for long periods of time trying to allocate a
     THP which frequently led to a recommendation of disabling THP. Such
     recommendations should be re-evaluated for &sle; 12 SP2.
   </para>
   <para>
     The behaviour of THP may be configured via the
     <option>transparent_hugepage=</option> kernel parameter or via
     sysfs. For example, it may be disabled by adding the kernel parameter
     <option>transparent_hugepage=never</option>, rebuild your grub2
     configuration, and reboot. Verify if THP is disabled with:

    </para>
    <screen>&prompt.root;cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]</screen>
     <para>
      If disabled, the value <literal>never</literal> is shown
      in square brackets like in the example above. A value of
      <literal>always</literal> will always try and use THP at fault
      time but defer to <command>khugepaged</command> if the allocation
      fails. A value of <literal>madvise</literal> will only allocate THP
      for address spaces explicitly specified by an application.
     </para>
     <variablelist>
      <varlistentry>
       <term><filename>/sys/kernel/mm/transparent_hugepage/defrag</filename>
       </term>
       <listitem>
        <para>
         This parameter controls how much effort an application commits
         when allocating a THP. A value of <literal>always</literal> is
         the default for &sle; 12 SP1 and earlier releases that supported
         THP. If a THP is not available, the application will try to
         defragment memory. It potentially incurs large stalls in an
         application if the memory is fragmented and a THP is not available.
        </para>
        <para>
         A value of <literal>madvise</literal> means that THP allocation
         requests will only defragment if the application explicitly requests
	 it. This is the default for &sle; 12 SP2.
        </para>
        <para>
         <literal>defer</literal> is only available on &sle; 12 SP2 and later
         releases. If a THP is not available, the application will
         fallback to using small pages if a THP is not available. It will
	 wake the <command>kswapd</command> and <command>kcompactd</command>
         kernel threads to defragment memory in the background and a THP
         will be allocated later <command>khugepaged</command>.
       </para>
       <para>
         The final option <literal>never</literal> will use small pages if
	 a THP is unavailable but no other action will take place.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
  </sect2>
  <sect2 xml:id="cha.tuning.memory.vm.khugepaged">
   <title>khugepaged Parameters</title>
   <para>
    khugepaged will be automatically started when
    <literal>transparent_hugepage</literal> is set to
    <literal>always</literal> or <literal>madvise</literal>, and it'll be
    automatically shutdown if it's set to <literal>never</literal>. Normally
    this runs at low frequency but the behaviour can be tuned.
   </para>
   <variablelist>
     <varlistentry>
      <term><filename>/sys/kernel/mm/transparent_hugepage/khugepaged/defrag</filename> </term>
      <listitem>
       <para>
        A value of 0 will disable <command>khugepaged</command> even though
        THP may still be used at fault time. This may be important for
        latency-sensitive applications that benefit from THP but cannot
        tolerate a stall if <command>khugepaged</command> tries update an
        applications memory usage.
       </para>
      </listitem>
     </varlistentry>

     <varlistentry>
      <term><filename>/sys/kernel/mm/transparent_hugepage/khugepaged/pages_to_scan</filename></term>
      <listitem>
      <para>
        This parameter controls how many pages are scanned by
        <command>khugepaged</command> in a single pass. A scan identifies
        small pages that can be reallocated as THP. Increasing this value
         will allocate THP in the background faster at the cost of CPU
         usage.
       </para>
      </listitem>
    </varlistentry>

     <varlistentry>
      <term><filename>/sys/kernel/mm/transparent_hugepage/khugepaged/scan_sleep_millisecs</filename></term>
      <listitem>
      <para>
       <command>khugepaged</command> sleeps for a short interval specified
       by this parameter after each pass to limit how much CPU usage is
       used. Reducing this value will allocate THP in the background faster
       at the cost of CPU usage. A value of 0 will force continual scanning.
       </para>
      </listitem>
     </varlistentry>

     <varlistentry>
      <term><filename>/sys/kernel/mm/transparent_hugepage/khugepaged/alloc_sleep_millisecs</filename>
      </term>
      <listitem>
       <para>
         This parameter controls how long <command>khugepaged</command> will
         sleep in the event it fails to allocate a THP in the background waiting
         for <command>kswapd</command> and <command>kcompactd</command> to
         take action.
       </para>
      </listitem>
    </varlistentry>
   </variablelist>

   <para>
     The remaining parameters for <command>khugepaged</command> are rarely
     useful for performance tuning but are fully documented in
     <filename>/usr/src/linux/Documentation/vm/transhuge.txt</filename>
   </para>
  </sect2>
  <sect2 xml:id="cha.tuning.memory.vm.more">
   <title>Further VM Parameters</title>
   <para>
    For the complete list of the VM tunable parameters, see
    <filename>/usr/src/linux/Documentation/sysctl/vm.txt</filename>
    (available after having installed the
    <systemitem class="resource">kernel-source</systemitem> package).
   </para>
  </sect2>
 </sect1>
<!-- fs 2015-05-28
     No longer supported: http://bugzilla.suse.com/show_bug.cgi?id=874971

 <sect1 id="cha.tuning.memory.numa">
  <title>Non-Uniform Memory Access (NUMA)</title>

  <para>
   Another increasingly important role of the VM is to provide good NUMA
   allocation strategies. NUMA stands for non-uniform memory access, and
   most of today's multi-socket servers are NUMA machines. NUMA is a
   secondary concern to managing swapping and caches in terms of
   performance, and there are lots of documents about improving NUMA memory
   allocations. One particular parameter interacts with page reclaim:
  </para>

  <variablelist>
   <varlistentry>
    <term><filename>/proc/sys/vm/zone_reclaim_mode</filename>
    </term>
    <listitem>
     <para>
      This parameter controls whether memory reclaim is performed on a local
      NUMA node even if there is plenty of memory free on other nodes. This
      parameter is automatically turned on on machines with more pronounced
      NUMA characteristics.
     </para>
     <para>
      If the VM caches are not being allowed to fill all of memory on a NUMA
      machine, it could be because of zone_reclaim_mode being set. Setting to 0
      will disable this behavior.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
-->
 <sect1 xml:id="cha.tuning.memory.monitoring">
  <title>Monitoring VM Behavior</title>

  <para>
   Some simple tools that can help monitor VM behavior:
  </para>

  <orderedlist spacing="normal">
   <listitem>
    <para>
     vmstat: This tool gives a good overview of what the VM is doing. See
     <xref linkend="sec.util.multi.vmstat"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     <filename>/proc/meminfo</filename>: This file gives a detailed
     breakdown of where memory is being used. See
     <xref linkend="sec.util.memory.meminfo"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     <command>slabtop</command>: This tool provides detailed information
     about kernel slab memory usage. buffer_head, dentry, inode_cache,
     ext3_inode_cache, etc. are the major caches. This command is available
     with the package <systemitem class="resource">procps</systemitem>.
    </para>
   </listitem>
   <listitem>
    <para>
    <filename>/proc/vmstat</filename>: This file gives a detailed
    breakdown of internal VM behaviour. The information contained within
    is implementation specific and may not always be available.  Some of
    the information is duplicated in <filename>/proc/meminfo</filename>
    and others can be presented in a friendly fashion by utilties. For
    maximum utility, this file needs to be monitored over time to observe
    rates of change. The most important pieces of information that are
    hard to derive from other sources are as follows;
    </para>
    <variablelist>
     <varlistentry>
      <term><literal>pgscan_kswapd_*, pgsteal_kswapd_*</literal></term>
      <listitem>
       <para>
        These report respectively the number of pages scanned and reclaimed
        by <command>kswapd</command> since the system started. The ratio
        between these values can be interpreted as the reclaim efficiency
        with a low efficiency implying that the system is struggling to
        reclaim memory and may be thrashing. Light activity here is
        generally not something to be concerned with.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>pgscan_direct_*, pgsteal_direct_*</literal></term>
      <listitem>
        <para>
         These report respectively the number of pages scanned and
         reclaimed by an application directly. It is correlated with
         increases in the <literal>allocstall</literal> counter. This is
         more serious than <command>kswapd</command> activity as these
         events indicate that processes are stalling. Heavy activity
         here combined with <command>kswapd</command> and high rates of
         <literal>pgpgin</literal>, <literal>pgpout</literal> and/or high
         rates of <literal>pswapin</literal> or <literal>pswpout</literal>
         are signs that a system is thrashing heavily.
        </para>
        <para>
         More detailed information can be obtained using tracepoints.
        </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>thp_fault_alloc, thp_fault_fallback</literal></term>
      <listitem>
       <para>
        These counters correspond to how many THPs were allocated directly
        by an application and how many times a THP was not available and
        small pages were used. Generally a high fallback rate is harmless
        unless the application is very sensitive to TLB pressure.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>thp_collapse_alloc, thp_collapse_alloc_failed</literal></term>
      <listitem>
       <para>
        These counters correspond to how many THPs were allocated by
        <command>khugepaged</command> and how many times a THP was not
        available and small pages were used. A high fallback rate implies
        that the system is fragmented and THPs are not being used even
        when the memory usage by applications would allow them. It is
        only a problem for applications that are sensitive to TLB pressure.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>compact_*_scanned, compact_stall, compact_fail,
      compact_success</literal></term> <listitem>
       <para>
        These counters may increase when THP is enabled and the system is
        fragmented. <literal>compact_stall</literal> is incremented when
        an application stalls allocating THP.  The remaining counters
        account for pages scanned, the number of defragmentation events
        that succeeded or failed.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </listitem>
  </orderedlist>
 </sect1>
</chapter>
