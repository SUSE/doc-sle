<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="libvirt_configuration_virsh.xml" version="5.0" xml:id="cha-libvirt-config-virsh">
  <title>Configuring virtual machines with &virsh;</title>
  <info>
    <abstract>
      <para>
        You can use &virsh; to configure virtual machines (VM) on the command
        line as an alternative to using the &vmm;. With &virsh;, you can
        control the state of a VM, edit the configuration of a VM or even
        migrate a VM to another host. The following sections describe how to
        manage VMs by using &virsh;.
      </para>
    </abstract>
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
      <dm:bugtracker/>
      <dm:translation>yes</dm:translation>
    </dm:docmanager>
  </info>
  <sect1 xml:id="sec-libvirt-config-editing-virsh">
    <title>Editing the VM configuration</title>

    <para>
      The configuration of a VM is stored in an XML file in
      <filename>/etc/libvirt/qemu/</filename> and looks like this:
    </para>

    <example>
      <title>Example XML configuration file</title>
<screen>
&lt;domain type='kvm'&gt;
  &lt;name&gt;sles15&lt;/name&gt;
  &lt;uuid&gt;ab953e2f-9d16-4955-bb43-1178230ee625&lt;/uuid&gt;
  &lt;memory unit='KiB'&gt;2097152&lt;/memory&gt;
  &lt;currentMemory unit='KiB'&gt;2097152&lt;/currentMemory&gt;
  &lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;
  &lt;os&gt;
    &lt;type arch='x86_64' machine='pc-q35-2.0'&gt;hvm&lt;/type&gt;
  &lt;/os&gt;
  &lt;features&gt;...&lt;/features&gt;
  &lt;cpu mode='custom' match='exact' check='partial'&gt;
    &lt;model fallback='allow'&gt;Skylake-Client-IBRS&lt;/model&gt;
  &lt;/cpu&gt;
  &lt;clock&gt;...&lt;/clock&gt;
  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;
  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;
  &lt;on_crash&gt;destroy&lt;/on_crash&gt;
  &lt;pm&gt;
    &lt;suspend-to-mem enabled='no'/&gt;
    &lt;suspend-to-disk enabled='no'/&gt;
  &lt;/pm&gt;
  &lt;devices&gt;
    &lt;emulator&gt;/usr/bin/qemu-system-x86_64&lt;/emulator&gt;
    &lt;disk type='file' device='disk'&gt;...&lt;/disk&gt;
  &lt;/devices&gt;
  ...
&lt;/domain&gt;
</screen>
    </example>

    <para>
      To edit the configuration of a &vmguest;, check if it is offline:
    </para>

<screen>&prompt.sudo;<command>virsh list --inactive</command></screen>

    <para>
      If your &vmguest; is in this list, you can safely edit its configuration:
    </para>

<screen>&prompt.sudo;<command>virsh edit <replaceable>NAME_OF_VM_GUEST</replaceable></command>
    </screen>

    <para>
      Before saving the changes, &virsh; validates your input against a RelaxNG
      schema.
    </para>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-machinetype-virsh">
    <title>Changing the machine type</title>

    <para>
      When installing with the <command>virt-install</command> tool, the
      machine type for a &vmguest; is <emphasis>pc-q35</emphasis> by default.
      The machine type is stored in the &vmguest;'s configuration file in the
      <tag>type</tag> element:
    </para>

<screen>&lt;type arch='x86_64' machine='pc-q35-2.3'&gt;hvm&lt;/type&gt;</screen>

    <para>
      As an example, the following procedure shows how to change this value to
      the machine type <literal>q35</literal>. The value <literal>q35</literal>
      is an Intel* chipset and includes
      <xref linkend="gloss-vt-acronym-pcie"/>, supports up to 12 USB ports, and
      has support for <xref linkend="gloss-vt-acronym-sata"/> and
      <xref linkend="gloss-vt-acronym-iommu"/>.
      <!-- IRQ routing has also
        been improved. -->
    </para>

    <procedure>
      <title>Changing machine type</title>
      <step>
        <para>
          Check whether your &vmguest; is inactive:
        </para>
<screen>&prompt.sudo;<command>virsh list --inactive</command>
Id    Name                           State
----------------------------------------------------
-     sles15                         shut off</screen>
      </step>
      <step>
        <para>
          Edit the configuration for this &vmguest;:
        </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
      </step>
      <step>
        <para>
          Replace the value of the <tag class="attribute">machine</tag>
          attribute with <tag class="attvalue">pc-q35-2.0</tag> :
        </para>
<screen>&lt;type arch='x86_64' machine='pc-q35-2.0'&gt;hvm&lt;/type&gt;</screen>
      </step>
      <step>
        <para>
          Restart the &vmguest;:
        </para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
      </step>
      <step>
        <para>
          Check if the machine type has changed. Log in to the &vmguest; and
          run the following command:
        </para>
<screen>&prompt.sudo;<command>dmidecode | grep Product</command>
Product Name: Standard PC (Q35 + ICH9, 2009)</screen>
      </step>
    </procedure>

    <tip>
      <title>Machine type update recommendations</title>
      <para>
        Whenever the QEMU version on the host system is upgraded (for example,
        when upgrading the &vmhost; to a new service pack), upgrade the machine
        type of the &vmguest;s to the latest available version. To check, use
        the command <command>qemu-system-x86_64 -M help</command> on the
        &vmhost;.
      </para>
      <para>
        The default machine type <literal>pc-i440fx</literal>, for example, is
        regularly updated. If your &vmguest; still runs with a machine type of
        <literal>pc-i440fx-1.<replaceable>X</replaceable></literal>, we
        strongly recommend an update to
        <literal>pc-i440fx-2.<replaceable>X</replaceable></literal>. This
        allows taking advantage of the most recent updates and corrections in
        machine definitions, and ensures better future compatibility.
      </para>
    </tip>
  </sect1>
  <sect1 xml:id="sec-libvirt-hypervisor-features-virsh">
    <title>Configuring hypervisor features</title>

    <para>
      <command>libvirt</command> automatically enables a default set of
      hypervisor features that are sufficient in most circumstances, but also
      allows enabling and disabling features as needed. As an example, Xen does
      not support enabling PCI pass-through by default. It must be enabled with
      the <literal>passthrough</literal> setting. Hypervisor features can be
      configured with &virsh;. Look for the <tag>&lt;features&gt;</tag> element
      in the &vmguest;'s configuration file and adjust its features as
      required. Continuing with the &xen; pass-through example:
    </para>

<screen>&prompt.sudo;virsh edit sle15sp1
 &lt;features&gt;
    &lt;xen&gt;
      &lt;passthrough/&gt;
    &lt;/xen&gt;
 &lt;/features&gt;
</screen>

    <para>
      Save your changes and restart the &vmguest;.
    </para>

    <para>
      See the <citetitle>Hypervisor features</citetitle> section of the libvirt
      <citetitle>Domain XML format</citetitle> manual at
      <link xlink:href="https://libvirt.org/formatdomain.html#elementsFeatures"/>
      for more information.
    </para>
  </sect1>
  <sect1 xml:id="libvirt-cpu-virsh">
    <title>Configuring CPU</title>

    <para>
      Many aspects of the virtual CPUs presented to &vmguest;s are configurable
      with &virsh;. The number of current and maximum CPUs allocated to a
      &vmguest; can be changed, as well as the model of the CPU and its feature
      set. The following subsections describe how to change the common CPU
      settings of a &vmguest;.
    </para>

    <sect2 xml:id="sec-libvirt-cpu-num-virsh">
      <title>Configuring the number of CPUs</title>
      <para>
        The number of allocated CPUs is stored in the &vmguest;'s XML
        configuration file in <filename>/etc/libvirt/qemu/</filename> in the
        <tag class="attribute">vcpu</tag> element:
      </para>
<screen>&lt;vcpu placement='static'&gt;1&lt;/vcpu&gt;</screen>
      <para>
        In this example, the &vmguest; has only one allocated CPU. The
        following procedure shows how to change the number of allocated CPUs
        for the &vmguest;:
      </para>
      <procedure>
        <step>
          <para>
            Check whether your &vmguest; is inactive:
          </para>
<screen>&prompt.sudo;<command>virsh list --inactive</command>
Id    Name                           State
----------------------------------------------------
-     sles15                         shut off</screen>
        </step>
        <step>
          <para>
            Edit the configuration for an existing &vmguest;:
          </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
        </step>
        <step>
          <para>
            Change the number of allocated CPUs:
          </para>
<screen>&lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;</screen>
        </step>
        <step>
          <para>
            Restart the &vmguest;:
          </para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
        </step>
        <step>
          <para>
            Check if the number of CPUs in the VM has changed.
          </para>
<screen>
&prompt.sudo;<command>virsh vcpuinfo sled15</command>
VCPU:           0
CPU:            N/A
State:          N/A
CPU time        N/A
CPU Affinity:   yy

VCPU:           1
CPU:            N/A
State:          N/A
CPU time        N/A
CPU Affinity:   yy
</screen>
        </step>
      </procedure>
      <para>
        You can also change the number of CPUs while the &vmguest; is running.
        CPUs can be hotplugged until the maximum number configured at &vmguest;
        start is reached. Likewise, they can be hot-unplugged until the lower
        limit of 1 is reached. The following example shows changing the
        active CPU count from 2 to a predefined maximum of 4.
      </para>
      <procedure>
        <step>
          <para>
            Check the current live vcpu count:
          </para>
<screen>&prompt.sudo;<command>virsh vcpucount sles15 | grep live</command>
maximum      live           4
current      live           2
</screen>
        </step>
        <step>
          <para>
            Change the current, or active, number of CPUs to 4:
          </para>
<screen>&prompt.sudo;<command>virsh setvcpus sles15 --count 4 --live</command></screen>
        </step>
        <step>
          <para>
            Check that the current live vcpu count is now 4:
          </para>
<screen>&prompt.sudo;<command>virsh vcpucount sles15 | grep live</command>
maximum      live           4
current      live           4
</screen>
        </step>
      </procedure>
      <important>
        <title>Exceeding 255 CPUs</title>
        <para>
          With &kvm;, it is possible to define a &vmguest; with more than 255
          CPUs. However, additional configuration is necessary to start and run
          the &vmguest;. The <literal>ioapic</literal> feature needs to be
          tuned and an IOMMU device needs to be added to the &vmguest;. Below
          is an example configuration for 288 CPUs.
        </para>
<screen>
&lt;domain&gt;
 &lt;vcpu placement='static'&gt;288&lt;/vcpu&gt;
 &lt;features&gt;
  &lt;ioapic driver='qemu'/&gt;
 &lt;/features&gt;
 &lt;devices&gt;
  &lt;iommu model='intel'&gt;
   &lt;driver intremap='on' eim='on'/&gt;
  &lt;/iommu&gt;
 &lt;/devices&gt;
&lt;/domain&gt;
</screen>
      </important>
    </sect2>

    <sect2 xml:id="sec-libvirt-cpu-model-virsh">
      <title>Configuring the CPU model</title>
      <para>
        The CPU model exposed to a &vmguest; can often influence the workload
        running within it. The default CPU model is derived from a CPU mode
        known as <literal>host-model</literal>.
      </para>
<screen>&lt;cpu mode='host-model'/&gt;</screen>
      <para>
        When starting a &vmguest; with the CPU mode <literal>host-model</literal>,
        &libvirt; copies its model of the host CPU into the &vmguest;
        definition. The host CPU model and features copied to the &vmguest;
        definition can be observed in the output of the <command>virsh
        capabilities</command>.
      </para>
      <para>
        Another interesting CPU mode is <literal>host-passthrough</literal>.
      </para>
<screen>&lt;cpu mode='host-passthrough'/&gt;</screen>
      <para>
        When starting a &vmguest; with the CPU mode
        <literal>host-passthrough</literal>, it is presented with a CPU that is
        exactly the same as the &vmhost; CPU. This can be useful when the
        &vmguest; workload requires CPU features not available in &libvirt;'s
        simplified <literal>host-model</literal> CPU. The
        <literal>host-passthrough</literal> CPU mode comes with the
        disadvantage of reduced migration flexibility. A &vmguest; with
        <literal>host-passthrough</literal> CPU mode can only be migrated to a
        &vmhost; with identical hardware.
      </para>
      <para>
        When using the <literal>host-passthrough</literal> CPU mode, it is
        still possible to disable undesirable features. The following
        configuration presents the &vmguest; with a CPU that is exactly the
        same as the host CPU but with the <literal>vmx</literal> feature
        disabled.
      </para>
<screen>
&lt;cpu mode='host-passthrough'&gt;
  &lt;feature policy='disable' name='vmx'/&gt;
  &lt;/cpu&gt;
</screen>
      <para>
        The <literal>custom</literal> CPU mode is another common mode used to
        define a normalized CPU that can be migrated throughout dissimilar
        hosts in a cluster. For example, in a cluster with hosts containing
        Nehalem, IvyBridge and SandyBridge CPUs, the &vmguest; can be
        configured with a <literal>custom</literal> CPU mode that contains a
        Nehalem CPU model.
      </para>
<screen>
&lt;cpu mode='custom' match='exact'&gt;
  &lt;model fallback='allow'&gt;Nehalem&lt;/model&gt;
  &lt;feature policy='require' name='vme'/&gt;
  &lt;feature policy='require' name='ds'/&gt;
  &lt;feature policy='require' name='acpi'/&gt;
  &lt;feature policy='require' name='ss'/&gt;
  &lt;feature policy='require' name='ht'/&gt;
  &lt;feature policy='require' name='tm'/&gt;
  &lt;feature policy='require' name='pbe'/&gt;
  &lt;feature policy='require' name='dtes64'/&gt;
  &lt;feature policy='require' name='monitor'/&gt;
  &lt;feature policy='require' name='ds_cpl'/&gt;
  &lt;feature policy='require' name='vmx'/&gt;
  &lt;feature policy='require' name='est'/&gt;
  &lt;feature policy='require' name='tm2'/&gt;
  &lt;feature policy='require' name='xtpr'/&gt;
  &lt;feature policy='require' name='pdcm'/&gt;
  &lt;feature policy='require' name='dca'/&gt;
  &lt;feature policy='require' name='rdtscp'/&gt;
  &lt;feature policy='require' name='invtsc'/&gt;
  &lt;/cpu&gt;
</screen>
      <para>
        For more information on &libvirt;'s CPU model and topology options, see
        the <citetitle>CPU model and topology</citetitle> documentation at
        <link xlink:href="https://libvirt.org/formatdomain.html#cpu-model-and-topology"/>.
      </para>
    </sect2>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-boot-menu-virsh">
    <title>Changing boot options</title>

    <para>
      The boot menu of the &vmguest; can be found in the <tag>os</tag> element
      and looks similar to this example:
    </para>

<screen>&lt;os&gt;
  &lt;type&gt;hvm&lt;/type&gt;
  &lt;loader&gt;readonly='yes' secure='no' type='rom'/&gt;/usr/lib/xen/boot/hvmloader&lt;/loader&gt;
  &lt;nvram template='/usr/share/OVMF/OVMF_VARS.fd'/&gt;/var/lib/libvirt/nvram/guest_VARS.fd&lt;/nvram&gt;
  &lt;boot dev='hd'/&gt;
  &lt;boot dev='cdrom'/&gt;
  &lt;bootmenu enable='yes' timeout='3000'/&gt;
  &lt;smbios mode='sysinfo'/&gt;
  &lt;bios useserial='yes' rebootTimeout='0'/&gt;
  &lt;/os&gt;</screen>

    <para>
      In this example, two devices are available,
      <tag class="attvalue">hd</tag> and <tag class="attvalue">cdrom</tag> .
      The configuration also reflects the actual boot order, so the
      <tag class="attvalue">hd</tag> comes before the
      <tag class="attvalue">cdrom</tag> .
    </para>

    <sect2 xml:id="sec-libvirt-config-bootorder-virsh">
      <title>Changing boot order</title>
      <para>
        The &vmguest;'s boot order is represented through the order of devices
        in the XML configuration file. As the devices are interchangeable, it
        is possible to change the boot order of the &vmguest;.
      </para>
      <procedure>
        <step>
          <para>
            Open the &vmguest;'s XML configuration.
          </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
        </step>
        <step>
          <para>
            Change the sequence of the bootable devices.
          </para>
<screen>
...
&lt;boot dev='cdrom'/&gt;
&lt;boot dev='hd'/&gt;
...
      </screen>
        </step>
        <step>
          <para>
            Check if the boot order was changed successfully by looking at the
            boot menu in the BIOS of the &vmguest;.
          </para>
        </step>
      </procedure>
    </sect2>

    <sect2 xml:id="sec-libvirt-config-directkernel-virsh">
      <title>Using direct kernel boot</title>
      <para>
        Direct Kernel Boot allows you to boot from a kernel and initrd stored
        on the host. Set the path to both files in the <tag>kernel</tag> and
        <tag>initrd</tag> elements:
      </para>
<screen>&lt;os&gt;
    ...
  &lt;kernel&gt;/root/f8-i386-vmlinuz&lt;/kernel&gt;
  &lt;initrd&gt;/root/f8-i386-initrd&lt;/initrd&gt;
    ...
&lt;os&gt;</screen>
      <para>
        To enable Direct Kernel Boot:
      </para>
      <procedure>
        <step>
          <para>
            Open the &vmguest;'s XML configuration:
          </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
        </step>
        <step>
          <para>
            Inside the <tag>os</tag> element, add a <tag>kernel</tag> element
            and the path to the kernel file on the host:
          </para>
<screen>...
&lt;kernel&gt;/root/f8-i386-vmlinuz&lt;/kernel&gt;
...</screen>
        </step>
        <step>
          <para>
            Add an <tag>initrd</tag> element and the path to the initrd file on
            the host:
          </para>
<screen>...
&lt;initrd&gt;/root/f8-i386-initrd&lt;/initrd&gt;
...</screen>
        </step>
        <step>
          <para>
            Start your VM to boot from the new kernel:
          </para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
        </step>
      </procedure>
    </sect2>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-memory-virsh">
    <title>Configuring memory allocation</title>

    <para>
      The amount of memory allocated for the &vmguest; can also be configured
      with &virsh;. It is stored in the <tag>memory</tag> element and defines
      the maximum allocation of memory for the &vmguest; at boot time. The
      optional <tag>currentMemory</tag> element defines the actual memory
      allocated to the &vmguest;. <tag>currentMemory</tag> can be less than
      <tag>memory</tag>, allowing for increasing (or
      <emphasis>ballooning</emphasis>) the memory while the &vmguest; is
      running. If <tag>currentMemory</tag> is omitted, it defaults to the same
      value as the <tag>memory</tag> element.
    </para>

    <para>
      You can adjust memory settings by editing the &vmguest; configuration,
      but be aware that changes do not take place until the next boot. The
      following steps demonstrate changing a &vmguest; to boot with 4G of
      memory, but allow later expansion to 8G:
    </para>

    <procedure>
      <step>
        <para>
          Open the &vmguest;'s XML configuration:
        </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
      </step>
      <step>
        <para>
          Search for the <tag>memory</tag> element and set to 8G:
        </para>
<screen>...
&lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
...</screen>
      </step>
      <step>
        <para>
          If the <tag>currentMemory</tag> element does not exist, add it below
          the <tag>memory</tag> element, or change its value to 4G:
        </para>
<screen>
[...]
&lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
&lt;currentMemory unit='KiB'&gt;4194304&lt;/currentMemory&gt;
[...]</screen>
      </step>
    </procedure>

    <para>
      Changing the memory allocation while the &vmguest; is running can be done
      with the <command>setmem</command> subcommand. The following example
      shows increasing the memory allocation to 8G:
    </para>

    <procedure>
      <step>
        <para>
          Check &vmguest; existing memory settings:
        </para>
<screen>&prompt.sudo;<command>virsh dominfo sles15 | grep memory</command>
Max memory:     8388608 KiB
Used memory:    4194608 KiB
</screen>
      </step>
      <step>
        <para>
          Change the used memory to 8G:
        </para>
<screen>&prompt.sudo;<command>virsh setmem sles15 8388608</command></screen>
      </step>
      <step>
        <para>
          Check the updated memory settings:
        </para>
<screen>&prompt.sudo;<command>virsh dominfo sles15 | grep memory</command>
Max memory:     8388608 KiB
Used memory:    8388608 KiB
</screen>
      </step>
    </procedure>

    <important>
      <title>Large memory &vmguest;s</title>
      <para>
        &vmguest;s with memory requirements of 4&nbsp;TB or more must either
        use the <literal>host-passthrough</literal> CPU mode, or explicitly
        specify the virtual CPU address size when using
        <literal>host-model</literal> or <literal>custom</literal> CPU modes.
        The default virtual CPU address size may not be sufficient for memory
        configurations of 4&nbsp;TB or more. The following example shows how to
        use the &vmhost;'s physical CPU address size when using the
        <literal>host-model</literal> CPU mode.
      </para>
<screen>
[...]
&lt;cpu mode='host-model' check='partial'&gt;
&lt;maxphysaddr mode='passthrough'&gt;
&lt;/cpu&gt;
[...]</screen>
      <para>
        For more information on specifying virtual CPU address size, see the
        <literal>maxphysaddr</literal> option in the <citetitle>CPU model and
        topology</citetitle> documentation at
        <link xlink:href="https://libvirt.org/formatdomain.html#cpu-model-and-topology"/>.
      </para>
    </important>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-pci-virsh">
    <title>Adding a PCI device</title>

    <para>
      To assign a PCI device to &vmguest; with &virsh;, follow these steps:
    </para>

    <procedure>
      <step>
        <para>
          Identify the host PCI device to assign to the &vmguest;. In the
          following example, we are assigning a DEC network card to the guest:
        </para>
<screen>&prompt.sudo;<command>lspci -nn</command>
[...]
<emphasis role="bold">03:07.0</emphasis> Ethernet controller [0200]: Digital Equipment Corporation DECchip \
21140 [FasterNet] [1011:0009] (rev 22)
[...]</screen>
        <para>
          Write down the device ID (<literal>03:07.0</literal> in this case).
        </para>
      </step>
      <step>
        <para>
          Gather detailed information about the device using <command>virsh
          nodedev-dumpxml <replaceable>ID</replaceable></command>. To get the
          <replaceable>ID</replaceable>, replace the colon and the period in
          the device ID (<literal>03:07.0</literal>) with underscores. Prefix
          the result with <quote>pci_0000_</quote>:
          <literal>pci_0000_03_07_0</literal>.
        </para>
<screen>&prompt.sudo;<command>virsh nodedev-dumpxml pci_0000_03_07_0</command>
&lt;device&gt;
  &lt;name&gt;pci_0000_03_07_0&lt;/name&gt;
  &lt;path&gt;/sys/devices/pci0000:00/0000:00:14.4/0000:03:07.0&lt;/path&gt;
  &lt;parent&gt;pci_0000_00_14_4&lt;/parent&gt;
  &lt;driver&gt;
    &lt;name&gt;tulip&lt;/name&gt;
  &lt;/driver&gt;
  &lt;capability type='pci'&gt;
    <emphasis role="bold">&lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;3&lt;/bus&gt;
    &lt;slot&gt;7&lt;/slot&gt;
    &lt;function&gt;0&lt;/function&gt;</emphasis>
    &lt;product id='0x0009'&gt;DECchip 21140 [FasterNet]&lt;/product&gt;
    &lt;vendor id='0x1011'&gt;Digital Equipment Corporation&lt;/vendor&gt;
    &lt;numa node='0'/&gt;
  &lt;/capability&gt;
&lt;/device&gt;</screen>
        <para>
          Write down the values for domain, bus and function (see the previous
          XML code printed in bold).
        </para>
      </step>
      <step>
        <para>
          Detach the device from the host system before attaching it to the
          &vmguest;:
        </para>
<screen>&prompt.sudo;<command>virsh nodedev-detach pci_0000_03_07_0</command>
  Device pci_0000_03_07_0 detached</screen>
        <tip>
          <title>Multi-function PCI devices</title>
          <para>
            When using a multi-function PCI device that does not support FLR
            (function level reset) or PM (power management) reset, you need to
            detach all its functions from the &vmhost;. The whole device must
            be reset for security reasons. <systemitem>libvirt</systemitem>
            refuses to assign the device if one of its functions is still in
            use by the &vmhost; or another &vmguest;.
          </para>
        </tip>
      </step>
      <step>
        <para>
          Convert the domain, bus, slot, and function value from decimal to
          hexadecimal. In our example, domain = 0, bus = 3, slot = 7, and
          function = 0. Ensure that the values are inserted in the right order:
        </para>
<screen>&prompt.user;<command>printf "&lt;address domain='0x%x' bus='0x%x' slot='0x%x' function='0x%x'/&gt;\n" 0 3 7 0</command></screen>
        <para>
          This results in:
        </para>
<screen>&lt;address domain='0x0' bus='0x3' slot='0x7' function='0x0'/&gt;</screen>
      </step>
      <step>
        <para>
          Run <command>virsh edit</command> on your domain, and add the
          following device entry in the <literal>&lt;devices&gt;</literal>
          section using the result from the previous step:
        </para>
<screen>&lt;hostdev mode='subsystem' type='pci' managed='yes'&gt;
  &lt;source&gt;
    &lt;address domain='0x0' bus='0x03' slot='0x07' function='0x0'/&gt;
  &lt;/source&gt;
&lt;/hostdev&gt;</screen>
        <tip xml:id="tip-libvirt-config-pci-virsh-managed">
          <title><literal>managed</literal> compared to <literal>unmanaged</literal></title>
          <para>
            <systemitem>libvirt</systemitem> recognizes two modes for handling
            PCI devices: they can be <literal>managed</literal> or
            <literal>unmanaged</literal>. In the managed case,
            <systemitem>libvirt</systemitem> handles all details of unbinding
            the device from the existing driver if needed, resetting the
            device, binding it to <systemitem>vfio-pci</systemitem> before
            starting the domain, etc. When the domain is terminated or the
            device is removed from the domain, <systemitem>libvirt</systemitem>
            unbinds from <systemitem>vfio-pci</systemitem> and rebinds to the
            original driver in the case of a managed device. If the device is
            unmanaged, the user must ensure that all these management aspects
            of the device are done before assigning it to a domain, and after
            the device is no longer used by the domain.
          </para>
        </tip>
        <para>
          In the example above, the <literal>managed='yes'</literal> option
          means that the device is managed. To switch the device mode to
          unmanaged, set <literal>managed='no'</literal> in the listing above.
          If you do so, you need to take care of the related driver with the
          <command>virsh nodedev-detach</command> and <command>virsh
          nodedev-reattach</command> commands. Before starting the &vmguest;,
          you need to detach the device from the host by running <command>virsh
          nodedev-detach pci_0000_03_07_0</command>. In case the &vmguest; is
          not running, you can make the device available for the host by
          running <command>virsh nodedev-reattach pci_0000_03_07_0</command>.
        </para>
      </step>
      <step>
        <para>
          Shut down the &vmguest; and disable &selnx; if it is running on the
          host.
        </para>
<screen>&prompt.sudo;<command>setsebool -P virt_use_sysfs 1</command></screen>
      </step>
      <step>
        <para>
          Start your &vmguest; to make the assigned PCI device available:
        </para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
      </step>
    </procedure>

    <important>
      <title>&slsa;11 SP4 &kvm; guests</title>
      <para>
        On a newer &qemu; machine type (pc-i440fx-2.0 or higher) with &slsa;11
        SP4 &kvm; guests, the <systemitem class="resource">acpiphp</systemitem>
        module is not loaded by default in the guest. This module must be
        loaded to enable hotplugging of disk and network devices. To load the
        module manually, use the command <command>modprobe acpiphp</command>.
        It is also possible to autoload the module by adding <literal>install
        acpiphp /bin/true</literal> to the
        <filename>/etc/modprobe.conf.local</filename> file.
      </para>
    </important>

    <important>
      <title>&kvm; guests using &qemu; Q35 machine type</title>
      <para>
        &kvm; guests using the &qemu; Q35 machine type have a PCI topology that
        includes a <literal>pcie-root</literal> controller and seven
        <literal>pcie-root-port</literal> controllers. The
        <literal>pcie-root</literal> controller does not support hotplugging.
        Each <literal>pcie-root-port</literal> controller supports hotplugging
        a single PCIe device. PCI controllers cannot be hotplugged, so plan
        accordingly and add more <literal>pcie-root-port</literal>s to hotplug
        more than seven PCIe devices. A <literal>pcie-to-pci-bridge</literal>
        controller can be added to support hotplugging legacy PCI devices. See
        <link xlink:href="https://libvirt.org/pci-hotplug.html"/> for more
        information about PCI topology between &qemu; machine types.
      </para>
    </important>

    <sect2 xml:id="tip-libvirt-config-zpci">
      <title>&pciback; for &zseries;</title>
      <para>
        To support &zseries;, &qemu; extended PCI representation by allowing the user to
        configure extra attributes. Two more
        attributes&mdash;<option>uid</option> and
        <option>fid</option>&mdash;were added to the
        <literal>&lt;zpci/&gt;</literal> &libvirt; specification.
        <option>uid</option> represents user-defined identifier, while
        <option>fid</option> represents PCI function identifier. These
        attributes are optional and if you do not specify them, they are
        automatically generated with non-conflicting values.
      </para>
      <para>
        To include zPCI attribute in your domain specification, use the
        following example definition:
      </para>
<screen>
&lt;controller type='pci' index='0' model='pci-root'/&gt;
&lt;controller type='pci' index='1' model='pci-bridge'&gt;
  &lt;model name='pci-bridge'/&gt;
  &lt;target chassisNr='1'/&gt;
  &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x0'&gt;
    &lt;zpci uid='0x0001' fid='0x00000000'/&gt;
  &lt;/address&gt;
&lt;/controller&gt;
&lt;interface type='bridge'&gt;
  &lt;source bridge='virbr0'/&gt;
  &lt;model type='virtio'/&gt;
  &lt;address type='pci' domain='0x0000' bus='0x01' slot='0x01' function='0x0'&gt;
    &lt;zpci uid='0x0007' fid='0x00000003'/&gt;
  &lt;/address&gt;
&lt;/interface&gt;
</screen>
    </sect2>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-usb-virsh">
    <title>Adding a USB device</title>

    <para>
      To assign a USB device to &vmguest; using &virsh;, follow these steps:
    </para>

    <procedure>
      <step>
        <para>
          Identify the host USB device to assign to the &vmguest;:
        </para>
<screen>&prompt.sudo;<command>lsusb</command>
[...]
Bus 001 Device 003: ID <emphasis role="bold">0557:2221</emphasis> ATEN International Co., Ltd Winbond Hermon
[...]</screen>
        <para>
          Write down the vendor and product IDs. In our example, the vendor ID
          is <literal>0557</literal> and the product ID is
          <literal>2221</literal>.
        </para>
      </step>
      <step>
        <para>
          Run <command>virsh edit</command> on your domain, and add the
          following device entry in the <literal>&lt;devices&gt;</literal>
          section using the values from the previous step:
        </para>
<screen>&lt;hostdev mode='subsystem' type='usb'&gt;
  &lt;source startupPolicy='optional'&gt;
   <emphasis role="bold">&lt;vendor id='0557'/&gt;
   &lt;product id='2221'/&gt;</emphasis>
  &lt;/source&gt;
&lt;/hostdev&gt;</screen>
        <tip>
          <title>Vendor/product or device's address</title>
          <para>
            Instead of defining the host device with
            <tag class="emptytag">vendor</tag> and
            <tag class="emptytag">product</tag> IDs, you can use the
            <tag class="emptytag">address</tag> element as described for host
            PCI devices in <xref linkend="sec-libvirt-config-pci-virsh"/>.
          </para>
        </tip>
      </step>
      <step>
        <para>
          Shut down the &vmguest; and disable &selnx; if it is running on the
          host:
        </para>
<screen>&prompt.sudo;<command>setsebool -P virt_use_sysfs 1</command></screen>
      </step>
      <step>
        <para>
          Start your &vmguest; to make the assigned PCI device available:
        </para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
      </step>
    </procedure>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-io">
    <title>Adding SR-IOV devices</title>

    <para>
      Single Root I/O Virtualization (<xref linkend="vt-io-sriov"/>) capable
      <xref linkend="gloss-vt-acronym-pcie"/> devices can replicate their
      resources, so they appear as multiple devices. Each of these
      <quote>pseudo-devices</quote> can be assigned to a &vmguest;.
    </para>

    <para>
      <xref linkend="vt-io-sriov"/> is an industry specification that was
      created by the Peripheral Component Interconnect Special Interest Group
      (PCI-SIG) consortium. It introduces physical functions (PF) and virtual
      functions (VF). PFs are full <xref linkend="gloss-vt-acronym-pcie"/>
      functions used to manage and configure the device. PFs also can move
      data. VFs lack the configuration and management part—they only can move
      data and a reduced set of configuration functions. As VFs do not have all
      <xref linkend="gloss-vt-acronym-pcie"/> functions, the host operating
      system or the <xref linkend="gloss-vt-hypervisor"/> must support
      <xref linkend="vt-io-sriov"/> to access and initialize VFs.
      The theoretical maximum for VFs is 256 per device (consequently the
      maximum for a dual-port Ethernet card would be 512). In practice, this
      maximum is much lower, since each VF consumes resources.
    </para>

    <sect2 xml:id="sec-libvirt-config-io-requirements">
      <title>Requirements</title>
      <para>
        The following requirements must be met to use
        <xref linkend="vt-io-sriov"/>:
      </para>
      <itemizedlist mark="bullet" spacing="normal">
        <listitem>
          <para>
            An <xref linkend="vt-io-sriov"/>-capable network card (as of
            <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise
            Server</phrase></phrase>
            <phrase role="productnumber"><phrase os="sles;sled">15</phrase></phrase>,
            only network cards support <xref linkend="vt-io-sriov"/>)
          </para>
        </listitem>
        <listitem>
          <para>
            An AMD64/Intel 64 host supporting hardware virtualization (AMD-V or
            Intel VT-x)<phrase os="sles;sled">, see
            <xref linkend="sec-kvm-requires-hardware"/> for more
            information</phrase>
          </para>
        </listitem>
        <listitem>
          <para>
            A chipset that supports device assignment (AMD-Vi or Intel
            <xref linkend="gloss-vt-acronym-vtd"/>)
          </para>
        </listitem>
        <listitem>
          <para>
            &libvirt; 0.9.10 or better
          </para>
        </listitem>
        <listitem>
          <para>
            <xref linkend="vt-io-sriov"/> drivers must be loaded and configured
            on the host system
          </para>
        </listitem>
        <listitem>
          <para>
            A host configuration that meets the requirements listed at
            <xref linkend="ann-vt-io-require"/>
          </para>
        </listitem>
        <listitem>
          <para>
            A list of the PCI addresses of the VFs assigned to &vmguest;s
          </para>
        </listitem>
      </itemizedlist>
      <tip>
        <title>Checking if a device is SR-IOV-capable</title>
        <para>
          The information whether a device is SR-IOV-capable can be obtained
          from its PCI descriptor by running <command>lspci</command>. A device
          that supports <xref linkend="vt-io-sriov"/> reports a capability
          similar to the following:
        </para>
<screen>Capabilities: [160 v1] Single Root I/O Virtualization (<xref linkend="vt-io-sriov"/>)</screen>
      </tip>
      <note>
        <title>Adding an SR-IOV device at &vmguest; creation</title>
        <para>
          Before adding an SR-IOV device to a &vmguest; when initially setting
          it up, the &vmhost; already needs to be configured as described in
          <xref linkend="sec-libvirt-config-io-config"/>.
        </para>
      </note>
    </sect2>

    <sect2 xml:id="sec-libvirt-config-io-config">
      <title>Loading and configuring the SR-IOV host drivers</title>
      <para>
        To access and initialize VFs, an SR-IOV-capable driver needs to be
        loaded on the host system.
      </para>
      <procedure>
        <step>
          <para>
            Before loading the driver, make sure the card is properly detected
            by running <command>lspci</command>. The following example shows
            the <command>lspci</command> output for the dual-port Intel 82576NS
            network card:
          </para>
<screen>&prompt.sudo;<command>/sbin/lspci | grep 82576</command>
01:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)</screen>
          <para>
            In case the card is not detected, the hardware virtualization
            support in the BIOS/EFI has probably not been enabled. To check if
            hardware virtualization support is enabled, look at the settings in
            the host's BIOS.
          </para>
        </step>
        <step>
          <para>
            Check whether the <xref linkend="vt-io-sriov"/> driver is already
            loaded by running <command>lsmod</command>. In the following
            example, a check for the igb driver (for the Intel 82576NS network
            card) returns a result. That means the driver is already loaded. If
            the command returns nothing, the driver is not loaded.
          </para>
<screen>&prompt.sudo;<command>/sbin/lsmod | egrep "^igb "</command>
igb                   185649  0</screen>
        </step>
        <step>
          <para>
            Skip the following step if the driver is already loaded. If the
            <xref linkend="vt-io-sriov"/> driver is not yet loaded, the
            non-<xref linkend="vt-io-sriov"/> driver needs to be removed first,
            before loading the new driver. Use <command>rmmod</command> to
            unload a driver. The following example unloads the
            non-<xref linkend="vt-io-sriov"/> driver for the Intel 82576NS
            network card:
          </para>
<screen>&prompt.sudo;<command>/sbin/rmmod igbvf</command></screen>
        </step>
        <step>
          <para>
            Load the <xref linkend="vt-io-sriov"/> driver subsequently using
            the <command>modprobe</command> command—the VF parameter
            (<literal>max_vfs</literal>) is mandatory:
          </para>
<screen>&prompt.sudo;<command>/sbin/modprobe igb max_vfs=8</command></screen>
        </step>
      </procedure>
      <remark>Unsure if the following procedure is really needed.</remark>
      <para>
        As an alternative, you can also load the driver via SYSFS:
      </para>
      <procedure>
        <step>
          <para>
            Find the PCI ID of the physical NIC by listing Ethernet devices:
          </para>
<screen>&prompt.sudo;<command>lspci | grep Eth</command>
06:00.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:00.1 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)</screen>
        </step>
        <step>
          <para>
            To enable VFs, echo the number of desired VFs to load to the
            <literal>sriov_numvfs</literal> parameter:
          </para>
<screen>&prompt.sudo;<command>echo 1 &gt; /sys/bus/pci/devices/0000:06:00.1/sriov_numvfs</command></screen>
        </step>
        <step>
          <para>
            Verify that the VF NIC was loaded:
          </para>
<screen>&prompt.sudo;<command>lspci | grep Eth</command>
06:00.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:00.1 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:08.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)</screen>
        </step>
        <step>
          <para>
            Obtain the maximum number of VFs available:
          </para>
<screen>&prompt.sudo;<command>lspci -vvv -s 06:00.1 | grep 'Initial VFs'</command>
                       Initial VFs: 32, Total VFs: 32, Number of VFs: 0,
Function Dependency Link: 01</screen>
        </step>
        <step>
          <para>
            Create a <filename>/etc/systemd/system/before.service</filename>
            file which loads VF via SYSFS on boot:
          </para>
<screen>[Unit]
Before=
[Service]
Type=oneshot
RemainAfterExit=true
ExecStart=/bin/bash -c "echo 1 &gt; /sys/bus/pci/devices/0000:06:00.1/sriov_numvfs"
# beware, executable is run directly, not through a shell, check the man pages
# systemd.service and systemd.unit for full syntax
[Install]
# target in which to start the service
WantedBy=multi-user.target
#WantedBy=graphical.target</screen>
        </step>
        <step>
          <para>
            Before starting the VM, it is required to create another service
            file (<filename>after-local.service</filename>) pointing to the
            <filename>/etc/init.d/after.local</filename> script that detaches
            the NIC. Otherwise the VM would fail to start:
          </para>
<screen>[Unit]
Description=/etc/init.d/after.local Compatibility
After=libvirtd.service
Requires=libvirtd.service
[Service]
Type=oneshot
ExecStart=/etc/init.d/after.local
RemainAfterExit=true

[Install]
WantedBy=multi-user.target</screen>
        </step>
        <step>
          <para>
            Copy it to <filename>/etc/systemd/system</filename>.
          </para>
<screen>#! /bin/sh
# ...
virsh nodedev-detach pci_0000_06_08_0</screen>
          <para>
            Save it as <filename>/etc/init.d/after.local</filename>.
          </para>
        </step>
        <step>
          <para>
            Reboot the machine and check if the SR-IOV driver is loaded by
            re-running the <command>lspci</command> command from the first step
            of this procedure. If the SR-IOV driver was loaded successfully you
            should see additional lines for the VFs:
          </para>
<screen>01:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
[...]
04:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
04:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
04:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
[...]</screen>
        </step>
      </procedure>
    </sect2>

    <sect2 xml:id="sec-libvirt-config-io-attach">
      <title>Adding a VF network device to a &vmguest;</title>
      <para>
        When the <xref linkend="vt-io-sriov"/> hardware is properly set up on
        the &vmhost;, you can add VFs to &vmguest;s. To do so, you need to
        collect specific data first.
      </para>
      <procedure>
        <title>Adding a VF network device to an existing &vmguest;</title>
        <para>
          The following procedure uses example data. Replace it with
          appropriate data from your setup.
        </para>
        <step>
          <para>
            Use the <command>virsh nodedev-list</command> command to get the
            PCI address of the VF you want to assign and its corresponding PF.
            Numerical values from the <command>lspci</command> output shown in
            <xref linkend="sec-libvirt-config-io-config"/> (for example
            <literal>01:00.0</literal> or <literal>04:00.1</literal>) are
            transformed by adding the prefix <literal>pci_0000_</literal> and
            by replacing colons and dots with underscores. So a PCI ID listed
            as <literal>04:00.0</literal> by <command>lspci</command> is listed
            as <literal>pci_0000_04_00_0</literal> by virsh. The following
            example lists the PCI IDs for the second port of the Intel 82576NS
            network card:
          </para>
<screen>&prompt.sudo;<command>virsh nodedev-list | grep 0000_04_</command>
<emphasis role="bold">pci_0000_04_00_0</emphasis>
<emphasis role="bold">pci_0000_04_00_1</emphasis>
pci_0000_04_10_0
pci_0000_04_10_1
pci_0000_04_10_2
pci_0000_04_10_3
pci_0000_04_10_4
pci_0000_04_10_5
pci_0000_04_10_6
pci_0000_04_10_7
pci_0000_04_11_0
pci_0000_04_11_1
pci_0000_04_11_2
pci_0000_04_11_3
pci_0000_04_11_4
pci_0000_04_11_5</screen>
          <para>
            The first two entries represent the
            <emphasis role="bold">PFs</emphasis>, whereas the other entries
            represent the VFs.
          </para>
        </step>
        <step>
          <para>
            Run the following <command>virsh nodedev-dumpxml</command> command
            on the PCI ID of the VF you want to add:
          </para>
<screen>&prompt.sudo;<command>virsh nodedev-dumpxml pci_0000_04_10_0</command>
&lt;device&gt;
  &lt;name&gt;pci_0000_04_10_0&lt;/name&gt;
  &lt;parent&gt;pci_0000_00_02_0&lt;/parent&gt;
  &lt;capability type='pci'&gt;
    <emphasis role="bold">&lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;4&lt;/bus&gt;
    &lt;slot&gt;16&lt;/slot&gt;
    &lt;function&gt;0&lt;/function&gt;</emphasis>
    &lt;product id='0x10ca'&gt;82576 Virtual Function&lt;/product&gt;
    &lt;vendor id='0x8086'&gt;Intel Corporation&lt;/vendor&gt;
    &lt;capability type='phys_function'&gt;
      &lt;address domain='0x0000' bus='0x04' slot='0x00' function='0x0'/&gt;
    &lt;/capability&gt;
  &lt;/capability&gt;
&lt;/device&gt;</screen>
          <para>
            The following data is needed for the next step:
          </para>
          <itemizedlist>
            <listitem>
              <para>
                <literal>&lt;domain&gt;0&lt;/domain&gt;</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>&lt;bus&gt;4&lt;/bus&gt;</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>&lt;slot&gt;16&lt;/slot&gt;</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>&lt;function&gt;0&lt;/function&gt;</literal>
              </para>
            </listitem>
          </itemizedlist>
        </step>
        <step>
          <para>
            Create a temporary XML file (for example
            <filename>/tmp/vf-interface.xml</filename> containing the data
            necessary to add a VF network device to an existing &vmguest;. The
            minimal content of the file needs to look like the following:
          </para>
<screen>&lt;interface type='hostdev'&gt;<co xml:id="sriov-iface"/>
 &lt;source&gt;
  &lt;address type='pci' domain='0' bus='11' slot='16' function='0'2/&gt;<co xml:id="sriov-data"/>
 &lt;/source&gt;
&lt;/interface&gt;</screen>
          <calloutlist>
            <callout arearefs="sriov-iface">
              <para>
                VFs do not get a fixed MAC address; it changes every time the
                host reboots. When adding network devices the
                <quote>traditional</quote> way with
                <tag class="attribute">hostdev</tag>, it would require to
                reconfigure the &vmguest;'s network device after each reboot of
                the host, because of the MAC address change. To avoid this kind
                of problem, &libvirt; introduced the
                <tag class="attvalue">hostdev</tag> value, which sets up
                network-specific data <emphasis>before</emphasis> assigning the
                device.
              </para>
            </callout>
            <callout arearefs="sriov-data">
              <para>
                Specify the data you acquired in the previous step here.
              </para>
            </callout>
          </calloutlist>
        </step>
        <step>
          <para>
            In case a device is already attached to the host, it cannot be
            attached to a &vmguest;. To make it available for guests, detach it
            from the host first:
          </para>
<screen>&prompt.sudo;<command>virsh nodedev-detach pci_0000_04_10_0</command></screen>
        </step>
        <step>
          <para>
            Add the VF interface to an existing &vmguest;:
          </para>
<screen>&prompt.sudo;<command>virsh attach-device <replaceable>GUEST</replaceable> /tmp/vf-interface.xml --<replaceable>OPTION</replaceable></command></screen>
          <para>
            <replaceable>GUEST</replaceable> needs to be replaced by the domain
            name, ID or UUID of the &vmguest;.
            --<replaceable>OPTION</replaceable> can be one of the following:
          </para>
          <variablelist>
            <varlistentry>
              <term><option>--persistent</option></term>
              <listitem>
                <para>
                  This option always adds the device to the domain's persistent
                  XML. If the domain is running, the device is hotplugged.
                </para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term><option>--config</option></term>
              <listitem>
                <para>
                  This option affects the persistent XML only, even if the
                  domain is running. The device appears in the &vmguest; on
                  next boot.
                </para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term><option>--live</option></term>
              <listitem>
                <para>
                  This option affects a running domain only. If the domain is
                  inactive, the operation fails. The device is not persisted in
                  the XML and becomes available in the &vmguest; on next boot.
                </para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term><option>--current</option></term>
              <listitem>
                <para>
                  This option affects the current state of the domain. If the
                  domain is inactive, the device is added to the persistent XML
                  and becomes available on next boot. If the domain is active,
                  the device is hotplugged but not added to the persistent XML.
                </para>
              </listitem>
            </varlistentry>
          </variablelist>
        </step>
        <step>
          <para>
            To detach a VF interface, use the <command>virsh
            detach-device</command> command, which also takes the options
            listed above.
          </para>
        </step>
      </procedure>
    </sect2>

    <sect2 xml:id="libvirt-config-io-pool">
      <title>Dynamic allocation of VFs from a pool</title>
      <para>
        If you define the PCI address of a VF into a &vmguest;'s configuration
        statically as described in
        <xref linkend="sec-libvirt-config-io-attach"/>, it is hard to migrate
        such guest to another host. The host must have identical hardware in
        the same location on the PCI bus, or the &vmguest; configuration must
        be modified before each start.
      </para>
      <para>
        Another approach is to create a &libvirt; network with a device pool
        that contains all the VFs of an <xref linkend="vt-io-sriov"/> device.
        The &vmguest; then references this network, and each time it is
        started, a single VF is dynamically allocated to it. When the &vmguest;
        is stopped, the VF is returned to the pool, available for another
        guest.
      </para>
      <sect3 xml:id="libvirt-config-io-pool-host">
        <title>Defining network with pool of VFs on &vmhost;</title>
        <para>
          The following example of network definition creates a pool of all VFs
          for the <xref linkend="vt-io-sriov"/> device with its physical
          function (PF) at the network interface <literal>eth0</literal> on the
          host:
        </para>
<screen>&lt;network&gt;
  &lt;name&gt;passthrough&lt;/name&gt;
    &lt;forward mode='hostdev' managed='yes'&gt;
      &lt;pf dev='eth0'/&gt;
    &lt;/forward&gt;
  &lt;/network&gt;</screen>
        <para>
          To use this network on the host, save the above code to a file, for
          example <filename>/tmp/passthrough.xml</filename>, and execute the
          following commands. Remember to replace <literal>eth0</literal> with
          the real network interface name of your <xref linkend="vt-io-sriov"/>
          device's PF:
        </para>
<screen>&prompt.sudo;<command>virsh net-define /tmp/passthrough.xml</command>
&prompt.sudo;<command>virsh net-autostart passthrough</command>
&prompt.sudo;<command>virsh net-start passthrough</command></screen>
      </sect3>
      <sect3 xml:id="libvirt-config-io-pool-guest">
        <title>Configuring &vmguest;s to use VF from the pool</title>
        <para>
          The following example of &vmguest; device interface definition uses a
          VF of the <xref linkend="vt-io-sriov"/> device from the pool created
          in <xref linkend="libvirt-config-io-pool-host"/>. &libvirt;
          automatically derives the list of all VFs associated with that PF the
          first time the guest is started.
        </para>
<screen>&lt;interface type='network'&gt;
  &lt;source network='passthrough'&gt;
&lt;/interface&gt;</screen>
        <para>
          After the first &vmguest; starts that uses the network with the pool
          of VFs, verify the list of associated VFs. Do so by running
          <command>virsh net-dumpxml passthrough</command> on the host.
        </para>
<screen>&lt;network connections='1'&gt;
  &lt;name&gt;passthrough&lt;/name&gt;
  &lt;uuid&gt;a6a26429-d483-d4ed-3465-4436ac786437&lt;/uuid&gt;
  &lt;forward mode='hostdev' managed='yes'&gt;
    &lt;pf dev='eth0'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x1'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x3'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x5'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x7'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x1'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x3'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x5'/&gt;
  &lt;/forward&gt;
  &lt;/network&gt;</screen>
      </sect3>
    </sect2>
  </sect1>
  <sect1 xml:id="libvirt-config-listing-host-devs">
    <title>Listing attached devices</title>

    <para>
      Although there is no mechanism in &virsh; to list all &vmhost;'s devices
      that have already been attached to its &vmguest;s, you can list all
      devices attached to a specific &vmguest; by running the following
      command:
    </para>

<screen>virsh dumpxml <replaceable>VMGUEST_NAME</replaceable> | xpath -e /domain/devices/hostdev</screen>

    <para>
      For example:
    </para>

<screen>
&prompt.sudo;virsh dumpxml sles12 | -e xpath /domain/devices/hostdev
Found 2 nodes:
-- NODE --
&lt;hostdev mode="subsystem" type="pci" managed="yes">
  &lt;driver name="xen" />
  &lt;source>
    &lt;address domain="0x0000" bus="0x0a" slot="0x10" function="0x1" />
  &lt;/source>
  &lt;address type="pci" domain="0x0000" bus="0x00" slot="0x0a" function="0x0" />
  &lt;/hostdev>
-- NODE --
&lt;hostdev mode="subsystem" type="pci" managed="yes">
  &lt;driver name="xen" />
  &lt;source>
    &lt;address domain="0x0000" bus="0x0a" slot="0x10" function="0x2" />
  &lt;/source>
  &lt;address type="pci" domain="0x0000" bus="0x00" slot="0x0b" function="0x0" />
&lt;/hostdev>
</screen>

    <tip xml:id="libvirt-config-listing-host-devs-sriov">
      <title>Listing SR-IOV devices attached via <literal>&lt;interface type='hostdev'&gt;</literal></title>
      <para>
        For SR-IOV devices that are attached to the &vmhost; via
        <literal>&lt;interface type='hostdev'&gt;</literal>, you need to use a
        different XPath query:
      </para>
<screen>virsh dumpxml <replaceable>VMGUEST_NAME</replaceable> | xpath -e /domain/devices/interface/@type</screen>
    </tip>
  </sect1>
  <sect1 xml:id="libvirt-config-storage-virsh">
    <title>Configuring storage devices</title>

    <para>
      Storage devices are defined within the <tag>disk</tag> element. The usual
      <tag>disk</tag> element supports several attributes. The following two
      attributes are the most important:
    </para>

    <itemizedlist>
      <listitem>
        <para>
          The <tag class="attribute">type</tag> attribute describes the source
          of the virtual disk device. Valid values are
          <tag class="attvalue">file</tag> , <tag class="attvalue">block</tag>
          , <tag class="attvalue">dir</tag> ,
          <tag class="attvalue">network</tag> , or
          <tag class="attvalue">volume</tag> .
        </para>
      </listitem>
      <listitem>
        <para>
          The <tag class="attribute">device</tag> attribute shows how the
          disk is exposed to the &vmguest; OS. As an example, possible values
          can include <tag
      class="attvalue">floppy</tag> ,
          <tag class="attvalue">disk</tag> , <tag class="attvalue">cdrom</tag>
          , and others.
        </para>
      </listitem>
    </itemizedlist>

    <para>
      The following child elements are the most important:
    </para>

    <itemizedlist>
      <listitem>
        <para>
          <tag>driver</tag> contains the driver and the bus. These are used by
          the &vmguest; to work with the new disk device.
        </para>
      </listitem>
      <listitem>
        <para>
          The <tag>target</tag> element contains the device name under which
          the new disk is shown in the &vmguest;. It also contains the optional
          bus attribute, which defines the type of bus on which the new disk
          should operate.
        </para>
      </listitem>
    </itemizedlist>

    <para>
      The following procedure shows how to add storage devices to the
      &vmguest;:
    </para>

    <procedure>
      <step>
        <para>
          Edit the configuration for an existing &vmguest;:
        </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
      </step>
      <step>
        <para>
          Add a <tag>disk</tag> element inside the <tag>disk</tag> element
          together with the attributes <tag class="attvalue">type</tag> and
          <tag class="attvalue">device</tag>:
        </para>
<screen>&lt;disk type='file' device='disk'&gt;</screen>
      </step>
      <step>
        <para>
          Specify a <tag>driver</tag> element and use the default values:
        </para>
<screen>&lt;driver name='qemu' type='qcow2'/&gt;</screen>
      </step>
      <step>
        <para>
          Create a disk image as a source for the new virtual disk device:
        </para>
<screen>&prompt.sudo;<command>qemu-img create -f qcow2 /var/lib/libvirt/images/sles15.qcow2 32G</command></screen>
      </step>
      <step>
        <para>
          Add the path for the disk source:
        </para>
<screen>&lt;source file='/var/lib/libvirt/images/sles15.qcow2'/&gt;</screen>
      </step>
      <step>
        <para>
          Define the target device name in the &vmguest; and the bus on which
          the disk should work:
        </para>
<screen>&lt;target dev='vda' bus='virtio'/&gt;</screen>
      </step>
      <step>
        <para>
          Restart your VM:
        </para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
      </step>
    </procedure>

    <para>
      Your new storage device should be available in the &vmguest; OS.
    </para>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-controllers-virsh">
    <title>Configuring controller devices</title>

    <para>
      <command>libvirt</command> manages controllers automatically
      based on the type of virtual devices used by the &vmguest;. If the
      &vmguest; contains PCI and SCSI devices, PCI and SCSI controllers are
      created and managed automatically. <command>libvirt</command> also models
      controllers that are hypervisor-specific, for example, a
      <literal>virtio-serial</literal> controller for KVM &vmguest;s or a
      <literal>xenbus</literal> controller for Xen &vmguest;s. Although the
      default controllers and their configuration are generally fine, there may
      be use cases where controllers or their attributes need to be adjusted
      manually. For example, a virtio-serial controller may need more ports, or
      a xenbus controller may need more memory or more virtual interrupts.
    </para>

    <para>
      The xenbus controller is unique in that it serves as the controller for
      all Xen paravirtual devices. If a &vmguest; has many disk and/or network
      devices, the controller may need more memory. Xen's
      <literal>max_grant_frames</literal> attribute sets how many grant frames,
      or blocks of shared memory, are allocated to the
      <literal>xenbus</literal> controller for each &vmguest;.
    </para>

    <para>
      The default of 32 is enough in most circumstances, but a &vmguest; with
      numerous I/O devices and an I/O-intensive workload may experience
      performance issues because of grant frame exhaustion. The
      <command>xen-diag</command> can check the current and maximum
      <literal>max_grant_frames</literal> values for dom0 and your &vmguest;s.
      The &vmguest;s must be running:
    </para>

<screen>&prompt.sudo;virsh list
 Id   Name             State
--------------------------------
 0    Domain-0         running
 3    sle15sp1         running

 &prompt.sudo;xen-diag gnttab_query_size 0
domid=0: nr_frames=1, max_nr_frames=256

&prompt.sudo;xen-diag gnttab_query_size 3
domid=3: nr_frames=3, max_nr_frames=32
</screen>

    <para>
      The <literal>sle15sp1</literal> guest is using only three frames out of
      32. If you are seeing performance issues, and log entries that point to
      insufficient frames, increase the value with &virsh;. Look for the
      <literal>&lt;controller type='xenbus'&gt;</literal> line in the guest's
      configuration file and add the <literal>maxGrantFrames</literal> control
      element:
    </para>

<screen>&prompt.sudo;virsh edit sle15sp1
 &lt;controller type='xenbus' index='0' maxGrantFrames='40'/>
</screen>

    <para>
      Save your changes and restart the guest. Now it should show your change:
    </para>

<screen>&prompt.sudo;xen-diag gnttab_query_size 3
domid=3: nr_frames=3, max_nr_frames=40
</screen>

    <para>
      Similar to maxGrantFrames, the xenbus controller also supports
      <option>maxEventChannels</option>. Event channels are like paravirtual
      interrupts, and in conjunction with grant frames, form a data transfer
      mechanism for paravirtual drivers. They are also used for inter-processor
      interrupts. &vmguest;s with a large number of vCPUs and/or many
      paravirtual devices may need to increase the maximum default value of
      1023. maxEventChannels can be changed similarly to maxGrantFrames:
    </para>

<screen>&prompt.sudo;virsh edit sle15sp1
 &lt;controller type='xenbus' index='0' maxGrantFrames='128' maxEventChannels='2047'/>
</screen>

    <para>
      See the <citetitle>Controllers</citetitle> section of the libvirt
      <citetitle>Domain XML format</citetitle> manual at
      <link xlink:href="https://libvirt.org/formatdomain.html#elementsControllers"/>
      for more information.
    </para>
  </sect1>
  <sect1 xml:id="libvirt-video-virsh">
    <title>Configuring video devices</title>

    <para>
      When using the Virtual Machine Manager, only the Video device model can
      be defined. The amount of allocated VRAM or 2D/3D acceleration can only
      be changed in the XML configuration.
    </para>

    <sect2 xml:id="libvirt-video-vram-virsh">
      <title>Changing the amount of allocated VRAM</title>
      <procedure>
        <step>
          <para>
            Edit the configuration for an existing &vmguest;:
          </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
        </step>
        <step>
          <para>
            Change the size of the allocated VRAM:
          </para>
<screen>&lt;video&gt;
&lt;model type='vga' vram='65535' heads='1'&gt;
...
&lt;/model&gt;
&lt;/video&gt;</screen>
        </step>
        <step>
          <para>
            Check if the amount of VRAM in the VM has changed by looking at the
            amount in the Virtual Machine Manager.
          </para>
        </step>
      </procedure>
    </sect2>

    <sect2 xml:id="libvirt-video-accel-virsh">
      <title>Changing the state of 2D/3D acceleration</title>
      <procedure>
        <step>
          <para>
            Edit the configuration for an existing &vmguest;:
          </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
        </step>
        <step>
          <para>
            To enable/disable 2D/3D acceleration, change the value of
            <literal>accel3d</literal> and <literal>accel2d</literal>
            accordingly:
          </para>
<screen>
&lt;video&gt;
 &lt;model&gt;
  &lt;acceleration accel3d='yes' accel2d='no'&gt;
 &lt;/model&gt;
&lt;/video&gt;</screen>
        </step>
      </procedure>
      <tip>
        <title>Enabling 2D/3D acceleration</title>
        <para>
          Only <literal>virtio</literal> and <literal>vbox</literal> video
          devices are capable of 2D/3D acceleration. You cannot enable it on
          other video devices.
        </para>
      </tip>
    </sect2>
  </sect1>
  <sect1 xml:id="virsh-network-devices">
    <title>Configuring network devices</title>

    <para>
      This section describes how to configure specific aspects of virtual
      network devices by using &virsh;.
    </para>

    <para>
      Find more details about &libvirt; network interface specification in
      <link xlink:href="https://libvirt.org/formatdomain.html#elementsDriverBackendOptions"/>.
    </para>

    <sect2 xml:id="virsh-multiqueue">
      <title>Scaling network performance with multiqueue virtio-net</title>
      <para>
        The multiqueue virtio-net feature scales the network performance by
        allowing the &vmguest;'s virtual CPUs to transfer packets in parallel.
        Refer to <xref linkend="kvm-qemu-multiqueue"/> for more general
        information.
      </para>
      <para>
        To enable multiqueue virtio-net for a specific &vmguest;, edit its XML
        configuration as described in
        <xref linkend="sec-libvirt-config-editing-virsh"/> and modify its
        network interface as follows:
      </para>
<screen>
&lt;interface type='network'>
 [...]
 &lt;model type='virtio'/>
 &lt;driver name='vhost' queues='<replaceable>NUMBER_OF_QUEUES</replaceable>'/>
&lt;/interface>
</screen>
    </sect2>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-direct">
    <title>Using macvtap to share &vmhost; network interfaces</title>

    <para>
      Macvtap provides direct attachment of a &vmguest; virtual interface to a
      host network interface. The macvtap-based interface extends the &vmhost;
      network interface and has its own MAC address on the same Ethernet
      segment. Typically, this is used to make both the &vmguest; and the
      &vmhost; show up directly on the switch that the &vmhost; is connected
      to.
    </para>

    <note>
      <title>Macvtap cannot be used with a Linux bridge</title>
      <para>
        Macvtap cannot be used with network interfaces already connected to a
        Linux bridge. Before attempting to create the macvtap interface, remove
        the interface from the bridge.
      </para>
    </note>

    <note>
      <title>&vmguest; to &vmhost; communication with macvtap</title>
      <para>
        When using macvtap, a &vmguest; can communicate with other &vmguest;s,
        and with other external hosts on the network. But it cannot communicate
        with the &vmhost; on which the &vmguest; runs. This is the defined
        behavior of macvtap, because of the way the &vmhost;'s physical
        Ethernet is attached to the macvtap bridge. Traffic from the &vmguest;
        into that bridge that is forwarded to the physical interface cannot be
        bounced back up to the &vmhost;'s IP stack. Similarly, traffic from the
        &vmhost;'s IP stack that is sent to the physical interface cannot be
        bounced back up to the macvtap bridge for forwarding to the &vmguest;.
      </para>
    </note>

    <para>
      Virtual network interfaces based on macvtap are supported by libvirt by
      specifying an interface type of <literal>direct</literal>. For example:
    </para>

<screen>&lt;interface type='direct'&gt;
   &lt;mac address='aa:bb:cc:dd:ee:ff'/&gt;
   &lt;source dev='eth0' mode='bridge'/&gt;
   &lt;model type='virtio'/&gt;
   &lt;/interface&gt;</screen>

    <para>
      The operation mode of the macvtap device can be controlled with the
      <literal>mode</literal> attribute. The following list shows its possible
      values and a description for each:
    </para>

    <itemizedlist mark="bullet" spacing="normal">
      <listitem>
        <para>
          <literal>vepa</literal>: all &vmguest; packets are sent to an
          external bridge. Packets whose destination is a &vmguest; on the same
          &vmhost; as where the packet originates from are sent back to the
          &vmhost; by the VEPA capable bridge (today's bridges are typically
          not VEPA capable).
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>bridge</literal>: packets whose destination is on the same
          &vmhost; as where they originate from are directly delivered to the
          target macvtap device. Both origin and destination devices need to be
          in <literal>bridge</literal> mode for direct delivery. If either
          of them is in <literal>vepa</literal> mode, a VEPA capable bridge is
          required.
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>private</literal>: all packets are sent to the external
          bridge and delivered to a target &vmguest; on the same &vmhost; if
          they are sent through an external router or gateway and that device
          sends them back to the &vmhost;. This procedure is followed if either
          the source or destination device is in private mode.
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>passthrough</literal>: a special mode that gives more power
          to the network interface. All packets are forwarded to the interface,
          allowing virtio &vmguest;s to change the MAC address or set
          promiscuous mode to bridge the interface or create VLAN interfaces on
          top of it. Note that a network interface is not shareable in
          <literal>passthrough</literal> mode. Assigning an interface to a
          &vmguest; disconnects it from the &vmhost;. For this reason SR-IOV
          virtual functions are often assigned to the &vmguest; in
          <literal>passthrough</literal> mode.
        </para>
      </listitem>
    </itemizedlist>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-disable-virtio-mellon">
    <title>Disabling a memory balloon device</title>

    <para>
      Memory Balloon has become a default option for KVM. The device is added
      to the &vmguest; explicitly, so you do not need to add this element in
      the &vmguest;'s XML configuration. If you want to disable Memory Balloon
      in the &vmguest; for any reason, you need to set
      <literal>model='none'</literal> as shown below:
    </para>

<screen>&lt;devices&gt;
   &lt;memballoon model='none'/&gt;
&lt;/device&gt;</screen>
  </sect1>
  <sect1 xml:id="virsh-video-dual-head">
    <title>Configuring multiple monitors (dual head)</title>

    <para>
      &libvirt; supports a dual head configuration to display the video output
      of the &vmguest; on multiple monitors.
    </para>

    <important>
      <title>No support for &xen;</title>
      <para>
        The &xen; hypervisor does not support dual head configuration.
      </para>
    </important>

    <procedure>
      <title>Configuring dual head</title>
      <step>
        <para>
          While the virtual machine is running, verify that the
          <package>xf86-video-qxl</package> package is installed in the
          &vmguest;:
        </para>
<screen>&prompt.user;rpm -q xf86-video-qxl</screen>
      </step>
      <step>
        <para>
          Shut down the &vmguest; and start editing its configuration XML as
          described in <xref linkend="sec-libvirt-config-editing-virsh"/>.
        </para>
      </step>
      <step>
        <para>
          Verify that the model of the virtual graphics card is
          <quote>qxl</quote>:
        </para>
<screen>
&lt;video>
 &lt;model type='qxl' ... />
</screen>
      </step>
      <step>
        <para>
          Increase the <option>heads</option> parameter in the graphics card
          model specification from the default <literal>1</literal> to
          <literal>2</literal>, for example:
        </para>
<screen>
&lt;video>
 &lt;model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='2' primary='yes'/>
 &lt;alias name='video0'/>
 &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x0'/>
&lt;/video>
</screen>
      </step>
      <step>
        <para>
          Configure the virtual machine to use the Spice display instead of
          VNC:
        </para>
<screen>
&lt;graphics type='spice' port='5916' autoport='yes' listen='0.0.0.0'>
 &lt;listen type='address' address='0.0.0.0'/>
&lt;/graphics>
</screen>
      </step>
      <step>
        <para>
          Start the virtual machine and connect to its display with
          <command>virt-viewer</command>, for example:
        </para>
<screen>&prompt.user;virt-viewer --connect qemu+ssh://<replaceable>USER@VM_HOST</replaceable>/system</screen>
      </step>
      <step>
        <para>
          From the list of VMs, select the one whose configuration you have
          modified and confirm with <guimenu>Connect</guimenu>.
        </para>
      </step>
      <step>
        <para>
          After the graphical subsystem (Xorg) loads in the &vmguest;, select
          <menuchoice><guimenu>View</guimenu><guimenu>Displays</guimenu><guimenu>Display
          2</guimenu></menuchoice> to open a new window with the second
          monitor's output.
        </para>
      </step>
    </procedure>
  </sect1>
  <sect1 xml:id="virsh-kvm-zseries-crypto">
    <title>Crypto adapter pass-through to &kvm; guests on &zseries;</title>

    <sect2 xml:id="virsh-kvm-zseries-crypto-intro">
      <title>Introduction</title>
      <para>
        &zseries; machines include cryptographic hardware with useful functions
        such as random number generation, digital signature generation, or
        encryption. &kvm; allows dedicating these crypto adapters to guests as
        pass-through devices. The means that the hypervisor cannot observe
        communications between the guest and the device.
      </para>
    </sect2>

    <sect2 xml:id="virsh-kvm-zseries-crypto-cover">
      <title>What is covered</title>
      <para>
        This section describes how to dedicate a crypto adapter and domains on
        an &zseries; host to a &kvm; guest. The procedure includes the
        following basic steps:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            Mask the crypto adapter and domains from the default driver on the
            host.
          </para>
        </listitem>
        <listitem>
          <para>
            Load the <literal>vfio-ap</literal> driver.
          </para>
        </listitem>
        <listitem>
          <para>
            Assign the crypto adapter and domains to the
            <literal>vfio-ap</literal> driver.
          </para>
        </listitem>
        <listitem>
          <para>
            Configure the guest to use the crypto adapter.
          </para>
        </listitem>
      </itemizedlist>
    </sect2>

    <sect2 xml:id="virsh-kvm-zseries-crypto-reqs">
      <title>Requirements</title>
      <itemizedlist>
        <listitem>
          <para>
            You need to have the &qemu; / &libvirt; virtualization environment
            correctly installed and functional.
          </para>
        </listitem>
        <listitem>
          <para>
            The <literal>vfio_ap</literal> and <literal>vfio_mdev</literal>
            modules for the running kernel need to be available on the host
            operating system.
          </para>
        </listitem>
      </itemizedlist>
    </sect2>

    <sect2 xml:id="virsh-kvm-zseries-crypto-proc">
      <title>Dedicate a crypto adapter to a &kvm; host</title>
      <procedure>
        <step>
          <para>
            Verify that the <literal>vfio_ap</literal> and
            <literal>vfio_mdev</literal> kernel modules are loaded on the host:
          </para>
<screen>&prompt.user;lsmod | grep vfio_</screen>
          <para>
            If any of them is not listed, load it manually, for example:
          </para>
<screen>&prompt.sudo;modprobe vfio_mdev</screen>
        </step>
        <step>
          <para>
            Create a new MDEV device on the host and verify that it was added:
          </para>
<screen>
uuid=$(uuidgen)
$ echo ${uuid} | sudo tee /sys/devices/vfio_ap/matrix/mdev_supported_types/vfio_ap-passthrough/create
dmesg | tail
[...]
[272197.818811] iommu: Adding device 24f952b3-03d1-4df2-9967-0d5f7d63d5f2 to group 0
[272197.818815] vfio_mdev 24f952b3-03d1-4df2-9967-0d5f7d63d5f2: MDEV: group_id = 0
</screen>
        </step>
        <step>
          <para>
            Identify the device on the host's logical partition that you intend
            to dedicate to a &kvm; guest:
          </para>
<screen>&prompt.user;ls -l /sys/bus/ap/devices/
[...]
lrwxrwxrwx 1 root root 0 Nov 23 03:29 00.0016 -> ../../../devices/ap/card00/00.0016/
lrwxrwxrwx 1 root root 0 Nov 23 03:29 card00 -> ../../../devices/ap/card00/
</screen>
          <para>
            In this example, it is card <literal>0</literal> queue
            <literal>16</literal>. To match the Hardware Management Console
            (HMC) configuration, you need to convert from <literal>16</literal>
            hexadecimal to <literal>22</literal> decimal.
          </para>
        </step>
        <step>
          <para>
            Mask the adapter from the <literal>zcrypt</literal> use:
          </para>
<screen>
&prompt.user;lszcrypt
CARD.DOMAIN TYPE MODE STATUS REQUEST_CNT
-------------------------------------------------
00 CEX5C CCA-Coproc online 5
00.0016 CEX5C CCA-Coproc online 5
</screen>
          <para>
            Mask the adapter:
          </para>
<screen>
&prompt.user;cat /sys/bus/ap/apmask
0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
echo -0x0 | sudo tee /sys/bus/ap/apmask
0x7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
</screen>
          <para>
            Mask the domain:
          </para>
<screen>
&prompt.user;cat /sys/bus/ap/aqmask
0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
echo -0x0 | sudo tee /sys/bus/ap/aqmask
0xfffffdffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
</screen>
        </step>
        <step>
          <para>
            Assign adapter 0 and domain 16 (22 decimal) to
            <literal>vfio-ap</literal>:
          </para>
<screen>
&prompt.sudo;echo +0x0 > /sys/devices/vfio_ap/matrix/${uuid}/assign_adapter
&prompt.user;echo +0x16 | sudo tee /sys/devices/vfio_ap/matrix/${uuid}/assign_domain
&prompt.user;echo +0x16 | sudo tee /sys/devices/vfio_ap/matrix/${uuid}/assign_control_domain
</screen>
        </step>
        <step>
          <para>
            Verify the matrix that you have configured:
          </para>
<screen>
&prompt.user;cat /sys/devices/vfio_ap/matrix/${uuid}/matrix
00.0016
</screen>
        </step>
        <step>
          <para>
            Either create a new VM (refer to <xref linkend="cha-kvm-inst"/>)
            and wait until it is initialized, or use an existing VM. In both
            cases, make sure the VM is shut down.
          </para>
        </step>
        <step>
          <para>
            Change its configuration to use the MDEV device:
          </para>
<screen>
&prompt.sudo;virsh edit <replaceable>VM_NAME</replaceable>
[...]
&lt;hostdev mode='subsystem' type='mdev' model='vfio-ap'>
 &lt;source>
  &lt;address uuid='24f952b3-03d1-4df2-9967-0d5f7d63d5f2'/>
 &lt;/source>
&lt;/hostdev>
[...]
</screen>
        </step>
        <step>
          <para>
            Restart the VM:
          </para>
<screen>&prompt.sudo;virsh reboot <replaceable>VM_NAME</replaceable></screen>
        </step>
        <step>
          <para>
            Log in to the guest and verify that the adapter is present:
          </para>
<screen>
&prompt.user;lszcrypt
CARD.DOMAIN TYPE MODE STATUS REQUEST_CNT
-------------------------------------------------
00 CEX5C CCA-Coproc online 1
00.0016 CEX5C CCA-Coproc online 1
</screen>
        </step>
      </procedure>
    </sect2>

    <sect2 xml:id="virsh-kvm-zseries-crypto-moreinfo">
      <title>Further reading</title>
      <itemizedlist>
        <listitem>
          <para>
            The installation of virtualization components is detailed in
            <xref linkend="cha-vt-installation"/>.
          </para>
        </listitem>
        <listitem>
          <para>
            The <literal>vfio_ap</literal> architecture is detailed in
            <link xlink:href="https://www.kernel.org/doc/Documentation/s390/vfio-ap.txt"/>.
          </para>
        </listitem>
        <listitem>
          <para>
            A general outline together with a detailed procedure is described
            in
            <link xlink:href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1787405"/>.
          </para>
        </listitem>
        <listitem>
          <para>
            The architecture of VFIO Mediated devices (MDEVs) is detailed in
            <link xlink:href="https://www.kernel.org/doc/html/latest/driver-api/vfio-mediated-device.html"/>.
          </para>
        </listitem>
      </itemizedlist>
    </sect2>
  </sect1>
</chapter>
