<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.qemu.host">
 <title>Setting Up a &kvm; &vmhost;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
  </dm:docmanager>
 </info>
 <para>
  This section documents how to set up and use &productname; &productnumber;
  as a &qemu;-&kvm; based virtual machine host.
 </para>
 <tip>
  <title>Resources</title>
  <para>
   In general, the virtual guest system needs the same hardware resources as
   when installed on a physical machine. The more guests you plan to run on
   the host system, the more hardware resources&mdash;CPU, disk, memory, and
   network&mdash;you need to add to the &vmhost;.
  </para>
 </tip>
 <sect1 xml:id="kvm.host.cpu">
  <title>CPU Support for Virtualization</title>

  <para>
   To run &kvm;, your CPU must support virtualization, and
   virtualization needs to be enabled in BIOS. The file
   <filename>/proc/cpuinfo</filename> includes information about your CPU
   features.
  </para>

  <para os="sles;sled">
   To find out whether your system supports virtualization, see
   <xref linkend="sec.kvm.requires.hardware"/>.
  </para>
 </sect1>
 <sect1 xml:id="kvm.host.soft">
  <title>Required Software</title>

  <para>
   The &kvm; host requires several packages to be installed. To install all
   necessary packages, do the following:
  </para>

  <procedure>
   <step>
    <para>
     Run <menuchoice><guimenu>&yast;</guimenu><guimenu>
     Virtualization</guimenu><guimenu>Install Hypervisor and
     Tools</guimenu></menuchoice>.
    </para>
    <figure os="sles;sled">
     <title>Installing the &kvm; Hypervisor and Tools</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_hypervisors.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_hypervisors.png" width="75%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>

    <figure os="osuse">
     <title>Installing the &kvm; Hypervisor and Tools</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_hypervisors_kde.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_hypervisors_kde.png" width="75%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>

   </step>
   <step>
    <para>
     Select <guimenu>KVM server</guimenu> and preferably also <guimenu>KVM
     tools</guimenu>, and confirm with <guimenu>Accept</guimenu>.
    </para>
   </step>
   <step>
    <para>
     During the installation process, you can optionally let &yast; create a
     <guimenu>Network Bridge</guimenu> for you automatically. If you do not
     plan to dedicate an additional physical network card to your virtual
     guests, network bridge is a standard way to connect the guest machines
     to the network.
    </para>
    <figure>
     <title>Network Bridge</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_netbridge.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_netbridge.png" width="75%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
   <step>
    <para>
     After all the required packages are installed (and new network setup
     activated), try to load the &kvm; kernel module relevant for your CPU
     type&mdash;<systemitem>kvm-intel</systemitem> or
     <systemitem>kvm-amd</systemitem>:
    </para>
<screen>&prompt.root;modprobe kvm-intel</screen>
    <para>
     Check if the module is loaded into memory:
    </para>
<screen>&prompt.user;lsmod | grep kvm
kvm_intel              64835  6
kvm                   411041  1 kvm_intel</screen>
    <para>
     Now the &kvm; host is ready to serve &kvm; &vmguest;s. For more
     information, see <xref linkend="cha.qemu.running"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="kvm.host.virtio">
  <title>&kvm; Host-Specific Features</title>

  <para>
   You can improve the performance of &kvm;-based &vmguest;s by letting them
   fully use specific features of the &vmhost;'s hardware
   (<emphasis>paravirtualization</emphasis>). This section introduces
   techniques to make the guests access the physical host's hardware
   directly&mdash;without the emulation layer&mdash;to make the most use of
   it.
  </para>

  <tip>
   <para>
    Examples included in this section assume basic knowledge of the
    <command>qemu-system-<replaceable>ARCH</replaceable></command> command
    line options. For more information, see
    <xref linkend="cha.qemu.running"/>.
   </para>
  </tip>

  <sect2 xml:id="kvm.virtio-scsi">
   <title>Using the Host Storage with <systemitem>virtio-scsi</systemitem></title>
   <para>
    <systemitem>virtio-scsi</systemitem> is an advanced storage stack for
    &kvm;. It replaces the former <systemitem>virtio-blk</systemitem> stack
    for SCSI devices pass-through. It has several advantages over
    <systemitem>virtio-blk</systemitem>:
   </para>
   <variablelist>
    <varlistentry>
     <term>Improved scalability</term>
     <listitem>
      <para>
       &kvm; guests have a limited number of PCI controllers, which results
       in a limited number of possibly attached devices.
       <systemitem>virtio-scsi</systemitem> solves this limitation by
       grouping multiple storage devices on a single controller. Each device
       on a <systemitem>virtio-scsi</systemitem> controller is represented
       as a logical unit, or <emphasis>LUN</emphasis>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Standard command set</term>
     <listitem>
      <para>
       <systemitem>virtio-blk</systemitem> uses a small set of
       commands that need to be known to both the
       <systemitem>virtio-blk</systemitem> driver and the virtual machine
       monitor, and so introducing a new command requires updating both the
       driver and the monitor.
      </para>
      <para>
       By comparison, <systemitem>virtio-scsi</systemitem> does not define
       commands, but rather a transport protocol for these commands following
       the industry-standard SCSI specification. This approach is shared with
       other technologies, such as Fibre Channel, ATAPI, and USB devices.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Device naming</term>
     <listitem>
      <para>
       <systemitem>virtio-blk</systemitem> devices are presented inside the
       guest as <filename>/dev/vd<replaceable>X</replaceable></filename>,
       which is different from device
       names in physical systems and may cause migration problems.
      </para>
      <para>
       <systemitem>virtio-scsi</systemitem> keeps the device names identical
       to those on physical systems, making the virtual machines easily
       relocatable.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>SCSI device pass-through</term>
     <listitem>
      <para>
       For virtual disks backed by a whole LUN on the host, it is preferable
       for the guest to send SCSI commands directly to the LUN
       (pass-through). This is limited in
       <systemitem>virtio-blk</systemitem>, as guests need to use the
       virtio-blk protocol instead of SCSI command pass-through, and,
       moreover, it is not available for Windows guests.
       <systemitem>virtio-scsi</systemitem> natively removes these
       limitations.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <sect3>
    <title><systemitem>virtio-scsi</systemitem> Usage</title>
    <para>
     &kvm; supports the SCSI pass-through feature with the
     <systemitem>virtio-scsi-pci</systemitem> device:
    </para>
<screen>qemu-system-x86_64 [...] \
-device virtio-scsi-pci,id=scsi</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="kvm.qemu.vnet">
   <title>Accelerated Networking with <systemitem>vhost-net</systemitem></title>
   <para>
    The <systemitem>vhost-net</systemitem> module is used to accelerate
    &kvm;'s paravirtualized network drivers. It provides better latency and
    greater network throughput. Use the <literal>vhost-net</literal>
    driver by starting the guest with the following example command line:
   </para>
<screen>qemu-system-x86_64 [...] \
-netdev tap,id=guest0,vhost=on,script=no \
-net nic,model=virtio,netdev=guest0,macaddr=00:16:35:AF:94:4B</screen>
   <para>
    Note that <literal>guest0</literal> is an identification string of the
    vhost-driven device.
   </para>
  </sect2>

  <sect2 xml:id="kvm.qemu.multiqueue">
   <title>Scaling Network Performance with Multiqueue virtio-net</title>
   <para>
    As the number of virtual CPUs increases in &vmguest;s, &qemu; offers a
    way of improving the network performance using
    <emphasis>multiqueue</emphasis>. Multiqueue virtio-net scales the
    network performance by allowing &vmguest; virtual CPUs to transfer
    packets in parallel. Multiqueue support is required on both the &vmhost;
    and &vmguest; sides.
   </para>
   <tip>
    <title>Performance Benefit</title>
    <para>
     The multiqueue virtio-net solution is most beneficial in the following
     cases:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       Network traffic packets are large.
      </para>
     </listitem>
     <listitem>
      <para>
       &vmguest; has many connections active at the same time, mainly
       between the guest systems, or between the guest and the host, or
       between the guest and an external system.
      </para>
     </listitem>
     <listitem>
      <para>
       The number of active queues is equal to the number of virtual CPUs in
       the &vmguest;.
      </para>
     </listitem>
    </itemizedlist>
   </tip>
   <note>
    <para>
     While multiqueue virtio-net increases the total network throughput, it
     increases CPU consumption as it uses of the virtual CPU's power.
    </para>
   </note>
   <procedure xml:id="kvm.qemu.mq.enable">
    <title>How to Enable Multiqueue virtio-net</title>
    <para>
     The following procedure lists important steps to enable the multiqueue
     feature with <command>qemu-system-ARCH</command>. It assumes that a tap
     network device with multiqueue capability (supported since kernel
     version 3.8) is set up on the &vmhost;.
    </para>
    <step>
     <para>
      In <command>qemu-system-ARCH</command>, enable multiqueue for the tap
      device:
     </para>
<screen>-netdev tap,vhost=on,queues=<replaceable>2*N</replaceable></screen>
     <para>
      where <literal>N</literal> stands for the number of queue pairs.
     </para>
    </step>
    <step>
     <para>
      In <command>qemu-system-ARCH</command>, enable multiqueue and specify
      MSI-X (Message Signaled Interrupt) vectors for the virtio-net-pci
      device:
     </para>
<screen>-device virtio-net-pci,mq=on,vectors=<replaceable>2*N+2</replaceable></screen>
     <para>
      where the formula for the number of MSI-X vectors results from: N
      vectors for TX (transmit) queues, N for RX (receive) queues, one for
      configuration purposes, and one for possible VQ (vector quantization)
      control.
     </para>
    </step>
    <step>
     <para>
      In &vmguest;, enable multiqueue on the relevant network interface
      (<literal>eth0</literal> in this example):
     </para>
<screen>ethtool -L eth0 combined 2*N</screen>
    </step>
   </procedure>
   <para>
    The resulting <command>qemu-system-ARCH</command> command line will look
    similar to the following example:
   </para>
<screen>qemu-system-x86_64 [...] -netdev tap,id=guest0,queues=8,vhost=on \
-device virtio-net-pci,netdev=guest0,mq=on,vectors=10</screen>
   <para>
    Note that the <literal>id</literal> of the network device
    (<literal>guest0</literal> ) needs to be identical for both options.
   </para>
   <para>
    Inside the running &vmguest;, specify the following command as
    &rootuser;:
   </para>
<screen>ethtool -L eth0 combined 8</screen>
   <para>
    Now the guest system networking uses the multiqueue support from the
    <command>qemu-system-ARCH</command> hypervisor.
   </para>
  </sect2>

  <sect2 xml:id="kvm.vfio">
   <title>VFIO: Secure Direct Access to Devices</title>
   <para>
    Directly assigning a PCI device to a &vmguest; (PCI pass-through) avoids
    performance issues caused by avoiding any emulation in performance-critical
    paths. VFIO replaces the traditional &kvm; &pciback; device
    assignment. A prerequisite for this feature is a &vmhost; configuration
    as described in <xref linkend="ann.vt.io.require"/>.
   </para>
   <para>
    To be able to assign a PCI device via VFIO to a &vmguest;, you need to
    find out which IOMMU Group it belongs to. The
    <xref
    linkend="gloss.vt.acronym.iommu"/> (input/output memory
    management unit that connects a direct memory access-capable I/O bus to
    the main memory) API supports the notion of groups. A group is a set of
    devices that can be isolated from all other devices in the system.
    Groups are therefore the unit of ownership used by
    <xref linkend="vt.io.vfio"/>.
   </para>
   <procedure>
    <title>Assigning a PCI Device to a &vmguest; via VFIO</title>
    <step>
     <para>
      Identify the host PCI device to assign to the guest.
     </para>
<screen>&prompt.user;sudo lspci -nn
[...]
00:10.0 Ethernet controller [0200]: Intel Corporation 82576 \
Virtual Function [8086:10ca] (rev 01)
[...]</screen>
     <para>
      Note down the device ID (<literal>00:10.0</literal> in this case) and
      the vendor ID (<literal>8086:10ca</literal>).
     </para>
    </step>
    <step>
     <para>
      Find the IOMMU group of this device:
     </para>
<screen>&prompt.user;sudo readlink /sys/bus/pci/devices/0000\:00\:10.0/iommu_group
../../../kernel/iommu_groups/20</screen>
     <para>
      The IOMMU group for this device is <literal>20</literal>. Now you can
      check the devices belonging to the same IOMMU group:
     </para>
<screen>ls -l /sys/bus/pci/devices/0000:01:10.0/iommu_group/devices/0000:01:10.0
[...] 0000:00:1e.0 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0
[...] 0000:01:10.0 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0/0000:01:10.0
[...] 0000:01:10.1 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0/0000:01:10.1</screen>
    </step>
    <step>
     <para>
      Unbind the device from the device driver:
     </para>
<screen>sudo echo "0000:01:10.0" &gt; /sys/bus/pci/devices/0000\:01\:10.0/driver/unbind</screen>
    </step>
    <step>
     <para>
      Bind the device to the vfio-pci driver using the vendor ID from step
      1:
     </para>
<screen>sudo echo "8086 153a" &gt; /sys/bus/pci/drivers/vfio-pci/new_id</screen>
     <para>
      A new device
      <filename>/dev/vfio/<replaceable>IOMMU_GROUP</replaceable></filename>
      will be created as a result, <filename>/dev/vfio/20</filename> in this
      case.
     </para>
    </step>
    <step>
     <para>
      Change the ownership of the newly created device:
     </para>
<screen>chown qemu.qemu /dev/vfio/<replaceable>DEVICE</replaceable></screen>
    </step>
    <step>
     <para>
      Now run the &vmguest; with the PCI device assigned.
     </para>
<screen>qemu-system-<replaceable>ARCH</replaceable> [...] -device
     vfio-pci,host=00:10.0,id=<replaceable>ID</replaceable></screen>
    </step>
   </procedure>
   <important>
    <title>No Hotplugging</title>
    <para>
     As of &productname; &productnumber; hotplugging of PCI devices passed
     to a &vmguest; via VFIO is not supported.
    </para>
   </important>
<!-- fs 2015-10-09: not yet supported
   <para>
    &productname; also supports PCI device hotplugging to a &vmguest;. To
    achieve this, you need to switch to a &qemu; monitor (see
    <xref linkend="cha.qemu.monitor"/> for more information) and issue the
    following commands:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      hot-add:
     </para>
<screen>device_add vfio-pci,host=00:10.0,id=<replaceable>ID</replaceable></screen>
    </listitem>
    <listitem>
     <para>
      hot-remove:
     </para>
<screen>device_del <replaceable>ID</replaceable></screen>
    </listitem>
   </itemizedlist>
-->
   <para>
    You can find more detailed information on the
    <xref linkend="vt.io.vfio"/> driver in the
    <filename>/usr/src/linux/Documentation/vfio.txt</filename> file (package
    <systemitem>kernel-source</systemitem> needs to be installed).
   </para>
  </sect2>

  <sect2 xml:id="kvm.qemu.virtfs">
   <title>VirtFS: Sharing Directories between Host and Guests</title>
   <para>
    &vmguest;s usually run in a separate computing space&mdash;they are
    provided their own memory range, dedicated CPUs, and file system space.
    The ability to share parts of the &vmhost;'s file system makes the
    virtualization environment more flexible by simplifying mutual data
    exchange. Network file systems, such as CIFS and NFS, have been the
    traditional way of sharing directories. But as they are not specifically
    designed for virtualization purposes, they suffer from major performance
    and feature issues.
   </para>
   <para>
    &kvm; introduces a new optimized method called
    <emphasis>VirtFS</emphasis> (sometimes called <quote>file system
    pass-through</quote>). VirtFS uses a paravirtual file system driver,
    which avoids converting the guest application file system operations
    into block device operations, and then again into host file system
    operations.
   </para>
   <para>
    You typically use VirtFS for the following situations:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      To access a shared directory from several guests, or to provide
      guest-to-guest file system access.
     </para>
    </listitem>
    <listitem>
     <para>
      To replace the virtual disk as the root file system to which the
      guest's RAM disk connects during the guest boot process.
     </para>
    </listitem>
    <listitem>
     <para>
      To provide storage services to different customers from a single host
      file system in a cloud environment.
     </para>
    </listitem>
   </itemizedlist>
   <sect3 xml:id="kvm.qemu.virtfs.implement">
    <title>Implementation</title>
    <para>
     In &qemu;, the implementation of VirtFS is simplified by defining two
     types of devices:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <literal>virtio-9p-pci</literal> device which transports protocol
       messages and data between the host and the guest.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>fsdev</literal> device which defines the export file system
       properties, such as file system type and security model.
      </para>
     </listitem>
    </itemizedlist>
    <example xml:id="ex.qemu.virtfs.host">
     <title>Exporting Host's File System with VirtFS</title>
<screen>qemu-system-x86_64 [...] \
-fsdev local,id=exp1<co xml:id="co.virtfs.host.id"/>,path=/tmp/<co xml:id="co.virtfs.host.path"/>,security_model=mapped<co xml:id="co.virtfs.host.sec_model"/> \
-device virtio-9p-pci,fsdev=exp1<co xml:id="co.virtfs.host.fsdev"/>,mount_tag=v_tmp<co xml:id="co.virtfs.host.mnt_tag"/></screen>
     <calloutlist>
      <callout arearefs="co.virtfs.host.id">
       <para>
        Identification of the file system to be exported.
       </para>
      </callout>
      <callout arearefs="co.virtfs.host.path">
       <para>
        File system path on the host to be exported.
       </para>
      </callout>
      <callout arearefs="co.virtfs.host.sec_model">
       <para>
        Security model to be used&mdash;<literal>mapped</literal> keeps the
        guest file system modes and permissions isolated from the host,
        while <literal>none</literal> invokes a <quote>pass-through</quote>
        security model in which permission changes on the guest's files are
        reflected on the host as well.
       </para>
      </callout>
      <callout arearefs="co.virtfs.host.fsdev">
       <para>
        The exported file system ID defined before with <literal>-fsdev
        id=</literal> .
       </para>
      </callout>
      <callout arearefs="co.virtfs.host.mnt_tag">
       <para>
        Mount tag used later on the guest to mount the exported file system.
       </para>
      </callout>
     </calloutlist>
     <para>
      Such an exported file system can be mounted on the guest as follows:
     </para>
<screen>sudo mount -t 9p -o trans=virtio v_tmp /mnt</screen>
     <para>
      where <literal>v_tmp</literal> is the mount tag defined earlier with
      <literal>-device mount_tag=</literal> and <literal>/mnt</literal> is
      the mount point where you want to mount the exported file system.
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2 xml:id="kvm.qemu.ksm">
   <title>KSM: Sharing Memory Pages between Guests</title>
   <para>
    Kernel Same Page Merging (<xref linkend="gloss.vt.acronym.ksm"/>) is a
    Linux kernel feature that merges identical memory pages from multiple
    running processes into one memory region. Because &kvm; guests run as
    processes under Linux, <xref linkend="gloss.vt.acronym.ksm"/> provides
    the memory overcommit feature to hypervisors for more efficient use of
    memory. Therefore, if you need to run multiple virtual machines on a
    host with limited memory, <xref linkend="gloss.vt.acronym.ksm"/> may be
    helpful to you.
   </para>
   <para>
    <xref linkend="gloss.vt.acronym.ksm"/> stores its status information in
    the files under the <filename>/sys/kernel/mm/ksm</filename> directory:
   </para>
<screen>&prompt.user;ls -1 /sys/kernel/mm/ksm
full_scans
merge_across_nodes
pages_shared
pages_sharing
pages_to_scan
pages_unshared
pages_volatile
run
sleep_millisecs</screen>
   <para>
    For more information on the meaning of the
    <filename>/sys/kernel/mm/ksm/*</filename> files, see
    <filename>/usr/src/linux/Documentation/vm/ksm.txt</filename> (package
    <systemitem>kernel-source</systemitem>).
   </para>
   <para>
    To use <xref linkend="gloss.vt.acronym.ksm"/>, do the following.
   </para>
   <procedure>
    <step>
     <para>
      Although &productnameshort; includes
      <xref linkend="gloss.vt.acronym.ksm"/> support in the kernel, it is
      disabled by default. To enable it, run the following command:
     </para>
<screen>&prompt.root;echo 1 > /sys/kernel/mm/ksm/run</screen>
    </step>
    <step>
     <para>
      Now run several &vmguest;s under &kvm; and inspect the content of
      files <filename>pages_sharing</filename> and
      <filename>pages_shared</filename>, for example:
     </para>
<screen>while [ 1 ]; do cat /sys/kernel/mm/ksm/pages_shared; sleep 1; done
13522
13523
13519
13518
13520
13520
13528</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
