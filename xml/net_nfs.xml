<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!--
  TODO for SLES12

  Reorganize chapter into the following structure (if possible):

  * Terminology/Conceptual Overview
    - General Infos about NFSv2, NFSv3, and NFSv4
      (may be separate subsections or
  * Installation
  * Configuration
     - YaST (merge procedure into one? Is it possible?)
       Also add information which files are modified by YaST
     - Manually

-->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.nfs">
<!--ix-->
<!-- == SLED ======================================================= -->
<!-- == osuse and SLES ============================================ -->
 <title>Sharing File Systems with NFS</title>
 <info>
  <abstract>
   <para>
    Distributing and sharing file systems over a network is a common
    task in corporate environments. The well-proven network file system
    (<emphasis>NFS</emphasis>) works with <emphasis>NIS</emphasis>, the
    yellow pages protocol. For a more secure protocol that works with
    <emphasis>LDAP</emphasis> and Kerberos, check
    <emphasis>NFSv4</emphasis> (default). Combined with pNFS, you can
    eliminate performance bottlenecks.
   </para>
   <para>
    NFS with NIS makes a network transparent to the user. With NFS, it is
    possible to distribute arbitrary file systems over the network. With an
    appropriate setup, users always find themselves in the same environment
    regardless of the terminal they currently use.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <!--ix-->
 <indexterm>
 <primary>NFS</primary></indexterm>
 <indexterm>
 <primary>Network File System</primary>
 <see>NFS</see>
 </indexterm>
 <important os="sles;osuse">
  <title>Need for DNS</title>
  <para>
   In principle, all exports can be made using IP addresses only. To avoid
   time-outs, you need a working DNS system. DNS is necessary at least for
   logging purposes, because the mountd daemon does reverse lookups.
  </para>
 </important>
 <sect1 xml:id="sec.nfs.term">
  <title>Terminology</title>

  <para>
   The following are terms used in the &yast; module.
  </para>

<!-- toms 2011-07-28: Used Samba file as a template: -->

  <variablelist>
   <varlistentry>
    <term>Exports</term>
    <listitem>
     <para>
      A directory <emphasis>exported</emphasis> by an NFS server, which
      clients can integrate it into their system.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS Client</term>
    <listitem>
     <para>
      The NFS client is a system that uses NFS services from an NFS server
      over the Network File System protocol. The TCP/IP protocol is already
      integrated into the Linux kernel; there is no need to install any
      additional software.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS Server</term>
    <listitem>
     <para>
      The NFS server provides NFS services to clients. A running server
      depends on the following daemons:
      <systemitem class="daemon">nfsd</systemitem> (worker),
      <systemitem class="daemon">idmapd</systemitem> (user and group name
      mappings to IDs and vice versa),
      <systemitem class="daemon">statd</systemitem> (file locking), and
      <systemitem class="daemon">mountd</systemitem> (mount requests).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFSv3</term>
    <listitem>
     <para>
      NFSv3 is the version 3 implementation, the <quote>old</quote>
      stateless NFS that supports client authentication.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFSv4</term>
    <listitem>
     <para>
      NFSv4 is the new version 4 implementation that supports secure user
      authentication via kerberos. NFSv4 requires one single port only and
      thus is better suited for environments behind a firewall than NFSv3.
     </para>
     <para>
      The protocol is specified as
      <link xlink:href="http://tools.ietf.org/html/rfc3530"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>pNFS</term>
    <listitem>
     <para>
      Parallel NFS, a protocol extension of NFSv4. Any pNFS clients can
      directly access the data on an NFS server.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
<!-- == SLED ======================================================= -->
 <sect1 os="sled" xml:id="sec.nfs.installation-sled">
<!-- For the moment, use different IDs -->

  <title>Installing NFS Server</title>

  <para>
   For installing and configuring an NFS server, see the &sls;
   documentation.
  </para>
 </sect1>
<!-- == osuse and SLES ============================================ -->
 <sect1 os="sles;osuse" xml:id="sec.nfs.installation">
  <title>Installing NFS Server</title>

  <para>
   The NFS server software is not part of the default installation. If you
   configure an NFS server as described in
   <xref linkend="sec.nfs.configuring-nfs-server"/> you will automatically
   be prompted to install the required packages. Alternatively, install the
   package <systemitem class="resource">nfs-kernel-server</systemitem> with
   &yast; or Zypper.
  </para>

  <para>
   Like NIS, NFS is a client/server system. However, a machine can be
   both&mdash;it can supply file systems over the network (export) and mount
   file systems from other hosts (import).
  </para>

  <note>
<!-- bnc#870129 -->
   <title>Mounting NFS Volumes Locally on the Exporting Server</title>
   <para>
    Mounting NFS volumes locally on the exporting server is not supported on
    &sle; systems, as is the case on all Enterprise-class Linux systems.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec.nfs.configuring-nfs-server" os="sles;osuse">
  <title>Configuring NFS Server</title>

  <para>
   Configuring an NFS server can be done either through &yast; or manually.
   For authentication, NFS can also be combined with Kerberos.
  </para>

  <sect2 xml:id="sec.nfs.export-yast2">
   <title>Exporting File Systems with &yast;</title><indexterm>
   <primary>NFS</primary>
   <secondary>servers</secondary></indexterm>
   <para>
    With &yast;, turn a host in your network into an NFS server&mdash;a server
    that exports directories and files to all hosts granted access to it or to
    all members of a group. Thus, the server can also provide applications
    without installing the applications locally on every host.
   </para>
   <para>
    To set up such a server, proceed as follows:
   </para>
   <procedure xml:id="pro.nfs.export-yast2.nfs">
    <title>Setting Up an NFS Server</title>
    <step>
     <para>
      Start &yast; and select <menuchoice> <guimenu>Network
      Services</guimenu> <guimenu>NFS Server</guimenu> </menuchoice>; see
      <xref linkend="fig.inst.nfsserver1"/>. You may be prompted to install
      additional software.
     </para>
     <figure xml:id="fig.inst.nfsserver1">
      <title>NFS Server Configuration Tool</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_inst_nfsserver1.png" width="75%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_inst_nfsserver1.png" width="75%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Activate the <guimenu>Start</guimenu> radio button.
     </para>
    </step>
    <step>
     <para>
      If a firewall is active on your system (&susefirewall;), check
      <guimenu>Open Ports in Firewall</guimenu>. &yast; adapts its
      configuration for the NFS server by enabling the
      <systemitem class="service">nfs</systemitem> service.
     </para>
    </step>
    <step>
     <para>
      Check whether you want to <guimenu>Enable NFSv4</guimenu>. If you
      deactivate NFSv4, &yast; will only support NFSv3.  For information
      about enabling NFSv2, see <xref
      linkend="sec.nfs.export.manual.nsfv2"/>.
     </para>
     <substeps performance="required">
      <step>
       <para>
        If NFSv4 is selected, additionally enter the appropriate NFSv4
        domain name.
       </para>
       <para>
        Make sure the name is the same as the one in the
        <filename>/etc/idmapd.conf</filename> file of any NFSv4 client that
        accesses this particular server. This parameter is for the
        <systemitem class="daemon">idmapd</systemitem> daemon that is
        required for NFSv4 support (on both server and client). Leave it as
        <literal>localdomain</literal> (the default) if you do not have any
        special requirements.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Click <guimenu>Enable GSS Security</guimenu> if you need secure access
      to the server. A prerequisite for this is to have Kerberos installed
      on your domain and to have both the server and the clients kerberized.
      <remark>emap 2011-0824: A reference to a Kerberos chapter
      would be good.</remark>
      Click <guimenu>Next</guimenu> to proceed with the next configuration
      dialog.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Add Directory</guimenu> in the upper half of the dialog
      to export your directory.
     </para>
    </step>
    <step>
     <para>
      If you have not configured the allowed hosts already, another dialog
      for entering the client information and options pops up automatically.
      Enter the host wild card (usually you can leave the default settings
      as they are).
     </para>
<!--<para>The default settings allow every host in your network to
         connect. In case you want to restrict to a set of allowed
         hosts, enter the IP addresses or host names in the
         <guimenu>Host Wild Card</guimenu> textfield. See
         <citetitle>man exports</citetitle> for a detailed view of all
         possible variants.</para>-->
     <para>
      There are four possible types of host wild cards that can be set for
      each host: a single host (name or IP address), netgroups, wild cards
      (such as <literal>*</literal> indicating all machines can access the
      server), and IP networks.
     </para>
     <para>
      For more information about these options, see the
      <literal>exports</literal> man page.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Finish</guimenu> to complete the configuration.
     </para>
    </step>
   </procedure>
<!--
   bind-mount is deprecated on SLES 12; see fate#315589
   -->
<!--
   <sect3 id="sec.nfs.exportv4">
    <title>Exporting for NFSv4 Clients</title>
    <para>
     For a fixed set of NFSv4 clients, there are two types of directories
     that can be exported&mdash;directories that act as pseudo root file
     systems and those that are bound to some subdirectory of the pseudo
     file system. This pseudo file system acts as a base point under which
     all file systems exported for the same client set take their place. For
     a client or set of clients, only one directory on the server can be
     configured as pseudo root for export. For this client, export multiple
     directories by binding them to some existing subdirectory in the pseudo
     root.
    </para>
    <para>
     For example, suppose that the directory <filename>/exports</filename>
     is chosen as the pseudo root directory for all the clients that can
     access the server. Then add this into the list of exported directories
     and make sure that the options entered for this directory include
     <literal>fsid=0</literal>. If there is another directory,
     <filename>/data</filename>, that also needs to be NFSv4 exported, add
     this directory also to the list. While entering options for this, make
     sure that <literal>bind=/exports/data</literal> is in the list and that
     <filename>/exports/data</filename> is an already existing subdirectory
     of <filename>/exports</filename>. Any change in the option
     <systemitem>bind=/target/path</systemitem>, whether addition, deletion,
     or change in value, is reflected in <guimenu>Bindmount
     Targets</guimenu>.
    </para>
    <para>
     To set up the server to export directories for NFSv4 clients,
     use the the general guideline in
     <xref
       linkend="pro.nfs.export-yast2.nfs"/>, but change the
     following steps:
    </para>
    <procedure>
     <step>
      <para>
       Check <guimenu>Enable NFSv4</guimenu> in the first dialog.
      </para>
     </step>
     <step>
      <para>
       Enter the appropriate NFSv4 domain name in the first dialog.
      </para>
      <para>
       Make sure the name is the same as the one in the
       <filename>/etc/idmapd.conf</filename> file of any NFSv4 client that
       accesses this particular server. This parameter is for the
       <systemitem class="daemon">idmapd</systemitem> daemon that is
       required for NFSv4 support (on both server and client). Leave it as
       <literal>localdomain</literal> (the default) if you do not have any
       special requirements.
      </para>
      <para>
       After clicking <guimenu>Next</guimenu>, the dialog that follows has
       two sections. The upper half consists of two columns named
       <guimenu>Directories</guimenu> and <guimenu>Bindmount
       Targets</guimenu>. The service will become available immediately.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Add Directory</guimenu> in the upper half of the
       dialog to export your directory and confirm with
       <guimenu>Ok</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Enter your host names in the <guimenu>Host Wild Card</guimenu>
       textfield and options.
      </para>
      <para>
       In <guimenu>Options</guimenu> textfield, include
       <literal>fsid=0</literal> in the comma-separated list of options to
       configure the directory as pseudo root. If this directory needs to be
       bound to another directory under an already configured pseudo root,
       make sure that a target bind path is given in the option list with
       <literal>bind=/target/path</literal>.
      </para>
      <para>
       The <guimenu>Bindmount Targets</guimenu> column is not a directly
       editable column, but instead summarizes directories and their nature.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Finish</guimenu> to complete the configuration.
      </para>
     </step>
    </procedure>
   </sect3>
   -->
<!--
   Thus, this is also superfluous
   <sect3 id="sec.nfs.exportv23">
    <title>NFSv3 and NFSv2 Exports</title>
    <para>
     Make sure that <guimenu>Enable NFSv4</guimenu> is not checked in the
     initial dialog before clicking <guimenu>Next</guimenu>.
    </para>
    <para>
     The next dialog has two parts. In the upper text field, enter the
     directories to export. Below, enter the hosts that should have access
     to them. There are four types of host wild cards that can be set for
     each host: a single host (name or IP address), netgroups, wild cards
     (such as <literal>*</literal> indicating all machines can access the
     server), and IP networks.
     <remark>rwalter: more examples might be useful in
            the future</remark>
    </para>
    <para>
     This dialog is shown in
     <xref linkend="fig.nfs.export23"
            xrefstyle="FigureOnPage"/>.
     Find a more thorough explanation of these options in <command>man
     exports</command>. Click <guimenu>Finish</guimenu> to complete the
     configuration.
    </para>
    <figure pgwide="0" id="fig.nfs.export23">
     <title>Exporting Directories with NFSv2 and v3</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="nfs_002a.png" width="75%" format="PNG"
              />
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="nfs_002a.png" width="75%" format="PNG"
              />
      </imageobject>
     </mediaobject>
    </figure>
   </sect3>
   -->
<!--
   <sect3 id="sec.nfs.export.coexisting">
    <title>Coexisting v3 and v4 Exports</title>
    <para>
     NFSv3 and NFSv4 exports can coexist on a server. After enabling the
     support for NFSv4 in the initial configuration dialog, those exports
     for which <systemitem>fsid=0</systemitem> and
     <systemitem>bind=/target/path</systemitem> are not included in the
     option list are considered v3 exports.
    </para>
    <para>
     Consider the example in <xref linkend="sec.nfs.exportv4"/>. If you add
     another directory, such as <filename>/data2</filename>, using
     <guimenu>Add Directory</guimenu> then in the corresponding options list
     do not mention either <systemitem>fsid=0</systemitem> or
     <systemitem>bind=/target/path</systemitem>, this export acts as a v3
     export.
    </para>
   -->
<!--
   <important>
     <para>
      Automatic Firewall Configuration
     </para>
     <para>
      If &susefirewall; is active on your system, &yast; adapts its
      configuration for the NFS server by enabling the
      <literal>nfs</literal> service when <guimenu>Open Ports in
      Firewall</guimenu> is selected.
     </para>
    </important>
   -->
<!--
     </sect3>
     -->
  </sect2>

<!-- ============================================================== -->

  <sect2 xml:id="sec.nfs.export.manual" os="sles;osuse">
   <title>Exporting File Systems Manually</title><indexterm>
   <primary>NFS</primary>
   <secondary>exporting</secondary></indexterm>
   <para>
    The configuration files for the NFS export service are
    <filename>/etc/exports</filename> and
    <filename>/etc/sysconfig/nfs</filename>. In addition to these files,
    <filename>/etc/idmapd.conf</filename> is needed for the NFSv4 server
    configuration. To start or restart the services, run the command
    <command>systemctl restart nfsserver</command>. This also starts
    the <literal>rpc.idmapd</literal> if NFSv4 is configured in
    <filename>/etc/sysconfig/nfs</filename>. The NFS server depends on a
    running RPC portmapper. Therefore, it also starts or restarts the
    portmapper service.
<!-- with <command>systemctl restart rpcbind</command>-->
   </para>
<!-- <sect3 id="sec.nfs.manexportv4"> -->
<!--  <title>Exporting File Systems with NFSv4</title> -->
   <note>
    <title>NFSv4</title>
    <para>
     NFSv4 is the latest version of NFS protocol available on &productname;.
     Configuring directories for export with NFSv4 is now the same as with
     NFSv3.
    </para>
    <para>
     On the previous &sls; 11 version, the bind mount in
     <filename>/etc/exports</filename> was mandatory. It is still supported,
     but now deprecated.
    </para>
   </note>
   <variablelist>
    <varlistentry>
     <term>/etc/exports</term>
     <listitem>
      <para>
       The <filename>/etc/exports</filename> file contains a list of
       entries. Each entry indicates a directory that is shared and how it
       is shared. A typical entry in <filename>/etc/exports</filename>
       consists of:
      </para>
<screen>/<replaceable>shared</replaceable>/<replaceable>directory</replaceable>   <replaceable>host</replaceable>(<replaceable>option_list</replaceable>)</screen>
      <para>
       For example:
      </para>
<screen>/export/data   192.168.1.2(rw,sync)</screen>
      <para>
       Here the IP address <literal>192.168.1.2</literal> is used to
       identify the allowed client. You can also use the name of the host, a
       wild card indicating a set of hosts (<literal>*.abc.com</literal>,
       <literal>*</literal>, etc.), or netgroups
       (<literal>@my-hosts</literal>).
      </para>
      <para>
       For a detailed explanation of all options and their meaning, refer to
       the man page of <command>exports</command> (<command>man
       exports</command>).
      </para>
<!--
     <para>
      When clients mount from this server, they just mount
      <literal>servername:/</literal> rather than
      <literal>servername:/export</literal>. It is not necessary to mount
      <literal>servername:/data</literal>, because it will automatically
      appear beneath wherever <literal>servername:/</literal> was mounted.
     </para>
     -->
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>/etc/sysconfig/nfs</term>
     <listitem>
      <para>
       The <filename>/etc/sysconfig/nfs</filename> file contains a few
       parameters that determine NFSv4 server daemon behavior. It is
       important to set the parameter <systemitem>NFS4_SUPPORT</systemitem>
       to <literal>yes</literal> (default).
       <systemitem>NFS4_SUPPORT</systemitem> determines whether the NFS
       server supports NFSv4 exports and clients.
      </para>
      <tip>
       <title>Mount Options</title>
       <para>
        On &sle; prior to version 12, the <option>--bind</option> mount in
        <filename>/etc/exports</filename> was mandatory. It is still
        supported, but now deprecated. Configuring directories for export
        with NFSv4 is now the same as with NFSv3.
       </para>
      </tip>
      <note xml:id="sec.nfs.export.manual.nsfv2">
       <!-- bsc#919708 -->
       <title>NFSv2</title>
       <para>
        If NFS clients still depend on NFSv2, enable it on the server in
        <filename>/etc/sysconfig/nfs</filename> by setting:
       </para>
       <screen>NFSD_OPTIONS="-V2"
MOUNTD_OPTIONS="-V2"</screen>
<para>
 After restarting the service, check whether version 2 is available with
 the command:
</para>
        <screen>&prompt.user;cat /proc/fs/nfsd/versions
+2 +3 +4 +4.1 -4.2</screen>
      </note>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>/etc/idmapd.conf</term>
     <listitem>
      <para>
       Every user on a Linux machine has a name and an ID. idmapd does the
       name-to-ID mapping for NFSv4 requests to the server and replies to
       the client. It must be running on both server and client for NFSv4,
       because NFSv4 uses only names for its communication.
      </para>
      <para>
       Make sure that there is a uniform way in which user names and IDs
       (uid) are assigned to users across machines that might probably be
       sharing file systems using NFS. This can be achieved by using NIS,
       LDAP, or any uniform domain authentication mechanism in your domain.
      </para>
      <para>
       The parameter <literal>Domain</literal> must be set the same for
       both, client and server in the <filename>/etc/idmapd.conf</filename>
       file. If you are not sure, leave the domain as
       <literal>localdomain</literal> in the server and client files. A
       sample configuration file looks like the following:
      </para>
<screen>[General]
Verbosity = 0
Pipefs-Directory = /var/lib/nfs/rpc_pipefs
Domain = localdomain

[Mapping]
Nobody-User = nobody
Nobody-Group = nobody</screen>
      <para>
       For more information, see the man pages of <literal>idmapd</literal>
       and <literal>idmapd.conf</literal> (<literal>man idmapd</literal> and
       <literal>man idmapd.conf</literal>).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    After changing <filename>/etc/exports</filename> or
    <filename>/etc/sysconfig/nfs</filename>, start or restart the NFS server
    service:
   </para>
<screen>systemctl restart nfsserver</screen>
   <para>
    After changing <filename>/etc/idmapd.conf</filename>, reload the
    configuration file:
   </para>
<screen>killall -HUP rpc.idmapd</screen>
   <para>
    If the NFS service needs to start at boot time, run:
   </para>
<screen>systemctl enable nfsserver</screen>
  </sect2>

<!-- ============================================================== -->

  <sect2 xml:id="sec.nfs.kerberos">
   <title>NFS with Kerberos</title>
   <para>
    To use Kerberos authentication for NFS, GSS security must be enabled.
    Select <guimenu>Enable GSS Security</guimenu> in the initial &yast; NFS
    Server dialog. You must have a working Kerberos server to use this
    feature. &yast; does not set up the server but only uses the provided
    functionality. If you want to use Kerberos authentication in addition to
    the &yast; configuration, complete at least the following steps before
    running the NFS configuration:
   </para>
   <procedure>
    <step>
     <para>
      Make sure that both the server and the client are in the same Kerberos
      domain. They must access the same KDC (Key Distribution Center) server
      and share their <filename>krb5.keytab</filename> file (the default
      location on any machine is <filename>/etc/krb5.keytab</filename>). For
      more information about Kerberos, see
      <xref linkend="cha.security.kerberos"/>.
     </para>
    </step>
    <step>
     <para>
      Start the gssd service on the client with <command>systemctl start
      rpc-gssd.service</command>.
     </para>
    </step>
    <step os="sles;osuse">
     <para>
      Start the svcgssd service on the server with <command>systemctl start
      rpc-svcgssd.service</command>.
     </para>
    </step>
   </procedure>
   <para>
    For more information about configuring kerberized NFS, refer to the
    links in <xref linkend="sec.nfs.info" xrefstyle="SectTitleOnPage"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.nfs.configuring-nfs-clients">
  <title>Configuring Clients</title>

  <para>
   To configure your host as an NFS client, you do not need to install
   additional software. All needed packages are installed by default.
  </para>

  <sect2 xml:id="sec.nfs.import-yast2">
   <title>Importing File Systems with &yast;</title><indexterm>
   <primary>NFS</primary>
   <secondary>clients</secondary></indexterm>
   <para>
    Authorized users can mount NFS directories from an NFS server into the
    local file tree using the &yast; NFS client module. Proceed as follows:
   </para>
   <procedure xml:id="pro.nfs.import-yast2">
    <title>Importing NFS Directories</title>
    <step>
     <para>
      Start the &yast; NFS client module.
     </para>
    </step>
<!-- Start: <guimenu>NFS Shares</guimenu> -->
    <step>
     <para>
      Click <guimenu>Add</guimenu> in the <guimenu>NFS Shares</guimenu> tab.
      Enter the host name of the NFS server, the directory to import, and
      the mount point at which to mount this directory locally.
     </para>
    </step>
<!-- End: NFS Shares -->
<!-- Start: <guimenu>NFS Settings</guimenu> -->
    <step>
     <para>
      When using NFSv4, select <guimenu>Enable NFSv4</guimenu> in the
      <guimenu>NFS Settings</guimenu> tab. Additionally, the <guimenu>NFSv4
      Domain Name</guimenu> must contain the same value as used by the NFSv4
      server. The default domain is <literal>localdomain</literal>.
     </para>
    </step>
    <step>
     <para>
      To use Kerberos authentication for NFS, GSS security must be enabled.
      Select <guimenu>Enable GSS Security</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enable <guimenu>Open Port in Firewall</guimenu> in the <guimenu>NFS
      Settings</guimenu> tab if you use a Firewall and want to allow access
      to the service from remote computers. The firewall status is displayed
      next to the check box.
     </para>
    </step>
<!-- End: NFS Settings -->
    <step>
     <para>
      Click <guimenu>OK</guimenu> to save your changes.
     </para>
    </step>
   </procedure>
   <para>
    The configuration is written to <filename>/etc/fstab</filename> and the
    specified file systems are mounted. When you start the &yast;
    configuration client at a later time, it also reads the existing
    configuration from this file.
   </para>
   <tip>
    <title>NFS as a Root File System</title>
    <para>
     On (diskless) systems where the root partition is mounted via network
     as an NFS share, you need to be careful when configuring the network
     device with which the NFS share is accessible.
    </para>
    <para>
     When shutting down or rebooting the system, the default processing
     order is to turn off network connections, then unmount the root
     partition. With NFS root, this order causes problems as the root
     partition cannot be cleanly unmounted as the network connection to the
     NFS share is already not activated. To prevent the system from
     deactivating the relevant network device, open the network device
     configuration tab as described in
     <xref linkend="sec.basicnet.yast.change.start"/>, and choose
     <guimenu>On NFSroot</guimenu> in the <guimenu>Device
     Activation</guimenu> pane.
    </para>
   </tip>
<!--
   <figure id="fig.yast2.nfs.client">
    <title>NFS Client Configuration with YaST</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast2_nfsclient.png" width="75%"
              format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast2_nfsclient.png" width="75%"
              format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
-->
  </sect2>

  <sect2 xml:id="sec.nfs.import">
   <title>Importing File Systems Manually</title>
<!--idx--><indexterm>
   <primary>NFS</primary>
   <secondary>importing</secondary></indexterm><indexterm>
   <primary>NFS</primary>
   <secondary>mounting</secondary></indexterm>
<!-- mention nfs.service according to bnc#884690 -->
   <para>
    The prerequisite for importing file systems manually from an NFS server is
    a running RPC port mapper. The <option>nfs</option> service takes care to
    start it properly; thus, start it by entering <command>systemctl start
    nfs</command> as <systemitem class="username">root</systemitem>. Then
    remote file systems can be mounted in the file system like local
    partitions using <command>mount</command>:
   </para>
<screen>mount <replaceable>host</replaceable>:<replaceable>remote-path</replaceable><replaceable>local-path</replaceable></screen>
   <para>
    To import user directories from the <systemitem>&nfsname;</systemitem>
    machine, for example, use:
   </para>
<screen>mount &nfsname;:/home /home</screen>
   <sect3 xml:id="sec.nfs.automount">
    <title>Using the Automount Service</title>
    <para>
     The autofs daemon can be used to mount remote file systems
     automatically. Add the following entry to the
     <filename>/etc/auto.master</filename> file:
    </para>
<screen>/nfsmounts /etc/auto.nfs</screen>
    <para>
     Now the <filename>/nfsmounts</filename> directory acts as the root for
     all the NFS mounts on the client if the <filename>auto.nfs</filename>
     file is filled appropriately. The name <filename>auto.nfs</filename> is
     chosen for the sake of convenience&mdash;you can choose any name. In
     <filename>auto.nfs</filename> add entries for all the NFS mounts as
     follows:
    </para>
<screen>localdata -fstype=nfs server1:/data
nfs4mount -fstype=nfs4 server2:/</screen>
    <para>
     Activate the settings with <command>systemctl start
     autofs</command> as &rootuser;. In this example,
     <filename>/nfsmounts/localdata</filename>, the
     <filename>/data</filename> directory of
     <systemitem>server1</systemitem>, is mounted with NFS and
     <filename>/nfsmounts/nfs4mount</filename> from
     <systemitem>server2</systemitem> is mounted with NFSv4.
    </para>
    <para>
     If the <filename>/etc/auto.master</filename> file is edited while the
     service autofs is running, the automounter must be restarted for the
     changes to take effect with <command>systemctl restart
     autofs</command>.
    </para>
   </sect3>
   <sect3 xml:id="sec.nfs.fstab">
    <title>Manually Editing <filename>/etc/fstab</filename></title>
    <para>
     A typical NFSv3 mount entry in <filename>/etc/fstab</filename> looks
     like this:
    </para>
<screen>&nfsname;:/data /local/path nfs rw,noauto 0 0</screen>
    <para>
     For NFSv4 mounts, use <literal>nfs4</literal> instead of
     <literal>nfs</literal> in the third column:
    </para>
<screen>&nfsname;:/data /local/pathv4 nfs4 rw,noauto 0 0</screen>
    <para>
     The <literal>noauto</literal> option prevents the file system from
     being mounted automatically at start-up. If you want to mount the
     respective file system manually, it is possible to shorten the mount
     command specifying the mount point only:
    </para>
<screen>mount /local/path</screen>
    <note>
     <title>Mounting at Start-Up</title>
     <para>
      If you do not enter the <literal>noauto</literal> option, the init
      scripts of the system will handle the mount of those file systems at
      start-up.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.nfs.pnfs">
   <title>Parallel NFS (pNFS)</title>
   <para>
    NFS is one of the oldest protocols, developed in the '80s. As such, NFS
    is usually sufficient if you want to share small files. However, when
    you want to transfer big files or large numbers of clients want to
    access data, an NFS server becomes a bottleneck and significantly
    impacts on the system performance. This is because of files quickly
    getting bigger, whereas the relative speed of your Ethernet has not
    fully kept up.
   </para>
   <para>
    When you request a file from a <quote>normal</quote> NFS server, the
    server looks up the file metadata, collects all the data and transfers
    it over the network to your client. However, the performance bottleneck
    becomes apparent no matter how small or big the files are:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      With small files most of the time is spent collecting the metadata.
     </para>
    </listitem>
    <listitem>
     <para>
      With big files most of the time is spent on transferring the data from
      server to client.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    pNFS, or parallel NFS, overcomes this limitation as it separates the
    file system metadata from the location of the data. As such, pNFS
    requires two types of servers:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      A <emphasis>metadata</emphasis> or <emphasis>control server</emphasis>
      that handles all the non-data traffic
     </para>
    </listitem>
    <listitem>
     <para>
      One or more <emphasis>storage server(s)</emphasis> that hold(s) the
      data
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The metadata and the storage servers form a single, logical NFS server.
    When a client wants to read or write, the metadata server tells the
    NFSv4 client which storage server to use to access the file chunks. The
    client can access the data directly on the server.
   </para>
   <para>
    &sle; supports pNFS on the client side only.
   </para>
   <sect3 xml:id="sec.nfs.pnfs.yast">
    <title>Configuring pNFS Client With &yast;</title>
    <para>
     Proceed as described in <xref linkend="pro.nfs.import-yast2"/>, but
     click the <guimenu>pNFS (v4.1)</guimenu> check box and optionally
     <guimenu>NFSv4 share</guimenu>. &yast; will do all the necessary steps
     and will write all the required options in the file
     <filename>/etc/exports</filename>.
     <remark>toms 2013-02-15:
           Is that enough? Any other files for pNFS?</remark>
    </para>
   </sect3>
   <sect3 xml:id="sec.nfs.pnfs.manual">
    <title>Configuring pNFS Client Manually</title>
    <para>
     Refer to <xref linkend="sec.nfs.import"/> to start. Most of the
     configuration is done by the NFSv4 server. For pNFS, the only
     difference is to add the <option>minorversion</option> option and the
     metadata server <replaceable>MDS_SERVER</replaceable> to your
     <command>mount</command> command:
    </para>
<screen>mount -t nfs4 -o minorversion=1 <replaceable>MDS_SERVER</replaceable> <replaceable>MOUNTPOINT</replaceable></screen>
    <para>
     To help with debugging, change the value in the
     <filename>/proc</filename> file system:
    </para>
<screen>echo 32767 &gt; /proc/sys/sunrpc/nfsd_debug
echo 32767 &gt; /proc/sys/sunrpc/nfs_debug</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.nfs.info">
  <title>For More Information</title>

  <para>
   In addition to the man pages of <command>exports</command>,
   <command>nfs</command>, and <command>mount</command>, information about
   configuring an NFS server and client is available in
   <filename>/usr/share/doc/packages/nfsidmap/README</filename>. For further
   documentation online refer to the following Web sites:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Find the detailed technical documentation online at
     <link xlink:href="http://nfs.sourceforge.net/">SourceForge</link>.
    </para>
   </listitem>
   <listitem>
    <para>
     For instructions for setting up kerberized NFS, refer to
     <link xlink:href="http://www.citi.umich.edu/projects/nfsv4/linux/krb5-setup.html">NFS
     Version 4 Open Source Reference Implementation</link>.
    </para>
   </listitem>
   <listitem>
    <para>
     If you have questions on NFSv4, refer to the
     <link xlink:href="http://www.citi.umich.edu/projects/nfsv4/linux/faq/">Linux
     NFSv4 FAQ</link>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
