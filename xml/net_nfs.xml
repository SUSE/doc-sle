<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<!-- 
  TODO for SLES12
  
  Reorganize chapter into the following structure (if possible):
  
  * Terminology/Conceptual Overview
    - General Infos about NFSv2, NFSv3, and NFSv4
      (may be separate subsections or 
  * Installation
  * Configuration
     - YaST (merge procedure into one? Is it possible?)
       Also add information which files are modified by YaST 
     - Manually
  
-->
<chapter id="cha.nfs">
 <title>Sharing File Systems with NFS</title>
<!--ix--><indexterm>
 <primary>NFS</primary></indexterm><indexterm>
 <primary>Network File System</primary>
 <see>NFS</see></indexterm>
 <abstract>
  <para>
   Distributing and sharing file systems over a network is a common task in
   corporate environments. The well-proven network file system
   (<emphasis>NFS</emphasis>) works together with <emphasis>NIS</emphasis>,
   the yellow pages protocol. For a more secure protocol that works together
   with <emphasis>LDAP</emphasis> and may also use Kerberos, check
   <emphasis>NFSv4</emphasis>. Combined with pNFS, you can eliminate
   performance bottlenecks.
  </para>

  <para>
   NFS with NIS makes a network transparent to the user. With NFS, it is
   possible to distribute arbitrary file systems over the network. With an
   appropriate setup, users always find themselves in the same environment
   regardless of the terminal they currently use.
  </para>
 </abstract>
 <important os="osuse;sles">
  <title>Need for DNS</title>
  <para>
   In principle, all exports can be made using IP addresses only. To avoid
   time-outs, you need a working DNS system. DNS is necessary at least for
   logging purposes, because the mountd daemon does reverse lookups.
  </para>
 </important>
 <sect1 id="sec.nfs.term">
  <title>Terminology</title>

  <para>
   The following are terms used in the &yast; module.
  </para>

<!-- toms 2011-07-28: Used Samba file as a template: -->

  <variablelist>
   <varlistentry>
    <term>Exports</term>
    <listitem>
     <para>
      A directory <emphasis>exported</emphasis> by an NFS server, which
      clients can integrate it into their system.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS Client</term>
    <listitem>
     <para>
      The NFS client is a system that uses NFS services from an NFS server
      over the Network File System protocol. The TCP/IP protocol is already
      integrated into the Linux kernel; there is no need to install any
      additional software.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS Server</term>
    <listitem>
     <para>
      The NFS server provides NFS services to clients. A running server
      depends on the following daemons:
      <systemitem class="daemon">nfsd</systemitem> (worker),
      <systemitem class="daemon">idmapd</systemitem> (user and group name
      mappings to IDs and vice versa),
      <systemitem class="daemon">statd</systemitem> (file locking), and
      <systemitem class="daemon">mountd</systemitem> (mount requests).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFSv3</term>
    <listitem>
     <para>
      NFSv3 is the version 3 implementation, the <quote>old</quote>
      stateless NFS that supports client authentication.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFSv4</term>
    <listitem>
     <para>
      NFSv4 is the new version 4 implementation that supports secure user
      authentication via kerberos.  NFSv4 requires one single port
      only and thus is better suited for firewalled environments than
      NFSv3.
     </para>
     <para>
      The protocol is spcified as <ulink
      url="http://tools.ietf.org/html/rfc3530"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>pNFS</term>
    <listitem>
     <para>
      Parallel NFS, a protocol extension of NFSv4. Any pNFS clients can
      directly access the data on an NFS server.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.nfs.installation">
  <title>Installing NFS Server</title>

  <para>
   The NFS server software is not part of the default installation. If you
   configure an NFS server as described in
   <xref
   linkend="sec.nfs.configuring-nfs-server"/> you will
   automatically be prompted to install the required packages.
   Alternatively, install the package
   <systemitem class="resource">nfs-kernel-server</systemitem> with &yast;
   or zypper.
  </para>

  <para>
   Like NIS, NFS is a client/server system. However, a machine can be
   both&mdash;it can supply file systems over the network (export) and mount
   file systems from other hosts (import).
  </para>
 </sect1>
 <sect1 id="sec.nfs.configuring-nfs-server">
  <title>Configuring NFS Server</title>

  <para>
   Configuring an NFS server can be done either through &yast; or manually.
   For authentication, NFS can also be combined with Kerberos.
  </para>

  <sect2 id="sec.nfs.export-yast2" os="osuse;sles">
   <title>Exporting File Systems with &yast;</title><indexterm>
   <primary>NFS</primary>
   <secondary>servers</secondary></indexterm>
   <para>
    With &yast;, turn a host in your network into an NFS server&mdash;a
    server that exports directories and files to all hosts granted access to
    it. The server can also provide applications to all members of a group
    without installing the applications locally on each and every host.
   </para>
   <para>
    To set up such a server, proceed as follows:
   </para>
   <procedure id="pro.nfs.export-yast2.nfsv3">
    <title>Setting Up an NFS Server</title>
    <step>
     <para>
      Start &yast; and select <menuchoice> <guimenu>Network
      Services</guimenu> <guimenu>NFS Server</guimenu> </menuchoice>; see
      <xref linkend="fig.inst.nfsserver1"/>. You may be prompted to install
      additional software.
     </para>
     <figure id="fig.inst.nfsserver1">
      <title>NFS Server Configuration Tool</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_inst_nfsserver1.png" width="75%"
                  format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_inst_nfsserver1.png" width="75%"
                  format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Activate the <guimenu>Start</guimenu> radio button.
     </para>
    </step>
    <step>
     <para>
      If a firewall is active on your system (SuSEfirewall2), check
      <guimenu>Open Ports in Firewall</guimenu>. &yast; adapts its
      configuration for the NFS server by enabling the
      <systemitem class="service">nfs</systemitem> service.
     </para>
    </step>
    <step>
     <para>
      Leave the <guimenu>Enable NFSv4</guimenu> check box disabled.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Enable GSS Security</guimenu> if you need secure access
      to the server. A prerequisite for this is to have Kerberos installed
      on your domain and to have both the server and the clients kerberized.
      <remark>emap 2011-0824: A reference to a Kerberos chapter
      would be good.</remark>
      Click <guimenu>Next</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Add Directory</guimenu> in the upper half of the dialog
      to export your directory.
     </para>
    </step>
    <step>
     <para>
      If you have not configured the allowed hosts already, another dialog
      for entering the client information and options pops up automatically.
      Enter the host wild card (usually you can leave the default settings
      as they are).
     </para>
<!--<para>The default settings allow every host in your network to
         connect. In case you want to restrict to a set of allowed
         hosts, enter the IP addresses or hostnames in the 
         <guimenu>Host Wild Card</guimenu> textfield. See
         <citetitle>man exports</citetitle> for a detailed view of all
         possible variants.</para>-->
     <para>
      There are four possible types of host wild cards that can be set for
      each host: a single host (name or IP address), netgroups, wild cards
      (such as <literal>*</literal> indicating all machines can access the
      server), and IP networks.
     </para>
     <para>
      For more information about these options, see the
      <literal>exports</literal> man page.
     </para>
   </step>
    <step>
     <para>
      Click <guimenu>Finish</guimenu> to complete the configuration.
     </para>
    </step>
   </procedure>
   <!--
   bind-mount is deprecated on SLES 12; see fate#315589
   -->
   <!--
   <sect3 id="sec.nfs.exportv4">
    <title>Exporting for NFSv4 Clients</title>
    <para>
     For a fixed set of NFSv4 clients, there are two types of directories
     that can be exported&mdash;directories that act as pseudo root file
     systems and those that are bound to some subdirectory of the pseudo
     file system. This pseudo file system acts as a base point under which
     all file systems exported for the same client set take their place. For
     a client or set of clients, only one directory on the server can be
     configured as pseudo root for export. For this client, export multiple
     directories by binding them to some existing subdirectory in the pseudo
     root.
    </para>
    <para>
     For example, suppose that the directory <filename>/exports</filename>
     is chosen as the pseudo root directory for all the clients that can
     access the server. Then add this into the list of exported directories
     and make sure that the options entered for this directory include
     <literal>fsid=0</literal>. If there is another directory,
     <filename>/data</filename>, that also needs to be NFSv4 exported, add
     this directory also to the list. While entering options for this, make
     sure that <literal>bind=/exports/data</literal> is in the list and that
     <filename>/exports/data</filename> is an already existing subdirectory
     of <filename>/exports</filename>. Any change in the option
     <systemitem>bind=/target/path</systemitem>, whether addition, deletion,
     or change in value, is reflected in <guimenu>Bindmount
     Targets</guimenu>.
    </para>
    <para>
     In order to set up the server to export directories for NFSv4 clients,
     use the the general guideline in
     <xref
       linkend="pro.nfs.export-yast2.nfsv3"/>, but change the
     following steps:
    </para>
    <procedure>
     <step>
      <para>
       Check <guimenu>Enable NFSv4</guimenu> in the first dialog.
      </para>
     </step>
     <step>
      <para>
       Enter the appropriate NFSv4 domain name in the first dialog.
      </para>
      <para>
       Make sure the name is the same as the one in the
       <filename>/etc/idmapd.conf</filename> file of any NFSv4 client that
       accesses this particular server. This parameter is for the
       <systemitem class="daemon">idmapd</systemitem> daemon that is
       required for NFSv4 support (on both server and client). Leave it as
       <literal>localdomain</literal> (the default) if you do not have any
       special requirements.
      </para>
      <para>
       After clicking <guimenu>Next</guimenu>, the dialog that follows has
       two sections. The upper half consists of two columns named
       <guimenu>Directories</guimenu> and <guimenu>Bindmount
       Targets</guimenu>. The service will become available immediately.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Add Directory</guimenu> in the upper half of the
       dialog to export your directory and confirm with
       <guimenu>Ok</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Enter your hostnames in the <guimenu>Host Wild Card</guimenu>
       textfield and options.
      </para>
      <para>
       In <guimenu>Options</guimenu> textfield, include
       <literal>fsid=0</literal> in the comma-separated list of options to
       configure the directory as pseudo root. If this directory needs to be
       bound to another directory under an already configured pseudo root,
       make sure that a target bind path is given in the option list with
       <literal>bind=/target/path</literal>.
      </para>
      <para>
       The <guimenu>Bindmount Targets</guimenu> column is not a directly
       editable column, but instead summarizes directories and their nature.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Finish</guimenu> to complete the configuration.
      </para>
     </step>
    </procedure>
   </sect3>
   -->
   <!--
   Thus, this is also superfluous
   <sect3 id="sec.nfs.exportv23">
    <title>NFSv3 and NFSv2 Exports</title>
    <para>
     Make sure that <guimenu>Enable NFSv4</guimenu> is not checked in the
     initial dialog before clicking <guimenu>Next</guimenu>.
    </para>
    <para>
     The next dialog has two parts. In the upper text field, enter the
     directories to export. Below, enter the hosts that should have access
     to them. There are four types of host wild cards that can be set for
     each host: a single host (name or IP address), netgroups, wild cards
     (such as <literal>*</literal> indicating all machines can access the
     server), and IP networks.
     <remark>rwalter: more examples might be useful in
            the future</remark>
    </para>
    <para>
     This dialog is shown in
     <xref linkend="fig.nfs.export23"
            xrefstyle="FigureOnPage"/>.
     Find a more thorough explanation of these options in <command>man
     exports</command>. Click <guimenu>Finish</guimenu> to complete the
     configuration.
    </para>
    <figure pgwide="0" id="fig.nfs.export23">
     <title>Exporting Directories with NFSv2 and v3</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="nfs_002a.png" width="75%" format="PNG"
              />
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="nfs_002a.png" width="75%" format="PNG"
              />
      </imageobject>
     </mediaobject>
    </figure>
   </sect3>
   -->
   <!--
   <sect3 id="sec.nfs.export.coexisting">
    <title>Coexisting v3 and v4 Exports</title>
    <para>
     NFSv3 and NFSv4 exports can coexist on a server. After enabling the
     support for NFSv4 in the initial configuration dialog, those exports
     for which <systemitem>fsid=0</systemitem> and
     <systemitem>bind=/target/path</systemitem> are not included in the
     option list are considered v3 exports.
    </para>
    <para>
     Consider the example in <xref linkend="sec.nfs.exportv4"/>. If you add
     another directory, such as <filename>/data2</filename>, using
     <guimenu>Add Directory</guimenu> then in the corresponding options list
     do not mention either <systemitem>fsid=0</systemitem> or
     <systemitem>bind=/target/path</systemitem>, this export acts as a v3
     export.
    </para>
   -->
   <important>
     <para>
      Automatic Firewall Configuration
     </para>
     <para>
      If SuSEfirewall2 is active on your system, &yast; adapts its
      configuration for the NFS server by enabling the
      <literal>nfs</literal> service when <guimenu>Open Ports in
      Firewall</guimenu> is selected.
     </para>
    </important>
    <!--
     </sect3>
     -->
  </sect2>

  <sect2 id="sec.nfs.export.manual" os="osuse;sles">
   <title>Exporting File Systems Manually</title><indexterm>
   <primary>NFS</primary>
   <secondary>exporting</secondary></indexterm>
   <para>
    The configuration files for the NFS export service are
    <filename>/etc/exports</filename> and
    <filename>/etc/sysconfig/nfs</filename>. In addition to these files,
    <filename>/etc/idmapd.conf</filename> is needed for the NFSv4 server
    configuration. To start or restart the services, run the command
    <command>systemctl restart nfsserver.service</command>. This also starts the
    <literal>rpc.idmapd</literal> if NFSv4 is configured in
    <filename>/etc/sysconfig/nfs</filename>. The NFS server depends on a
    running RPC portmapper. Therefore, also start or restart the portmapper
    service with <command>systemctl restart rpcbind.service</command>.
   </para>
   <sect3 id="sec.nfs.manexportv4">
    <title>Exporting File Systems with NFSv4</title>
    <para>
     NFSv4 is the latest version of NFS protocol available on &productname;.
     Configuring directories for export with NFSv4 differs slightly from
     previous NFS versions.
    </para>
    <sect4 id="sec.nfs.exports">
<!-- really need this subsection? -->
<!-- don't know, this is more like a listing in a bigger scope
     Better using sect3 than variablelist. 2010-07-07, ke -->
     <title>/etc/exports</title>
     <para>
      The <filename>/etc/exports</filename> file contains a list of entries.
      Each entry indicates a directory that is shared and how it is shared.
      A typical entry in <filename>/etc/exports</filename> consists of:
     </para>
<screen>/shared/directory   host(option_list)</screen>
     <para>
      For example:
     </para>
<screen>/export   192.168.1.2(rw,fsid=0,sync,crossmnt)
/export/data   192.168.1.2(rw,bind=/data,sync)
</screen>
     <para>
      Here the IP address <literal>192.168.1.2</literal> is used to identify
      the allowed client. You can also use the name of the host, a wild card
      indicating a set of hosts (<literal>*.abc.com</literal>,
      <literal>*</literal>, etc.), or netgroups
      (<literal>@my-hosts</literal>).
     </para>
     <para>
      The directory which specifies <literal>fsid=0</literal> is special. It
      is the root of the filesystem that is exported, sometimes referred to
      as the pseudo root filesystem. This directory must also have the
      <literal>crossmnt</literal> for correct operation with NFSv4. All
      other directories exported via NFSv4 must be mounted below this point.
      If you want to export a directory that is not within the exported
      root, it needs to be bound into the exported tree. This can be done
      using the <literal>bind=</literal> syntax.
     </para>
     <para>
      In the example above, <filename>/data</filename> is not within
      <filename>/export</filename>, so we export
      <filename>/export/data</filename>, and specify that the
      <filename>/data</filename> directory should be bound to that name. The
      directory <filename>/export/data</filename> must exist and should
      normally be empty.
     </para>
     <para>
      When clients mount from this server, they just mount
      <literal>servername:/</literal> rather than
      <literal>servername:/export</literal>. It is not necessary to mount
      <literal>servername:/data</literal>, because it will automatically
      appear beneath wherever <literal>servername:/</literal> was mounted.
     </para>
    </sect4>
    <sect4 id="sec.nfs.sysconfig">
     <title>/etc/sysconfig/nfs</title>
     <para>
      The <filename>/etc/sysconfig/nfs</filename> file contains a few
      parameters that determine NFSv4 server daemon behavior. It is
      important to set the parameter <systemitem>NFS4_SUPPORT</systemitem>
      to <literal>yes</literal>. <systemitem>NFS4_SUPPORT</systemitem>
      determines whether the NFS server supports NFSv4 exports and clients.
     </para>
    </sect4>
    <sect4 id="sec.nfs.idmapd">
     <title>/etc/idmapd.conf</title>
     <para>
      Every user on a Linux machine has a name and an ID. idmapd does the
      name-to-ID mapping for NFSv4 requests to the server and replies to the
      client. It must be running on both server and client for NFSv4,
      because NFSv4 uses only names for its communication.
     </para>
     <para>
      Make sure that there is a uniform way in which usernames and IDs (uid)
      are assigned to users across machines that might probably be sharing
      file systems using NFS. This can be achieved by using NIS, LDAP, or
      any uniform domain authentication mechanism in your domain.
     </para>
     <para>
      The parameter <literal>Domain</literal> must be set the same for both,
      client and server in the <filename>/etc/idmapd.conf</filename> file.
      If you are not sure, leave the domain as
      <literal>localdomain</literal> in the server and client files. A
      sample configuration file looks like the following:
     </para>
<screen>[General] 

Verbosity = 0 
Pipefs-Directory = /var/lib/nfs/rpc_pipefs
Domain = localdomain

[Mapping]

Nobody-User = nobody
Nobody-Group = nobody</screen>
     <para>
      For further reference, read the man page of <literal>idmapd</literal>
      and <literal>idmapd.conf</literal>; <literal>man idmapd</literal>,
      <literal>man idmapd.conf</literal>.
     </para>
    </sect4>
    <sect4 id="sec.nfs.services">
     <title>Starting and Stopping Services</title>
     <para>
      After changing <filename>/etc/exports</filename> or
      <filename>/etc/sysconfig/nfs</filename>, start or restart the NFS
      server service with <command>systemctl restart nfsserver.service</command>. After
      changing <filename>/etc/idmapd.conf</filename>, reload the
      configuration file with the command <command>killall -HUP
      rpc.idmapd</command>.
     </para>
     <para>
      If the NFS service needs to start at boot time, run the command
      <command>systemctl enable nfsserver.service</command>.
     </para>
    </sect4>
   </sect3>
   <sect3 id="sec.nfs.manualexportv23">
    <title>Exporting File Systems with NFSv2 and NFSv3</title>
    <para>
     This section is specific to NFSv3 and NFSv2 exports. <!-- Refer to
     <xref linkend="sec.nfs.exportv4"
            xrefstyle="HeadingOnPage"/>
     for exporting with NFSv4.
     -->
    </para>
    <para>
     Exporting file systems with NFS involves two configuration files:
     <filename>/etc/exports</filename> and
     <filename>/etc/sysconfig/nfs</filename>. A typical
     <filename>/etc/exports</filename> file entry is in the format:
    </para>
<screen>/shared/directory   host(list_of_options)</screen>
    <para>
     For example:
    </para>
<screen>/export   192.168.1.2(rw,sync)</screen>
    <para>
     Here, the directory <filename>/export</filename> is shared with the
     host <literal>192.168.1.2</literal> with the option list
     <systemitem>rw,sync</systemitem>. This IP address can be replaced with
     a client name or set of clients using a wild card (such as
     <literal>*.abc.com</literal>) or even netgroups.
    </para>
    <para>
     For a detailed explanation of all options and their meaning, refer to
     the man page of <command>exports</command> (<command>man
     exports</command>).
    </para>
    <para>
     After changing <filename>/etc/exports</filename> or
     <filename>/etc/sysconfig/nfs</filename>, start or restart the NFS
     server using the command <command>systemctl restart nfsserver.service</command>.
    </para>
   </sect3>
  </sect2>

  <sect2 id="sec.nfs.kerberos">
   <title>NFS with Kerberos</title>
   <para>
    To use Kerberos authentication for NFS, GSS security must be enabled.
    Select <guimenu>Enable GSS Security</guimenu> in the initial &yast; NFS
    Server dialog. You must have a working Kerberos server to use this
    feature. &yast; does not set up the server but just uses the provided
    functionality. If you want to use Kerberos authentication in addition to
    the &yast; configuration, complete at least the following steps before
    running the NFS configuration:
   </para>
   <procedure>
    <step>
     <para>
      Make sure that both the server and the client are in the same Kerberos
      domain. They must access the same KDC (Key Distribution Center) server
      and share their <filename>krb5.keytab</filename> file (the default
      location on any machine is <filename>/etc/krb5.keytab</filename>). For
      more information about Kerberos, see
      <xref
              linkend="cha.security.kerberos"/>.
     </para>
    </step>
    <step>
     <para>
      Start the gssd service on the client with 
      <command>systemctl start gssd.service</command>.
     </para>
    </step>
    <step os="osuse;sles">
     <para>
      Start the svcgssd service on the server with 
      <command>systemctl start svcgssd.service</command>.
     </para>
    </step>
   </procedure>
   <para>
    For more information about configuring kerberized NFS, refer to the
    links in
    <xref linkend="sec.nfs.info"
          xrefstyle="SectTitleOnPage"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.nfs.configuring-nfs-clients">
  <title>Configuring Clients</title>

  <para>
   To configure your host as an NFS client, you do not need to install
   additional software. All needed packages are installed by default.
  </para>

  <sect2 id="sec.nfs.import-yast2">
   <title>Importing File Systems with &yast;</title><indexterm>
   <primary>NFS</primary>
   <secondary>clients</secondary></indexterm>
   <para>
    Authorized users can mount NFS directories from an NFS server into the
    local file tree using the &yast; NFS client module. Proceed as follows:
   </para>
   <procedure id="pro.nfs.import-yast2">
    <title>Importing NFS Directories</title>
    <step>
     <para>
      Start the &yast; NFS client module.
     </para>
    </step>
    <step>
     <para>
      Click on <guimenu>Add</guimenu> in the <guimenu>NFS Shares</guimenu>
      tab. Enter the hostname of the NFS server, the directory to import,
      and the mount point at which to mount this directory locally.
     </para>
    </step>
    <step>
     <para>
      Enable <guimenu>Open Port in Firewall</guimenu> in the <guimenu>NFS
      Settings</guimenu> tab if you use a Firewall and want to allow access
      to the service from remote computers. The firewall status is displayed
      next to the check box.
     </para>
    </step>
    <step>
     <para>
      When using NFSv4, make sure that the check box <guimenu>Enable
      NFSv4</guimenu> is selected and that the <guimenu>NFSv4 Domain
      Name</guimenu> contains the same value as used by the NFSv4 server.
      The default domain is <literal>localdomain</literal>.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>OK</guimenu> to save your changes.
     </para>
    </step>
   </procedure>
   <para>
    The configuration is written to <filename>/etc/fstab</filename> and the
    specified file systems are mounted. When you start the &yast;
    configuration client at a later time, it also reads the existing
    configuration from this file.
   </para>
<!--
   <figure id="fig.yast2.nfs.client">
    <title>NFS Client Configuration with YaST</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast2_nfsclient.png" width="75%"
              format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast2_nfsclient.png" width="75%"
              format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
-->
  </sect2>

  <sect2 id="sec.nfs.import">
   <title>Importing File Systems Manually</title><indexterm>
   <primary>NFS</primary>
   <secondary>importing</secondary></indexterm><indexterm>
   <primary>NFS</primary>
   <secondary>mounting</secondary></indexterm>
   <para>
    The prerequisite for importing file systems manually from an NFS server
    is a running RPC port mapper. Start it by entering <command>systemctl start
     rpcbind.service</command> as
    <systemitem class="username">root</systemitem>. Then remote file systems
    can be mounted in the file system like local partitions using
    <command>mount</command>:
   </para>
<screen>mount <replaceable>host</replaceable>:<replaceable
        >remote-path</replaceable><replaceable
          >local-path</replaceable></screen>
   <para>
    To import user directories from the <systemitem>&nfsname;</systemitem>
    machine, for example, use:
   </para>
<screen>mount &nfsname;:/home /home</screen>
   <sect3 id="sec.nfs.automount">
    <title>Using the Automount Service</title>
    <para>
     The autofs daemon can be used to mount remote file systems
     automatically. Add the following entry in the your
     <filename>/etc/auto.master</filename> file:
    </para>
<screen>/nfsmounts /etc/auto.nfs</screen>
    <para>
     Now the <filename>/nfsmounts</filename> directory acts as the root for
     all the NFS mounts on the client if the <filename>auto.nfs</filename>
     file is filled appropriately. The name <filename>auto.nfs</filename> is
     chosen for the sake of convenience&mdash;you can choose any name. In
     <filename>auto.nfs</filename> add entries for all the NFS mounts as
     follows:
    </para>
<screen>localdata -fstype=nfs server1:/data
nfs4mount -fstype=nfs4 server2:/</screen>
    <para>
     Activate the settings with <command>systemctl start autofs.service</command> as
     &rootuser;. In this example, <filename>/nfsmounts/localdata</filename>,
     the <filename>/data</filename> directory of
     <systemitem>server1</systemitem>, is mounted with NFS and
     <filename>/nfsmounts/nfs4mount</filename> from
     <systemitem>server2</systemitem> is mounted with NFSv4.
    </para>
    <para>
     If the <filename>/etc/auto.master</filename> file is edited while the
     service autofs is running, the automounter must be restarted for the
     changes to take effect with <command>systemctl restart autofs.service</command>.
    </para>
   </sect3>
   <sect3 id="sec.nfs.fstab">
    <title>Manually Editing <filename>/etc/fstab</filename></title>
    <para>
     A typical NFSv3 mount entry in <filename>/etc/fstab</filename> looks
     like this:
    </para>
<screen>&nfsname;:/data /local/path nfs rw,noauto 0 0</screen>
    <para>
     NFSv4 mounts may also be added to the <filename>/etc/fstab</filename>
     file. For these mounts, use <literal>nfs4</literal> instead of
     <literal>nfs</literal> in the third column and make sure that the
     remote file system is given as <filename>/</filename> after the
     <replaceable>&nfsname;:</replaceable> in the first column. A sample
     line for an NFSv4 mount in <filename>/etc/fstab</filename> looks like
     this:
    </para>
<screen>&nfsname;:/ /local/pathv4 nfs4 rw,noauto 0 0</screen>
    <para>
     The <literal>noauto</literal> option prevents the file system from
     being mounted automatically at start up. If you want to mount the
     respective file system manually, it is possible to shorten the mount
     command specifying the mount point only:
    </para>
<screen>mount /local/path</screen>
    <para>
     Note, that if you do not enter the <literal>noauto</literal> option,
     the initialization scripts of the system will handle the mount of those
     file systems at start up.
    </para>
   </sect3>
  </sect2>

  <sect2 id="sec.nfs.pnfs">
   <title>Parallel NFS (pNFS)</title>
   <para>
    NFS is one of the oldest protocols, developed in the '80s. As such, NFS
    is usually sufficient if you want to share small files. However, when
    you want to transfer big files or large numbers of clients want to
    access data, an NFS server becomes a bottleneck and significantly
    impacts on the system performance. This is due to files quickly getting
    bigger, whereas the relative speed of your Ethernet just has not fully
    kept up.
   </para>
   <para>
    When you request a file from a <quote>normal</quote> NFS server, the
    server looks up the file metadata, collects all the data and transfers
    it over the network to your client. However, the performance bottleneck
    becomes apparent no matter how small or big the files are:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      With small files most of the time is spent collecting the metadata
     </para>
    </listitem>
    <listitem>
     <para>
      With big files most of the time is spent on transfering the data from
      server to client
     </para>
    </listitem>
   </itemizedlist>
   <para>
    pNFS, or parallel NFS, overcomes this limitation as it separates the
    file system metadata from the location of the data. As such, pNFS
    requires two types of servers:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      A <emphasis>metadata</emphasis> or <emphasis>control server</emphasis>
      which handels all the non-data traffic
     </para>
    </listitem>
    <listitem>
     <para>
      One or more <emphasis>storage server(s)</emphasis> which hold(s) the
      data
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The metadata and the storage servers form a single, logical NFS server.
    When a client wants to read or write, the metadata server tells the
    NFSv4 client which storage server to use to access the file chunks. The
    client can access the data directly on the server.
   </para>
   <para>
    &sle; supports pNFS on the client side only.
   </para>
   <sect3 id="sec.nfs.pnfs.yast">
    <title>Configuring pNFS Client With &yast;</title>
    <para>
     Proceed as described in
     <xref
         linkend="pro.nfs.import-yast2"/>, but click the
     <guimenu>pNFS (v4.1)</guimenu> check box and optionally <guimenu>NFSv4
     share</guimenu>. &yast; will do all the necessary steps and will write
     all the required options in the file <filename>/etc/exports</filename>
     <remark>toms 2013-02-15:
           Is that enough? Any other files for pNFS?</remark>
     .
    </para>
   </sect3>
   <sect3 id="sec.nfs.pnfs.manual">
    <title>Configuring pNFS Client Manually</title>
    <para>
     Refer to <xref linkend="sec.nfs.import"/> to start. Most of the
     configuration is done by the NFSv4 server. For pNFS, the only
     difference is to add the <option>minorversion</option> option and the
     metadata server <replaceable>MDS_SERVER</replaceable> to your
     <command>mount</command> command:
    </para>
<screen>mount -t nfs4 -o minorversion=1 <replaceable
         >MDS_SERVER</replaceable> <replaceable>MOUNTPOINT</replaceable></screen>
    <para></para>
    <para>
     To help with debugging, change the value in the
     <filename>/proc</filename> file system:
    </para>
<screen>echo 32767 > /proc/sys/sunrpc/nfsd_debug
echo 32767 > /proc/sys/sunrpc/nfs_debug</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 id="sec.nfs.info">
  <title>For More Information</title>

  <para>
   In addition to the man pages of <command>exports</command>,
   <command>nfs</command>, and <command>mount</command>, information about
   configuring an NFS server and client is available in
   <filename>/usr/share/doc/packages/nfsidmap/README</filename>. For further
   documentation online refer to the following Web sites:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Find the detailed technical documentation online at
     <ulink url="http://nfs.sourceforge.net/">SourceForge</ulink>.
    </para>
   </listitem>
   <listitem>
    <para>
     For instructions for setting up kerberized NFS, refer to
     <ulink url="http://www.citi.umich.edu/projects/nfsv4/linux/krb5-setup.html">NFS
     Version 4 Open Source Reference Implementation</ulink>.
    </para>
   </listitem>
   <listitem>
    <para>
     If you have questions on NFSv4, refer to the
     <ulink url="http://www.citi.umich.edu/projects/nfsv4/linux/faq/">Linux
     NFSv4 FAQ</ulink>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
