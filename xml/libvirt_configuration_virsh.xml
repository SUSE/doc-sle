<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="libvirt_configuration_virsh.xml" version="5.0" xml:id="cha.libvirt.config_virsh">
 <title>Configuring Virtual Machines with virsh</title>
 <info>
  <abstract>
   <para>
    In addition to the Virtual Machine Manager, the VM Guest can also be configured with <command>virsh</command>.
    <command>virsh</command> is a command line tool to manage virtual machines based on virtualization infrastructures such as
    KVM, Xen, VMware, or LXC.
    <command>virsh</command> can control the state of a VM, edit a the configuration of a VM or even migrate a VM to another host.
    In the following sections the configuration of a VM with <command>virsh</command> will be shown.
    For example CPU, Memory, Input devices or Video devices.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec.libvirt.config.editing.virsh">
  <title>Editing VM Configuration</title>
    <para>
    The configuration of a VM is stored in a XML file in <filename>/etc/libvirtd/qemu/</filename>.
    It usually looks like this:
    </para>
    <screen>
    &lt;domain type='kvm'&gt;
      &lt;name&gt;sles15&lt;/name&gt;
      &lt;uuid&gt;ab953e2f-9d16-4955-bb43-1178230ee625&lt;/uuid&gt;
      &lt;memory unit='KiB'&gt;2097152&lt;/memory&gt;
      &lt;currentMemory unit='KiB'&gt;2097152&lt;/currentMemory&gt;
      &lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;
      &lt;os&gt;
        &lt;type arch='x86_64' machine='pc-i440fx-2.11'&gt;hvm&lt;/type&gt;
      &lt;/os&gt;
      &lt;features&gt;...&lt;/features&gt;
      &lt;cpu mode='custom' match='exact' check='partial'&gt;
        &lt;model fallback='allow'&gt;Skylake-Client-IBRS&lt;/model&gt;
      &lt;/cpu&gt;
      &lt;clock&gt;...&lt;/clock&gt;
      &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;
      &lt;on_reboot&gt;restart&lt;/on_reboot&gt;
      &lt;on_crash&gt;destroy&lt;/on_crash&gt;
      &lt;pm&gt;
        &lt;suspend-to-mem enabled='no'/&gt;
        &lt;suspend-to-disk enabled='no'/&gt;
      &lt;/pm&gt;
      &lt;devices&gt;
        &lt;emulator&gt;/usr/bin/qemu-system-x86_64&lt;/emulator&gt;
        &lt;disk type='file' device='disk'&gt;...&lt;/disk&gt;
      &lt;/devices&gt;
      ...
    &lt;/domain&gt;
    </screen>
    <para>
    You can edit this XML configuration of the VM Guest with
    </para>
<screen><prompt>tux &gt; </prompt><command>sudo virsh edit [name of VM Guest]</command>
    </screen>
    <tip>
      <para>
      You can only edit the configuration of a VM Guest while it's offline.
      You can ensure this by doing:
      </para>
      <screen><command>virsh list -\-inactive</command></screen>
      <para>
      If the VM Guest is in this list, the configuration can be edited.
      </para>
    </tip>
    <para>
    Before saving the changes, <command>virsh</command> validates your input against a RelaxNG
    schema.
    </para>
</sect1>


<sect1 xml:id="sec.libvirt.config.machinetype.virsh">
<title>Changing the Machine Type</title>

<para>
  By default, when installing with the <command>virt-install</command> tool, the machine type for VM Guest is
  <emphasis>pc-i440fx</emphasis>. The machine type is stored in the
  VM Guest's XML configuration file in
  <filename>/etc/libvirt/qemu/</filename> in the tag <tag>type</tag>:
</para>

<screen>&lt;type arch='x86_64' machine='pc-i440fx-2.3'&gt;hvm&lt;/type&gt;</screen>

<para>
  As an example, the following procedure shows how to change this value to the
  machine type <literal>q35</literal>. The value <literal>q35</literal> is an Intel*
  chipset. It includes <xref linkend="gloss.vt.acronym.pcie"/>, supports up to
  12 USB ports, and has support for
  <xref linkend="gloss.vt.acronym.sata"/> and
  <xref linkend="gloss.vt.acronym.iommu"/>. IRQ routing has also
  been improved.
</para>

<procedure>
  <title>Changing Machinge Type <remark>toms 2018-08-13: Check title</remark></title>
  <step>
  <para>
    Check whether your VM Guest is inactive:
  </para>
<screen><prompt>tux &gt; </prompt><command>sudo virsh list --inactive</command>
Id    Name                           State
----------------------------------------------------
-     sles15                         shut off</screen>
  </step>
  <step>
  <para>
    Edit the configuration for this VM Guest:
  </para>
<screen><prompt>tux &gt; </prompt><command>sudo virsh edit sles15</command></screen>
  </step>
  <step>
  <para>
    Change the value of the
    <tag class="attribute">machine</tag>
    attribute:
  </para>
<screen>&lt;type arch='x86_64' machine='pc-q35-2.0'&gt;hvm&lt;/type&gt;</screen>
  </step>
  <step>
  <para>
    Restart the VM Guest:
  </para>
<screen><prompt>tux &gt; </prompt><command>sudo virsh start sles15</command></screen>
  </step>
  <step>
  <para>
    Check that the machine type has changed. Log in to the VM Guest as
    root and run the following command:
  </para>
<screen><prompt>tux &gt; </prompt><command>sudo dmidecode | grep Product</command>
Product Name: Standard PC (Q35 + ICH9, 2009)</screen>
  </step>
</procedure>

<tip>
  <title>Machine Type Update Recommendations</title>
  <para>
  Whenever the QEMU version on the host system is upgraded (for example,
  when upgrading the VM Host Server to a new service pack), upgrade the machine type
  of the VM Guests to the latest
  available version. To check, use the command <command>qemu-system-x86_64 -M
  help</command> on the VM Host Server.
  </para>
  <para>
  The default machine type <literal>pc-i440fx</literal>, for example, is
  regularly updated. If your VM Guest still runs with a machine type of
  <literal>pc-i440fx-1.<replaceable>X</replaceable></literal>, an update
  to <literal>pc-i440fx-2.<replaceable>X</replaceable></literal> is
  strongly recommended. This allows taking advantage of the most recent
  updates and corrections in machine definitions, and ensures
  better future compatibility.
  </para>
</tip>
</sect1>

<sect1 xml:id="libvirt.cpu_virsh">
<title>Changing CPU Configuration</title>
<sect2 xml:id="libvirt.cpu_alloc_virsh">
  <title>Configure the CPU allocation</title>
  <para>
    The number of allocated CPUs is stored in the VM Guest's XML
    configuration file in <filename>/etc/libvirt/qemu/</filename> in the tag type:
  </para>
  <screen>&lt;vcpu placement='static'&gt;1&lt;/vcpu&gt;</screen>
  <para>In this example the VM Guest has only one allocated CPU.
    The following prcedure shows how to change the number of allocated
    CPUs for the VM Guest.
  </para>
    <procedure>
    <step>
      <para>
      Edit the configuration for the<remark>toms 2018-08-13: shouldn't it
        be "the" replaced by "an existing"?
      </remark> VM Guest:</para>
      <screen><prompt>tux &gt; </prompt><command>sudo virsh edit sles15</command></screen>
    </step>
    <step>
      <para>
      Change the number of allocated CPUs:</para>
      <screen>&lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;</screen>
    </step>
    <step>
      <para>
      Restart the VM Guest:
      </para>
      <screen><prompt>tux &gt; </prompt><command>sudo virsh start sles15</command></screen>
    </step>
    <step>
    <para>
    Check if the number of the CPUs in the VM has changed.</para>
    <screen>
<prompt>tux &gt; </prompt><command>sudo virsh vcpuinfo sled15</command>
VCPU:           0
CPU:            N/A
State:          N/A
CPU time        N/A
CPU Affinity:   yy

VCPU:           1
CPU:            N/A
State:          N/A
CPU time        N/A
CPU Affinity:   yy
      </screen>
    </step>
    </procedure>
</sect2>
</sect1>

<sect1 xml:id="sec.libvirt.config.boot_menu.virsh">
  <title>Changing Boot Options</title>
  <para>
  The boot menu of the VM Guest can be found in the tag type
  <tag>os</tag> and usually looks like this:
  </para>
  <screen>&lt;os&gt;
  &lt;type&gt;hvm&lt;/type&gt;
  &lt;loader&gt;readonly='yes' secure='no' type='rom'/&gt;/usr/lib/xen/boot/hvmloader&lt;/loader&gt;
  &lt;nvram template='/usr/share/OVMF/OVMF_VARS.fd'/&gt;/var/lib/libvirt/nvram/guest_VARS.fd&lt;/nvram&gt;
  &lt;boot dev='hd'/&gt;
  &lt;boot dev='cdrom'/&gt;
  &lt;bootmenu enable='yes' timeout='3000'/&gt;
  &lt;smbios mode='sysinfo'/&gt;
  &lt;bios useserial='yes' rebootTimeout='0'/&gt;
  &lt;/os&gt;</screen>
  <para>
  In this example two devices are available, <tag class="attvalue">hd</tag> and <tag class="attvalue">cdrom</tag>.
  It also represents the actual boot order, so the <tag class="attvalue">cdrom</tag> comes
  before the <tag class="attvalue">hd</tag>.
  </para>

  <sect2 xml:id="sec.libvirt.config.bootorder.virsh">
    <title>Changing Boot Order</title>
    <para>
    The VM Guest's boot order is represented through the order of devices in the XML configuration file.
    As the devices are interchangeable, it's possible to change the boot order of the VM Guest.
    </para>
    <procedure>
      <step>
        <para>
        Open the VM Guest's XML configuration.
        </para>
        <screen><command>sudo virsh edit sles15</command></screen>
      </step>
      <step>
        <para>
        Change the sequence of the bootable devices.
        </para>
        <screen>...
        &lt;boot dev='cdrom'/&gt;
        &lt;boot dev='hd'/&gt;
        ...</screen>
      </step>
      <step>
        <para>
        Check if the boot order was changed successfully by
        looking at the boot menu in the BIOS of the VM Guest.
        </para>
      </step>
    </procedure>
  </sect2>

  <sect2 xml:id="sec.libvirt.config.directkernel.virsh">
    <title>Direct Kernel Boot</title>
    <para>
    libvirt also supports booting from a kernel and initrd stored on the host.
    The path to both files has to be set in the <tag>kernel</tag> and <tag>initrd</tag> tag.
    Here is an example:
    </para>
    <screen>&lt;os&gt;
    ...
  &lt;kernel&gt;/root/f8-i386-vmlinuz&lt;/kernel&gt;
  &lt;initrd&gt;/root/f8-i386-initrd&lt;/initrd&gt;
    ...
&lt;os&gt;</screen>
    <para>
    To enable Direct Kernel Boot, follow these steps:
    </para>
    <procedure>
    <step>
        <para>
        Open the VM Guest's XML configuration:
        </para>
        <screen><command>sudo virsh edit sles15</command></screen>
      </step>
      <step>
        <para>
        Add a <tag>kernel</tag> element and the path to the kernel file on the host:
        </para>
        <screen>
...
&lt;kernel&gt;/root/f8-i386-vmlinuz&lt;/kernel&gt;
...
        </screen>
      </step>
      <step>
        <para>
        Add a <tag>initrd</tag> and the path to the initrd file on the host.
        </para>
        <screen>
...
&lt;initrd&gt;/root/f8-i386-initrd&lt;/initrd&gt;
...
        </screen>
      </step>
      <step>
        <para>
        Start your VM to boot from the new kernel. 
        </para>
        <screen><command>sudo virsh start sles15</command></screen>
      </step>
    </procedure>
  </sect2>
</sect1>

<sect1 xml:id="sec.libvirt.config.memory.virsh">
  <title>Configuring Memory Allocation</title>
  <para>
  The amount of memory allocated for the VM Guest can also be configured with virsh.
  It's stored in the <tag>memory</tag> element tag and can be changed by doing the following steps:
  </para>
  <procedure>
    <step>
        <para>
        Open the VM Guest's XML configuration:
        </para>
        <screen><command>sudo virsh edit sles15</command></screen>
      </step>
      <step>
        <para>
        Look for the <tag>memory</tag> and set the amount of allocated RAM:
        </para>
        <screen>
...
&lt;memory unit='KiB'&gt;524288&lt;/memory&gt;
...
        </screen>
      </step>
      <step>
        <para>
        Check the amount of allocated RAM in your VM by running:
        <command>cat /proc/meminfo</command>
        </para>
      </step> 
  </procedure>
</sect1>

<sect1 xml:id="sec.libvirt.config.pci.virsh">
   <title>Adding a PCI Device</title>
   <para>
    To assign a PCI device to VM Guest with <command>virsh</command>,
    follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Identify the host PCI device to assign to the guest. In the following
      example, we are assigning a DEC network card to the guest:
     </para>
<screen><prompt>tux &gt; </prompt><command>sudo lspci -nn</command>
[...]
<emphasis role="bold">03:07.0</emphasis> Ethernet controller [0200]: Digital Equipment Corporation DECchip \
21140 [FasterNet] [1011:0009] (rev 22)
[...]</screen>
     <para>
      Write down the device ID (<literal>03:07.0</literal> in this case).
     </para>
    </step>
    <step>
     <para>
      Gather detailed information about the device using <command>virsh
      nodedev-dumpxml <replaceable>ID</replaceable></command>. To get the
      <replaceable>ID</replaceable>, you need to replace colon and period in
      the device ID (<literal>03:07.0</literal>) with underscore and prefix
      the result with <quote>pci_0000_</quote>
      (<literal>pci_0000_03_07_0</literal>).
     </para>
<screen><prompt>tux &gt; </prompt><command>virsh nodedev-dumpxml pci_0000_03_07_0</command>
&lt;device&gt;
  &lt;name&gt;pci_0000_03_07_0&lt;/name&gt;
  &lt;path&gt;/sys/devices/pci0000:00/0000:00:14.4/0000:03:07.0&lt;/path&gt;
  &lt;parent&gt;pci_0000_00_14_4&lt;/parent&gt;
  &lt;driver&gt;
    &lt;name&gt;tulip&lt;/name&gt;
  &lt;/driver&gt;
  &lt;capability type='pci'&gt;
    <emphasis role="bold">&lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;3&lt;/bus&gt;
    &lt;slot&gt;7&lt;/slot&gt;
    &lt;function&gt;0&lt;/function&gt;</emphasis>
    &lt;product id='0x0009'&gt;DECchip 21140 [FasterNet]&lt;/product&gt;
    &lt;vendor id='0x1011'&gt;Digital Equipment Corporation&lt;/vendor&gt;
    &lt;numa node='0'/&gt;
  &lt;/capability&gt;
&lt;/device&gt;</screen>
     <para>
      Write down the values for domain, bus, and function (see the previous
      XML code printed in bold).
     </para>
    </step>
    <step>
     <para>
      Detach the device from the host system prior to attaching it to
      VM Guest.
     </para>
<screen><prompt>tux &gt; </prompt><command>virsh nodedev-detach pci_0000_03_07_0</command>
  Device pci_0000_03_07_0 detached</screen>
     <tip>
      <title>Multi-Function PCI Devices</title>
      <para>
       When using a multi-function PCI device that does not support FLR
       (function level reset) or PM (power management) reset, you need to
       detach all its functions from the VM Host Server. The whole device must be
       reset for security reasons. <systemitem>libvirt</systemitem> will
       refuse to assign the device if one of its functions is still in use
       by the VM Host Server or another VM Guest.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Convert the domain, bus, slot, and function value from decimal to
      hexadecimal, and prefix with <literal>0x</literal> to tell the
      system that the value is hexadecimal. In our example, domain = 0,
      bus = 3, slot = 7, and function = 0. Their hexadecimal values are:
     </para>
     <remark>toms 2018-08-13: IDEA: You can make it easier for our readers
     if you combine all the steps in one go:
     printf "&lt;address domain='0x%x' bus='0x%x' slot='0x%x' function='0x%x'/&gt;\n" 0 3 7 0
     The resulting XML fragment can you copy into the target XML.
     </remark>
<screen><prompt>tux &gt; </prompt><command>printf %x 0</command>
0
<prompt>tux &gt; </prompt><command>printf %x 3</command>
3
<prompt>tux &gt; </prompt><command>printf %x 7</command>
7</screen>
     <para>
      This results in domain = 0x0000, bus = 0x03, slot = 0x07 and function
      = 0x00.
     </para>
    </step>
    <step>
     <para>
      Run <command>virsh edit</command> on your domain, and add the
      following device entry in the <literal>&lt;devices&gt;</literal>
      section using the values from the previous step:
     </para>
<screen>&lt;hostdev mode='subsystem' type='pci' managed='yes'&gt;
  &lt;source&gt;
    &lt;address domain='0x0' bus='0x03' slot='0x07' function='0x0'/&gt;
  &lt;/source&gt;
&lt;/hostdev&gt;</screen>
     <tip xml:id="tip.libvirt.config.pci.virsh.managed">
      <title><literal>managed</literal> Compared to <literal>unmanaged</literal></title>
      <para>
       <systemitem>libvirt</systemitem> recognizes two modes for handling
       PCI devices: they can be either <literal>managed</literal> or
       <literal>unmanaged</literal>. In the managed case,
       <systemitem>libvirt</systemitem> will handle all details of
       unbinding the device from the existing driver if needed, resetting
       the device, binding it to <systemitem>vfio-pci</systemitem> before
       starting the domain, etc. When the domain is terminated or the device
       is removed from the domain, <systemitem>libvirt</systemitem> will
       unbind from <systemitem>vfio-pci</systemitem> and rebind to the
       original driver in the case of a managed device. If the device is
       unmanaged, the user must ensure all of these management
       aspects of the device are done before assigning it to a domain, and
       after the device is no longer used by the domain.
      </para>
      <para>
       In the example above, the <literal>managed='yes'</literal> option
       means that the device is managed. To switch the device mode to
       unmanaged, set <literal>managed='no'</literal> in the listing above.
       If you do so, you need to take care of the related driver with the
       <command>virsh nodedev-detach</command> and <command>virsh
       nodedev-reattach</command> commands. That means you need to run
       <command>virsh nodedev-detach pci_0000_03_07_0</command> prior to
       starting the VM Guest to detach the device from the host. In case
       the VM Guest is not running, you can make the device available for
       the host by running <command>virsh nodedev-reattach
       pci_0000_03_07_0</command>.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Shut down the VM Guest and restart it to make the assigned PCI device
      available.
     </para>
     <tip>
      <title>SELinux</title>
      <para>
       If you are running SELinux on your VM Host Server, you need to disable it
       prior to starting the VM Guest with
      </para>
<screen><command>setsebool -P virt_use_sysfs 1</command></screen>
     </tip>
    </step>
   </procedure>
  </sect1>

  <sect1 xml:id="sec.libvirt.config.usb.virsh">
   <title>Adding a USB Device</title>
   <para>
    To assign a USB device to VM Guest using <command>virsh</command>,
    follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Identify the host USB device to assign to the guest:
     </para>
<screen><prompt>tux &gt; </prompt><command>sudo lsusb</command>
[...]
Bus 001 Device 003: ID <emphasis role="bold">0557:2221</emphasis> ATEN International Co., Ltd Winbond Hermon
[...]</screen>
     <para>
      Write down the vendor and product IDs. In our example, the vendor ID is
      <literal>0557</literal> and the product ID is <literal>2221</literal>.
     </para>
    </step>
    <step>
     <para>
      Run <command>virsh edit</command> on your domain, and add the
      following device entry in the <literal>&lt;devices&gt;</literal>
      section using the values from the previous step:
     </para>
<screen>&lt;hostdev mode='subsystem' type='usb'&gt;
  &lt;source startupPolicy='optional'&gt;
   <emphasis role="bold">&lt;vendor id='0557'/&gt;
   &lt;product id='2221'/&gt;</emphasis>
  &lt;/source&gt;
&lt;/hostdev&gt;</screen>
     <tip>
      <title>Vendor/Product or Device's Address</title>
      <para>
       Instead of defining the host device with &lt;vendor/&gt; and &lt;product/&gt; IDs, you
     can use the &lt;address/&gt; element as described for host PCI devices in <xref linkend="sec.libvirt.config.pci.virsh"/>.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Shut down the VM Guest and restart it to make the assigned USB device
      available.
     </para>
     <tip>
      <title>SELinux</title>
      <para>
       If you are running SELinux on your VM Host Server, you need to disable it
       prior to starting the VM Guest with
      </para>
<screen><prompt>tux &gt; </prompt><command>setsebool -P virt_use_sysfs 1</command></screen>
     </tip>
    </step>
   </procedure>
  </sect1>

   <sect1 xml:id="sec.libvirt.config.io">
  <title>Adding SR-IOV Devices</title>

  <para>
   Single Root I/O Virtualization (<xref linkend="vt.io.sriov"/>) capable
   <xref linkend="gloss.vt.acronym.pcie"/> devices can replicate their
   resources, so they appear to be multiple devices. Each of these
   <quote>pseudo-devices</quote> can be assigned to a VM Guest.
  </para>

  <para>
   <xref linkend="vt.io.sriov"/> is an industry specification that was
   created by the Peripheral Component Interconnect Special Interest Group
   (PCI-SIG) consortium. It introduces physical functions (PF) and virtual
   functions (VF). PFs are full <xref linkend="gloss.vt.acronym.pcie"/>
   functions used to manage and configure the device. PFs also can move
   data. VFs lack the configuration and management part—they only can
   move data and a reduced set of configuration functions. Since VFs do not
   have all <xref linkend="gloss.vt.acronym.pcie"/> functions, the host
   operating system or the <xref linkend="gloss.vt.hypervisor"/> must
   support <xref linkend="vt.io.sriov"/> to be able to access and
   initialize VFs. The theoretical maximum for VFs is 256 per device
   (consequently the maximum for a dual-port Ethernet card would be 512). In
   practice this maximum is much lower, since each VF consumes resources.
  </para>

  <sect2 xml:id="sec.libvirt.config.io.requirements">
   <title>Requirements</title>
   <para>
    The following requirements must be met to be able to use
    <xref linkend="vt.io.sriov"/>:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      An <xref linkend="vt.io.sriov"/>-capable network card (as of
      <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise Server</phrase></phrase> <phrase role="productnumber"><phrase os="sles;sled">15</phrase></phrase>, only network cards support
      <xref linkend="vt.io.sriov"/>)
     </para>
    </listitem>
    <listitem>
     <para>
      An AMD64/Intel 64 host supporting hardware virtualization (AMD-V or Intel
      VT-x)<phrase os="sles;sled">, see <xref linkend="sec.kvm.requires.hardware"/> for more information</phrase>
     </para>
    </listitem>
    <listitem>
     <para>
      A chipset that supports device assignment (AMD-Vi or Intel
      <xref linkend="gloss.vt.acronym.vtd"/>)
     </para>
    </listitem>
    <listitem>
     <para>
      libvirt-0.9.10 or better
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="vt.io.sriov"/> drivers must be loaded and configured on
      the host system
     </para>
    </listitem>
    <listitem>
     <para>
      A host configuration that meets the requirements listed at
      <xref linkend="ann.vt.io.require"/>
     </para>
    </listitem>
    <listitem>
     <para>
      A list of the PCI addresses of the VF(s) that will be assigned to
      VM Guests
     </para>
    </listitem>
   </itemizedlist>
   <tip>
    <title>Checking if a Device is SR-IOV-Capable</title>
    <para>
     The information whether a device is SR-IOV-capable can be obtained from
     its PCI descriptor by running <command>lspci</command>. A device that
     supports <xref linkend="vt.io.sriov"/> reports a capability similar to
     the following:
    </para>
<screen>Capabilities: [160 v1] Single Root I/O Virtualization (<xref linkend="vt.io.sriov"/>)</screen>
   </tip>
   <note>
    <title>Adding an SR-IOV Device at VM Guest Creation</title>
    <para>
     Before adding an SR-IOV device to a VM Guest when initially
     setting it up, the VM Host Server already needs to be configured as described
     in <xref linkend="sec.libvirt.config.io.config"/>.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="sec.libvirt.config.io.config">
   <title>Loading and Configuring the SR-IOV Host Drivers</title>
   <para>
    To be able to access and initialize VFs, an SR-IOV-capable driver needs
    to be loaded on the host system.

   </para>
   <procedure>
    <step>
     <para>
      Before loading the driver, make sure the card is properly detected by
      running <command>lspci</command>. The following example shows the
      <command>lspci</command> output for the dual-port Intel 82576NS
      network card:
     </para>
<screen><prompt>tux &gt; </prompt><command>sudo</command> /sbin/lspci | grep 82576
01:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)</screen>
     <para>
      In case the card is not detected, it is likely that the hardware
      virtualization support in the BIOS/EFI has not been enabled.
      <remark>toms 2018-08-13: I think, a hint is missing to go to the BIOS/EFI to
      enable the hardware virtualization support.</remark>
     </para>
    </step>
    <step>
     <para>
      Check whether the <xref linkend="vt.io.sriov"/> driver is already
      loaded by running <command>lsmod</command>. In the following example a
      check for the igb driver (for the Intel 82576NS network card) returns
      a result. That means the driver is already loaded. If the command
      returns nothing, the driver is not loaded.
     </para>
<screen><prompt>tux &gt; </prompt><command>sudo/sbin/lsmod | egrep "^igb "</command>
igb                   185649  0</screen>
    </step>
    <step>
     <para>
      Skip this step if the driver is already loaded.
     </para>
     <remark>toms 2018-08-13: the following paras should go into substeps.</remark>
     <para>
      If the <xref linkend="vt.io.sriov"/> driver is not yet loaded, the
      non-<xref linkend="vt.io.sriov"/> driver needs to be removed first,
      before loading the new driver. Use <command>rmmod</command> to unload
      a driver. The following example unloads the
      non-<xref linkend="vt.io.sriov"/> driver for the Intel 82576NS network
      card:
     </para>
     <screen><prompt>tux &gt; </prompt><command>sudo /sbin/rmmod igbvf</command></screen>
     
     
     <para>
      Load the <xref linkend="vt.io.sriov"/> driver subsequently using
      the <command>modprobe</command> command—the VF parameter
      (<literal>max_vfs</literal>) is mandatory:
     </para>
     <screen><prompt>tux &gt; </prompt><command>sudo /sbin/modprobe igb max_vfs=8</command></screen>
     <para>
      Or load the driver via SYSFS:
     </para>
     <para>
      Find the PCI ID of the physical NIC by listing Ethernet devices:
     </para>
<screen><prompt>tux &gt; </prompt><command>sudo lspci | grep Eth</command>
06:00.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:00.1 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)</screen>
     <para>
      To enable VFs, echo the number of desired VFs to load to the
      <literal>sriov_numvfs</literal> parameter:
     </para>

<screen><prompt>tux &gt; </prompt><command>sudo echo 1 &gt; /sys/bus/pci/devices/0000:06:00.1/sriov_numvfs</command></screen>

<para>Verify that the VF NIC was loaded:</para>
<screen><prompt>tux &gt; </prompt><command>sudo lspci | grep Eth</command>
06:00.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:00.1 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:08.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)</screen>

<para>Obtain the maximum number of VFs available:</para>

<screen><prompt>tux &gt; </prompt><command>sudo lspci -vvv -s 06:00.1 | grep 'Initial VFs'</command>
                       Initial VFs: 32, Total VFs: 32, Number of VFs: 0,
Function Dependency Link: 01</screen>

    </step>

    <step>
     <para>
      Create a <filename>/etc/systemd/system/before.service</filename> file
      which loads VF via SYSFS on boot:
     </para>
     <remark>toms 2018-08-13: the following paras are too much for this single
      step. They should be rewritten/rephrased or substeps should be added.
     </remark>
<screen>[Unit]
Before=
[Service]
Type=oneshot
RemainAfterExit=true
ExecStart=/bin/bash -c "echo 1 &gt; /sys/bus/pci/devices/0000:06:00.1/sriov_numvfs"
# beware, executable is run directly, not through a shell, check the man pages
# systemd.service and systemd.unit for full syntax
[Install]
# target in which to start the service
WantedBy=multi-user.target
#WantedBy=graphical.target</screen>
<para>
 Additionally, it is required to create another service file
 (<filename>after-local.service</filename>) pointing to
 <filename>/etc/init.d/after.local</filename> script that detaches the
 NIC prior to starting the VM, otherwise the VM would fail to start:
</para>

<screen>[Unit]
Description=/etc/init.d/after.local Compatibility
After=libvirtd.service
Requires=libvirtd.service
[Service]
Type=oneshot
ExecStart=/etc/init.d/after.local
RemainAfterExit=true

[Install]
WantedBy=multi-user.target</screen>
<para>
And copy it to <filename>/etc/systemd/system</filename>.
</para>

<screen>#! /bin/sh
# ...
virsh nodedev-detach pci_0000_06_08_0</screen>
<para>
Then save it as <filename>/etc/init.d/after.local</filename>.
</para>
    </step>

    <step>
     <para>
      Reboot the machine and check if the SR-IOV driver is loaded by
      re-running the <command>lspci</command> command from the first step of
      this procedure. If the SR-IOV driver was loaded successfully you
      should see additional lines for the VFs:
     </para>
<screen>01:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
[...]
04:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
04:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
04:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
[...]</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.libvirt.config.io.attach">
   <title>Adding a VF Network Device to an Existing VM Guest</title>
   <para>
    When the <xref linkend="vt.io.sriov"/> hardware is properly set up on
    the VM Host Server, you can add VFs to VM Guests. To do so, you need to
    collect some data first.
   </para>
   <remark>toms 2018-08-13: Add title to the procedure</remark>
   <procedure>
    <para>
     The following procedure is using example data. Make sure to
     replace it by appropriate data from your setup.
    </para>
    <step>
     <para>
      Use the <command>virsh nodedev-list</command> command to get the PCI
      address of the VF you want to assign and its corresponding PF.
      Numerical values from the <command>lspci</command> output shown in
      <xref linkend="sec.libvirt.config.io.config"/> (for example
      <literal>01:00.0</literal> or <literal>04:00.1</literal>) are
      transformed by adding the prefix "pci_0000_" and by replacing colons
      and dots with underscores. So a PCI ID listed as "04:00.0" by
      <command>lspci</command> is listed as "pci_0000_04_00_0" by virsh. The
      following example lists the PCI IDs for the second port of the Intel
      82576NS network card:
     </para>
<screen><prompt>tux &gt; </prompt><command>sudo virsh nodedev-list | grep 0000_04_</command>
pci_0000_04_00_0
pci_0000_04_00_1
pci_0000_04_10_0
pci_0000_04_10_1
pci_0000_04_10_2
pci_0000_04_10_3
pci_0000_04_10_4
pci_0000_04_10_5
pci_0000_04_10_6
pci_0000_04_10_7
pci_0000_04_11_0
pci_0000_04_11_1
pci_0000_04_11_2
pci_0000_04_11_3
pci_0000_04_11_4
pci_0000_04_11_5</screen>
     <para>
      The first two entries represent the PFs, whereas the other entries
      represent the VFs.
     </para>
    </step>
    <step>
     <para>
      Get more data that will be needed by running the command
      <command>virsh nodedev-dumpxml</command> on the PCI ID of the VF you
      want to add:
     </para>
<screen><prompt>tux &gt; </prompt><command>sudo virsh nodedev-dumpxml pci_0000_04_10_0</command>
&lt;device&gt;
  &lt;name&gt;pci_0000_04_10_0&lt;/name&gt;
  &lt;parent&gt;pci_0000_00_02_0&lt;/parent&gt;
  &lt;capability type='pci'&gt;
    <emphasis role="bold">&lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;4&lt;/bus&gt;
    &lt;slot&gt;16&lt;/slot&gt;
    &lt;function&gt;0&lt;/function&gt;</emphasis>
    &lt;product id='0x10ca'&gt;82576 Virtual Function&lt;/product&gt;
    &lt;vendor id='0x8086'&gt;Intel Corporation&lt;/vendor&gt;
    &lt;capability type='phys_function'&gt;
      &lt;address domain='0x0000' bus='0x04' slot='0x00' function='0x0'/&gt;
    &lt;/capability&gt;
  &lt;/capability&gt;
&lt;/device&gt;</screen>
     <para>
      The following data is needed for the next step:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        <literal>&lt;domain&gt;0&lt;/domain&gt;</literal>
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>&lt;bus&gt;4&lt;/bus&gt;</literal>
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>&lt;slot&gt;16&lt;/slot&gt;</literal>
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>&lt;function&gt;0&lt;/function&gt;</literal>
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      Create a temporary XML file (for example
      <filename>/tmp/vf-interface.xml</filename> containing the data
      necessary to add a VF network device to an existing VM Guest. The
      minimal content of the file needs to look like the following:
     </para>
<screen>&lt;interface type='hostdev'&gt;<co xml:id="sriov.iface"/>
 &lt;source&gt;
  &lt;address type='pci' domain='0' bus='11' slot='16' function='0'2/&gt;<co xml:id="sriov.data"/>
 &lt;/source&gt;
&lt;/interface&gt;</screen>
     <calloutlist>
      <callout arearefs="sriov.iface">
       <para>
        VFs do not get a fixed MAC address; it changes every time the host
        reboots. When adding network devices the <quote>traditional</quote>
        way with &lt;hostdev&gt;, it would require to reconfigure the
        VM Guest's network device after each reboot of the host, because of
        the MAC address change. To avoid this kind of problem, libvirt
        introduced the <tag class="attvalue">hostdev</tag> value,
        which sets up network-specific data <emphasis>before</emphasis>
        assigning the device.
       </para>
      </callout>
      <callout arearefs="sriov.data">
       <para>
        Specify the data you acquired in the previous step here.
       </para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>
      In case a device is already attached to the host, it cannot be
      attached to a guest. To make it available for guests, detach it from
      the host first:
     </para>
<screen><prompt>tux &gt; </prompt><command>sudo virsh nodedev-detach pci_0000_04_10_0</command></screen>
    </step>
    <step>
     <para>
      Add the VF interface to an existing VM Guest:
     </para>
<screen><prompt>tux &gt; </prompt><command>sudo virsh attach-device <replaceable>GUEST</replaceable> /tmp/vf-interface.xml --<replaceable>OPTION</replaceable></command></screen>
     <para>
      <replaceable>GUEST</replaceable> needs to be replaced by the domain
      name, id or uuid of the VM Guest and
      --<replaceable>OPTION</replaceable> can be one of the following:
     </para>
     <variablelist>
      <varlistentry>
       <term><option>--persistent</option>
       </term>
       <listitem>
        <para>
         This option will always add the device to the domain's persistent
         XML. In addition, if the domain is running, it will be hotplugged.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>--config</option>
       </term>
       <listitem>
        <para>
         This option will only affect the persistent XML, even if the domain
         is running. The device will only show up in the guest on next boot.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>--live</option>
       </term>
       <listitem>
        <para>
         This option will only affect a running domain. If the domain is
         inactive, the operation will fail. The device is not persisted in
         the XML and will not be available in the guest on next boot.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>--current</option></term>
       <listitem>
        <para>
         This option affects the current state of the domain. If the domain
         is inactive, the device is added to the persistent XML and will be
         available on next boot. If the domain is active, the device is
         hotplugged but not added to the persistent XML.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </step>
    <step>
     <para>
      To detach a VF interface, use the <command>virsh
      detach-device</command> command, which also takes the options listed
      above.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="libvirt.config.io.pool">
   <title>Dynamic Allocation of VFs from a Pool</title>
   <para>
    If you define the PCI address of a VF into a guest's configuration
    statically as described in
    <xref linkend="sec.libvirt.config.io.attach"/>, it is hard to migrate
    such guest to another host. The host must have identical hardware in the
    same location on the PCI bus, or the guest configuration must be
    modified prior to each start.
   </para>
   <para>
    Another approach is to create a <systemitem class="library">libvirt</systemitem> network with a device pool
    that contains all the VFs of an <xref linkend="vt.io.sriov"/> device.
    The guest then references this network, and each time it is started, a
    single VF is dynamically allocated to it. When the guest is stopped, the
    VF is returned to the pool, available for another guest.
   </para>
   <sect3 xml:id="libvirt.config.io.pool.host">
    <title>Defining Network with Pool of VFs on VM Host Server</title>
    <para>
     The following example of network definition creates a pool of all VFs
     for the <xref linkend="vt.io.sriov"/> device with its physical function
     (PF) at the network interface eth0 on the host:
    </para>
<screen>&lt;network&gt;
  &lt;name&gt;passthrough&lt;/name&gt;
    &lt;forward mode='hostdev' managed='yes'&gt;
      &lt;pf dev='eth0'/&gt;
    &lt;/forward&gt;
  &lt;/network&gt;</screen>
    <para>
     To use this network on the host, save the above code to a file, for
     example <filename>/tmp/passthrough.xml</filename>, and execute the
     following commands. Remember to replace eth0 with the real network
     interface name of your <xref linkend="vt.io.sriov"/> device's PF:
    </para>
<screen><prompt>tux &gt; </prompt><command>sudo virsh net-define /tmp/passthrough.xml</command>
<prompt>tux &gt; </prompt><command>sudo</command> virsh net-autostart passthrough
<prompt>tux &gt; </prompt><command>sudo</command> virsh net-start passthrough</screen>
   </sect3>
   <sect3 xml:id="libvirt.config.io.pool.guest">
    <title>Configuring VM Guest to Use VF from the Pool</title>
    <para>
     The following example of guest device interface definition uses a VF of
     the <xref linkend="vt.io.sriov"/> device from the pool created in
     <xref linkend="libvirt.config.io.pool.host"/>. <systemitem class="library">libvirt</systemitem> automatically
     derives the list of all VFs associated with that PF the first time the
     guest is started.
    </para>
<screen>&lt;interface type='network'&gt;
  &lt;source network='passthrough'&gt;
&lt;/interface&gt;</screen>
    <para>
     To verify the list of associated VFs, run <command>virsh net-dumpxml
     passthrough</command> on the host after the first guest that uses the
     network with the pool of VFs starts.
    </para>
<screen>&lt;network connections='1'&gt;
  &lt;name&gt;passthrough&lt;/name&gt;
  &lt;uuid&gt;a6a26429-d483-d4ed-3465-4436ac786437&lt;/uuid&gt;
  &lt;forward mode='hostdev' managed='yes'&gt;
    &lt;pf dev='eth0'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x1'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x3'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x5'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x7'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x1'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x3'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x5'/&gt;
  &lt;/forward&gt;
  &lt;/network&gt;</screen>
   </sect3>
  </sect2>
 </sect1>

 <sect1 xml:id="libvirt.config.storage.virsh">
  <title>Configuring Storage Devices</title>
  <para>
  Information about storage devices is defined under the <tag>disk</tag> element.
   <remark>toms 2018-08-16: check the following para and itemizedlist.</remark>
  A useful <tag>disk</tag> element supports several attributes, but the
   following two attributes are the most important:
  </para>
  <itemizedlist>
   <listitem>
    <para>The <tag class="attribute">type</tag> attribute describes the source
     of the virtual disk device. Valid values are <tag class="attvalue">file</tag>,
     <tag class="attvalue">block</tag>, <tag class="attvalue">dir</tag>,
     <tag class="attvalue">network</tag>, or <tag class="attvalue">volume</tag>.</para>
   </listitem>
   <listitem>
    <para>The <tag class="attribute">device</tag> attribute indicates, how
     the disk is to be exposed to the guest OS. As an example, possible values
     include <tag
      class="attvalue">floppy</tag>, <tag class="attvalue">disk</tag>,
     <tag class="attvalue">cdrom</tag>, and others.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The following child elements are the most important:
  </para>
  <remark>toms 2018-08-16: check the following list.</remark>
  <itemizedlist>
   <listitem>
    <para><tag>driver</tag> contains the driver and the bus, which is used by
     the VM guest to work with the new disk device.</para>
   </listitem>
   <listitem>
    <para>The <tag>target</tag> element contains the device name, under which
     the new disk is shown in the VM guest.
     as well as the optional bus attribute, which defines the type of bus,
     on which the new disk should operate.</para>
   </listitem>
  </itemizedlist>
<!--  <para>
  There are multiple types such as <tag class="attvalue">file</tag>, <tag class="attvalue">block</tag>,
  <tag class="attvalue">dir</tag>, <tag class="attvalue">network</tag> or <tag class="attvalue">volume</tag>.
  The <tag>type</tag> element describes the source of the virtual disk device.
  A <tag>source</tag> element has to be set accordingly.
  Virtual disks can be treated by the VM Guest in several different ways, for example
  <tag class="attvalue">floppy</tag>, <tag class="attvalue">disk</tag> or <tag class="attvalue">cdrom</tag>.
  Each disk contains a <tag class="attribute">type</tag> attribute which can have the values: <tag class="attvalue">file</tag>, <tag class="attvalue">block</tag>,
  <tag class="attvalue">dir</tag>, <tag class="attvalue">network</tag> or <tag class="attvalue">volume</tag>.
  A <tag>disk</tag> element can contain several child elements, whereas <tag>driver</tag>, <tag>source</tag>, and <tag>target</tag> are the most important.
  <tag>driver</tag> contains the driver and the bus, which is used by the VM guest to work with the new disk device.
  In the <tag>source</tag> element, the path to the disk's source is stored.
  The <tag>target</tag> element contains the device name, under which the new disk is shown in the VM Guest, as well as the optional bus attribute,
  which defines the type of bus, on which the new disk should operate.
  </para>
  -->
  <para>
  The following procedure shows how to add storage devices to the VM guest:
  </para>
  <procedure>
    <step>
        <para>Edit the configuration for an existing VM guest:</para>
        <screen>&prompt.user;<command>sudo virsh edit sles15</command></screen>
      </step>
      <step>
        <para>Add a <tag>disk</tag> element together with the attributes <tag class="attvalue">type</tag> and
        <tag class="attvalue">device</tag>.</para>
        <screen>&lt;disk type='file' device='disk'&gt;</screen>
      </step>
      <step>
        <para>Specify a <tag>driver</tag> element and use the default values:</para>
        <screen>&lt;driver name='qemu' type='qcow2'/&gt;</screen>
      </step>
      <step>
        <para>Add the path for the disk source:</para>
       <remark>toms 2018-08-16: it would be good to refer to another section
       how to create such a image file. Or, if feasible, add another step
       to create a device.</remark>
        <screen>&lt;source file='/var/lib/libvirt/images/sles15.qcow2'/&gt;</screen>
      </step>
      <step>
        <para>Define the target device name in the VM guest and the bus, on which the disk should work:</para>
        <screen>&lt;target dev='vda' bus='virtio'/&gt;</screen>
      </step>
      <step>
        <para>Restart your VM:</para>
        <screen><command>sudo virsh start sles15</command></screen>
      </step>
  </procedure>
  <para>Your new storage device should be available in the Guest OS.</para>
 </sect1>

  <sect1 xml:id="libvirt.video_virsh">
    <title>Configuring Video devices</title>
    <para>
    When using the <command>Virtual Machine Manager</command>,
    only the Video device model can be defined. The amount of allocated VRAM
    or 2D/3D acceleration can only be changed in the XML configuration.
    </para>
    <sect2 xml:id="libvirt.video_vram_virsh">
      <title>Changing the amount of allocated VRAM</title>
      <procedure>
      <step>
        <para>Edit the configuration for an existing VM Guest:</para>
        <screen><prompt>tux &gt; </prompt><command>sudo virsh edit sles15</command></screen>
      </step>
      <step>
        <para>Change the size of the allocated VRAM:</para>
        <screen>
&lt;video&gt;
&lt;model type='vga' vram='65535' heads='1'&gt;
...
&lt;/model&gt;
&lt;/video&gt;
        </screen>
      </step>
      <step>
      <para>
      Check if the amount of VRAM in the VM has changed by looking at
      the amount in the <command>Virtual Machine Manager</command>.
      </para>
      </step>
      </procedure>
    </sect2>

    <sect2 xml:id="libvirt.video_accel_virsh">
      <title>Changing the state of 2D/3D acceleration</title>
      <procedure>
        <step>
        <para>Edit the configuration for an existing VM Guest:</para>
        <screen><prompt>tux &gt; </prompt><command>sudo virsh edit sles15</command></screen>
      </step>
      <step>
        <para>To enable/disable 2D/3D acceleration, change the value of 'accel3d' and 'accel2d'
        accordingly.</para>
        <screen>
&lt;video&gt;
&lt;acceleration accel3d='yes' accel2d='no'&gt;
...
&lt;/model&gt;
&lt;/video&gt;
        </screen>
      </step>
      </procedure>
      <tip>
        <title>Note on enabling 2D/3D acceleration</title>
        <para>
        Please note that only 'vbox' video devices are capable of 2D/3D acceleration.
        You cannot enable it on other video devices.
        </para>
      </tip>
    </sect2>
  </sect1>
</chapter>
