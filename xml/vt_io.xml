<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE sect1
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<sect1 xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="sec.vt.io">
 <title>I/O Virtualization</title>

 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
  </dm:docmanager>
 </info>

 <para>
  &vmguest;s not only share CPU and memory resources of the host system, but
  also the I/O subsystem. Because software I/O virtualization techniques
  deliver less performance than bare metal, hardware solutions that deliver
  almost <quote>native</quote> performance have been developed recently.
  &productname; supports the following I/O virtualization techniques:
 </para>

 <variablelist>
  <varlistentry xml:id="vt.io.fullv">
   <term>Full Virtualization</term>
   <listitem>
    <para>
     Fully Virtualized (FV) drivers emulate widely supported real devices,
     which can be used with an existing driver in the &vmguest;. The guest is
     also called <emphasis>Hardware Virtual Machine</emphasis> (HVM).
     Since the physical device on the &vmhost; may differ from the emulated
     one, the hypervisor needs to process all I/O operations before handing
     them over to the physical device. Therefore all I/O operations need to
     traverse two software layers, a process that not only significantly
     impacts I/O performance, but also consumes CPU time.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt.io.parav">
   <term>Paravirtualization</term>
   <listitem>
    <para>
     Paravirtualization (PV) allows direct communication between the hypervisor
     and the &vmguest;. With less overhead involved, performance is much better
     than with full virtualization. However, paravirtualization requires either
     the guest operating system to be modified to support the
     paravirtualization API or paravirtualized drivers. <phrase
     os="sles;sled">See <xref linkend="sec.kvm.requires.guests.virt_drivers"/>
     for a list of guest operating systems supporting
     paravirtualization.</phrase>
    </para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt.io.pvhvm">
   <term>PVHVM</term>
   <listitem>
    <para>
     This type of virtualization enhances HVM (see
     <xref linkend="vt.io.fullv"/>) with paravirtualized (PV) drivers, and PV
     interrupt and timer handling.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt.io.vfio">
   <term>VFIO</term>
   <listitem>
    <para>
     VFIO stands for <emphasis>Virtual Function I/O</emphasis> and is a new
     user-level driver framework for Linux. It replaces the traditional &kvm;
     &pciback; device assignment. The VFIO driver exposes direct device access
     to user space in a secure memory
     (<xref
     linkend="gloss.vt.acronym.iommu"/>) protected environment.
     With VFIO, a &vmguest; can directly access hardware devices on the
     &vmhost; (pass-through), avoiding performance issues caused by emulation
     in performance critical paths. This method does not allow to share
     devices&mdash;each device can only be assigned to a single &vmguest;. VFIO
     needs to be supported by the &vmhost; CPU, chipset and the BIOS/EFI.
    </para>
    <para>
     Compared to the legacy &kvm; PCI device assignment, VFIO has the following
     advantages:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       Resource access is compatible with secure boot.
      </para>
     </listitem>
     <listitem>
      <para>
       Device is isolated and its memory access protected.
      </para>
     </listitem>
     <listitem>
      <para>
       Offers a user space device driver with more flexible device ownership
       model.
      </para>
     </listitem>
     <listitem>
      <para>
       Is independent of &kvm; technology, and not bound to x86 architecture
       only.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     As of <phrase os="sles;sled">&sls; 12 SP2</phrase><phrase
     os="osuse">openSUSE 42.2</phrase>, the USB and PCI Pass-through methods of
     device assignment are considered deprecated and were superseded by the
     VFIO model.
   </para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt.io.sriov">
   <term>SR-IOV</term>
   <listitem>
    <para>
     The latest I/O virtualization technique, Single Root I/O Virtualization
     SR-IOV combines the benefits of the aforementioned
     techniques&mdash;performance and the ability to share a device with
     several &vmguest;s. SR-IOV requires special I/O devices, that are capable
     of replicating resources so they appear as multiple separate devices. Each
     such <quote>pseudo</quote> device can be directly used by a single guest.
     However, for network cards for example the number of concurrent queues
     that can be used is limited, potentially reducing performance for the
     &vmguest; compared to paravirtualized drivers. On the &vmhost;, SR-IOV
     must be supported by the I/O device, the CPU and chipset, the BIOS/EFI and
     the hypervisor&mdash;for setup instructions see
     <xref linkend="sec.libvirt.config.pci"/>.
<!-- fs 2014-02-21: no list available
     ATM &productname; supports the SRV-IOV capable network cards listed below
     -->
    </para>
   </listitem>
  </varlistentry>
 </variablelist>

 <important xml:id="ann.vt.io.require">
  <title>Requirements for VFIO and SR-IOV</title>
  <para>
   To be able to use the VFIO and SR-IOV features, the &vmhost; needs to
   fulfill the following requirements:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     IOMMU needs to be enabled in the BIOS/EFI.
    </para>
   </listitem>
   <listitem>
    <para>
     For Intel CPUs, the kernel parameter <literal>intel_iommu=on</literal>
     needs to be provided on the kernel command line. For more information,
     see <xref linkend="sec.grub2.yast2.config.advanced.kernel_parameters"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     The VFIO infrastructure needs to be available. This can be achieved by
     loading the kernel module
     <systemitem class="resource">vfio_pci</systemitem>. For more information,
     see <xref linkend="sec.boot.systemd.advanced.kernel_modules"/>.
    </para>
   </listitem>
  </itemizedlist>
 </important>
</sect1>
