<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.qemu.cachemodes">
 <title>&kvm; Disk Cache Modes</title>
 <info/>
 <para/>
 <sect1>
  <title>Disk Interface Cache Modes</title>

  <para>
   <command>qemu-system-ARCH</command> allows for various storage caching
   strategies to be specified when configuring a KVM guest. Each guest disk
   interface can have one of the following cache modes specified:
   <emphasis>writethrough</emphasis>, <emphasis>writeback</emphasis>,
   <emphasis>none</emphasis>, <emphasis>directsync</emphasis>, or
   <emphasis>unsafe</emphasis>. If no cache mode is specified,
   <command>qemu-system-ARCH</command> uses an appropriate default cache
   mode. These cache modes influence how host-based storage is accessed, as
   follows:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Read/write data may be cached in the host page cache.
    </para>
   </listitem>
   <listitem>
    <para>
     The guest's storage controller is informed whether a write cache is
     present, allowing for the use of a flush command.
    </para>
   </listitem>
   <listitem>
    <para>
     Synchronous write mode may be used, in which write requests are
     reported complete only when committed to the storage device.
    </para>
   </listitem>
   <listitem>
    <para>
     Flush commands (generated by the guest storage controller) may be
     ignored for performance reasons.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   In the event of a disorderly disconnection between the guest and its
   storage, the cache mode in use will affect whether data loss occurs. The
   cache mode can also affect disk performance significantly. Additionally,
   some cache modes are incompatible with live migration, depending on a
   number of factors. There are no simple rules about what combination of
   cache mode, disk image format, image placement, or storage sub-system is
   best. The user should plan each guest's configuration carefully and
   experiment with various configurations to determine the optimal
   performance.
  </para>
 </sect1>
 <sect1>
  <title>Description of Cache Modes</title>

  <variablelist>
   <varlistentry>
    <term>cache mode unspecified</term>
    <listitem>
     <para>
      In <command>qemu-system-ARCH</command> versions older than v1.2 (e.g.
      SLES11 SP2), not specifying a cache mode meant that
      <emphasis>writethrough</emphasis> would be used as the default. Since
      that version, the various <command>qemu-system-ARCH</command> guest
      storage interfaces have been fixed to handle
      <emphasis>writeback</emphasis> or <emphasis>writethrough</emphasis>
      semantics more correctly, allowing for the default caching mode to be
      switched to <emphasis>writeback</emphasis>. The guest driver for each
      of <literal>ide</literal>, <literal>scsi</literal>, and
      <literal>virtio</literal> have within their power to disable the write
      back cache, causing the caching mode used to revert to
      <emphasis>writethrough</emphasis>. The typical guest's storage drivers
      will maintain the default caching mode as
      <emphasis>writeback</emphasis>, however.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="cache.writethrough">
    <term>cache = writethrough</term>
    <listitem>
     <para>
      This mode causes <command>qemu-system-ARCH</command> to interact with
      the disk image file or block device with O_DSYNC semantics, where
      writes are reported as completed only when the data has been committed
      to the storage device. The host page cache is used in what can be
      termed a writethrough caching mode. The guest's virtual storage
      adapter is informed that there is no writeback cache, so the guest
      would not need to send down flush commands to manage data integrity.
      The storage behaves as if there is a writethrough cache.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="cache.writeback">
    <term>cache = writeback</term>
    <listitem>
     <para>
      This mode causes <command>qemu-system-ARCH</command> to interact with
      the disk image file or block device with neither O_DSYNC nor O_DIRECT
      semantics, so the host page cache is used and writes are reported to
      the guest as completed when placed in the host page cache, and the
      normal page cache management will handle commitment to the storage
      device. Additionally, the guest's virtual storage adapter is informed
      of the writeback cache, so the guest would be expected to send down
      flush commands as needed to manage data integrity. Analogous to a raid
      controller with RAM cache.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="cache.none">
    <term>cache = none</term>
    <listitem>
     <para>
      This mode causes <command>qemu-system-ARCH</command> to interact with
      the disk image file or block device with O_DIRECT semantics, so the
      host page cache is bypassed and I/O happens directly between the
      <command>qemu-system-ARCH</command> userspace buffers and the storage
      device. Because the actual storage device may report a write as
      completed when placed in its write queue only, the guest's virtual
      storage adapter is informed that there is a writeback cache, so the
      guest would be expected to send down flush commands as needed to
      manage data integrity. Performance-wise, it is equivalent to direct
      access to your host's disk.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="cache.unsafe">
    <term>cache = unsafe</term>
    <listitem>
     <para>
      This mode is similar to the <literal>cache=writeback</literal> mode
      discussed above. The key aspect of this <quote>unsafe</quote> mode, is
      that all flush commands from the guests are ignored. Using this mode
      implies that the user has accepted the trade-off of performance over
      risk of data loss in the event of a host failure. Useful, for example,
      during guest installation, but not for production workloads.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="cache.directsync">
    <term>cache = directsync</term>
    <listitem>
     <para>
      This mode causes <command>qemu-system-ARCH</command> to interact with
      the disk image file or block device with both O_DSYNC and O_DIRECT
      semantics, where writes are reported as completed only when the data
      has been committed to the storage device, and when it is also
      desirable to bypass the host page cache. Like
      <xref linkend="cache.writethrough"/>, it is helpful to guests that do
      not send flushes when needed. It was the last cache mode added,
      completing the possible combinations of caching and direct access
      semantics.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1>
  <title>Data Integrity Implications of Cache Modes</title>

  <variablelist>
   <varlistentry>
    <term>cache = writethrough, cache = none, cache=directsync</term>
    <listitem>
     <para>
      These are the safest modes, and considered equally safe, given that
      the guest operating system is <quote>modern and well behaved</quote>,
      which means that it uses flushes as needed. If you have a suspect
      guest, use <emphasis>writethough</emphasis>, or
      <emphasis>directsync</emphasis>. Note that some file systems are not
      compatible with <literal>cache=none</literal> or
      <literal>cache=directsync</literal>, as they do not support O_DIRECT,
      which these cache modes rely on.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>cache = writeback</term>
    <listitem>
     <para>
      This mode informs the guest of the presence of a write cache, and
      relies on the guest to send flush commands as needed to maintain data
      integrity within its disk image. This is a common storage design which
      is completely accounted for within modern file systems. But it should
      be noted that because there is a window of time between the time a
      write is reported as completed, and that write being committed to the
      storage device, this mode exposes the guest to data loss in the
      unlikely event of a host failure.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>cache = unsafe</term>
    <listitem>
     <para>
      This mode is similar to writeback caching except the guest flush
      commands are ignored, nullifying the data integrity control of these
      flush commands, and resulting in a higher risk of data loss due to
      host failure. The name <quote>unsafe</quote> should serve as a warning
      that there is a much higher potential for data loss due to a host
      failure than with the other modes. Note that as the guest terminates,
      the cached data is flushed at that time.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1>
  <title>Performance Implications of Cache Modes</title>

  <para>
   The choice to make full use of the page cache, or to write through it, or
   to bypass it altogether can have dramatic performance implications. Other
   factors that influence disk performance include the capabilities of the
   actual storage system, what disk image format is used, the potential size
   of the page cache and the IO scheduler used. Additionally, not flushing
   the write cache increases performance, but with risk, as noted above. As
   a general rule, high-end systems typically perform best with
   <literal>cache = none</literal>, because of the reduced data copying that
   occurs. The potential benefit of having multiple guests share the common
   host page cache, the ratio of reads to writes, and the use of
   <literal>aio = native</literal> (see below) should also be considered.
  </para>
 </sect1>
 <sect1 xml:id="sec.cache.mode.live.migration">
  <title>Effect of Cache Modes on Live Migration</title>

  <para>
   The caching of storage data and meta-data restricts the configurations
   that support live migration. Currently, only <literal>raw</literal>,
   <literal>qcow2</literal> and <literal>qed</literal> image formats can be
   used for live migration. If a clustered file system is used, all cache
   modes support live migration. Otherwise the only cache mode that supports
   live migration on read/write shared storage is <literal>cache =
   none</literal>.
  </para>

  <para>
   The <systemitem>libvirt</systemitem> management layer includes checks for
   migration compatibility based on a number of factors. If the guest
   storage is hosted on a clustered file system, is read-only or is marked
   sharable, then the cache mode is ignored when determining if migration
   can be allowed. Otherwise <systemitem>libvirt</systemitem> will not allow
   migration unless the cache mode is set to <literal>none</literal>.
   However, this restriction can be overridden with the
   <quote>unsafe</quote> option to the migration APIs, which is also
   supported by <command>virsh</command>, as for example in
  </para>

<screen>virsh migrate --live --unsafe</screen>

  <tip>
   <para>
    <literal>cache = none</literal> is required for the IO mode setting
    <literal>aio = native</literal>. If another cache mode is used, then the
    IO mode will silently be switched back to the default <literal>aio =
    threads</literal>. <command>qemu-system-ARCH</command> implements the
    guest flush within the host by using
    <systemitem>fdatasync()</systemitem>.
   </para>
  </tip>
 </sect1>
</chapter>
