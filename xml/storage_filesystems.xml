<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="filesystems" lang="en">
  <title>Overview of File Systems in Linux</title>
  <para>
   &productname; ships with a number of different file
   systems from which to choose, including Btrfs, Ext3, Ext2, ReiserFS, and
   XFS. Each file system has its own advantages and disadvantages.
  </para>
  <para>
   Professional high-performance setups might require a highly available
   storage systems. To meet the requirements of high-performance clustering
   scenarios, &productname; includes OCFS2 (Oracle Cluster File
   System 2) and the Distributed Replicated Block Device (DRBD) in the &hasi;
   add-on. These advanced
   storage systems are not covered in this guide. For information, see the
   <citetitle>&sle; &hasi; &haguide;</citetitle> at <ulink url="http://www.suse.com/doc"/>.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="sec_filesystems_glossary" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec_filesystems_major" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec_filesystems_add" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec_filesystems_lfs" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sect_stor_limits" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="bwkbhpd" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec_filesystems_info" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="sec_filesystems_glossary">
   <title>Terminology</title>

   <variablelist>
    <varlistentry id="bgchzar">
     <term>metadata</term>
     <listitem>
      <para>
       A data structure that is internal to the file system. It assures that
       all of the on-disk data is properly organized and accessible.
       Essentially, it is <quote>data about the data.</quote> Almost every
       file system has its own structure of metadata, which is on reason
       that the file systems show different performance characteristics. It
       is extremely important to maintain metadata intact, because otherwise
       all data on the file system could become inaccessible.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="bgchzas">
     <term>inode</term>
     <listitem>
      <para>
       A data structure on a file system that contains various information
       about a file, including size, number of links, pointers to the disk
       blocks where the file contents are actually stored, and date and time
       of creation, modification, and access.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="bgchzat">
     <term>journal</term>
     <listitem>
      <para>
       In the context of a file system, a journal is an on-disk structure
       containing a type of log in which the file system stores what it is
       about to change in the file system’s metadata. Journaling greatly
       reduces the recovery time of a file system because it has no need for
       the lengthy search process that checks the entire file system at
       system startup. Instead, only the journal is replayed.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect1>
  <sect1 id="sec_filesystems_major">
   <title>Major File Systems in Linux</title>

   <para>
    &productname; offers a variety of file systems from which
    to choose. This section contains an overview of how these file systems
    work and which advantages they offer.
   </para>

   <para>
    It is very important to remember that no file system best suits all
    kinds of applications. Each file system has its particular strengths and
    weaknesses, which must be taken into account. In addition, even the most
    sophisticated file system cannot replace a reasonable backup strategy.
   </para>

   <para>
    The terms <emphasis>data integrity</emphasis> and <emphasis>data
    consistency</emphasis>, when used in this section, do not refer to the
    consistency of the user space data (the data your application writes to
    its files). Whether this data is consistent must be controlled by the
    application itself.
   </para>

   <important>
    <para>
     Unless stated otherwise in this section, all the steps required to set
     up or change partitions and file systems can be performed by using
     &yast;.
    </para>
   </important>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="bwk8gda" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_filesystems_major_ext2" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_filesystems_major_ext3" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_filesystems_major_reiser" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_filesystems_major_xfs" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="bwkryet" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="bwk8gda">
    <title>Btrfs</title>
    <para>
     Btrfs is a copy-on-write (COW) file system developed by Chris Mason. It
     is based on COW-friendly B-trees developed by Ohad Rodeh. Btrfs is a
     logging-style file system. Instead of journaling the block changes, it
     writes them in a new location, then links the change in. Until the last
     write, the new changes are not committed.
    </para>
    <important>
     <para>
      Because Btrfs is capable of storing snapshots of the file system, it
      is advisable to reserve twice the amount of disk space than the
      standard storage proposal. This is done automatically by the &yast;
      Partitioner in the Btrfs storage proposal for the root file system.
     </para>
    </important>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="b15tkq5m" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkq5n" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5j" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5k" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5l" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5m" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5n" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5o" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b15tkr5p" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="b15tkq5m">
     <title>Key Features</title>
     <para>
      Btrfs provides fault tolerance, repair, and easy management features,
      such as the following:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Writable snapshots that allow you to easily roll back your system if
        needed after applying updates, or to back up files.
       </para>
      </listitem>
      <listitem>
       <para>
        Multiple device support that allows you to grow or shrink the file
        system. The feature is planned to be available in a future release
        of the &yast; Partitioner.
       </para>
       <para>
        Use Btrfs commands to set up transparent compression. Compression
        and Encryption functionality for Btrfs is currently under
        development and is currently not supported on &productname;.
       </para>
      </listitem>
      <listitem>
       <para>
        Different RAID levels for metadata and user data.
       </para>
      </listitem>
      <listitem>
       <para>
        Different checksums for metadata and user data to improve error
        detection.
       </para>
      </listitem>
      <listitem>
       <para>
        Integration with Linux Logical Volume Manager (LVM) storage objects.
       </para>
      </listitem>
      <listitem>
       <para>
        Integration with the &yast; Partitioner and &ay; on SUSE Linux.
       </para>
      </listitem>
      <listitem>
       <para>
        Offline migration from existing Ext2, Ext3, and Ext4 file systems.
       </para>
      </listitem>
     </itemizedlist>
    </sect3>
    <sect3 id="b15tkq5n">
     <title>Bootloader Support</title>
     <para>
      Bootloader support for <filename>/boot</filename> on Btrfs is planned
      to be available beginning in &sle; 12.
     </para>
    </sect3>
    <sect3 id="b15tkr5j">
     <title>Btrfs Subvolumes</title>
     <para>
      Btrfs creates a default subvolume in its assigned pool of space. It
      allows you to create additional subvolumes that act as individual file
      systems within the same pool of space. The number of subvolumes is
      limited only by the space allocated to the pool.
     </para>
     <para>
      If Btrfs is used for the root (<filename>/</filename>) file system,
      the &yast; Partitioner automatically prepares the Btrfs file system for
      use with Btrfs subvolumes. You can cover any subdirectory as a
      subvolume. For example,
      <xref linkend="b11qhfrl" xrefstyle="TableTitleOnPage"/> identifies the
      subdirectories that we recommend you treat as subvolumes because they
      contain files that you should not snapshot for the reasons given:
     </para>
     <table id="b11qhfrl" frame="topbot" rowsep="1" pgwide="0" >
      <title>Default Subvolume Handling for Btrfs in &yast;</title>
      <tgroup cols="2">
       <colspec colnum="1" colname="1" colwidth="2381*"/>
       <colspec colnum="2" colname="2" colwidth="7620*"/>
       <thead>
        <row id="bwk8pe0">
         <entry>
          <para>
           Path
          </para>
         </entry>
         <entry>
          <para>
           Reason to Cover as a Subvolume
          </para>
         </entry>
        </row>
       </thead>
       <tbody>
        <row id="b11qhf62">
         <entry>
          <para>
           <filename>/opt</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains third-party add-on application software packages.
          </para>
         </entry>
        </row>
        <row id="b11qhf63">
         <entry>
          <para>
           <filename>/srv</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains <filename>http</filename> and <filename>ftp</filename>
           files.
          </para>
         </entry>
        </row>
        <row id="b11qhf64">
         <entry>
          <para>
           <filename>/tmp</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains temporary files.
          </para>
         </entry>
        </row>
        <row id="b11qhf65">
         <entry>
          <para>
           <filename>/var/crash</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains memory dumps of crashed kernels.
          </para>
         </entry>
        </row>
        <row id="b11qhf66">
         <entry>
          <para>
           <filename>/var/log</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains system and applications’ log files, which should never
           be rolled back.
          </para>
         </entry>
        </row>
        <row id="b11qhf67">
         <entry>
          <para>
           <filename>/var/run</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains run-time variable data.
          </para>
         </entry>
        </row>
        <row id="b11qhf68">
         <entry>
          <para>
           <filename>/var/spool</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains data that is awaiting processing by a program, user, or
           administrator, such as news, mail, and printer queues.
          </para>
         </entry>
        </row>
        <row id="b11qhf69">
         <entry>
          <para>
           <filename>/var/tmp</filename>
          </para>
         </entry>
         <entry>
          <para>
           Contains temporary files or directories that are preserved
           between system reboots.
          </para>
         </entry>
        </row>
       </tbody>
      </tgroup>
     </table>
     <para>
      After the installation, you can add or remove Btrfs subvolumes by
      using the &yast; Expert Partitioner. For information, see
      <xref
      linkend="yast2.btrfs.yast"/>.
     </para>
    </sect3>
    <sect3 id="b15tkr5k">
     <title>Snapshots for the Root File System</title>
     <para>
      Btrfs provides writable snapshots with the SUSE Snapper infrastructure
      that allow you to easily roll back your system if needed after
      applying updates, or to back up files. Snapper allows you to create
      and delete snapshots, and to compare snapshots and revert the
      differences between them. If Btrfs is used for the root
      (<filename>/</filename>) file system, &yast; automatically enables
      snapshots for the root file system.
     </para>
     <para>
      For information about Snapper and its integration in ZYpp
      (<filename>snapper-zypp-plugin</filename>) and &yast;
      (<filename>yast2-snapper</filename>), see
      <xref linkend="cha.snapper"/>.
     </para>
     <para>
      To prevent snapshots from filling up the system disk, you can change
      the Snapper cleanup defaults to be more aggressive in the
      <filename>/etc/snapper/configs/root</filename> configuration file, or
      for other mount points. Snapper provides three algorithms to clean up
      old snapshots that are executed in a daily cron-job. The cleanup
      frequency is defined in the Snapper configuration for the mount point.
      Lower the TIMELINE_LIMIT parameters for daily, monthly, and yearly to
      reduce how long and the number of snapshots to be retained. For
      information, see <xref linkend="sec.snapper.config"/>.
     </para>
     <para>
      For information about the SUSE Snapper project, see the
      <ulink url="http://en.opensuse.org/Portal:Snapper">Snapper Portal wiki
      at OpenSUSE.org</ulink>.
     </para>
    </sect3>
    <sect3 id="b15tkr5l">
     <title>Online Check and Repair Functionality</title>
     <para>
      The <command>scrub</command> check and repair functionality is
      available as part of the Btrfs command line tools. It verifies the
      integrity of data and metadata, assuming the tree structures is fine.
      You can run <command>scrub</command> periodically on a mounted file
      system; it runs as a background process during normal operation.
     </para>
    </sect3>
    <sect3 id="b15tkr5m">
     <title>RAID and Multipath Support</title>
     <para>
      You can create Btrfs on Multiple Devices (MD) and Device Mapper (DM)
      storage configurations by using the &yast; Partitioner.
     </para>
    </sect3>
    <sect3 id="b15tkr5n">
     <title>Migration from Ext File Systems to Btrfs</title>
     <para>
      You can migrate data volumes from existing Ext file systems (Ext2,
      Ext3, or Ext4) to the Btrfs file system. The conversion process occurs
      offline and in place on the device. The file system needs least 15% of
      available free space on the device.
     </para>
     <para>
      To convert the Ext file system to Btrfs, take the file system offline,
      then enter:
     </para>
<screen>
btrfs-convert &lt;<replaceable>device</replaceable>&gt;
</screen>
     <para>
      To roll back the migration to the original Ext file system, take the
      file system offline, then enter:
     </para>
<screen>
btrfs-convert -r &lt;<replaceable>device</replaceable>&gt;
</screen>
     <important>
      <para>
       When rolling back to the original Ext file system, all data will be
       lost that you added after the conversion to Btrfs. That is, only the
       original data is converted back to the Ext file system.
      </para>
     </important>
    </sect3>
    <sect3 id="b15tkr5o">
     <title>Btrfs Administration</title>
     <para>
      Btrfs is integrated in the &yast; Partitioner and &ay;. It is
      available during the installation to allow you to set up a solution
      for the root file system. You can use the &yast; Partitioner after the
      install to view and manage Btrfs volumes.
     </para>
     <para>
      Btrfs administration tools are provided in the
      <filename>btrfsprogs</filename> package. For information about using
      Btrfs commands, see the <command>btrfs(8)</command>,
      <command>btrfsck(8)</command>, <command>mkfs.btrfs(8)</command>, and
      <command>btrfsctl(8)</command> man pages. For information about Btrfs
      features, see the
      <ulink url="http://btrfs.wiki.kernel.org"><citetitle>Btrfs
      wiki</citetitle></ulink>.
     </para>
     <para>
      The <command>fsck.btrfs(8)</command> tool will soon be available in
      the &sle; update repositories.
     </para>
    </sect3>
    <sect3 id="b15tkr5p">
     <title>Btrfs Quota Support for Subvolumes</title>
     <para>
      The Btrfs &rootuser; file system subvolumes
      <filename>/var/log</filename>, <filename>/var/crash</filename> and
      <filename>/var/cache</filename> can use all of the available disk
      space during normal operation, and cause a system malfunction. To help
      avoid this situation, &sle; now offers Btrfs quota
      support for subvolumes. See the <filename>btrfs(8)</filename> manual
      page for more details.
     </para>
    </sect3>
   </sect2>

   <sect2 id="sec_filesystems_major_ext2">
    <title>Ext2</title>
    <para>
     The origins of Ext2 go back to the early days of Linux history. Its
     predecessor, the Extended File System, was implemented in April 1992
     and integrated in Linux 0.96c. The Extended File System underwent a
     number of modifications and, as Ext2, became the most popular Linux
     file system for years. With the creation of journaling file systems and
     their short recovery times, Ext2 became less important.
    </para>
    <para>
     A brief summary of Ext2’s strengths might help understand why it
     was&mdash;and in some areas still is&mdash;the favorite Linux file
     system of many Linux users.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bi6ymkx" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzb1" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bi6ymkx">
     <title>Solidity and Speed</title>
     <para>
      Being quite an <quote>old-timer,</quote> Ext2 underwent many
      improvements and was heavily tested. This might be the reason why
      people often refer to it as rock-solid. After a system outage when the
      file system could not be cleanly unmounted, e2fsck starts to analyze
      the file system data. Metadata is brought into a consistent state and
      pending files or data blocks are written to a designated directory
      (called <filename>lost+found</filename>). In contrast to journaling
      file systems, e2fsck analyzes the entire file system and not only the
      recently modified bits of metadata. This takes significantly longer
      than checking the log data of a journaling file system. Depending on
      file system size, this procedure can take half an hour or more.
      Therefore, it is not desirable to choose Ext2 for any server that
      needs high availability. However, because Ext2 does not maintain a
      journal and uses significantly less memory, it is sometimes faster
      than other file systems.
     </para>
    </sect3>
    <sect3 id="bgchzb1">
     <title>Easy Upgradability</title>
     <para>
      Because Ext3 is based on the Ext2 code and shares its on-disk format
      as well as its metadata format, upgrades from Ext2 to Ext3 are very
      easy.
     </para>
    </sect3>
   </sect2>

   <sect2 id="sec_filesystems_major_ext3">
    <title>Ext3</title>
    <para>
     Ext3 was designed by Stephen Tweedie. Unlike all other next-generation
     file systems, Ext3 does not follow a completely new design principle.
     It is based on Ext2. These two file systems are very closely related to
     each other. An Ext3 file system can be easily built on top of an Ext2
     file system. The most important difference between Ext2 and Ext3 is
     that Ext3 supports journaling. In summary, Ext3 has three major
     advantages to offer:
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bgchzb3" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzb4" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="sec.filesystems.major.ext22ext3" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="sec_fileystems_major_ext3inodesize" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bgchzb3">
     <title>Easy and Highly Reliable Upgrades from Ext2</title>
     <para>
      The code for Ext2 is the strong foundation on which Ext3 could become
      a highly-acclaimed next-generation file system. Its reliability and
      solidity are elegantly combined in Ext3 with the advantages of a
      journaling file system. Unlike transitions to other journaling file
      systems, such as ReiserFS or XFS, which can be quite tedious (making
      backups of the entire file system and recreating it from scratch), a
      transition to Ext3 is a matter of minutes. It is also very safe,
      because re-creating an entire file system from scratch might not work
      flawlessly. Considering the number of existing Ext2 systems that await
      an upgrade to a journaling file system, you can easily see why Ext3
      might be of some importance to many system administrators. Downgrading
      from Ext3 to Ext2 is as easy as the upgrade. Perform a clean unmount
      of the Ext3 file system and remount it as an Ext2 file system.
     </para>
    </sect3>
    <sect3 id="bgchzb4">
     <title>Reliability and Performance</title>
     <para>
      Some other journaling file systems follow the
      <quote>metadata-only</quote> journaling approach. This means your
      metadata is always kept in a consistent state, but this cannot be
      automatically guaranteed for the file system data itself. Ext3 is
      designed to take care of both metadata and data. The degree of
      <quote>care</quote> can be customized. Enabling Ext3 in the
      <option>data=journal</option> mode offers maximum security (data
      integrity), but can slow down the system because both metadata and
      data are journaled. A relatively new approach is to use the
      <option>data=ordered</option> mode, which ensures both data and
      metadata integrity, but uses journaling only for metadata. The file
      system driver collects all data blocks that correspond to one metadata
      update. These data blocks are written to disk before the metadata is
      updated. As a result, consistency is achieved for metadata and data
      without sacrificing performance. A third option to use is
      <option>data=writeback</option>, which allows data to be written into
      the main file system after its metadata has been committed to the
      journal. This option is often considered the best in performance. It
      can, however, allow old data to reappear in files after crash and
      recovery while internal file system integrity is maintained. Ext3 uses
      the <option>data=ordered</option> option as the default.
     </para>
    </sect3>
    <sect3 id="sec.filesystems.major.ext22ext3">
     <title>Converting an Ext2 File System into Ext3</title>
     <para>
      To convert an Ext2 file system to Ext3:
     </para>
     <procedure id="bgchzb5">
      <step id="bgchzb6">
       <para>
        Create an Ext3 journal by running <command>tune2fs -j</command> as
        the &rootuser; user.
       </para>
       <para>
        This creates an Ext3 journal with the default parameters.
       </para>
       <para>
        To specify how large the journal should be and on which device it
        should reside, run <command>tune2fs <option>-J</option></command>
        instead together with the desired journal options
        <option>size=</option> and <option>device=</option>. More
        information about the <command>tune2fs</command> program is
        available in the <command>tune2fs</command> man page.
       </para>
      </step>
      <step id="bgchzb7">
       <para>
        Edit the file <filename>/etc/fstab</filename> as the
        &rootuser; user to change the file system type
        specified for the corresponding partition from
        <literal>ext2</literal> to <literal>ext3</literal>, then save the
        changes.
       </para>
       <para>
        This ensures that the Ext3 file system is recognized as such. The
        change takes effect after the next reboot.
       </para>
      </step>
      <step id="bgchzb8">
       <para>
        To boot a root file system that is set up as an Ext3 partition,
        include the modules <literal>ext3</literal> and
        <literal>jbd</literal> in the <filename>initrd</filename>.
       </para>
       <substeps>
        <step id="bi6z1gc">
         <para>
          Edit <filename>/etc/dracut.conf.d/01-dist.conf</filename>, the
          following line:
         </para>
<screen>force_drivers+="ext3 jbd"</screen>
        </step>
        <step id="bi6z1rw">
         <para>
          Run the <command>dracut <option>-f</option></command> command.
         </para>
         <para>
          This builds a new initrd and prepares it for use.
         </para>
        </step>
       </substeps>
      </step>
      <step id="bi6z29v">
       <para>
        Reboot the system.
       </para>
      </step>
     </procedure>
    </sect3>
<!--  <sect2 id="sec.filesystems.major.reiser4">   <title>Reiser4</title>   <indexterm class="startofrange" id="idx.file_systems_Reiser4">    <primary>file systems</primary>    <secondary>Reiser4</secondary>   </indexterm>   <para>    Right after kernel 2.6 had been released, the family of journaling file    systems was joined by another member: Reiser4. Reiser4 is fundamentally    different from its predecessor ReiserFS (version 3.6). It introduces the    concept of plug-ins to tweak the file system functionality and a finer    grained security concept.     </para>   <variablelist>   <varlistentry>    <term>Fine Grained Security Concept</term>    <listitem>     <para>      In designing Reiser4, its developers put an emphasis on the      implementation of security-relevant features. Reiser4 therefore comes      with a set of dedicated security plug-ins. The most important one      introduces the concept of file <quote>items.</quote> Currently, file      access controls are defined per file. If there is a large file      containing information relevant to several users, groups, or applications,      the access rights had be fairly imprecise to include all parties      involved. In Reiser4, you can split those files into smaller portions      (the <quote>items</quote>). Access rights can then be set for      each item and each user separately, allowing a much more precise file      security management. A perfect example would be      <filename>/etc/passwd</filename>. To date, only <systemitem       class="username">root</systemitem> can read and edit the file while      non-<systemitem class="username">root</systemitem> users only get read      access to this file. Using the item concept of Reiser4, you could split      this file in a set of items (one item per user) and allow users or      applications to modify their own data but not      access other users' data. This concept adds both to security and       flexibility.     </para>    </listitem>   </varlistentry>    <varlistentry>     <term>Extensibility through Plug-Ins</term>     <listitem>      <para>       Many file system functions and external functions normally used by a       file system are implemented as plug-ins in Reiser4. These plug-ins can       easily be added to the base system. You no longer need to recompile the       kernel or reformat the hard disk to add new functions to your       file system.      </para>     </listitem>    </varlistentry>   <varlistentry>    <term>Better File System Layout through Delayed Allocation</term>    <listitem>     <para>      Like XFS, Reiser4 supports delayed allocation. See <xref       linkend="sec.filesystems.major.xfs"/>. Using delayed allocation even      for metadata can result in better overall layout.     </para>    </listitem>   </varlistentry>  </variablelist>   <indexterm class="endofrange" startref="idx.file_systems_Reiser4"/>  </sect2>-->
    <sect3 id="sec_fileystems_major_ext3inodesize">
     <title>Ext3 File System Inode Size and Number of Inodes</title>
     <para>
      An inode stores information about the file and its block location in
      the file system. To allow space in the inode for extended attributes
      and ACLs, the default inode size for Ext3 was increased from 128 bytes
      on SLES 10 to 256 bytes on SLES 11. As compared to SLES 10, when you
      make a new Ext3 file system on SLES 11, the default amount of space
      pre-allocated for the same number of inodes is doubled, and the usable
      space for files in the file system is reduced by that amount. Thus,
      you must use larger partitions to accommodate the same number of
      inodes and files than were possible for an Ext3 file system on SLES
      10.
     </para>
     <para>
      When you create a new Ext3 file system, the space in the inode table
      is pre-allocated for the total number of inodes that can be created.
      The bytes-per-inode ratio and the size of the file system determine
      how many inodes are possible. When the file system is made, an inode
      is created for every bytes-per-inode bytes of space:
     </para>
<screen>
number of inodes = total size of the file system divided by the number of bytes per inode
</screen>
     <para>
      The number of inodes controls the number of files you can have in the
      file system: one inode for each file. To address the increased inode
      size and reduced usable space available, the default for the
      bytes-per-inode ratio was increased from 8192 bytes on SLES 10 to
      16384 bytes on SLES 11. The doubled ratio means that the number of
      files that can be created is one-half of the number of files possible
      for an Ext3 file system on SLES 10.
     </para>
     <important>
      <para>
       After the inodes are allocated, you cannot change the settings for
       the inode size or bytes-per-inode ratio. No new inodes are possible
       without recreating the file system with different settings, or unless
       the file system gets extended. When you exceed the maximum number of
       inodes, no new files can be created on the file system until some
       files are deleted.
      </para>
     </important>
     <para>
      When you make a new Ext3 file system, you can specify the inode size
      and bytes-per-inode ratio to control inode space usage and the number
      of files possible on the file system. If the blocks size, inode size,
      and bytes-per-inode ratio values are not specified, the default values
      in the <filename>/etc/mked2fs.conf</filename> file are applied. For
      information, see the <filename>mke2fs.conf(5)</filename> man page.
     </para>
     <para>
      Use the following guidelines:
     </para>
     <itemizedlist>
      <listitem>
       <formalpara id="b14jepus">
        <title>Inode size:</title>
        <para>
         The default inode size is 256 bytes. Specify a value in bytes that
         is a power of 2 and equal to 128 or larger in bytes and up to the
         block size, such as 128, 256, 512, and so on. Use 128 bytes only if
         you do not use extended attributes or ACLs on your Ext3 file
         systems.
        </para>
       </formalpara>
      </listitem>
      <listitem>
       <formalpara id="b14jeput">
        <title>Bytes-per-inode ratio:</title>
        <para>
         The default bytes-per-inode ratio is 16384 bytes. Valid
         bytes-per-inode ratio values must be a power of 2 equal to 1024 or
         greater in bytes, such as 1024, 2048, 4096, 8192, 16384, 32768, and
         so on. This value should not be smaller than the block size of the
         file system, because the block size is the smallest chunk of space
         used to store data. The default block size for the Ext3 file system
         is 4 KB.
        </para>
       </formalpara>
       <para>
        In addition, you should consider the number of files and the size of
        files you need to store. For example, if your file system will have
        many small files, you can specify a smaller bytes-per-inode ratio,
        which increases the number of inodes. If your file system will have
        a very large files, you can specify a larger bytes-per-inode ratio,
        which reduces the number of possible inodes.
       </para>
       <para>
        Generally, it is better to have too many inodes than to run out of
        them. If you have too few inodes and very small files, you could
        reach the maximum number of files on a disk that is practically
        empty. If you have too many inodes and very large files, you might
        have free space reported but be unable to use it because you cannot
        create new files in space reserved for inodes.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      If you do not use extended attributes or ACLs on your Ext3 file
      systems, you can restore the SLES 10 behavior specifying 128 bytes as
      the inode size and 8192 bytes as the bytes-per-inode ratio when you
      make the file system. Use any of the following methods to set the
      inode size and bytes-per-inode ratio:
     </para>
     <itemizedlist>
      <listitem>
       <formalpara id="b14irnj5">
        <title>Modifying the default settings for all new Ext3 files:</title>
        <para>
         In a text editor, modify the <literal>defaults</literal> section of
         the <filename>/etc/mke2fs.conf</filename> file to set the
         <literal>inode_size</literal> and <literal>inode_ratio</literal> to
         the desired default values. The values apply to all new Ext3 file
         systems. For example:
        </para>
       </formalpara>
<screen>
blocksize = 4096
inode_size = 128
inode_ratio = 8192
</screen>
      </listitem>
      <listitem>
       <formalpara id="b14irnj6">
        <title>At the command line:</title>
        <para>
         Pass the inode size (<literal>-I 128</literal>) and the
         bytes-per-inode ratio (<literal>-i 8192</literal>) to the
         <command>mkfs.ext3(8)</command> command or the
         <command>mke2fs(8)</command> command when you create a new Ext3
         file system. For example, use either of the following commands:
        </para>
       </formalpara>
<screen>
mkfs.ext3 -b 4096 -i 8092 -I 128 /dev/sda2

mke2fs -t ext3 -b 4096 -i 8192 -I 128 /dev/sda2
</screen>
      </listitem>
      <listitem>
       <formalpara id="b12dp9td">
        <title>During installation with &yast;:</title>
        <para>
         Pass the inode size and bytes-per-inode ratio values when you
         create a new Ext3 file system during the installation. In the &yast;
         Partitioner on the <guimenu>Edit Partition</guimenu> page
         under<guimenu> Formatting Options</guimenu>, select <guimenu>Format
         partition</guimenu><guimenu>Ext3</guimenu>, then click
         <guimenu>Options</guimenu>. In the <guimenu>File system
         options</guimenu> dialog box, select the desired values from the
         <guimenu>Block Size in Bytes</guimenu>,
         <guimenu>Bytes-per-inode</guimenu>, and <guimenu>Inode
         Size</guimenu> drop-down lists.
        </para>
       </formalpara>
       <para>
        For example, select 4096 for the <guimenu>Block Size in
        Bytes</guimenu> drop-down list, select 8192 from the <guimenu>Bytes
        per inode</guimenu> drop-down list, select 128 from the
        <guimenu>Inode Size</guimenu> drop-down list, then click
        <guimenu>OK</guimenu>.
       </para>
       <informalfigure pgwide="0">
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="ext3_inode_yast_a.png" width="305pt" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="ext3_inode_yast_a.png" width="305pt" format="PNG"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
      </listitem>
      <listitem>
       <formalpara id="b12dpiab">
        <title>During installation with autoyast:</title>
        <para>
         In an autoyast profile, you can use the <literal>fs_options
         </literal>tag to set the <literal>opt_bytes_per_inode</literal>
         ratio value of 8192 for -i and the
         <literal>opt_inode_density</literal> value of 128 for -I:
        </para>
       </formalpara>
<screen>
&lt;partitioning config:type="list"&gt;
    &lt;drive&gt;
      &lt;device&gt;/dev/sda&lt;/device&gt;
      &lt;initialize config:type="boolean"&gt;true&lt;/initialize&gt;
      &lt;partitions config:type="list"&gt;
        &lt;partition&gt;
          &lt;filesystem config:type="symbol"&gt;ext3&lt;/filesystem&gt;
          &lt;format config:type="boolean"&gt;true&lt;/format&gt;
          &lt;fs_options&gt;
            &lt;opt_bytes_per_inode&gt;
              &lt;option_str&gt;-i&lt;/option_str&gt;
              &lt;option_value&gt;8192&lt;/option_value&gt;
            &lt;/opt_bytes_per_inode&gt;
            &lt;opt_inode_density&gt;
              &lt;option_str&gt;-I&lt;/option_str&gt;
              &lt;option_value&gt;128&lt;/option_value&gt;
            &lt;/opt_inode_density&gt;
          &lt;/fs_options&gt;
          &lt;mount&gt;/&lt;/mount&gt;
          &lt;partition_id config:type="integer"&gt;131&lt;/partition_id&gt;
          &lt;partition_type&gt;primary&lt;/partition_type&gt;
          &lt;size&gt;25G&lt;/size&gt;
        &lt;/partition&gt;
</screen>
      </listitem>
     </itemizedlist>
     <para>
      For information,
      see<ulink url="http://www.novell.com/support/kb/doc.php?id=7009075"><citetitle>
      SLES11 ext3 partitions can only store 50% of the files that can be
      stored on SLES10</citetitle> [Technical Information Document
      7009075]</ulink>.
     </para>
    </sect3>
   </sect2>

   <sect2 id="sec_filesystems_major_reiser">
    <title>ReiserFS</title>
    <para>
     Officially one of the key features of the 2.4 kernel release, ReiserFS
     has been available as a kernel patch for 2.2.x SUSE kernels since
     version 6.4. ReiserFS was designed by Hans Reiser and the Namesys
     development team. It has proven itself to be a powerful alternative to
     Ext2. Its key assets are better disk space utilization, better disk
     access performance, faster crash recovery, and reliability through data
     journaling.
    </para>
    <important>
     <para>
      Existing ReiserFS partitions are supported for the lifetime of SUSE
      Linux Enterprise Server 12 specifically for migration purposes.
      Support for creating new ReiserFS file systems has been removed
      starting with &productname; 12.
     </para>
    </important>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bgchzav" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzaw" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzax" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzay" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bgchzav">
     <title>Better Disk Space Utilization</title>
     <para>
      In ReiserFS, all data is organized in a structure called a B*-balanced
      tree. The tree structure contributes to better disk space utilization
      because small files can be stored directly in the B* tree leaf nodes
      instead of being stored elsewhere and maintaining a pointer to the
      actual disk location. In addition to that, storage is not allocated in
      chunks of 1 or 4 KB, but in portions of the exact size needed. Another
      benefit lies in the dynamic allocation of inodes. This keeps the file
      system more flexible than traditional file systems, like Ext2, where
      the inode density must be specified at file system creation time.
     </para>
    </sect3>
    <sect3 id="bgchzaw">
     <title>Better Disk Access Performance</title>
     <para>
      For small files, file data and <quote>stat_data</quote> (inode)
      information are often stored next to each other. They can be read with
      a single disk I/O operation, meaning that only one access to disk is
      required to retrieve all the information needed.
     </para>
    </sect3>
    <sect3 id="bgchzax">
     <title>Fast Crash Recovery</title>
     <para>
      Using a journal to keep track of recent metadata changes makes a file
      system check a matter of seconds, even for huge file systems.
     </para>
    </sect3>
    <sect3 id="bgchzay">
     <title>Reliability through Data Journaling</title>
     <para>
      ReiserFS also supports data journaling and ordered data modes similar
      to the concepts outlined in
      <xref linkend="sec_filesystems_major_ext3" xrefstyle="HeadingOnPage"/>.
      The default mode is <literal>data=ordered</literal>, which ensures
      both data and metadata integrity, but uses journaling only for
      metadata.
     </para>
    </sect3>
   </sect2>

   <sect2 id="sec_filesystems_major_xfs">
    <title>XFS</title>
    <para>
     Originally intended as the file system for their IRIX OS, SGI started
     XFS development in the early 1990s. The idea behind XFS was to create a
     high-performance 64-bit journaling file system to meet extreme
     computing challenges. XFS is very good at manipulating large files and
     performs well on high-end hardware. However, even XFS has a drawback.
     Like ReiserFS, XFS takes great care of metadata integrity, but less
     care of data integrity.
    </para>
    <para>
     A quick review of XFS’s key features explains why it might prove to
     be a strong competitor for other journaling file systems in high-end
     computing.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="bgchzba" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzbb" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="bgchzbc" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="bgchzba">
     <title>High Scalability through the Use of Allocation Groups</title>
     <para>
      At the creation time of an XFS file system, the block device
      underlying the file system is divided into eight or more linear
      regions of equal size. Those are referred to as <emphasis>allocation
      groups</emphasis>. Each allocation group manages its own inodes and
      free disk space. Practically, allocation groups can be seen as file
      systems in a file system. Because allocation groups are rather
      independent of each other, more than one of them can be addressed by
      the kernel simultaneously. This feature is the key to XFS’s great
      scalability. Naturally, the concept of independent allocation groups
      suits the needs of multiprocessor systems.
     </para>
    </sect3>
    <sect3 id="bgchzbb">
     <title>High Performance through Efficient Management of Disk Space</title>
     <para>
      Free space and inodes are handled by B<superscript>+</superscript>
      trees inside the allocation groups. The use of
      B<superscript>+</superscript> trees greatly contributes to XFS’s
      performance and scalability. XFS uses <emphasis>delayed
      allocation</emphasis>, which handles allocation by breaking the
      process into two pieces. A pending transaction is stored in RAM and
      the appropriate amount of space is reserved. XFS still does not decide
      where exactly (in file system blocks) the data should be stored. This
      decision is delayed until the last possible moment. Some short-lived
      temporary data might never make its way to disk, because it is
      obsolete by the time XFS decides where actually to save it. In this
      way, XFS increases write performance and reduces file system
      fragmentation. Because delayed allocation results in less frequent
      write events than in other file systems, it is likely that data loss
      after a crash during a write is more severe.
     </para>
    </sect3>
    <sect3 id="bgchzbc">
     <title>Preallocation to Avoid File System Fragmentation</title>
     <para>
      Before writing the data to the file system, XFS
      <emphasis>reserves</emphasis> (preallocates) the free space needed for
      a file. Thus, file system fragmentation is greatly reduced.
      Performance is increased because the contents of a file are not
      distributed all over the file system.
     </para>
    </sect3>
   </sect2>

   <sect2 id="bwkryet">
    <title>File System Feature Comparison</title>
    <para>
     For a side-by-side feature comparison of the major operating systems in
     &productname;,
     see<ulink url="http://www.suse.com/products/server/technical-information/#FileSystem">
     File System Support and Sizes</ulink> on the
     <ulink url="http://www.suse.com/products/server/technical-information/">SUSE
     Linux Enterprise Server Technical Information Web site</ulink>.
    </para>
   </sect2>
  </sect1>
  <sect1 id="sec_filesystems_add">
   <title>Other Supported File Systems</title>

   <para role="intro">
    <xref linkend="tab.dateisysteme" xrefstyle="TableXRef"/> summarizes some
    other file systems supported by Linux. They are supported mainly to
    ensure compatibility and interchange of data with different kinds of
    media or foreign operating systems.
   </para>

   <table id="tab.dateisysteme" frame="topbot" rowsep="1" pgwide="0">
    <title>File System Types in Linux</title>
    <tgroup cols="2">
     <colspec colnum="1" colname="1" colwidth="2381*"/>
     <colspec colnum="2" colname="2" colwidth="7620*"/>
     <thead>
      <row id="bi6z95b">
       <entry>
        <para>
         File System Type
        </para>
       </entry>
       <entry>
        <para>
         Description
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row id="bwk81ba">
       <entry>
        <para>
         <systemitem>cramfs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Compressed ROM file system: A compressed read-only file system for
         ROMs.
        </para>
       </entry>
      </row>
      <row id="bwk81bb">
       <entry>
        <para>
         <systemitem>hpfs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         High Performance File System: The IBM OS/2 standard file system.
         Only supported in read-only mode.
        </para>
       </entry>
      </row>
      <row id="bwk81bc">
       <entry>
        <para>
         <systemitem>iso9660</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Standard file system on CD-ROMs.
        </para>
       </entry>
      </row>
      <row id="bwk81bd">
       <entry>
        <para>
         <systemitem>minix</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         This file system originated from academic projects on operating
         systems and was the first file system used in Linux. Today, it is
         used as a file system for floppy disks.
        </para>
       </entry>
      </row>
      <row id="bwk81be">
       <entry>
        <para>
         <systemitem>msdos</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         <filename>fat</filename>, the file system originally used by DOS,
         is today used by various operating systems.
        </para>
       </entry>
      </row>
      <row id="bwk81bf">
       <entry>
        <para>
         <systemitem>ncpfs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         File system for mounting Novell volumes over networks.
        </para>
       </entry>
      </row>
      <row id="bwk81bg">
       <entry>
        <para>
         <systemitem>nfs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Network File System: Here, data can be stored on any machine in a
         network and access might be granted via a network.
        </para>
       </entry>
      </row>
      <row id="bwk81bh">
       <entry>
        <para>
         <systemitem>ntfs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Windows&nbsp;NT file system; read-only.
        </para>
       </entry>
      </row>
      <row id="bwk81bi">
       <entry>
        <para>
         <systemitem>smbfs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Server Message Block is used by products such as Windows to enable
         file access over a network.
        </para>
       </entry>
      </row>
      <row id="bwk81bj">
       <entry>
        <para>
         <systemitem>sysv</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Used on SCO UNIX, Xenix, and Coherent (commercial UNIX systems for
         PCs).
        </para>
       </entry>
      </row>
      <row id="bwk81bk">
       <entry>
        <para>
         <systemitem>ufs</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Used by BSD, SunOS, and NextStep. Only supported in read-only mode.
        </para>
       </entry>
      </row>
      <row id="bwk81bl">
       <entry>
        <para>
         <systemitem>umsdos</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         UNIX on MS-DOS: Applied on top of a standard
         <filename>fat</filename> file system, achieves UNIX functionality
         (permissions, links, long filenames) by creating special files.
        </para>
       </entry>
      </row>
      <row id="bwk81bm">
       <entry>
        <para>
         <systemitem>vfat</systemitem>
        </para>
       </entry>
       <entry>
        <para>
         Virtual FAT: Extension of the <literal>fat</literal> file system
         (supports long filenames).
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </sect1>
  <sect1 id="sec_filesystems_lfs">
   <title>Large File Support in Linux</title>

   <para>
    Originally, Linux supported a maximum file size of 2&nbsp;GiB
    (2<superscript>31</superscript> bytes). Unless a file system comes with
    large file support, the maximum file size on a 32-bit system is 2 GiB.
   </para>

   <para>
    Currently, all of our standard file systems have LFS (large file
    support), which gives a maximum file size of
    2<superscript>63</superscript> bytes in theory.
    <xref linkend="tab.maxsize" xrefstyle="TableXRef"/> offers an overview
    of the current on-disk format limitations of Linux files and file
    systems. The numbers in the table assume that the file systems are using
    4 KiB block size, which is a common standard. When using different block
    sizes, the results are different. The maximum file sizes in
    <xref linkend="tab.maxsize" xrefstyle="TableXRef"/> can be larger than
    the file system's actual size when using sparse blocks.
   </para>

   <note>
    <para>
     In this document: 1024 Bytes = 1 KiB; 1024 KiB = 1 MiB; 1024 MiB = 1
     GiB; 1024 GiB = 1 TiB; 1024 TiB = 1 PiB; 1024 PiB = 1 EiB (see also
     <ulink url="http://physics.nist.gov/cuu/Units/binary.html"><citetitle>NIST:
     Prefixes for Binary Multiples</citetitle></ulink>.
    </para>
   </note>

   <table id="tab.maxsize" frame="topbot" rowsep="1" pgwide="0">
    <title>Maximum Sizes of Files and File Systems (On-Disk Format, 4 KiB Block Size)</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="3334*"/>
     <colspec colnum="2" colname="2" colwidth="3334*"/>
     <colspec colnum="3" colname="3" colwidth="3334*"/>
     <thead>
      <row id="b12r1f2q">
       <entry>
        <para>
         File System (4 KiB Block Size)
        </para>
       </entry>
       <entry>
        <para>
         Maximum File System Size
        </para>
       </entry>
       <entry>
        <para>
         Maximum File Size
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row id="b12r1f2r">
       <entry>
        <para>
         Btrfs
        </para>
       </entry>
       <entry>
        <para>
         16 EiB
        </para>
       </entry>
       <entry>
        <para>
         16 EiB
        </para>
       </entry>
      </row>
      <row id="b12r1f2s">
       <entry>
        <para>
         Ext3
        </para>
       </entry>
       <entry>
        <para>
         16 TiB
        </para>
       </entry>
       <entry>
        <para>
         2 TiB
        </para>
       </entry>
      </row>
      <row id="b12r1f2t">
       <entry>
        <para>
         OCFS2 (a cluster-aware file system available in the High
         Availability Extension)
        </para>
       </entry>
       <entry>
        <para>
         16 TiB
        </para>
       </entry>
       <entry>
        <para>
         1 EiB
        </para>
       </entry>
      </row>
      <row id="b12r1f2u">
       <entry>
        <para>
         ReiserFS v3.6
        </para>
       </entry>
       <entry>
        <para>
         16 TiB
        </para>
       </entry>
       <entry>
        <para>
         1 EiB
        </para>
       </entry>
      </row>
      <row id="b12r1f2v">
       <entry>
        <para>
         XFS
        </para>
       </entry>
       <entry>
        <para>
         8 EiB
        </para>
       </entry>
       <entry>
        <para>
         8 EiB
        </para>
       </entry>
      </row>
      <row id="b12r1f2w">
       <entry>
        <para>
         NFSv2 (client side)
        </para>
       </entry>
       <entry>
        <para>
         8 EiB
        </para>
       </entry>
       <entry>
        <para>
         2 GiB
        </para>
       </entry>
      </row>
      <row id="b12r1f2x">
       <entry>
        <para>
         NFSv3 (client side)
        </para>
       </entry>
       <entry>
        <para>
         8 EiB
        </para>
       </entry>
       <entry>
        <para>
         8 EiB
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>

   <important>
    <para>
     <xref linkend="tab.maxsize" xrefstyle="TableXRef"/> describes the
     limitations regarding the on-disk format. The Linux kernel imposes its
     own limits on the size of files and file systems handled by it. These
     are as follows:
    </para>
    <variablelist>
     <varlistentry id="bgchzc6">
      <term>File Size</term>
      <listitem>
       <para>
        On 32-bit systems, files cannot exceed 2 TiB
        (2<superscript>41</superscript> bytes).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="bgchzc7">
      <term>File System Size</term>
      <listitem>
       <para>
        File systems can be up to 2<superscript>73</superscript> bytes in
        size. However, this limit is still out of reach for the currently
        available hardware.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </important>
  </sect1>
  <sect1 id="sect_stor_limits">
   <title>Linux Kernel Storage Limitations</title>

   <para>
    <xref linkend="b12o27j6" xrefstyle="TableXRef"/> summarizes the kernel
    limits for storage associated with &productname;.
   </para>

   <table id="b12o27j6"  frame="topbot" rowsep="1" pgwide="0">
    <title>Storage Limitations</title>
    <tgroup cols="2">
     <colspec colnum="1" colname="1" colwidth="5001*"/>
     <colspec colnum="2" colname="2" colwidth="5001*"/>
     <thead>
      <row id="b12o27j7">
       <entry>
        <para>
         Storage Feature
        </para>
       </entry>
       <entry>
        <para>
         Limitation
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row id="b12o27j8">
       <entry>
        <para>
         Maximum number of LUNs supported
        </para>
       </entry>
       <entry>
        <para>
         16384 LUNs per target.
        </para>
       </entry>
      </row>
      <row id="b12o27j9">
       <entry>
        <para>
         Maximum number of paths per single LUN
        </para>
       </entry>
       <entry>
        <para>
         No limit per se. Each path is treated as a normal LUN.
        </para>
        <para>
         The actual limit is given by the number of LUNs per target and the
         number of targets per HBA (16777215 for a Fibre Channel HBA).
        </para>
       </entry>
      </row>
      <row id="b12o27ja">
       <entry>
        <para>
         Maximum number of HBAs
        </para>
       </entry>
       <entry>
        <para>
         Unlimited. The actual limit is determined by the amount of PCI
         slots of the system.
        </para>
       </entry>
      </row>
      <row id="b12o27jb">
       <entry>
        <para>
         Maximum number of paths with device-mapper-multipath (in total) per
         operating system
        </para>
       </entry>
       <entry>
        <para>
         Approximately1024. The actual number depends on the length of the
         device number strings. It is a compile-time variable within
         multipath-tools, which can be raised if this limit poses to be a
         problem.
        </para>
       </entry>
      </row>
      <row id="b12r1mnq">
       <entry>
        <para>
         Maximum size per block device
        </para>
       </entry>
       <entry>
        <para>
         For X86, up to 16 TiB.
        </para>
        <para>
         For x86_64, ia64, s390x, and ppc64, up to 8 EiB.
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </sect1>
  <sect1 id="bwkbhpd">
   <title>Managing Devices with the &yast; Partitioner</title>

   <para>
    You can use the &yast; Partitioner to create and manage file systems and
    RAID devices. For information, see <xref linkend="cha.advdisk"/>.
   </para>
  </sect1>
  <sect1 id="sec_filesystems_info">
   <title>Additional Information</title>

   <para>
    Each of the file system projects described above maintains its own home
    page on which to find mailing list information, further documentation,
    and FAQs:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      <ulink url="http://e2fsprogs.sourceforge.net/"><citetitle>E2fsprogs:
      Ext2/3/4 File System Utilities</citetitle></ulink>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="http://www.ibm.com/developerworks/linux/library/l-fs7/"><citetitle>Introducing
      Ext3</citetitle></ulink>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="http://oss.sgi.com/projects/xfs/"><citetitle>XFS: A
      High-Performance Journaling Filesytem</citetitle></ulink>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="http://oss.oracle.com/projects/ocfs2/"><citetitle>OCFS2
      Project</citetitle></ulink>
     </para>
    </listitem>
   </itemizedlist>

   <para>
    A comprehensive multipart tutorial about Linux file systems can be found
    at IBM&nbsp;developerWorks in the
    <ulink url="https://www.ibm.com/developerworks/linux/library/l-fs/"><citetitle>Advanced
    File System Implementor’s Guide</citetitle></ulink>.
   </para>

   <para>
    An in-depth comparison of file systems (not only Linux file systems) is
    available from the Wikipedia project in
    <ulink url="http://en.wikipedia.org/wiki/Comparison_of_file_systems#Comparison"><citetitle>Comparison
    of File Systems</citetitle></ulink>.
   </para>
  </sect1>
 </chapter>
