<?xml version="1.0"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.xen.ha">
 <title>&xen; as a High-Availability Virtualization Host</title>
 <para>
  Setting up two &xen; hosts as a failover system has several advantages
  compared to a setup where every server runs on dedicated hardware.
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Failure of a single server does not cause major interruption of the
    service.
   </para>
  </listitem>
  <listitem>
   <para>
    A single big machine is normally way cheaper than multiple smaller
    machines.
   </para>
  </listitem>
  <listitem>
   <para>
    Adding new servers as needed is a trivial task.
   </para>
  </listitem>
  <listitem>
   <para>
    The utilization of the server is improved, which has positive effects on
    the power consumption of the system.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  The setup of migration for &xen; hosts is described in
  <xref
   linkend="sec.xen.manage.migrate"/>. In the following, several
  typical scenarios are described.
 </para>
 <sect1 id="sec.xen.ha.remote_storage">
  <title>&xen; HA with Remote Storage</title>

<!--
  iSCSI, NBD
  FC/NPIV
  NFS
  -->

  <para>
   &xen; can directly provide a number of remote block devices to the
   respective &xen; guest systems. These include iSCSI, NPIV, and NBD. All of
   these may be used to do live migrations. When a storage system is already
   in place, first try to use the same device type you already used in the
   network.
  </para>

  <para>
   If the storage system cannot be used directly but provides a possibility
   to offer the needed space over NFS, it is also possible to create image
   files on NFS. If the NFS file system is available on all &xen; host
   systems, this method also allows live migrations of &xen; guests.
  </para>

  <para>
   When setting up a new system, one of the main considerations is whther a
   dedicated storage area network should be implemented. The following
   possibilities are available:
  </para>

<!-- method, complexity/cost, comments -->

  <table id="tab.xen.remote_storage">
   <title>&xen; Remote Storage</title>
   <tgroup cols="3">
    <thead>
     <row>
      <entry>
       <para>
        Method
       </para>
      </entry>
      <entry>
       <para>
        Complexity
       </para>
      </entry>
      <entry>
       <para>
        Comments
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        Ethernet
       </para>
      </entry>
      <entry>
       <para>
        low
       </para>
      </entry>
      <entry>
       <para>
        Note that all block device traffic goes over the same Ethernet
        interface as the network traffic. This may be limiting the
        performance of the guest.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Ethernet dedicated to storage.
       </para>
      </entry>
      <entry>
       <para>
        medium
       </para>
      </entry>
      <entry>
       <para>
        Running the storage traffic over a dedicated Ethernet interface may
        eliminate a bottleneck on the server side. However, planning your
        own network with your own IP address range and possibly a VLAN
        dedicated to storage requires numerous considerations.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        NPIV
       </para>
      </entry>
      <entry>
       <para>
        high
       </para>
      </entry>
      <entry>
       <para>
        NPIV is a method to virtualize fibre channel connections. This is
        available with adapters that support a data rate of at least 4
        Gbit/s and allows the setup of complex storage systems.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <para>
   Typically, a 1 Gbit/s Ethernet device will be able to fully use a typical
   hard disk or storage system. When using very fast storage systems, such
   an Ethernet device will probably limit the speed of the system.
  </para>
 </sect1>
 <sect1 id="sec.xen.ha.local_storage">
  <title>&xen; HA with Local Storage</title>

  <para>
   For space or budget reasons, it may be necessary to rely on storage that
   is local to the &xen; host systems. To still maintain the possibility of
   live migrations, it is necessary to build block devices that are mirrored
   to both &xen; hosts. The software that allows this is called Distributed
   Replicated Block Device (DRBD).
  </para>

  <para>
   If a system that uses DRBD to mirror the block devices or files between
   two &xen; hosts should be set up, both hosts should use the identical
   hardware. If one of the hosts has slower hard disks, both hosts will
   suffer from this limitation.
  </para>

  <para>
   During the setup, each of the required block devices should use its own
   DRBD device. The setup of such a system is quite a complex task.
  </para>
 </sect1>
 <sect1 id="sec.xen.ha.private_bridge">
  <title>&xen; HA and Private Bridges</title>

  <para>
   When using several guest systems that need to communicate between each
   other, it is possible to do this over the regular interface. However, for
   security reasons it may be advisable to create a bridge that is only
   connected to guest systems.
  </para>

  <para>
   In a HA environment that also should support live migrations, such a
   private bridge must be connected to the other &xen; hosts. This is
   possible by using dedicated physical Ethernet devices, and also using a
   dedicated network.
  </para>

  <para>
   A different implementation method is using VLAN interfaces. In that case,
   all the traffic goes over the regular Ethernet interface. However, the
   VLAN interface does not get the regular traffic, because only the VLAN
   packets that are tagged for the correct VLAN are forwarded.
  </para>

  <para>
   For more information about the setup of a VLAN interface see
   <xref
    linkend="sec.xen.net.vlan"/>.
  </para>
 </sect1>
</chapter>
