<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article	  [
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<!-- Converted by suse-upgrade version 1.1 -->
<article xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="article.vt.best.practices" xml:lang="en">
 <?suse-quickstart color="suse"?>
 <title>Virtualization Best Practices</title>
 <subtitle>&sle; &productname; &productnumber;</subtitle>
 <info>
  <productnumber>&productnumber;</productnumber>
  <productname>&sle; &productname;</productname>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker>
   </dm:bugtracker>
  </dm:docmanager>
 </info>
 <!--
     we can write it based on on scenario?
     Virtualization Capabilities:
     Consolidation (hardware1+hardware2 -> hardware)
     Isolation
     Migration (hardware1 -> hardware2)
     Disaster recovery
     Dynamic load balancing
 -->
 <sect1 xml:id="vt.best.scenario">
  <title>Virtualization Scenarios</title>
  <para>
   Virtualization offers your environment a lot of capabilities. It can
   be used in multiple scenarios. To get more details about
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_capabilities.html">Virtualization
   Capabilities</link> and
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_benefits.html">Virtualization
   Benefits</link> refer to the
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/book_virt.html">Virtualization Guide</link>.
  </para>
  <para>
   This best practice will provide advice to help you make the best choices
   for your environment; it will recommend or discourage options based on
   your virtualization use case. Optimal configuration adjustments and 
   performance tuning can increase &vmguest; performance close to bare metal.
  </para>
  <remark>
   Bruce 20150806: There is no mention of caching considerations, of migration inhibitors, or
   basic strategies for how to do your storage or networking infrastructure,
   and only a little bit about how to map your virtualization requirements to
   host capabilities.
   I see that the doc doesn't address the issue of when to use Xen PV vs Xen FV
   vs KVM at all.
  </remark>
  <!-- Todo
       <Table Rowsep="1">
       <title>Scenario</title>
       <tgroup cols="3">
       <colspec colnum="1" colname="1" colwidth=""/>
       <colspec colnum="2" colname="2" colwidth=""/>
       <thead>
       <row>
       <entry>
       <para>Scenarios</para>
       </entry>
       <entry>
       <para>Option Recommended for</para>
       </entry>
       <entry>
       <para>Option Not recommended for</para>
       </entry>
       </row>
       </thead>
       <tbody>
       <row>
       <entry>
       <para>Consolidation</para>
       </entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Isolation</para>
       </entry>
       <entry></entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Migration</para>
       </entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Disaster Recovery</para>
       </entry>
       <entry>
       <mediaobject>
       <imageobject>
       <imagedata fileref="thumbup_green.png" width="25%" format="PNG"/>
       </imageobject>
       </mediaobject>
       </entry>
       </row>
       <row>
       <entry>
       <para>Dynamic Load Balancing</para>
       </entry>
       <entry></entry>
       <entry>
       <mediaobject>
       <imageobject>
       <imagedata fileref="thumbdown_red.png" width="25%" format="PNG"/>
       </imageobject>
       </mediaobject>
       </entry>
       </row>
       </tbody>
       </tgroup>
       </table>
  -->
 </sect1>
 <sect1 xml:id="vt.best.intro">
  <title>Before Any Modification</title>
  <sect2 xml:id="vt.best.intro.backup">
   <title>Back Up First</title>
   <para>
    Adjusting &vmguest; and VM Host configuration can lead to data loss
    or an unstable state. It is very important to create backup configuration files,
    data, images, etc. before making any changes. Backups ensure the
    original configuration can be restored in the event of data loss or
    a miss-configuration.
   </para>
   <warning>
    <para>
     Backup is mandatory before doing any tests, ensuring the ability to
     roll back to a usable/stable system or configuration. Do not perform
     tests or experiments on online, production systems.
    </para>
   </warning>
  </sect2>
  <sect2 xml:id="vt.best.intro.testing">
   <title>Test Your Configurations</title>
   <para>
    The efficiency of a virtualization environment depends on many factors.
    This guide is provided as a reference to help make good choices when
    configuring virtualization in a production environment. Nothing is
    <emphasis>carved in stone</emphasis>. Hardware, workloads, resource
    capacity, etc. should all be considered when planning, testing, and
    deploying your virtualization infra-structure. Testing your virtualized
    workloads is vital to a successful virtualization implementation.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="vt.best.reco">
  <title>Recommended Usage</title>
  <sect2 xml:id="vt.best.intro.libvirt">
   <title>Prefer &libvirt; Framework</title>
   <remark>add some explanation about how to enable/disable kvm with
   qemu-system-arch</remark>
   <para>
    In SUSE Linux Enterprise it is recommended to use the &libvirt;
    framework to use management operations on hosts, containers and &vmguest;.
    Using management tools out-of-band prevents libvirt from recongnizing
    the changes. For example, creating a system image manually with
    <command>qemu-img create data.raw 10G</command> will not be displayed in
    the <command>virt-manager</command> pool section. A VM started directly
    with the <command>qemu-system-arch</command> command will not be visible
    under &libvirt;. So you should be careful if using out-of-band management tools
    and keep in mind their usage probably won't be reflected in other
    virtualization tools.
   </para>
   <note>
    <para>
     Read
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/cha_libvirt_overview.html#">&libvirt;
     overview </link> for more information.
    </para>
   </note>
  </sect2>
  <sect2 xml:id="vt.best.intro.qemu">
   <title>qemu-system-i386 Vs. qemu-system-x86_64</title>
   <remark>Lin 20150808: Should we add some explanation about how to enable/disable kvm
   with qemu-system-arch</remark>
   <para>
    Just as a modern 64-bit x86 PC supports running a 32-bit OS and a
    64-bit OS, <command>qemu-system-x86_64</command> runs 32-bit OS's
    perfectly fine, and in fact usually provides better performance to 32-bit 
    guests than <command>qemu-system-i386</command>, which provides a 32-
    bit guest environment only. Hence we recommend using
    <command>qemu-system-x86_64</command> over
    <command>qemu-system-i386</command> for all guest types. Where
    <command>qemu-system-i386</command> is known to perform better is in
    configurations that SUSE does not support.
   </para>
  </sect2>
 </sect1>
 
 <sect1>
  <title>I/O in Virtualization</title>
  <para>
   SUSE products support various I/O Virtualization technologies. The
   following table lists advantages and drawbacks of each. For more
   information about I/O in virtualization refer to <link
   xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_vt_io.html">Virtualization
   Guide, I/O in Virtualization</link>.
  </para>
  <table>
   <title>I/O Virtualization Solutions</title>
   <tgroup cols="3">
    <colspec colnum="1" colname="1" colwidth="20%"/>
    <colspec colnum="2" colname="2" colwidth=""/>
    <colspec colnum="1" colname="1" colwidth=""/>
    <thead>
     <row>
      <entry>
       <para>Technology</para>
      </entry>
      <entry>
       <para>Advantages</para>
      </entry>
      <entry>
       <para>Drawbacks</para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>Device Assignment</para><para>(pass-through)</para>
      </entry>
      <entry>
       <itemizedlist>
	<listitem>
	 <para>Device accessed directly by the guest</para>
	</listitem>
	<listitem>
	 <para>High performance</para>
	</listitem>
       </itemizedlist>
      </entry>
      <entry>
       <itemizedlist>
        <listitem>
	 <para>No sharing among multiple guests</para>
	</listitem>
	<listitem>
	 <para>Live migration is complex</para>
	</listitem>
	<listitem>
	 <para>PCI device limit is 8 per guest</para>
	</listitem>
	<listitem>
         <para>Limited number of slots on a server</para>
	</listitem>
       </itemizedlist>
      </entry>
     </row>
     <row>
      <entry>
       <para>Full virtualization</para><para>(IDE, SATA, SCSI, e1000)</para>
      </entry>
      <entry>
       <itemizedlist>
	<listitem>
	 <para>&vmguest; compatibility</para>
	</listitem>
	<listitem>
	 <para>Easy for live migration</para>
	</listitem>
       </itemizedlist>
      </entry>
      <entry>
       <itemizedlist>
	<listitem>
	 <para>Bad performance</para>
	</listitem>
	<listitem>
	 <para>Emulated operation</para>
	</listitem> 
       </itemizedlist>
      </entry>
     </row>
     <row>
      <entry>
       <para>Para-virtualization</para><para>(virtio-blk, virtio-net, virtio-scsi)</para>
      </entry>
      <entry>
       <itemizedlist>
	<listitem>
	 <para>Good performance</para>
	</listitem>
	<listitem>
	 <para>Easy for live migration</para>
	</listitem>
	<listitem>
	 <para>Efficient host communication with &vmguest; </para>
	</listitem>
       </itemizedlist>
      </entry>
      <entry>
       <itemizedlist>
	<listitem>
	 <para>Modified guest (PV drivers)</para>
	</listitem>
       </itemizedlist>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </sect1>

 <sect1 xml:id="vt.best.hostlevel">
  <title>Host-Level Configuration and Settings</title>
  <para></para>

  <sect2 xml:id="vt.best.mem">
   <title>Memory and Pages</title>
   <para>
    Linux manages memory in units called pages. On most systems the default
    page size is 4 KB. Linux and the CPU need to know which pages belongs to
    which process. That information is stored in a page table. If you have
    a lot of processes running, it takes more time to find where the memory is
    mapped, due to the time required to search the page table. To speed up
    the search of the table, the TLB (Translation Lookaside Buffer) was
    invented. But on a system with a lot of memory the TLB is not enough. To
    avoid any fallback to normal page table (resulting in a cache miss, which
    is time consuming), huge pages can be used. Using huge pages will reduce TLB
    overhead and TLB misses (pagewalk). A host with
    32 GB (32*1014*1024 = 33554432KB) of memory and a 4 KB page size
    has a TLB with: <emphasis>3355443/4 = 8388608</emphasis> entries.
    Usig a 2 MB (2048 KB) page size, the TLB only has <emphasis>3355443/ 2048 = 16384</emphasis> entries, greatly reducing TLB misses.
   </para>
   <sect3>
    <title>Huge Pages</title>
    <para>
     Current CPU architectures support larger pages than 4 KB: huge pages.
     To determine  the size of huge pages available on your system
     (could be 2 MB or 1 GB), check the flag in <filename>/proc/cpuinfo</filename>.
    </para>
    <table>
     <title>DETERMINE HUGE PAGES SIZE</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="10%"/>
      <colspec colnum="2" colname="2" colwidth=""/>
      <thead>
       <row>
	<entry>
         <para>CPU flag</para>
	</entry>
	<entry>
         <para>Huge pages size available</para>
	</entry>
       </row>
      </thead>
      <tbody>
       <row>
	<entry><para>Empty string</para></entry>
	<entry><para>No huge pages available</para></entry>
       </row>
       <row>
	<entry><para>pse</para></entry>
	<entry><para>2 MB</para></entry>
       </row>
       <row>
	<entry><para>pdpe1gb</para></entry>
	<entry><para>1 GB</para></entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para>
     Using huge pages should improve performance of &vmguest; and reduce host
     memory consumption.
    </para>
    <para>
     By default the system uses THP, to make huge pages available on your system
     you need to activate it at boot time with <option>hugepages=1</option>, 
     and optionally add the huge pages size with <option>hugepagesz=2MB</option>.
    </para>
    <note>
     <title>1 GB huge pages</title>
     <para>
      1 GB pages can only be allocated at boot time and cannot be freed afterwards.
     </para>
    </note>
    <para>
     To allocate and use huge page table (HugeTlbPage) you need to
     mount <filename>hugetlbfs</filename> with correct permissions.
    </para>
    <procedure>
     <step>
      <para>
       Mount <option>hugetlbfs</option> to <filename>/dev/hugepages</filename>:
      </para>
      <screen># mount -t hugetlbfs hugetlbfs /dev/hugepages</screen>
     </step>
     <step>
      <para>
       To reserve memory for huge pages use <command>sysctl</command> or
       <command>echo <replaceable>NUMBER</replaceable></command>. If your
       system has a <remark>dublin 2015-11-04:  should there be a space
         between the words?</remark><emphasis>Hugepagesize</emphasis> of 2 MB (2048 KB), and you
       want to reserve 1 GB (1048576KB) for your &vmguest;, you need <emphasis>1048576/2048=512</emphasis>
       pages in the pool.
      </para>
      <screen># sysctl vm.nr_hugepages=512</screen>
      <para>Or:</para>
      <screen># echo 512 > /proc/sys/vm/nr_hugepages<co
      xml:id="co.hp.nrhp"/></screen>
      <calloutlist>
       <callout arearefs="co.hp.nrhp">
	<para>
	 Current number of <emphasis>persistent</emphasis> huge pages in the
	 kernel's huge page pool.
	 <emphasis>Persistent</emphasis> huge pages will be returned to the huge
	page pool when freed by a task</para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       Add the <emphasis>memoryBacking</emphasis> element in the &vmguest; config:
      </para>
      <screen>&lt;memoryBacking&gt;
  &lt;hugepages/&gt;
&lt;/memoryBacking&gt;</screen> 
     </step>
     <step>
      <para>
       Start your &vmguest; and check on the host whether it uses hugepages:
      </para>
      <screen># cat /proc/meminfo | grep HugePages_
HugePages_Total:<co xml:id="co.hp.total"/>    512
HugePages_Free:<co xml:id="co.hp.free"/>       92
HugePages_Rsvd:<co xml:id="co.hp.rsvd"/>        0
HugePages_Surp:<co xml:id="co.hp.surp"/>        0</screen>
      <calloutlist>
       <callout arearefs="co.hp.total">
	<para>Size of the pool of huge pages</para>
       </callout>
       <callout arearefs="co.hp.free">
	<para>Number of huge pages in the pool that are not yet allocated</para>
       </callout>
       <callout arearefs="co.hp.rsvd">
	<para>Number of huge pages for which a commitment to allocate from the pool has been made, but no allocation has yet been made</para>
       </callout>
       <callout arearefs="co.hp.surp">
	<para>Number of huge pages in the pool above the value in
	<filename>/proc/sys/vm/nr_hugepages</filename>. The maximum number of surplus huge pages is controlled by
	<filename>/proc/sys/vm/nr_overcommit_hugepages</filename></para>
       </callout>
      </calloutlist>
     </step>
    </procedure>
    <note>
     <para>
      Even if huge pages provide the best performance, they do come with some
      drawbacks. You lose features such as Memory ballooning (see <xref linkend="vt.best.virtio.balloon"/>),
      KSM (see <xref linkend="vt.best.perf.ksm"/>, and huge pages cannot be
      swapped.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="vt.best.mem.thp">
    <title>Transparent Huge Pages</title>
    <para>
     Transparent huge pages (THP) provide a way to dynamically allocate huge
     pages with the <command>khugepaged</command> kernel thread, instead of
     manually managing their allocation and use. Workloads with contiguous
     memory access patterns can benefit greatly from THP. A 1000 fold
     decrease in page faults can be observed when running synthetic workloads
     with contiguous memory access patterns. Conversely, workloads with sparse
     memory access patterns (like databases) may perform poorly with THP. In
     such cases it may be preferable to disable THP by adding the kernel
     parameter <option>transparent_hugepage=never</option>, rebuild your
     grub2 configuration, and reboot. Verify THP is disabled with:
    </para>
    <screen># cat /sys/kernel/mm/transparent_hugepage/enabled 
always madvise [never]</screen>
    <note>
     <para>THP is not available under &xen;</para>
    </note>
   </sect3>
   <sect3 xml:id="vt.best.mem.hot">
    <title>Memory Hotplug</title>
    <para>
     To optimize the usage of your host memory, it may be useful to hot plug more
     memory for a running &vmguest; when required. To support memory hotplug,
     you must first configure the <emphasis>&lt;maxMemory&gt;</emphasis> tag
     in the XML configuration:
    </para>
    <screen>&lt;maxMemory<co xml:id="co.mem.hot.max"/> slots='16'<co   xml:id="co.mem.hot.slots"/> unit='KiB'&gt;20971520<co   xml:id="co.mem.hot.size"/>&lt;/maxMemory&gt;
  &lt;memory<co   xml:id="co.mem.hot.mem"/> unit='KiB'&gt;1048576&lt;/memory&gt;
&lt;currentMemory<co xml:id="co.mem.hot.curr"/> unit='KiB'&gt;1048576&lt;/currentMemory&gt;</screen>
    <calloutlist>
     <callout arearefs="co.mem.hot.max">
      <para>Run time maximum memory allocation of the guest. </para>
     </callout>
     <callout arearefs="co.mem.hot.slots">
      <para>Number of slots available for adding memory to the guest
      </para>
     </callout>
     <callout arearefs="co.mem.hot.size">
      <para>Valid units are:</para>
      <itemizedlist>
       <listitem><para>"KB" for kilobytes (1,000 bytes)</para></listitem>
       <listitem><para>"k" or "KiB" for kibibytes (1,024 bytes)</para></listitem>
       <listitem><para>"MB" for megabytes (1,000,000 bytes)</para></listitem>
       <listitem><para>"M" or "MiB" for mebibytes (1,048,576 bytes)</para></listitem>
       <listitem><para>"GB" for gigabytes (1,000,000,000 bytes)</para></listitem>
       <listitem><para>"G" or "GiB" for gibibytes (1,073,741,824 bytes)</para></listitem>
       <listitem><para>"TB" for terabytes (1,000,000,000,000 bytes)</para></listitem>
       <listitem><para>"T" or "TiB" for tebibytes (1,099,511,627,776 bytes)</para></listitem>
      </itemizedlist>
     </callout>
     <callout arearefs="co.mem.hot.mem">
      <para>Maximum allocation of memory for the guest at boot time</para>
     </callout>
     <callout arearefs="co.mem.hot.curr">
      <para>Actual allocation of memory for the guest</para>
     </callout>
    </calloutlist>
    <para>
     To hot plug a memory devices into the slots, use <command>virsh
     attach-device vm-name mem-dev.xml</command>.
    </para>
    <screen># cat mem-dev.xml
&lt;memory model='dimm'&gt;
  &lt;target&gt;
  &lt;size unit='KiB'>524287&lt;/size&gt;
  &lt;node>0&lt;/node&gt;
  &lt;/target&gt;
&lt;/memory&gt;</screen>
    <para>
     For memory device hotplug, the guest must have at least 1 NUMA cell
     defined (see <xref linkend="vt.best.numa.topo"/>).
    </para>
   </sect3>
   <sect3>
    <title>Xen-specific Memory Notes</title>
    <sect4>
     <title>Managing Domain-0 Memory</title>
     <para>
      When using the Xen hypervisor, by default a small percentage of system
      memory is reserved for the hypervisor, with all remaining memory
      automatically allocated to Domain-0. When virtual machines are created,
      memory is ballooned out of Domain-0 to provide memory for the virtual
      machine. This process is called "autoballooning".
     </para>
     <para>
      Autoballooning has several drawbacks:
     </para>
     <itemizedlist>
      <listitem><para>Reduced performance while dom0 is ballooning down to free memory for the new domain.</para></listitem>
      <listitem><para>Memory freed by ballooning is not confined to a specific NUMA node. This can result in performance problems in the new domain due to using a non-optimal NUMA configuration.</para></listitem>
      <listitem><para>Failure to start large domains due to delays while ballooning large amounts of memory from dom0.</para></listitem>
     </itemizedlist>
     <para>
      For these reasons, it is strongly recommended to disable autoballooning
      and give Domain-0 a pre-defined amount of memory.
     </para>
     <para>
      Autoballooning is controlled by the toolstack used to manage your Xen
      installation. For the xl/libxl toolstack, autoballooning is controlled
      by the <option>autoballoon</option> setting in
      <filename>/etc/xen/xl.conf</filename>. For the libvirt+libxl toolstack,
      autoballooning is controlled by the <option>autoballoon</option> setting
      in <filename>/etc/libvirt/libxl.conf</filename>.
     </para>
     <para>
      The amount of memory initially allocated to Domain-0 is controlled by
      the Xen hypervisor dom0_mem parameter. E.g. to set Domain-0's memory
      to 8GB, add <option>dom0_mem=8G</option> to the Xen hypervisor
      parameters.
     </para>
     <para>
      To set dom0_mem on SLE11 products, modify <filename>/boot/grub/menu.lst</filename>,
      adding <option>dom0_mem=XX</option> to the Xen hypervisor (xen.gz)
      parameters. The change will be applied at next reboot.
    </para>
    <para>
     To set dom0_mem on SLE12 products, modify <filename>/etc/default/grub</filename>,
     adding <option>dom0_mem-XX</option> to <option>GRUB_CMDLINE_XEN_DEFAULT</option>.
     Changes to <filename>/etc/default/grub</filename> require rebuilding the
     grub configuration with <command>grub2-mkconfig -o /boot/grub2/grub.cfg</command>.
     and rebooting the host.
    </para>
     <para>
      Autoballooning is enabled by default since it is extremely difficult
      to determine a pre-defined amount of memory required by Domain-0.
      Memory needed by Domain-0 is heavily dependent on the number of hosted
      virtual machines and their configuration. Users must ensure Domain-0 has
      sufficient memory resources to accommodate virtual machine workloads.
     </para>
    </sect4>
    <sect4>
     <title>xenstore in tmpfs</title>
     <para>
      When using Xen, it is recommended to place the xenstore database
      on tmpfs. xenstore is used as a control plane by the xm/xend and
      xl/libxl toolstacks and the frontend and backend drivers servicing
      domain I/O devices. The load on xenstore increases linearly as the
      number of running domains increase. If you anticipate hosting many
      &vmguest; on a Xen host, move the xenstore database onto tmpfs to
      improve overall performance of the control plane.
      Mount <filename>/var/lib/xenstored</filename> directory on tmpfs:
     </para>
     <screen># mount -t tmpfs tmpfs /var/lib/xenstored/</screen>
    </sect4>
   </sect3>
  </sect2>

  <sect2 xml:id="vt.best.perf.ksm">
   <title>KSM and Page Sharing</title>
   <para>
    Kernel Samepage Merging is a kernel feature that allows packing more
    virtual machines into physical memory, by sharing the data common between
    them. The KSM daemon ksmd periodically scans user memory looking for pages
    of identical content which can be replaced by a single write-protected
    page. The duplicate pages are then available to run more &vmguest; on the
    host.  You can enable KSM with <command>echo 1 &gt; /sys/kernel/mm/ksm/run</command>.
    One advantage of using KSM from a &vmguest; perspective is that all guest
    memory is backed by host anonymous memory, so you can share
    <emphasis>pagecache</emphasis>, <emphasis>tmpfs</emphasis> or any kind
    of memory allocated in the guest.
   </para>
   <para>
    KSM is controlled by <emphasis>sysfs</emphasis>. You can check KSM's
    values in <filename>/sys/kernel/mm/ksm/</filename>:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>pages_shared</emphasis>: How many shared pages are being
      used (read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>pages_sharing</emphasis>: How many sites are sharing the
      pages (read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>pages_unshared</emphasis>: How many pages are unique but
      repeatedly checked for merging (read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>pages_volatile</emphasis>: How many pages are changing too fast
      to be considered for merging (read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>full_scans</emphasis>: How many times all mergeable areas
      have been scanned (read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>sleep_millisecs</emphasis>: How many milliseconds
      <command>ksmd</command> should sleep before the next scan. A low value
      will overuse the CPU, consuming CPU time that could be used for other
      tasks. A value greater than <option>1000</option> is recommended.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>pages_to_scan</emphasis>: How many present pages to scan
      before ksmd goes to sleep. A high value will overuse the CPU. It is
      recommended to start with a value of <option>1000</option>, and then
      adjust as necessary based on the KSM results observed while testing
      your deployment.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>merge_across_nodes</emphasis>: by default the system merges
      pages across NUMA nodes. Set this option to <option>0</option> to
      disable this behavior.
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <para>
     KSM is a good technique to over-commit host memory when running
     multiple instances of the same application or &vmguest;. When
     applications and &vmguest; are heterogeneous and do not share
     any common data, it is preferable to disable KSM. In a mixed
     heterogeneous and homogeneous environment, KSM can be enabled
     on the host but disabled on a per &vmguest; basis. Use <command>
     virsh edit</command> to disable page sharing of a &vmguest; by
     adding the following to the guest's XML configuration:
    </para>
    <screen>&lt;memoryBacking&gt;
   &lt;nosharepages/&gt;
&lt;/memoryBacking&gt;</screen>
   </note>
   <warning>
    <para>
     KSM can free up some memory on the host system, but the administrator
     should also reserve enough swap to avoid out-of-memory conditions
     in the event sharable memory decreases. A decrease in the amount of
     sharable memory results in an increase in the use of physical memory.
    </para>
   </warning>
   <warning>
    <para>
     By default, KSM will merge common pages across NUMA nodes. This
     may degrade &vmguest; performance if the merged, common page is now
     located on a distant NUMA node, relative to the node running the
     &vmguest; VCPUs. If increased memory access latencies are noticed
     in the &vmguest;, disable cross-node merging with the <emphasis>
     merge_across_nodes</emphasis> sysfs control:
     <command>echo 0 &gt; /sys/kernel/mm/ksm/merge_across_nodes</command>.
    </para>
   </warning>
  </sect2>
  
  <sect2 xml:id="vt.best.perf.swap">
   <title>Swapping</title>
   <para>
    <emphasis>Swap</emphasis> is mostly used by the system to store
    underused physical memory (low usage, or not accessed for a long
    time). To prevent the system running out of memory, setting up a minimum
    swap is highly recommended.
   </para>
   <sect3>
    <title>swappiness</title>
    <para>
     The <emphasis>swappiness</emphasis> setting controls your systems swap
     behavior. It defines how memory pages are swapped to disk. A high
     value of <emphasis>swappiness</emphasis> results in a
     system that swaps more. Values available are from <option>0</option> to
     <option>100</option>. A value of <option>100</option> tells the system
     to find inactive pages and put them in swap. A value of
     <option>0</option> reduces the systems tendency to swap
      <remark>dublin 2015-11-04: is userland correct?</remark>userland processes
     but does not disable swap completely (this is now the case with kernel =&gt; 3.5).
    </para>
    <para>
     To change the value and do some testing on a live system, you
     need to make an echo of the value, and check your system memory usage (i.e.,
     with the <command>free</command> command):
    </para>
    <screen># echo 35 &gt; /proc/sys/vm/swappiness</screen>
    <screen># free
total       used       free     shared    buffers     cached
Mem:      24616680    4991492   19625188     167056     144340    2152408
-/+ buffers/cache:    2694744   21921936
Swap:      6171644          0    6171644</screen>
    <para>
     To get this permanently add a line in
     <filename>/etc/systcl.conf</filename>:
    </para>
    <screen>vm.swappiness = 35</screen>
    <para>
     You can also control the swap by using the
     <emphasis>swap_hard_limit</emphasis> 
     element in the XML configuration of your &vmguest;. It is highly recommended to
     do some testing before setting this parameter and using it in a production
     environment as the host can kill the domain if the value is too low.
    </para>
    <screen>&lt;memtune&gt;<co xml:id="co.mem.1"/>
  &lt;hard_limit unit='G'&gt;1&lt;/hard_limit&gt;<co xml:id="co.mem.hard"/>
  &lt;soft_limit unit='M'&gt;128&lt;/soft_limit&gt;<co xml:id="co.mem.soft"/>
  &lt;swap_hard_limit unit='G'&gt;2&lt;/swap_hard_limit&gt;<co xml:id="co.mem.swap"/>
&lt;/memtune&gt;</screen>
    <calloutlist>
     <callout arearefs="co.mem.1">
      <para>This element provides memory tunable parameters for the domain. 
      If this is omitted, it defaults to the OS provided defaults</para>
     </callout>
     <callout arearefs="co.mem.hard">
      <para>Maximum memory the guest can use. To avoid any problems on the
      &vmguest; it is strongly recommended to do not use this parameter</para>
     </callout>
     <callout arearefs="co.mem.soft">
      <para>The memory limit to enforce during memory contention</para>
     </callout>
     <callout arearefs="co.mem.swap">
      <para>The maximum memory plus swap the &vmguest; can use</para>
     </callout>
    </calloutlist>
   </sect3>
  </sect2>

  <sect2 xml:id="vt.best.perf.io">
   <title>I/O Scheduler</title>
   <para>
    The default I/O scheduler is Completely Fair Queuing (CFQ). The main
    aim of the CFQ scheduler is to provide a fair allocation of the disk I/O
    bandwidth for all processes that request an I/O operation. You can
    have different I/O schedulers for different devices.
   </para>
   <para>
    To get better performance in host and &vmguest; it is recommended to use
    <emphasis>noop</emphasis> in the &vmguest; (disable the I/O scheduler)
    and the <emphasis>deadline</emphasis> scheduler for a virtualization
    host.
   </para>
   <sect3>
    <title>Checking the Current I/O Scheduler</title>
    <para>
     To check your current I/O scheduler for your disk (replace
     <replaceable>sdX</replaceable> by the disk you want to check):
    </para>
    <screen># cat /sys/block/<replaceable>sdX</replaceable>/queue/scheduler
noop [deadline] cfq</screen>
    <para>
     In our example the <emphasis>deadline</emphasis> scheduler is selected.
    </para>
   </sect3>
   <sect3>
    <title>Changing I/O Scheduler at Runtime</title>
    <para>
     You can change it at runtime with <command>echo</command>:
    </para>
    <screen># echo noop &gt; /sys/block/<replaceable>sdX</replaceable>/queue/scheduler</screen>
   </sect3>
   <sect3>
    <title>SLE 11 SPX Product</title>
    <para>
     To change the value at boot time for the SLE 11 SPX product, you need to
     modify your <filename>/boot/grub/menu.lst</filename> file, adding
     <option>elevator=deadline</option> for host and
     <option>elevator=noop</option> for &vmguest; (changes will be taken into
     account at next reboot). This will be applied to all devices on your
     system.
    </para>
    <para>
     Changing the <option>elevator=</option> for the boot command line will
     apply the <option>elevator</option> to all devices on your system.
    </para>
   </sect3>
   <sect3>
    <title>SLE 12 SPX Product</title>
    <para>
     To change the value at boot time for the SLE 12 SPX product, you need to
     modify your <filename>/etc/default/grub</filename> file. Find the
     variable starting with <option>GRUB_CMDLINE_LINUX_DEFAULT</option> and
     add at the end <option>elevator=deadline</option> (or change it with
     the correct value if it is already available).
    </para>
    <para>
     Now you need to regenerate your grub2 config with:
    </para>
    <screen># grub2-mkconfig -o /boot/grub2/grub.cfg</screen>
    <para>
     If you want to have a different parameter for each disk, create a
     <filename>/usr/lib/tmpfiles.d/IO_ioscheduler.conf</filename> file with:
    </para>
    <screen>w /sys/block/sda/queue/scheduler - - - - deadline
w /sys/block/sdb/queue/scheduler - - - - noop</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="vt.best.perf.cpu">
   <title>Understanding CPU</title>
   <para>
    Allocation of resources for &vmguest; is a crucial point in VM
    administration. Each &vmguest; should be <emphasis>sized</emphasis> to
    be able to run a certain amount of services, but over-allocating
    resources for &vmguest; may impact the host and all other &vmguest;s. If
    all &vmguest;s suddenly requested all their resources, the host wouldn't be
    able to provide all of them, and this would impact the host's performance
    and degrade all other services running on the host.
   </para>
   <para>
    CPU's Host <emphasis>components</emphasis> will be
    <emphasis>translated</emphasis> as vCPU in the &vmguest;, but even if
    you have a multi-core CPU with Hyper threading, you should understand
    that a main CPU unit and a multi-core and Hyper threading do not
    provide the same computation capabilities:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>CPU processor</emphasis>: this describes the main CPU unit, it
      can be multi-core and Hyper threaded, and most dedicated servers
      have multiple CPU processors on their motherboard.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>CPU core</emphasis>: a main CPU unit can provide more than
      one core, and the proximity of cores speeds up the computation process and reduces
      energy costs.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>CPU Hyper threading</emphasis>: this implementation is used
      to improve parrallelization of computations, but this is not efficient
      as a dedicated core.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="vt.best.perf.cpuparam">
   <title>CPU Parameter</title>
   <para>
    You should avoid CPU over-commit. Unless you know exactly how many vCPU
    you require for your &vmguest; you should start with 1 vCPU per
    &vmguest;. Each vCPU should match one hardware processor or core. You
    should target a workload of 70% inside your VM (this could be checked with a
    lot of tools like the <command>top</command>). If you allocate more
    processor than needed in the VM, this will add overhead, and will
    degrade cycle efficiency, the unused vCPU will consume timer interrupts
    and will idle loop, then this will impact the &vmguest;, but also the
    host. To optimize the performance usage it is recommended to know whether your
    applications are single threaded or not, so as to avoid any over-allocation of
    vCPU.
   </para>
   <sect3>
    <title>vCPU Model and Features</title>
    <para>
     CPU model and topology
     can be specified for each &vmguest;. The vCPU definition could be very
     specific excluding some CPU features, listing exact ones, etc. You
     can use predefined models available in the
     <filename>/usr/share/libvirt/cpu_map.xml</filename> file to exactly
     match your needs. Even if it seems beneficial to state a very
     specific vCPU for a &vmguest;, you should keep in mind that the normal
     vCPU model and features simplify migration among heterogeneous hosts
     (see
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_libvirt_admin_migrate.html">libvirt
     migration guide</link>). This is because changing the vCPU type requires the
     &vmguest; to be off, which is a constraint in a production environment.
     You should also keep in mind that multiple sockets with a single core and
     thread generally give the best performance.
    </para>
    <para>To get all capabilities and topology available on your system, use the
    <command>virsh capabilities</command> command.</para>
    <note>
     <title>host-passthrough</title>
     <para>
      The CPU visible to the guest should be exactly the same as the host
      CPU even in the aspects that &libvirt; does not understand. However the
      downside of this mode is that the guest environment cannot be reproduced
      on different hardware. Thus, if you hit any bugs, you are on your own.
     </para>
    </note>
    <para>
     To specify a strict model of a CPU for a &vmguest; like the
     new Broadwell CPU model:
    </para>
    <screen>&lt;cpu mode='custom' match='exact'&gt;
  &lt;model&gt;Broadwell&lt;/model&gt;
&lt;/cpu&gt;</screen>
    <para>
     If you want to add the <emphasis>invtsc</emphasis> (Invariant TSC) CPU feature,
     add a <emphasis>feature</emphasis> element between the
     <emphasis>&lt;cpu&gt;</emphasis> element:
    </para>
    <screen>&lt;cpu&gt;
  ....
  &lt;feature name='invtsc'/&gt;
&lt;/cpu&gt;</screen>
   </sect3>
   <sect3>
    <title>vCPU Pinning</title>
    <para>
     This is a method to constrain vCPU
     threads to a NUMA node. The <emphasis>vcpupin</emphasis> element
     specifies which of the host's physical CPUs the domain vCPU will be pinned
     to. If this is omitted, and the attribute <emphasis>cpuset</emphasis> of the
     element <emphasis>vcpu</emphasis> is not specified, the vCPU is pinned
     to all the physical CPUs by default.
    </para>
    <para>
     You can pin a vCPU to a specific physical CPU, in which case the vCPU
     will increase the CPU cache hit ratio. <remark>dublin 2015-11-04:
       It does not seem right to position the "Note" text here, between the 
       instruction "...core:" and the sample code. Can the "Note" text be
       positioned below the code?
     </remark>To pin a vCPU to a specific core:
    </para>

    <screen>virsh# vcpupin <replaceable>DOMAIN_ID</replaceable> --vcpu <replaceable>vCPU_NUMBER</replaceable>
VCPU: CPU Affinity
----------------------------------
0: 0-7
virsh # vcpupin SLE12 --vcpu 0 0 --config</screen>
    <note>
     <para>vCPU pinning can also be used for a non-numa node</para>
    </note>
    <para>In the XML configuration of your domain you should now have:</para>
    <screen>&lt;cputune&gt;
   &lt;vcpupin vcpu='0' cpuset='0'/&gt;
&lt;/cputune&gt;</screen>
    <figure xml:id="fig.qemu-img.vmnuma">
     <title>NUMA vCPU placement</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="vm_numa.png" width="95%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="vm_numa.png" width="95%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <warning>
     <title>Live Migration</title>
     <para>
      Even if <emphasis>vcpupin</emphasis> can improve performance, you should
      take into consideration that live migration of a pinned &vmguest; is
      difficult, because the resource may not be available on the host, or the
      NUMA topology could be different on the destination host.
      For more recommendations about Live Migration see <link
      xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_libvirt_admin_migrate.html#libvirt_admin_live_migration_requirements">Virtualization
      Live Migration Requirements</link>.
     </para>
    </warning>
   </sect3>
   <sect3>
    <title>libvirt and CPU Configuration</title>
    <para>
     For more information about vCPU configuration and tuning parameters
     refer to the
     <link xlink:href="https://libvirt.org/formatdomain.html#elementsCPU">libvirt</link>
     documentation.
    </para>
   </sect3>
  </sect2>
  <sect2>
   <title>&xen;: Moving from PV to FV</title>
   <para>
    This chapter explains how to convert a &xen; para-virtual machine into a &xen;
    fully virtualized machine.
   </para>
   <para>
    First you need to change to the <emphasis>-default</emphasis> kernel, if
    not already installed, you must install it while in PV mode.
   </para>
   <para>
    In case you are using <emphasis>vda*</emphasis> disk naming, you
    need to change this to <emphasis>hd*</emphasis> in
    <filename>/etc/fstab</filename>, <filename>/boot/grub/menu.lst</filename>
    and <filename>/boot/grub/device.map</filename>.
   </para>
   <note>
    <title>Prefer UUIDs</title>
    <para>
     You should use UUIDs or logical volumes within your
     <filename>/etc/fstab</filename>. Using UUID simplifies attached network
     storage, multipathing, and virtualization.
    </para>
   </note>
   <para>
    Moving from PV to FV will lead to disk driver modules being missing from the
    initrd. The modules expected are <emphasis>xen-vbd</emphasis> (and
    <emphasis>xen-vnif</emphasis> for network). These are the PV drivers for
    a fully virtualized &vmguest;. All other modules like <emphasis>ata_piix</emphasis>,
    <emphasis>ata_generic</emphasis> and <emphasis>libata</emphasis> should
    be added automatically.
   </para>
   <tip>
    <title>Adding Modules to the initrd</title>
    <para>
     On &slsa; 11, you can add modules to the
     <emphasis>INITRD_MODULES</emphasis> line in the
     <filename>/etc/sysconfig/kernel</filename> file, for example:
    </para>
    <screen>INITRD_MODULES="xen-vbd xen-vnif"</screen>
    <para>
     Run <command>mkinitrd</command> to build a new initrd containing the
     modules.
    </para>
    <para>
     On &slsa; 12, open <filename>/etc/dracut.conf.d/01-dist.conf</filename>
     and add the the modules with <literal>force-drivers</literal> by adding a
     line as in the example below:
    </para>
    <screen>force-drivers+="xen-vbd xen-vnif"</screen>
    <para>
     Run <command>dracut -f</command> to build a new initrd containing the
     modules.
    </para>
   </tip>
   <para>
    You need to change a few parameters in the XML configuration file to describe
    your &vmguest;:
   </para>
   <para>
    Set the OS section to something like:
   </para>
   <screen>&lt;os&gt;
  &lt;type arch='x86_64' machine='xenfv'&gt;hvm&lt;/type&gt;
  &lt;loader&gt;/usr/lib/xen/boot/hvmloader&lt;/loader&gt;
  &lt;boot dev='hd'/&gt;
&lt;/os&gt;</screen>
   <para>
    In the devices section, you need to add:
   </para>
   <screen>&lt;emulator&gt;/usr/lib/xen/bin/qemu-system-i386&lt;/emulator&gt;</screen>
   <para>
    Replace the <emphasis>xen</emphasis> disk bus with
    <emphasis>ide</emphasis>, and the <emphasis>xvda</emphasis> target device
    with <emphasis>hda</emphasis>.
   </para>
   <note>
    <title>guestfs-tools</title>
    <para>
     If you want to script this process, or work on disk images directly, you
     can use the
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_guestfs_tools.html">guestfs-tools</link>
     suite, as numerous tools exist there to help to modify disk images.
    </para>
   </note>
  </sect2>
  <sect2 xml:id="vt.best.perf.numa">
   <title>NUMA Affinity</title>
   <para>
    Using <emphasis>NUMA</emphasis> can potentially have a huge impact on performance. You should consider your host topology when sizing guests. First check that your host has NUMA capabilities:
   </para>
   <screen># numactl --hardware
available: 4 nodes (0-3)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 72 73 74 75 76 77 78
79 80 81 82 83 84 85 86 87 88 89
node 0 size: 31975 MB
node 0 free: 31120 MB
node 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 90 91 92 93
94 95 96 97 98 99 100 101 102 103 104 105 106 107
node 1 size: 32316 MB
node 1 free: 31673 MB
node 2 cpus: 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 108 109 110
111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
node 2 size: 32316 MB
node 2 free: 31726 MB
node 3 cpus: 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 126 127 128
129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
node 3 size: 32314 MB
node 3 free: 31387 MB
node distances:
node   0   1   2   3 
0:  10  21  21  21 
1:  21  10  21  21 
2:  21  21  10  21 
3:  21  21  21  10</screen>
   <sect3>
    <title>Numa Node Tuning (host)</title>
    <para>
     NUMA is the acronym of Non Uniform Memory Access. A NUMA system has
     multiple physical CPUs with local memory attached.
     Moreover each CPU can access other CPUs' Memory this is what we call
     remote memory, and access is slower than accessing local memory.
    </para>
    <para>
     You can control the NUMA policy performance <remark>toms 2015-11-04: 
       Should this say "for the domain process" or "for domain processes"?
     </remark>for domain processes by using the <emphasis>numatune</emphasis>
      element:
    </para>
    <screen>&lt;numatune&gt;
    &lt;memory mode="strict"<co xml:id="co.numat.mode"/> nodeset="1-4,^3"<co xml:id="co.numat.nodeset"/>/&gt;
    &lt;memnode<co xml:id="co.numat.memnode"/> cellid="0"<co xml:id="co.numat.cellid"/> mode="strict" nodeset="1"/&gt;
    &lt;memnode cellid="2" placement="strict"<co xml:id="co.numat.placement"/> mode="preferred" nodeset="2"/&gt;
&lt;/numatune&gt;</screen>
    <calloutlist>
     <callout arearefs="co.numat.mode">
      <para>
       Policies available are: <emphasis>interleave</emphasis> (round-robin
       like), <emphasis>strict</emphasis> (default) or <emphasis>preferred</emphasis>
      </para>
     </callout>
     <callout arearefs="co.numat.nodeset">
      <para>
       Specify the NUMA nodes
      </para>
     </callout>
     <callout arearefs="co.numat.memnode">
      <para>
       Specify memory allocation policies for each guest NUMA node (if this
       element is not defined then this will fall back and use the
       <emphasis>memory</emphasis> element)
      </para>
     </callout>
     <callout arearefs="co.numat.cellid">
      <para>
       Addresses the guest NUMA node for which the settings are applied
      </para>
     </callout>
     <callout arearefs="co.numat.placement">
      <para>
       The placement attribute can be used to indicate the memory placement mode
        <remark>dublin 2015-11-04: 
          Should this say "for the domain process" or "for domain processes"?
        </remark>for domain processes, the value can be 
        <emphasis>auto</emphasis> or <emphasis>strict</emphasis>
      </para>
     </callout>
    </calloutlist>
    <warning>
     <title>Memory and CPU across NUMA nodes</title>
     <para>
      You should avoid allocating &vmguest; memory across NUMA nodes, and
      prevent vCPUs from floating across NUMA nodes.
     </para>
    </warning>
   </sect3>
   <sect3>
    <title>NUMA Balancing</title>
    <para>
     On NUMA machines, there is a performance penalty if remote memory is
     accessed by a CPU.
     Automatic NUMA balancing scans a task's address space and unmaps pages to
     detect whether pages are properly placed or whether the data should be migrated to a
     memory node local to where the task is running. Every scan delay
     (<emphasis>numa_balancing_scan_delay_ms</emphasis>), the task
     scans the next scan size
     (<emphasis>numa_balancing_scan_size_mb</emphasis>) 
     number of pages in its address space. When the
     end of the address space is reached the scanner restarts from the beginning.
    </para>
    <para>
     Higher scan rates incur higher system overhead as page faults must be
     trapped and potentially data may need to be migrated. However, the higher the scan
     rate, the more quickly a task's memory is migrated to a local node if the
     workload pattern changes, thereby minimizing performance impact due to remote
     memory accesses. These <command>sysctls</command> control the thresholds for scan delays and
     the number of pages scanned.
    </para>
    <screen># sysctl -a | grep numa_balancing
kernel.numa_balancing = 1<co xml:id="co.numa.balancing"/>
kernel.numa_balancing_scan_delay_ms = 1000<co xml:id="co.numa.delay"/>
kernel.numa_balancing_scan_period_max_ms = 60000<co xml:id="co.numa.pmax"/>
kernel.numa_balancing_scan_period_min_ms = 1000<co xml:id="co.numa.pmin"/>
kernel.numa_balancing_scan_size_mb = 256<co xml:id="co.numa.size"/></screen>
    <calloutlist>
     <callout arearefs="co.numa.balancing">
      <para>Enables/disables <remark>dublin 2015-11-04:
        Should this say the following?: "automatic page fault-based NUMA balancing"
      </remark>automatic page fault-based NUMA memory</para>
     </callout>
     <callout arearefs="co.numa.delay">
      <para>Starting scan delay used for a task when it initially forks</para>
     </callout>
     <callout arearefs="co.numa.pmax">
      <para>Maximum time in milliseconds to scan a task's virtual memory
      </para>
     </callout>
     <callout arearefs="co.numa.pmin">
      <para>
       Minimum time in milliseconds to scan a task's virtual memory
      </para>
     </callout>
     <callout arearefs="co.numa.size">
      <para>
       How many megabytes worth of pages are scanned for a given scan
      </para>
     </callout>
    </calloutlist>
    <para>
     The main goal of automatic NUMA balancing is to reschedule tasks on the same
     node's memory, so the CPU follows the memory, or to copy the memory's pages to
     the same node, so the memory follows the CPU.
    </para>
    <warning>
     <title>Task Placement</title>
     <para>
      There are no rules to define the best place to run a task, because
      tasks could share memory with another task, so you should group such
      tasks on the same node to obtain the best performance. Check NUMA
      statistics with <command># cat /proc/vmstat | grep numa_</command>.
     </para>
    </warning>
   </sect3>
   <sect3 xml:id="vt.best.numa.topo">
    <title>Guest NUMA Topology</title>
    <para>
     &vmguest; NUMA topology can be specified using the
     <emphasis>numa</emphasis> element in the XML configuration:
    </para>
    <screen>&lt;cpu&gt;
...
  &lt;numa&gt;
    &lt;cell<co xml:id="co.numa.cell"/> id='0'<co xml:id="co.numa.id"/> cpus='0-1'<co xml:id="co.numa.cpus"/> memory='512000' unit='KiB'/&gt;
    &lt;cell id='1' cpus='2-3' memory='256000'<co xml:id="co.numa.mem"/>
    unit='KiB'<co xml:id="co.numa.unit"/> memAccess='shared'<co
    xml:id="co.numa.memaccess"/>/&gt;
  &lt;/numa&gt;
  ...
&lt;/cpu&gt;</screen>
    <calloutlist>
     <callout arearefs="co.numa.cell">
      <para>
       Each <emphasis>cell</emphasis> element specifies a NUMA cell or a NUMA node
      </para>
     </callout>
     <callout arearefs="co.numa.id">
      <para>
       All cells should have an <emphasis>id</emphasis> attribute in case
       it is necessary to refer to any cell in the code. Otherwise the 
       cells are assigned ids in ascending order starting from 0
      </para>
     </callout>
     <callout arearefs="co.numa.cpus">
      <para>
       cpus specifies the CPU or range of CPUs that are part of the node
      </para>
     </callout>
     <callout arearefs="co.numa.mem">
      <para>
       The node memory in kilobytes (i.e. blocks of 1024 bytes)
      </para>
     </callout>
     <callout arearefs="co.numa.unit">
      <para>
       Attribute to define units in which memory is specified
      </para>
     </callout>
     <callout arearefs="co.numa.memaccess">
      <para>
       Optional attribute which can control whether the memory is to be
       mapped as <option>shared</option> or <option>private</option>.
       This is valid only for hugepages-backed memory. 
      </para>
     </callout>
    </calloutlist>
    <para>
     To find where the &vmguest; has allocated its pages. use: <command>cat /proc/<replaceable>PID</replaceable>/numa_maps</command> and <command>cat /sys/fs/cgroup/memory/sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>/memory.numa_stat</command>.
    </para>
    <warning>
     <title>NUMA specification</title>
     <para>The &libvirt; guest NUMA specification is currently only available for QEMU/KVM.</para>
    </warning>
   </sect3>
   <sect3>
    <title>Cpuset Memory Policy</title>
    <para>
     There are three memory cpuset policy modes available: interleave, bind and
     preferred. 
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>interleave</emphasis>: This is a memory placement policy
       which is also know as round-robin. 
       This policy can provide substantial improvements for jobs that need
       to place <remark>dublin 2015-11-04:
         Is this word ok here, or should it be deleted, or incorporated into the sentence in some other way?
       </remark>thread local data on the corresponding node. When the interleave destination
       is not available, it will be moved to another nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>bind</emphasis>: This will place memory only on one node, which means in case of insufficient memory, the allocation will fail.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>preferred</emphasis>: This policy will apply a preference to allocate
       memory to a node, but if there is not enough space for memory on this node, it will
       fall back to another node.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     You can change the memory policy mode with the <command>cgset</command> tool:
    </para>
    <screen># cgset -r cpuset.mems=<replaceable>NODE</replaceable> sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>/emulator</screen>
    <para>To migrate pages to a node, use the <command>migratepages</command> tool:</para>
    <screen># migratepages <replaceable>PID</replaceable> <replaceable>FROM-NODE</replaceable> <replaceable>TO-NODE</replaceable></screen>
    <para>To check everything is fine. use: <command>cat /proc/<replaceable>PID</replaceable>/status | grep Cpus</command>.</para>   
    <note>
     <title>Kernel NUMA/cpuset memory policy</title>
     <para>
      For more information see <link xlink:href="https://www.kernel.org/doc/Documentation/vm/numa_memory_policy.txt">Kernel
      NUMA memory policy</link> and <link xlink:href="https://www.kernel.org/doc/Documentation/cgroups/cpusets.txt">cpusets
      memory policy</link>. Check also the <link xlink:href="https://libvirt.org/formatdomain.html#elementsNUMATuning">Libvirt
      NUMA Tuning documentation</link>.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Memory Pinning</title>
    <sect4>
     <title>non-vNUMA Guest</title>
     <para>
      On a non-vNUMA guest, pinning memory to host NUMA nodes would be done with:
     </para>
     <screen>&lt;numatune&gt;
   &lt;memory mode="strict" nodeset="0-1"/&gt;
&lt;/numatune&gt;</screen>
     <para>
      This says memory should be allocated from host <emphasis>nodes
      0</emphasis> and <emphasis>1</emphasis>. Starting the guest will fail if its memory requirements cannot be fulfilled by <emphasis>nodes 0</emphasis> and <emphasis>1</emphasis>. <command>virt-install</command> also supports this configuration with the <option>--numatune</option> option.
     </para>
    </sect4>
    <sect4>
     <title>vNUMA Guest</title>
     <para>
      If the guest has a vNUMA topology, the method is as follows:
     </para>
     <screen>&lt;numatune&gt;
   &lt;memnode cellid="0" mode="strict" nodeset="0"/&gt;
   &lt;memnode cellid="1" mode="strict" nodeset="1"/&gt;
&lt;/numatune&gt;</screen>
     <para>
      This says vNUMA <emphasis>cell 0</emphasis> gets its memory from host <emphasis>node 0</emphasis>, and <emphasis>cell 1</emphasis> from host <emphasis>node 1</emphasis>. The amount of memory in each vNUMA cell is defined as part of the topology description (see <xref linkend="vt.best.numa.topo"/>).
     </para>
    </sect4>
    <sect4>
     <title>virt-install / virt-manager</title>
     <para>
      Configuring vNUMA and pinning guest memory to host nodes is not
      supported by <command>virt-install</command> or <command>virt-manager</command>.
     </para>
    </sect4>
   </sect3>
  </sect2>
 </sect1>

 <sect1 xml:id="vt.best.multihost">
  <title>Multi-Host Configuration and Settings</title>
  <para></para>

  <sect2 xml:id="vt.best.stor.fs">
   <title>Storage and File System</title>
   <para>
   </para>
   <!--
       <sect2 id="vt.best.stor.general">
       <title>General</title>
       <para>
       Access CD/DVD -> storage pool
       </para>
       <para>
       deleting pool
       </para>
       <para>
       Brtfs and guest image
       </para>
       <para>
       qemu direct access to host drives (-drive file=)
       </para>
       </sect2>
   -->

   <sect3 xml:id="vt.best.stor.blxvsimage">
    <title>Block Devices Vs. Image Files</title>
    <para>A block device is a storage device (e.g.
    <filename>/dev/xvd<replaceable>X</replaceable></filename>,
    <filename>/dev/sd<replaceable>X</replaceable></filename>,
    <filename>/dev/hd<replaceable>X</replaceable></filename>).
    A disk image file is stored on the host or accessible over a network.
    </para>
    <table>
     <title>Block devices Vs. Disk Images</title>
     <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="10%"/>
      <colspec colnum="2" colname="2" colwidth=""/>
      <colspec colnum="1" colname="1" colwidth=""/>
      <thead>
       <row>
	<entry>
	 <para>Technology</para>
	</entry>
	<entry>
	 <para>Advantages</para>
	</entry>
	<entry>
	 <para>Drawbacks</para>
	</entry>
       </row>
      </thead>
      <tbody>
       <row>
	<entry>
	 <para>
	  Block devices
	 </para>
	</entry>
	<entry>
	 <itemizedlist>
	  <listitem><para>
	   Better performance
	  </para></listitem>
	  <listitem><para>
	   Use standard tools for administration/disk modification
	  </para></listitem>
	  <listitem><para>
	   Accessible from host (pro and con)
	  </para></listitem>
	 </itemizedlist>
	</entry>
	<entry>
	 <itemizedlist>
	  <listitem><para>
	   Live migration is complex
	  </para></listitem>
	  <listitem><para>
	   Impossible to increase capacity 
	  </para></listitem>
	 </itemizedlist>
	</entry>
       </row>
       <row>
	<entry>
	 <para>
	  Image files
	 </para>
	</entry>
	<entry>
	 <itemizedlist>
	  <listitem><para>
	   Easier system management
	  </para></listitem>
	  <listitem><para>
	   Easily move, clone, expand, back up domains
	  </para></listitem>
	  <listitem><para>
	   Comprehensive toolkit (guestfs) for image manipulation
	  </para></listitem>
	  <listitem><para>
	   Reduce overhead through sparse files
	  </para></listitem>
	  <listitem><para>
	   Fully allocate for best performance
	  </para></listitem>
	 </itemizedlist>
	</entry>
	<entry>
	 <itemizedlist>
          <listitem><para>
	   Less performance than block devices
	  </para></listitem>
	 </itemizedlist>
	</entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </sect3>
   <sect3>
    <title>NFS Storage for Images</title>
    <para>
     If your image is stored on an NFS share, you should check some server
     and client parameters to improve access to the &vmguest; image.
    </para>
    <sect4>
     <title>NFS Read/Write (Client)</title>
     <para>
      Options <option>rsize</option> and <option>wsize</option>
      specify the size of the chunks of data that the
      client and server pass back and forth to each other.
      You should ensure NFS read/write sizes are sufficiently large, especially for
      large I/O. Change the <option>rsize</option> and
      <option>wsize</option> parameter in your
      <filename>/etc/fstab</filename> by increasing the value to 16 KB,
      which will ensure that all operations can be <remark>dublin 2015-11-04:
        This does not sound right. Would it be ok to say "frozen"?
      </remark>stuck if there is any instance of hanging.     
     </para>
     <screen>nfs_server:/exported/vm_images<co xml:id="co.nfs.server"/> /mnt/images<co xml:id="co.nfs.mnt"/> nfs<co xml:id="co.nfs.nfs"/> rw<co xml:id="co.nfs.rw"/>,hard<co xml:id="co.nfs.hard"/>,sync<co xml:id="co.nfs.sync"/>, rsize=8192<co xml:id="co.nfs.rsize"/>,wsize=8192<co xml:id="co.nfs.wsize"/> 0 0</screen>
     <calloutlist>
      <callout arearefs="co.nfs.server">
       <para>
	NFS server's host name and export path name
       </para>
      </callout>
      <callout arearefs="co.nfs.mnt">
       <para>
	Where to mount the NFS exported share
       </para>
      </callout>
      <callout arearefs="co.nfs.nfs">
       <para>
	This is an <option>nfs</option> mount point
       </para>
      </callout>
      <callout arearefs="co.nfs.rw">
       <para>
	This mount point will be accessible in read/write
       </para>
      </callout>
      <callout arearefs="co.nfs.hard">
       <para>
	Determines the recovery behavior of the NFS client after an NFS request
	times out. <option>hard</option> is the best option to avoid data corruption 
       </para>
      </callout>
      <callout arearefs="co.nfs.sync">
       <para>
	Any system call that writes data to files on that mount point causes
	that data to be flushed to the server before the system call returns
	control to user space
       </para>
      </callout>
      <callout arearefs="co.nfs.rsize">
       <para>
	Maximum number of bytes in each network READ request that the NFS
	client can receive when reading data from a file on an NFS server
       </para>
      </callout>
      <callout arearefs="co.nfs.wsize">
       <para>
	Maximum number of bytes per network WRITE request that the NFS client
	can send when writing data to a file on an NFS server. 
       </para>
      </callout>
     </calloutlist>
    </sect4>
    <sect4>
     <title>NFS Threads (Server)</title>
     <para>
      Your NFS server should have enough NFS threads to handle multi-threaded
      workloads. Use the <command>nfsstat</command> tool to get some RPC statistics
      on your server:
     </para>
     <screen># nfsstat -rc
Client rpc stats:
calls      retrans    authrefrsh
6401066    198          0          0</screen>
     <para>If the <emphasis>retrans</emphasis> is equal to 0 everything is
     fine. Otherwise, the client needs to retransmit, so increase the 
     <emphasis>USE_KERNEL_NFSD_NUMBER</emphasis> variable
     in <filename>/etc/sysconfig/nfs</filename>, and adjust accordingly
     until <emphasis>retrans</emphasis> is equal to <emphasis>0</emphasis>.
     </para>
    </sect4>
   </sect3>
  </sect2>
 </sect1>

 <sect1 xml:id="vt.best.stor">
  <title>&vmguest; Images</title>
  <para></para>
  <sect2 xml:id="vt.best.stor.imageformat">
   <title>&vmguest; Image Format</title>
    <para>
     Certain storage formats which &qemu; recognizes have their origins in
     other virtualization technologies. By recognizing these formats, &qemu;
     can leverage either data stores or entire guests that were
     originally targeted to run under these other virtualization
     technologies. Some  formats are supported only in read-only
     mode, enabling either direct use of that read-only data store in a
     &qemu; guest or conversion to a fully supported &qemu; storage format
     (using <command>qemu-img</command>) which could then be used in
     read/write mode. See &sle;
     <link xlink:href="https://www.suse.com/releasenotes/x86_64/SUSE-SLES/12/#fate-317891">Release
     Notes</link> to get the list of supported formats.
    </para>
    <note>
     <para>
      It is recommended to convert the disk images to either
      raw or qcow2 to achieve good performance.
     </para>
    </note>
    <warning>
     <title>Encryption</title>
     <para>
      When you create an image, you cannot use compression (<option>-c</option>) in the output file
      with the encryption option (<option>-e</option>).
     </para>
    </warning>
    <sect3>
     <title>Raw Format</title>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
	This format is simple and easily exportable to all other emulators/hypervisors
       </para>
      </listitem>
      <listitem>
       <para>
	It provides best performance (least I/O overhead)
       </para>
      </listitem>
      <listitem>
       <para>
	If your file system supports holes (for example in ext2 or ext3 on
	Linux or NTFS on Windows*), then only the written sectors will
	reserve space
       </para>
      </listitem>
      <listitem>
       <para>
	The raw format allows to copy a &vmguest; image to a physical device
	(<command>dd if=<replaceable>vmguest.raw</replaceable> of=<replaceable>/dev/sda</replaceable></command>)
       </para>
      </listitem>
      <listitem>
       <para>
	It is byte-for-byte the same as what the &vmguest; sees, so this wastes
	a lot of space
       </para>
      </listitem>
     </itemizedlist>
    </sect3>
    <sect3>
     <title>qcow2 Format</title>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
	Use this to have smaller images (useful if your file system does not supports holes, for example on
	Windows*)
       </para>
      </listitem>
      <listitem>
       <para>
	It has optional AES encryption
       </para>
      </listitem>
      <listitem>
       <para>
	Zlib-based compression option
       </para>
      </listitem>
      <listitem>
       <para>
	Support of multiple VM snapshots (internal, external)
       </para>
      </listitem>
      <listitem>
       <para>
	Improved performance and stability
       </para>
      </listitem>
      <listitem>
       <para>
	Supports changing the backing file
       </para>
      </listitem>
      <listitem>
       <para>
	Supports consistency checks
       </para>
      </listitem>
      <listitem>
       <para>
	Less performance than raw format
       </para>
      </listitem>
     </itemizedlist>
     <note>
      <title>12-cache-size</title>
      <para>
       qcow2 can provide the same performance in random access read/write as raw format,
       but it needs a well-sized cache size. By default cache size is set to
       1 MB. This will give good performance up to a disk size of 8 GB. If you need 
       a bigger disk size, you need to adjust the cache size. For a disk size
       of 64 GB (64*1024 = 65536), you need 65536 / 8192B = 8 MB of cache
       (<option>-drive format=qcow2,12-cache-size=8M</option>).
      </para>
     </note>
     <sect4>
      <title>Cluster Size</title>
      <para>The qcow2 format offers the capability to change the cluster
      size. The value must be between 512&nbsp;KB and 2&nbsp;MB. Smaller cluster sizes can
      improve the image file size whereas larger cluster sizes
      generally provide better performance.</para>
     </sect4>
     <sect4>
      <title>Pre-allocation</title>
      <para>
       An image with preallocated metadata is initially larger but can
       improve performance when the image needs to grow.
      </para>
     </sect4>
     <sect4>
      <title>Lazy Refcounts</title>
      <para>Reference count updates are postponed with the goal of
      avoiding metadata I/O and improving performance. This is
      particularly beneficial with <option>cache=writethrough</option>, which does not
      batch metadata updates, but in case of host crash, the
      reference count tables must be rebuilt, this is done automatically
      at the next open with <command>qemu-img check -r all</command>,
      but this takes some time.</para>
     </sect4>
    </sect3>
    <sect3>
     <title>qed format</title>
     <para>
      qed is the next-generation qcow (Qemu Copy On Write). Its
      characteristics include:
     </para>
     <itemizedlist>
      <listitem>
       <para>
	Strong data integrity because of simple design 
       </para>
      </listitem>
      <listitem>
       <para>
	Retains sparseness over non-sparse channels (e.g. HTTP) 
       </para>
      </listitem>
      <listitem>
       <para>
	Supports changing the backing file
       </para>
      </listitem>
      <listitem>
       <para>
	Supports consistency checks
       </para>
      </listitem>
      <listitem>
       <para> 
	Fully asynchronous I/O path 
       </para>
      </listitem>  
      <listitem>
       <para>
	Does not support internal snapshots
       </para>
      </listitem>
      <listitem>
       <para>
	Relies on the host file system and cannot be stored on a logical volume directly
       </para>
      </listitem>
     </itemizedlist>
    </sect3>
    <sect3>
     <title>VMDK format</title>
     <para>
      VMware 3, 4, or 6 image format, for exchanging images with that product.
     </para>
    </sect3>
    <sect3>
     <title>Image Information</title>
     <para>
      Use <command>qemu-img info
      <replaceable>vmguest.img</replaceable></command> to get an image's
      information,such as:
      the format, the virtual size, the physical size, snapshots if available.
     </para>
    </sect3>
    <sect3>
     <title>qemu-img Reference</title>
     <para>
      Refer to <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html#cha.qemu.guest_inst.qemu-img.create">SLE12
      qemu-img documentation</link> for more information on the <command>qemu-img</command> tool and examples.
     </para>
    </sect3>
    <sect3 xml:id="vt.best.overlay">
     <title>Overlay Storage Image</title>
     <para>
      The qcow2 and qed formats provide a way to create a base image, but also a way
      to create available overlay disk images on top of the
      base image. A backing file is useful to be able to revert to a known
      state and discard the overlay.
     </para>
     <para>
      To create an overlay image:
     </para>
     <screen># qemu-img create -o<co xml:id="co.1.minoro"/>backing_file=vmguest.raw<co xml:id="co.1.backingfile"/>,backing_fmt=raw<co xml:id="co.1.backingfmt"/>\
     -f<co xml:id="co.1.minorf"/> qcow2 vmguest.cow<co xml:id="co.1.imagename"/></screen>
     <calloutlist>
      <callout arearefs="co.1.minoro">
       <para>use <option>-o ?</option> for an overview of available options</para>
      </callout>
      <callout arearefs="co.1.backingfile">
       <para>the backing file name</para>
      </callout>
      <callout arearefs="co.1.backingfmt">
       <para>specify the file format for the backing file</para>
      </callout>
      <callout arearefs="co.1.minorf">
       <para>specify the image format for the &vmguest;</para>
      </callout>
      <callout arearefs="co.1.imagename">
       <para>image name of the &vmguest;, it will only record the
       differences from the backing file</para>
      </callout>
     </calloutlist>
     <para>
      Now you can start your &vmguest;, use it make some changes
      etc., and the backing image will be untouched and all changes to the
      storage will be recorded in the overlay image file. The backing file will
      never be modified unless you use the <option>commit</option> monitor command (or
      <command>qemu-img commit</command>).
     </para>
     <warning>
      <title>Backing Image Path</title>
      <para>
       You should not change the path to the backing image, otherwise you will need to
       adjust it. The path is stored in the overlay image file. If you
       want to update the path, you should make a symbolic link from the
       original path to the new path and then use the
      <command>qemu-img</command> <option>rebase</option> option.</para>
      <screen># ln -sf /var/lib/images/vmguest.raw  /var/lib/images/SLE12/vmguest.raw</screen>
      <screen># qemu-img rebase<co xml:id="co.2.rebase"/>-u<co xml:id="co.2.unsafe"/> -b<co xml:id="co.2.minorb"/> /var/lib/images/vmguest.raw /var/lib/images/SLE12/vmguest.cow<co xml:id="co.2.image"/></screen>
      <calloutlist>
       <callout arearefs="co.2.rebase">
	<para>change the backing file image</para>
       </callout>
       <callout arearefs="co.2.unsafe">
        <para>use unsafe mode (see note below)</para>
       </callout>
       <callout arearefs="co.2.minorb">
	<para>specify the backing file image to use</para>
       </callout>
       <callout arearefs="co.2.image">
        <para>specify the image that will be affected</para>
       </callout>
      </calloutlist>
      <para>There are two different modes in which <option>rebase</option> can operate:</para>
      <itemizedlist>
       <listitem>
	<para>
	 <emphasis>Safe</emphasis>: This is the default mode and performs a real rebase
	 operation. The safe mode is <remark>dublin 2015-11-04: 
	 Is it ok to say "an expensive" operation? Is there any other option that
	 would be more suitable? (Perhaps "a time-consuming" or "a data-hungry"
	 operation?)</remark>an expensive operation.
	</para>
       </listitem>
       <listitem>
	<para>
	 <emphasis>Unsafe</emphasis>: The unsafe mode
	 (<option>-u</option>) only changes the backing files name and
	 the format of the file name without making any checks on the files
	 contents. You should use this mode to rename or moving a
	 backing file.
	</para>
       </listitem>
      </itemizedlist>
     </warning>
     <para>
      A common use is to initiate a new guest with the backing file. Let's
      assume we have a <filename>sle12_base.img</filename> &vmguest;
      ready to use (fresh installation without any modification). This will
      be our backing file.
      Now you need to test a new package, on an updated system and on a
      system with a different kernel.
      We can use <filename>sle12_base.img</filename> to instantiate the new &sle; &vmguest; by 
      creating a qcow2 overlay file pointing to this backing file
      (<filename>sle12_base.img</filename>).
     </para>
     <para>
      In our example we will use <filename>sle12_updated.qcow2</filename>
      for the updated system, and
      <filename>sle12_kernel.qcow2</filename>
      for the system with a different kernel.
     </para>
     <para>To create the two thin provisioned systems use the
     <command>qemu-img</command> command
     line with the <option>-b</option> option:</para>
     <screen># qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
     /var/lib/libvirt/sle12_updated.qcow2
     Formatting 'sle12_updated.qcow2', fmt=qcow2 size=17179869184
     backing_file='sle12_base.img' encryption=off cluster_size=65536 
     lazy_refcounts=off nocow=off</screen>
     <screen># qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
     /var/lib/libvirt/sle12_kernel.qcow2
     Formatting 'sle12_kernel.qcow2', fmt=qcow2 size=17179869184
     backing_file='vmguest-sle12_base.img' encryption=off cluster_size=65536 
     lazy_refcounts=off nocow=off</screen>
     <para>
      The images are now usable, and you can do your test without touching the
      initial <filename>sle12_base.img</filename> backing file, all
      changes will be stored in the new images. Moreover, you can also use
      these new images as a backing file, and create a new overlay.
     </para>
     <screen># qemu-img create -b sle12_kernel.qcow2 -f qcow2 sle12_kernel_TEST.qcow2</screen>
     <para>
      <command>qemu-img info</command> with the option
      <option>--backing-chain</option> will return all information about the
      entire backing chain recursively:
     </para>
     <screen># qemu-img info --backing-chain<co xml:id="co.3.backingchain"/> \
/var/lib/libvirt/images/sle12_kernel_TEST.qcow2
image: sle12_kernel_TEST.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: sle12_kernel.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle12_kernel.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: SLE12.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle12_base.img
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 16G
cluster_size: 65536
Format specific information:
compat: 1.1
lazy refcounts: true</screen>
     <calloutlist>
      <callout arearefs="co.3.backingchain">
       <para>will list information about backing files in a disk image chain</para>
      </callout>
     </calloutlist>
     <figure xml:id="fig.qemu-img.overlay">
      <title>Understanding Image Overlay </title>
      <mediaobject>
       <imageobject role="fo">
	<imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
	<imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </sect3>
   </sect2>

   <!--
       <sect2 id="vt.best.stor.diskio">
       <title>Disk IO Modes</title>
       <table>
       <title>Notation Conventions</title>
       <tgroup cols="2">
       <colspec colnum="1" colname="1"/>
       <colspec colnum="2" colname="2"/>
       <thead>
       <row>
       <entry>
       <para>Mode</para>
       </entry>
       <entry>
       <para>Description</para>
       </entry>
       </row>
       </thead>
       <tbody>
       <row>
       <entry>
       <para>Disk IO Modes
       </para>
       </entry>
       <entry>
       <para>Native</para>
       </entry>
       </row>
       <row>
       <entry>
       <para>kernel asynchronous IO</para>
       </entry>
       <entry>
       <para>threads</para>
       </entry>
       </row>
       <row>
       <entry>
       <para>host user-mode based threads</para>
       </entry>
       <entry>
       <para>default, 'threads' mode in SLES</para>
       </entry>
       </row>
       </tbody>
       </tgroup>
       </table>
       </sect2>
   -->


  <sect2>
   <title>Open a &vmguest; Image</title>
   <para>
    You should use <emphasis>guestfs-tools</emphasis> to access the
    file system image of your &vmguest;. If you do not have this tool installed
    you can mount it with other Linux tools, but you should avoid accessing
    an untrusted or unknown &vmguest;'s image system because this can lead to
    security issues (read <link
    xlink:href="https://www.berrange.com/posts/2013/02/20/a-reminder-why-you-should-never-mount-guest-disk-images-on-the-host-os/">D.Berrang's
    post</link> for more information).
   </para>
   <sect3>
    <title>Raw Image</title>
    <sect4>
     <title>Mounting a Raw Image</title>
     <procedure>
      <step>
       <para>
	To be able to mount the image, first you need to find a free loop
	device:
       </para>
       <screen># losetup -f<co xml:id="co.losetup.find"/>
/dev/loop1</screen>
       <calloutlist>
	<callout arearefs="co.losetup.find">
	 <para>Find first unused device</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>
	Associate the image with the loop device:
       </para>
       <screen># losetup /dev/loop1 SLE12.raw</screen>
      </step>
      <step>
       <para>
	Check everything is fine:
       </para>
       <screen># losetup -l<co xml:id="co.losetup.list"/>
NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop1         0      0         0  0    /var/lib/libvirt/images/SLE12.raw</screen>
       <calloutlist>
	<callout arearefs="co.losetup.list">
	 <para>List information about <remark>dublin 2015-11-04: 
	 Is there a word missing after "all or specified"? It seems to need a
	 noun (in the plural form).</remark>all or specified</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>
	Check the image's partitions with <command>kpartx</command>:
       </para>
       <screen># kpartx -a<co xml:id="co.kpartx.a"/> -v<co xml:id="co.kpartx.v"/> /dev/loop1
add map loop1p1 (254:1): 0 29358080 linear /dev/loop1 2048</screen>
       <calloutlist>
        <callout arearefs="co.kpartx.a">
	 <para>Add partition devmappings</para>
	</callout>
        <callout arearefs="co.kpartx.v">
	 <para>Verbose mode</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>
	You can now mount the image partition(s):
       </para>
       <screen># mkdir /mnt/sle12mount
# mount /dev/mapper/loop1p1 /mnt/sle12mount</screen>
      </step>
     </procedure>
     <note>
      <title>Raw image with LVM</title>
      <para>If your raw image contains an LVM volume group you should use
      LVM tools to mount the partition. Refer to <xref linkend="sect4.lvm.found"/></para>
     </note>
    </sect4>
    <sect4>
     <title>Unmount a Raw Image</title>
     <procedure>
      <step>
       <para>Unmount all mounted partitions</para>
       <screen># umount /mnt/sle12mount</screen>
      </step>
      <step xml:id="step.umount.raw">
       <para>Delete partition devmappings with the <command>kpartx</command> and <option>-d</option> options</para>
       <screen># kpartx -d /dev/loop1</screen>
      </step>
      <step>
       <para>Detach the devices with <command>losetup</command></para>
       <screen># losetup -d /dev/loop1</screen>
      </step>
     </procedure>
    </sect4>
   </sect3>
   <sect3>
    <title>Qcow2 Image</title>
    <sect4>
     <title>Mount a Qcow2 Image</title>
     <para>
      This procedure describes the step-by-step process you should follow to
      mount a qcow2 image.
     </para>
     <procedure>
      <step>
       <para>First you need to probe the <emphasis>nbd</emphasis> 
       (network block devices) modules.
       </para>
       <screen># modprobe nbd max_part=16<co xml:id="co.nbd.maxpart"/>
# dmesg | grep nbd
[89155.142425] nbd: registered device at major 43</screen>
       <calloutlist>
	<callout arearefs="co.nbd.maxpart">
	 <para>Number of partitions per device</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>Check that <filename>/dev/nbd0</filename> is not used, and
       connect the &vmguest; image (i.e. SLE12.qcow2) to the NBD device witht the  
       <command>qemu-nbd</command> command:</para>
       <screen># qemu-nbd -c<co xml:id="co.qemunbd.minusc"/> /dev/nbd0<co xml:id="co.qemunbd.device"/> SLE12.qcow2<co xml:id="co.qemunbd.image"/></screen>
       <calloutlist>
	<callout arearefs="co.qemunbd.minusc">
	 <para>Connect <filename>SLE12.qcow2</filename> to the local NBD device <filename>/dev/nbd0</filename></para>
	</callout>
	<callout arearefs="co.qemunbd.device">
	 <para>NBD device to use</para>
	</callout>
	<callout arearefs="co.qemunbd.image">
	 <para>&vmguest; image to use</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>
	Inform the operating system about partition table changes with 
	<command>partprobe</command>
       </para>
       <screen># partprobe /dev/nbd0 -s<co xml:id="co.partprobe.sum"/>
/dev/nbd0: msdos partitions 1 2
# dmesg | grep nbd0 | tail -1
[89699.082206]  nbd0: p1 p2</screen>
       <calloutlist>
	<callout arearefs="co.partprobe.sum">
	 <para>Print a summary of contents</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>
	Two partitions are available in the SLE12.qcow2 image:
	<filename>/dev/nbd0p1</filename> and <filename>/dev/nbd0p2</filename>.
	Before mounting this partition you need to check that there is no LVM volume
	with <command>vgscan</command>.
       </para>
       <screen># vgscan 
No volume groups found</screen>
      </step>
      <step>
       <para>
	No LVM volume has been found, so you can mount the partition with
	<command>mount</command>. Refer to <xref linkend="sect4.lvm.found"/> to see how to handle LVM volumes.
       </para>
       <screen># mkdir /mnt/nbd0p2
# mount /dev/nbd0p1 /mnt/nbd0p2</screen>
       <para>You can now access your &vmguest; file system.</para>
      </step>
     </procedure>
    </sect4>
    <sect4>
     <title>Unmount a Qcow2 Image</title>
     <procedure>
      <title>Cleanup</title>
      <step>
       <para>
	To clean up everything, use <command>umount</command> to unmount the
         partition:
       </para>
       <screen># umount /mnt/nbd0p2</screen>
      </step>
      <step xml:id="step.umount.qcow2">
       <para>
	Disconnect the image from the <filename>/dev/nbd0</filename> device
       </para>
       <screen># qemu-nbd -d<co xml:id="co.qemunbd.minusd"/> /dev/nbd0</screen>
       <calloutlist>
	<callout arearefs="co.qemunbd.minusd">
	 <para>Disconnect the specified device</para>
	</callout>
       </calloutlist>
      </step>
     </procedure>
     <note>
      <title>Free NBD devices</title>
      <para>It is pretty easy to detect whether an NBD device is free. Run the
      following command:
      </para>
      <screen># lsof /dev/nbd0
COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF  NODE NAME
qemu-nbd 15149 root   10u   BLK   43,0      0t0 47347 /dev/nbd0</screen>
      <para>If it produces a line, the NBD device is busy. This can also
      be confirmed by the presence of the
      <filename>/sys/devices/virtual/block/nbd0/pid</filename> file.</para>
     </note>
    </sect4>
   </sect3>
   <sect3 xml:id="sect4.lvm.found">
    <title>Image with LVM</title>
    <para>
     In case an LVM volume group has been found,
     <command>vgscan</command> will return nothing, until you have passed
     the <option>-v</option> (verbose) parameter.
    </para>
    <screen># vgscan -v
connect() failed on local socket: No such file or directory
Internal cluster locking initialisation failed.
WARNING: Falling back to local file-based locking.
Volume Groups with the clustered attribute will be inaccessible.
Wiping cache of LVM-capable devices
Wiping internal VG cache
Reading all physical volumes.  This may take a while...
Finding all volume groups
Finding volume group "system"
Found volume group "system" using metadata type lvm2</screen>
    <para>
     The <emphasis>system</emphasis> LVM volume group has been found on the
     system. You can get more information about this volume with <command>vgdisplay
     <replaceable>VOLUMEGROUPNAME</replaceable></command> (in our case
     <replaceable>VOLUMEGROUPNAME</replaceable> is <emphasis>system</emphasis>).
     You should activate this volume group to expose LVM partitions as devices
     so the system can mount them. Use <command>vgchange</command>:
    </para>
    <screen># vgchange -ay<co xml:id="co.lvm.a"/> -v<co xml:id="co.lvm.v"/>
connect() failed on local socket: No such file or directory
Internal cluster locking initialisation failed.
WARNING: Falling back to local file-based locking.
Volume Groups with the clustered attribute will be inaccessible.
Finding all volume groups
Finding volume group "system"
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/home
Creating system-home
Loading system-home table (254:0)
Resuming system-home (254:0)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/root
Creating system-root
Loading system-root table (254:1)
Resuming system-root (254:1)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/swap
Creating system-swap
Loading system-swap table (254:2)
Resuming system-swap (254:2)
Activated 3 logical volumes in volume group system
    3 logical volume(s) in volume group "system" now active</screen>
    <calloutlist>
     <callout arearefs="co.lvm.a">
      <para>Activate the volume group</para>
     </callout>
     <callout arearefs="co.lvm.v">
      <para>Verbose mode, without this parameter the output is quiet</para>
     </callout>
    </calloutlist>
    <para>All partitions in the volume group will be listed in the
    <filename>/dev/mapper</filename> directory. You can simply mount them now.</para>
    <screen># ls /dev/mapper/system-*
/dev/mapper/system-home  /dev/mapper/system-root /dev/mapper/system-swap

# mkdir /mnt/system-root
# mount  /dev/mapper/system-root /mnt/system-root

# ls /mnt/system-root/
bin   dev  home  lib64       mnt  proc        root  sbin     srv  tmp  var
boot  etc  lib   lost+found  opt  read-write  run   selinux  sys  usr</screen>
    <para>To clean up:</para>
    <procedure>
     <step><para>Unmount all partitions (with
     <command>umount</command>)</para>
     <screen># umount /mnt/system-root</screen>
     </step>
     <step>
      <para>Deactivate the LVM volume group (with <command>vgchange -an
      <replaceable>VOLUMEGROUPNAME</replaceable></command>)</para>
      <screen># vgchange -an -v system
connect() failed on local socket: No such file or directory
Internal cluster locking initialisation failed.
WARNING: Falling back to local file-based locking.
Volume Groups with the clustered attribute will be inaccessible.
Using volume group(s) on command line
Finding volume group "system"
Found volume group "system"
Removing system-home (254:0)
Found volume group "system"
Removing system-root (254:1)
Found volume group "system"
Removing system-swap (254:2)
Deactivated 3 logical volumes in volume group system
0 logical volume(s) in volume group "system" now active</screen>
     </step>
     <step>
      <itemizedlist>
       <listitem>
	<para>
	 In case of Qcow2 format, end the procedure from 
	 <xref linkend="step.umount.qcow2"/> (<command>qemu-nbd -d
	/dev/nbd0</command>)      </para>
       </listitem>
       <listitem>
	<para>
	 In case of Raw format, end the procedure from 
	 <xref linkend="step.umount.raw"/> (<command>kpartx -d
	 /dev/loop1</command>; <command>losetup -d /dev/loop1</command>) 
	</para>
       </listitem>
      </itemizedlist>
     </step>
    </procedure>
    <warning>
     <title>Check cleanup is OK</title>
     <para>
      You should double-check that your cleanup procedure is ok by using a
      system command like <command>losetup</command> 
       <remark>dublin 2015-11-04: Should "or" be replaced by a comma here 
         and moved to before the last item, "vgscan"? Or is there a reason
         for writing it here?</remark>or
      <command>qemu-nbd</command>, <command>mount</command>,
      <command>vgscan</command>. If this is not the case you may have trouble
      using the &vmguest; because the system image will be used in 
      
      <remark>dublin 2015-11-04: This should be changed to either "in
        different places" or "in a different place". Please choose the most
        suitable option. </remark>different place.
     </para>
    </warning>
   </sect3>
  </sect2>

  <sect2>
   <title>File System Sharing</title>
   <para>
    You can access a host directory in the &vmguest; using the <emphasis>file system</emphasis>
    element. In the following example we will share the
    <filename>/data/shared</filename> directory and mount it under the
    &vmguest;.
    Note that the <emphasis>accessmode</emphasis> parameter only works with
    <emphasis>type='mount'</emphasis> for the QEMU/KVM drive. 
     <remark>dublin 2015-11-04: Is it ok to say this?:
       Most other instances of type are for the...</remark>All other
    <emphasis>type</emphasis> are mostly available for LXC driver.
   </para>
   <screen>&lt;filesystem type='mount'<co xml:id="co.fs.mount"/> accessmode='mapped'<co xml:id="co.fs.mode"/>&gt;
   &lt;source dir='/data/shared'<co xml:id="co.fs.sourcedir"/>&gt;
   &lt;target dir='shared'<co xml:id="co.fs.targetdir"/>/&gt;
&lt;/filesystem&gt;</screen>
   <calloutlist>
    <callout arearefs="co.fs.mount">
     <para>A host directory to mount &vmguest;</para>
    </callout>
    <callout arearefs="co.fs.mode">
     <para>Access mode (the security mode) set to <emphasis>mapped</emphasis> will give access
     with the permissions of the hypervisor. Use
     <emphasis>passthrough</emphasis> to access this share with the 
     permissions of the user inside the &vmguest;</para>
    </callout>
    <callout arearefs="co.fs.sourcedir">
     <para>Path to share with the &vmguest;</para>
    </callout>
    <callout arearefs="co.fs.targetdir">
     <para>Name or label of the path for the mount command</para>
    </callout>
   </calloutlist>
   <para>
    Under the &vmguest; now you need to mount the <emphasis>target
    dir='shared'</emphasis>:       
   </para>
   <screen># mkdir /opt/mnt_shared
# mount shared -t 9p /opt/mnt_shared -o trans=virtio
# mount | grep shared shared on /opt/mnt_shared type 9p (rw,relatime,sync,dirsync,trans=virtio)</screen>
   <para>
    Read <link xlink:href="https://libvirt.org/formatdomain.html#elementsFilesystems">&libvirt;
    File System </link> and <link xlink:href="http://wiki.qemu.org/Documentation/9psetup">&qemu; 9psetup</link> for more information.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="vt.best.vm.perf">
  <title>Configurations and Settings Performed within the &vmguest;</title>
  <para></para>
  <sect2 xml:id="vt.best.perf.virtio">
   <title>Virtio Driver</title>
   <para>
    To increase &vmguest; performance it is recommended to use PV drivers,
    the host implementation is in user space, so no driver is needed in the
    host. Virtio is a virtualization standard, so the guest's device driver
    is aware that it is running in a virtual environment. Note that virtio
    used in &kvm; is different, but architecturally similar to the &xen;
    paravirtualized device drivers (like
    <link xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link> in
    a Windows* guest).
   </para>
   <note>
    <title>I/O in virtualization</title>
    <para>
     To have a better understanding of this topic refer to the 
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_vt_io.html">I/O
     Virtualization</link> section in the official Virtualization guide.
    </para>
   </note>
   <sect3>
    <title>virtio blk</title>
    <para>
     <emphasis>virtio_blk</emphasis> is the virtio block device for disk.
    </para>
    <warning>
     <title>&qemu; option</title>
     <para>
      The <option>-hd[abcd]</option> for virtio disk won't work, you must use
      <option>-drive</option> instead.
     </para>
    </warning>
    <warning>
     <title>Disk name change</title>
     <para>
      For a Linux guest, the disk will show up as <option>/dev/vd[a-z][1-9]</option>. If you
      migrate from a non-virtio disk you need to change
      <option>root=</option> in GRUB config, and regenerate the
      <filename>initrd</filename> file, otherwise the system will not be able to boot.
     </para>
    </warning>
    <para>
     Example of a virtio disk definition:
    </para>
    <screen>&lt;disk type='....' device='disk'&gt;
    ....
    &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</screen>
    <para>
     This is preferable to removing every disk block in the XML configuration
     containing <emphasis>&lt;address .*&gt;</emphasis> because it will be
     regenerated automatically.
    </para>
   </sect3>
   <sect3>
    <title>virtio net</title>
    <para>
     <emphasis>virtio_net</emphasis> is the virtio network device. The
     kernel modules should be insmoded automatically in the guest at boot
     time. You need to start the service to make the network available.
    </para>
    <screen>&lt;interface type='network'&gt;
    ...
    &lt;model type='virtio' /&gt;
&lt;/interface&gt;</screen>
   </sect3>
   <sect3 xml:id="vt.best.virtio.balloon">
    <title>virtio balloon</title>
    <para>
     The virtio balloon is used for host memory over-commits for guests.
     For Linux guests, the balloon driver runs in the guest kernel, whereas
     for Windows 
     guests,
     the balloon driver is in the VMDP package.
     <emphasis>virtio_balloon</emphasis> is a PV driver to give or take
     memory from a &vmguest;. 
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>Inflate balloon</emphasis>: Return memory from guest to host
       kernel (for kvm) or
       to hypervisor (for xen)
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Deflate balloon</emphasis>: Guest will have more available memory
      </para>
     </listitem>
    </itemizedlist>
    <para>
     It is controlled by the 
     <emphasis>currentMemory</emphasis> and <emphasis>memory</emphasis>
     options.
    </para>
    <screen>&lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;1048576&lt;/currentMemory&gt;
    [...]
    &lt;devices&gt;
    &lt;memballoon model='virtio'/&gt;
    &lt;/devices&gt;</screen>
    <para>
     You can also use <command>virsh</command> to change it:
    </para>
    <screen># virsh setmem <replaceable>DOMAIN_ID</replaceable> <replaceable>MEMORY in KB</replaceable></screen>
   </sect3>
   <sect3>
    <title>Checking virtio Presence</title>
    <para>
     You can check the virtio block pci with:
    </para>
    <screen># find /sys/devices/ -name virtio*
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2</screen>
    <para>
     To find the <remark>dublin 2015-11-04: 
     Is it ok to say this?: "block device associated with vdX:"
     </remark>block device <filename>vdX</filename> associated:
    </para>
    <screen># find /sys/devices/ -name virtio* -print  -exec ls {}/block 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2
vda</screen>
    <para>
     To get more information on the virtio block:
    </para>
    <screen># udevadm info -p /sys/devices/pci0000:00/0000:00:08.0/virtio2
P: /devices/pci0000:00/0000:00:08.0/virtio2
E: DEVPATH=/devices/pci0000:00/0000:00:08.0/virtio2
E: DRIVER=virtio_blk
E: MODALIAS=virtio:d00000002v00001AF4
E: SUBSYSTEM=virtio</screen>
    <para>
     To check all virtio drivers being used:
    </para>
    <screen># find /sys/devices/ -name virtio* -print  -exec ls -l {}/driver 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
lrwxrwxrwx 1 root root 0 Jun 17 15:48 /sys/devices/pci0000:00/0000:00:06.0/virtio0/driver -&gt; ../../../../bus/virtio/drivers/virtio_console
/sys/devices/pci0000:00/0000:00:07.0/virtio1
lrwxrwxrwx 1 root root 0 Jun 17 15:47 /sys/devices/pci0000:00/0000:00:07.0/virtio1/driver -&gt; ../../../../bus/virtio/drivers/virtio_balloon
/sys/devices/pci0000:00/0000:00:08.0/virtio2
lrwxrwxrwx 1 root root 0 Jun 17 14:35 /sys/devices/pci0000:00/0000:00:08.0/virtio2/driver -&gt; ../../../../bus/virtio/drivers/virtio_blk</screen>
   </sect3>
   <!--
       <para>
       Currently performance is much better when using a host kernel configured with <emphasis>CONFIG_HIGH_RES_TIMERS</emphasis>. Another option is use HPET/RTC and <option>-clock=</option> &qemu; option.
       </para>
   -->
   <sect3>
    <title>Find Device Driver Options</title>
    <para>Virtio devices and other drivers have various options, to list all of
    them use the <option>help</option> parameter of the<command>qemu-system-ARCH</command> command.</para>
    <screen># qemu-system-x86_64 -device virtio-net,help
virtio-net-pci.ioeventfd=on/off
virtio-net-pci.vectors=uint32
virtio-net-pci.indirect_desc=on/off
virtio-net-pci.event_idx=on/off
virtio-net-pci.any_layout=on/off
.....</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="vt.best.perf.cirrus">
   <title>Cirrus Video Driver</title>
   <para>
    To get 16-bit color, high compatibility and better performance it is
    recommended to use the <emphasis>cirrus</emphasis> video driver.
   </para>
   <note>
    <title>&libvirt;</title>
    <para>
     &libvirt; ignores the vram value because video size has been hardcoded in
     &qemu;.
    </para>
   </note>
   <screen>&lt;video&gt;
   &lt;model type='cirrus' vram='9216' heads='1'/&gt;
&lt;/video&gt;</screen>
  </sect2>

  <sect2 xml:id="vt.best.entropy">
   <title>Better Entropy</title>
   <para>
    Virtio RNG is a paravirtualized device that is exposed as a hardware RNG
    device to the guest. On the host side, it can be wired up to one of
    several sources of entropy, including a real hardware RNG device as well
    as the host's <filename>/dev/random</filename> if hardware support does not exist.
    The Linux kernel contains the guest driver for the device from version
    2.6.26 and higher.
   </para>
   <para>
    The system entropy is collected from various non-deterministic hardware
    events and is mainly used by cryptographic applications.
    The virtual random number generator device (paravirtualized device) allows the host to pass through
    entropy to &vmguest; operating systems. This results in a better entropy in the &vmguest;.
   </para>
   <para>To use the <filename>hwrng</filename>, add the device in the XML configuration:</para>
   <screen>&lt;devices&gt;
   &lt;rng model='virtio'&gt;
   &lt;backend model='random'&gt;/dev/random&lt;/backend&gt;
   &lt;/rng&gt;
&lt;/devices&gt;</screen>
   <para>
    The host now should used <filename>/dev/random</filename>:
   </para>
   <screen># lsof /dev/random
qemu-syst 4926 qemu    6r   CHR    1,8      0t0 8199 /dev/random</screen>
   <para>
    On the &vmguest; the source of entropy can be checked with <command>cat
    /sys/devices/virtual/misc/hw_random/rng_available</command> and the
    current device used for entropy 
     <remark>dublin 2015-11-04: 
     Should this say the following?: "can be checked with:"
     </remark>with:
   </para>
   <screen># cat /sys/devices/virtual/misc/hw_random/rng_current
virtio_rng.0</screen>
   <para>
    You should install the <emphasis>rn-tools</emphasis> package on the &vmguest;,
    enable the service, and start it. Under SLE12 do the following:
   </para>
   <screen># zypper in rng-tools
# systemctl enable rng-tools
# systemctl start rng-tools</screen>
  </sect2>

  <sect2 xml:id="vt.best.perf.disable">
   <title>Disable Unused Tools and Devices</title>
   <para>
    Per host, use one virtualization technology only.
    For example, do not use KVM and Containers on the same computer.
    Otherwise, you may find yourself with a reduced amount of available
    resources, increased security risk and a longer software update queue.
    Even when the amount of resources allocated to each of the technologies is
    configured carefully, the host may suffer from reduced overall availability
    and degraded performance.
   </para>
   <para>
    Minimize the amount of software and services available on hosts.
    Most default installations of operating systems are not
    optimized for VM usage. Install what you really need and remove all other
    components in the &vmguest;.
   </para>
   <para>
    Windows* Guest:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Disable the screen saver
     </para>
    </listitem>
    <listitem>
     <para>
      Remove all graphical effects
     </para>
    </listitem>
    <listitem>
     <para>
      Disable indexing of hard disks if not necessary
     </para>
    </listitem>
    <listitem>
     <para>
      Check the list of started services and disable the ones you do not need
     </para>
    </listitem>
    <listitem>
     <para>
      Check and remove all unneeded devices
     </para>
    </listitem>
    <listitem>
     <para>
      Disable system update if not needed, or configure it to avoid any delay
      while rebooting or shutting down the host
     </para>
    </listitem>
    <listitem>
     <para>
      Check the Firewall rules
     </para>
    </listitem>
    <listitem>
     <para>
      Schedule backups and anti-virus updates appropriately
     </para>
    </listitem>
    <listitem>
     <para>
      Install the 
      <link xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link>
      para-virtualized driver for best performance
     </para>
    </listitem>
    <listitem>
     <para>
      Check the operating system recommendations, such as on the
      <link xlink:href="http://windows.microsoft.com/en-us/windows/optimize-windows-better-performance#optimize-windows-better-performance=windows-7">Microsoft
      Windows* 7 better performance</link> Web page.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Linux Guest:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Remove Xorg start if not needed
     </para>
    </listitem>
    <listitem>
     <para>
      Check the list of started services and disable the ones you do not need
     </para>
    </listitem>
    <listitem>
     <para>
      Check the OS recommendations for kernel parameters that enable better
      performance
     </para>
    </listitem>
    <listitem>
     <para>
      Only install software that you really need
     </para>
    </listitem>
    <listitem>
     <para>
      Optimize the scheduling of predictable tasks (system updates, hard
      disk checks, etc.)
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="vt.best.perf.mtype">
   <title>Updating the Guest Machine Type</title>
   <para>
    &qemu; machine types define some details of the architecture that are
    particularly relevant in the context of migration and save/restore,
    where all the details of the virtual machine ABI need to be carefully
    accounted for. As changes or improvements to &qemu; are made or certain
    types of fixes are done, new machine types are created which include
    those changes. Though the older machine types used for supported
    versions of &qemu; are still valid to use, the user would do well to try
    to move to the latest machine type supported by the current release, so
    as to be able to take advantage of all the changes represented in that
    machine type.
   </para>
   <para>
    Changing the guest's machine type for a Linux guest will mostly be
    transparent, whereas for Windows* guests, it is probably a good idea to
    take a snapshot or backup of the guest in case Windows* has issues with
    the changes it detects and subsequently the user decides to revert to
    the original machine type the guest was created with.
   </para>
   <note>
    <title>Changing the machine type</title>
    <para>
     Refer to the Virtualization guide section
     <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html#sec.libvirt.config.mahcinetype.virsh">Change
     Machine Type</link> for documentation.
    </para>
   </note>
  </sect2>
 </sect1>
 
 <sect1 xml:id="vt.best.vm.setup.config">
  <title>&vmguest;-Specific Configurations and Settings</title>
  <sect2 xml:id="vt.best.acpi">
   <title>ACPI Testing</title>
   <para>
    The ability to change a &vmguest;'s state heavily depends on the
    operating system. It is very important to test this feature before any
    use of your &vmguest; in production. For example most Linux operating system disable
    this capability by default, so this requires you to enable this
    operation (mostly through Policy Kit).
   </para>
   <para>
    ACPI must be enabled in the guest for a graceful shutdown to work. To
    check if ACPI is enabled, run:
   </para>
   <screen># virsh dumpxml <replaceable>VMNAME</replaceable> | grep acpi</screen>
   <para>
    If nothing is printed, ACPI is not enabled for your machine. Use
    <command>virsh edit</command> to add the following XML under
    &lt;domain&gt;:
   </para>
   <screen>&lt;features&gt;
   &lt;acpi/&gt;
&lt;/features&gt;</screen>
   <para>
    If ACPI was enabled during a Windows* Server 20XX guest installation,
    turning it on in the &vmguest; configuration alone is not sufficient.
    See the following articles for more information:
   </para>
   <simplelist>
    <member><link xlink:href="http://support.microsoft.com/kb/314088/EN-US/"/>
    </member>
    <member><link xlink:href="http://support.microsoft.com/?kbid=309283"/>
    </member>
   </simplelist>
   <para>
    A graceful shutdown is of course always possible from within the guest
    operating system, regardless of the &vmguest;'s configuration.
   </para>
  </sect2>
  <sect2 xml:id="vt.best.guest.kbd">
   <title>Keyboard Layout</title>
   <para>
    Though it is possible to specify the keyboard layout from a
    <command>qemu-system-ARCH</command> command, it is recommended to do this
    configuration in the &libvirt; XML file. To change the keyboard layout
    while connecting to a remote &vmguest; using vnc, you should edit the
    &vmguest; XML configuration file. The XML is located at
    <filename>/etc/libvirt/<replaceable>HYPERVISOR</replaceable></filename>.
    For example, to add an "en-us" keymap, add in the <emphasis>&lt;devices&gt;</emphasis> section:
   </para>
   <screen>&lt;graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/&gt;</screen>
   <para>
    Check the vncdisplay configuration and connect to your &vmguest;:
   </para>
   <screen># virsh vncdisplay sles12 127.0.0.1:0</screen>
  </sect2>
  <sect2 xml:id="vt.best.guest.spice_default_url">
   <title>Spice default listen URL</title>
   <para>
    If no network interface other than <emphasis>lo</emphasis> is assigned
    an IPv4 address on the host, the default address the spice server will
    listen on will not work. An error like the following one will occur:
   </para>
   <screen># virsh start sles12
error: Failed to start domain sles12
error: internal error: process exited while connecting to monitor: ((null):26929): Spice-Warning **: reds.c:2330:reds_init_socket: getaddrinfo(127.0.0.1,5900): Address family for hostname not supported
2015-08-12T11:21:14.221634Z qemu-system-x86_64: failed to initialize spice server</screen>
   <para>To change this you can change the default
   <emphasis>spice_listen</emphasis> value in <filename>/etc/libvirt/qemu.conf</filename> using
   the local ipv6 address <emphasis>::1</emphasis>. The spice server listening address
   can also be changed on a per &vmguest; basis, use <command>virsh edit</command> to add
   the listen XML attribute to the <emphasis>graphics type='spice'</emphasis> element:
   </para>
   <screen>&lt;graphics type='spice' listen='::1' autoport='yes'/&gt;></screen>
  </sect2>
  <sect2>
   <title>XML to QEMU command line</title>
   <para>
    Sometimes it could be useful to get the QEMU command line to launch the
    &vmguest; from the XML file. Use <command>virsh</command> with
   <option>domxml</option>.</para>
   <screen># virsh domxml-to-native<co xml:id="co.domxml.native"/> qemu-argv<co xml:id="co.domxml.argv"/> SLE12.xml<co xml:id="co.domxml.file"/></screen>
   <calloutlist>
    <callout arearefs="co.domxml.native">
     <para>Convert the file xml in domain XML format to the native guest configuration</para>
    </callout>
    <callout arearefs="co.domxml.argv">
     <para>For QEMU/KVM hypervisor, the format argument must be qemu-argv</para>
    </callout>
    <callout arearefs="co.domxml.file">
     <para>Domain XML file to use</para>
    </callout>
   </calloutlist>
   <screen># virsh domxml-to-native qemu-argv /etc/libvirt/qemu/SLE12.xml 
   LC_ALL=C PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
   QEMU_AUDIO_DRV=none /usr/bin/qemu-system-x86_64 -name SLE12 -machine \
   pc-i440fx-2.3,accel=kvm,usb=off -cpu SandyBridge -m 4048 -realtime \
   mlock=off -smp 4,sockets=4,cores=1,threads=1 -uuid 8616d00f-5f05-4244-97cc-86aeaed8aea7 \
   -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/SLE12.monitor,server,nowait \
   -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew \
   -global kvm-pit.lost_tick_policy=discard -no-hpet \
   -no-shutdown -global PIIX4_PM.disable_s3=1 -global PIIX4_PM.disable_s4=1 \
   -boot strict=on -device ich9-usb-ehci1,id=usb,bus=pci.0,addr=0x4.0x7 \
   -device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,multifunction=on,addr=0x4 \
   -device ich9-usb-uhci2,masterbus=usb.0,firstport=2,bus=pci.0,addr=0x4.0x1 \
   -device ich9-usb-uhci3,masterbus=usb.0,firstport=4,bus=pci.0,addr=0x4.0x2 \
   -drive file=/var/lib/libvirt/images/SLE12.qcow2,if=none,id=drive-virtio-disk0,format=qcow2,cache=none \
   -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=2 \
   -drive if=none,id=drive-ide0-0-1,readonly=on,format=raw  \
   -device ide-cd,bus=ide.0,unit=1,drive=drive-ide0-0-1,id=ide0-0-1 -netdev tap,id=hostnet0  \
   -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:28:04:a9,bus=pci.0,addr=0x3,bootindex=1 \
   -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 \
   -vnc 127.0.0.1:0 -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 \
   -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on</screen>
  </sect2>
  <sect2>
   <title>Add Device to an XML Configuration</title>
   <para>
    If you want to create a new &vmguest; based on an XML file, and you
    are not familiar with XML configuration syntax, you can specify the QEMU
    command line using the special tag <emphasis>qemu:commandline</emphasis>.
    For example if you want to add a virtio-balloon-pci, add this block at the
    end of the XML configuration file (before the &lt;/domain&gt; tag).
   </para>
   <screen>&lt;qemu:commandline&gt;
    &lt;qemu:arg value='-device'/&gt;
    &lt;qemu:arg value='virtio-balloon-pci,id=balloon0'/&gt;
&lt;/qemu:commandline&gt;</screen>
  </sect2>
 </sect1>
 <!-- TODO
      <sect2 id="vt.best.guest.clock">
      <title>Clock Setting</title>
      <para>
      
      clock setting (common source)
      kvm clock and ntp   
      </para>
      Use Time Stamp Counter (TSC) as 
      clocksource (if host CPU supports TSC)

kvm-clock
hpet
acpi_pm
jiffies
tsc
</sect2>

<sect2 id="vt.guest.guest.bio">
<title>Bio-based</title>
<para>
bio-based driver on slow devices
</para>
</sect2>
 -->


 



 <!--
     <sect1 id="vt.best.snapsav">
     <title>Saving, Migrating and Snapshoting</title>
     <para>
     Migration requirements/Recomendations
     save VM and start/boot (memory invalid)
     snapshot naming importance
     avoid qemu-img snapshot
     cache mode in live migration
     guestfs and live system
     </para>
     </sect1>


<sect1 id="vt.best.security">
<title>Security consideration</title>
<para>
Connection to guest: security policy
Authentication for libvirtd and VNC need to be configured separately
&qemu; Guest Agent
The VNC TLS (set at start)
</para>
</sect1>

<sect1 id="vt.best.pcipass">
<title>pcpipass</title>
<para>
Pci device (not online!, managed/unmanaged)
howto check SR-IOV capabilities
</para>
</sect1>
 -->
 <!--
     <sect1 xml:id="vt.best.net">
     <title>Network Tips</title>
     <para>
     </para>

<sect2 xml:id="vt.best.net.vnic">


<title>Virtual NICs</title>
<para>
virtio-net (KVM) : multi-queue option
vhost-net (KVM) : Default vNIC, best performance
netbk (Xen) : kernel threads vs tasklets
</para>
</sect2>

<sect2 xml:id="vt.best.net.enic">
<title>Emulated NICs</title>
<para>
e1000: Default and preferred emulated NIC
rtl8139
</para>
</sect2>

<sect2 xml:id="vt.best.net.sharednic">
<title>Shared Physical NICs</title>
<para>
SR-IOV: macvtap
Physicial NICs : PCI pass-through
</para>
</sect2>

<sect2 xml:id="vt.best.general">
<title>Network General</title>
<para>
use multi-network to avoid congestion
admin, storage, migration ...
use arp-filter to prevent arp flux

same MTU in all devices to avoid fragmentation
yast to configure bridge
Network MAC address
bridge configuration in bridge.conf file
PCI pass-through Vfio to improve performance
</para>
</sect2>
</sect1>
 -->
 <!--
     <sect1 xml:id="vt.best.debug">
     <title>Troubleshooting/Debugging</title>
     <para>
     </para>

<sect2 xml:id="vt.best.debug.xen">
<title>Xen</title>
<para>
</para>
<sect3 xml:id="vt.best.debug.xen.log">
<title>Xen Log Files</title>
<para>
libxl logs:
<filename>/var/log/xen/*</filename>
qemu-dm-domain.log, xl-domain.log
bootloader.log, vm-install, xen-hotplug
Process specific logs, often requiring debug log levels to be useful
Some logs require 'set -x' to be added to /etc/xen/scripts/*

libvirt logs:
<filename>/var/log/libvirt/libxl</filename>
libxl-driver.log
domain.log
</para>
</sect3>
<sect3 xml:id="vt.best.debug.xen.hypervisor">
<title>Daemon and Hypervisor Logs</title>
<para>
View systemd journal for specific units/daemons: <command>journalctl
<command>journalctl [\-\-follow] unit xencommons.service</command>
<command>journalctl /usr/sbin/xenwatchdogd</command>
xl dmesg
Xen hypervisor logs
</para>
</sect3>

<sect3 xml:id="vt.best.debug.xen.loglevel">
<title>Increasing Logging Levels</title>
<para>
Log levels are increased through xen parameters:
</para>
<screen>loglvl=all</screen>
<para>
Increased logging for Xen hypervisor
</para>
<screen>guest_loglvl=all</screen>
<para>
Increased logging for guest domain actions Grub2 config:

Edit <filename>/etc/default/grub</filename>, then recreate <filename>grub.cfg</filename>:
GRUB_CMDLINE_XEN_DEFAULT=loglvl=all guest_loglvl=all
<command>grub2-mkconfig -o /boot/grub/grub.cfg</command>
</para>
</sect3>
</sect2>

<sect2 xml:id="vt.best.debug.support">
<title>Support</title>
<para></para>
<sect3 xml:id="vt.best.debug.support.config">
<title>Supportconfig and Virtualization</title>
<para>
Core files:
basic-environment.txt
Reports detected virtualization hypervisor
Under some hypervisors (xen), subsequent general checks might be incomplete

Hypervisor specific files:
kvm.txt, xen.txt
Both logs contain general information:
RPM version/verification of important packages
Kernel, hardware, network details
</para>
</sect3>

<sect3 xml:id="vt.best.debug.support.kvm">
<title>kvm.txt</title>
<para>
libvirt details
General libvirt details
Libvirt daemon status
KVM statistics
virsh version, capabilities, nodeinfo, etc...

Domain list and configurations
Conf and log files
<filename>/etc/libvirt/libvirtd.conf</filename>
Last 500 lines from <filename>/var/log/libvirt/qemu/domain.log</filename>
</para>
</sect3>

<sect3 xml:id="vt.best.debug.support.xen">
<title>xen.txt</title>
<para>
Daemon status
xencommons, xendomains and xen-watchdog daemons
grub/grub2 configuration (for xen.gz parameters)

libvirt details
Domain list and configurations

xl details
Domain list and configurations
Conf and Log files
<filename>/etc/xen/xl.conf</filename>, <filename>/etc/libvirt/libvirtd.conf</filename>
Last 500 lines from <filename>/var/log/xen/*</filename>,
<filename>/var/log/libvirt/libxl/*</filename> 
Output of <command>xl dmesg</command> and <command>xl info</command>
</para>
</sect3>
</sect2>

<sect2 xml:id="vt.best.debug.advanced">
<title>Advanced Debugging Options</title>
<para>
Serial console
</para>
<screen>GRUB_CMDLINE_XEN_DEFAULT=loglvl=all guest_loglvl=all console=com1 com1=115200,8n1</screen>
<screen>GRUB_CMDLINE_LINUX_DEFAULT=console=ttyS0,115200</screen>
<para>
Debug keys
<command>xl debug keys h; xl dmesg</command>
<command>xl debug keys q; xl dmesg</command>
Additional Xen debug tools:
<command>xenstore-{ls,read,rm,watch,write}, xentrace, xentop</command>

Capturing Guest Logs
Capturing guest logs during triggered problem:
Connect to domain:
<command>virsh console domname</command>
Execute problem command
Capturing domain boot messages
</para>
<screen>xl create -c VM config file</screen>
<screen>virsh create VM config file \-\-console</screen>
</sect2>
<sect2 xml:id="vt.best.trouble">
<title>Troubleshooting Installations</title>
<para>
virt-manager and virt-install logs:
Found in <filename>~/.cache/virt-manager</filename>

Debugging virt-manager:
<command>virt-manager \-\-no-fork</command>
Sends messages directly to screen and log file
</para>
<screen>LIBVIRT_DEBUG=1 virt-manager \-\-no-fork</screen>
<para>
See libvirt messages in <filename>/var/log/messages</filename>

Use <command>xl</command> to rule out libvirt layer
</para>
</sect2>
<sect2 xml:id="vt.best.trouble.libvirt"> 
<title>Troubleshooting Libvirt</title>
<para>
Client side troubleshooting
</para>
<screen>LIBVIRT_DEBUG=1
1: debug, 2: info, 3: warning, 4: error</screen>
<para>
Server side troubleshooting
<filename>/etc/libvirt/libvirtd.conf</filename> (restart libvirtd after changes)
log_level = 1
log_output = 1:file:/var/log/libvirtd.log
log_filters = 1:qemu 3:remote
</para>
</sect2>

<sect2 xml:id="vt.best.trouble.kernel">
<title>Kernel Cores</title>
<para>
Host cores -vs- guest domain cores
Host cores are enabled through Kdump YaST module
For Xen dom0 cores, 'crashkernel=size@offset' should be added as a Xen hypervisor parameter

Guest cores require:
on_crash[action]on_crash tag
Possible coredump actions are:
coredump-restart     Dump core, then restart the VM
coredump-destroy    Dump core, then terminate the VM
Crashes are written to:
<filename>/var/lib/libvirt/{libxl,qemu}/dump</filename>
<filename>/var/lib/xen/dump</filename>  (if using xl).
</para>
</sect2>


<sect2 xml;id="vt.best.debug.other">
<title>Other</title>
<para>
VGA trouble debug
</para>
</sect2>
</sect1>
 -->
 <sect1>
  <title>Hypervisors Vs. Containers</title>
  <para/>
  <table>
   <title>Hypervisors Vs. Containers</title>
   <tgroup cols="3">
    <colspec colnum="1" colwidth="20%"/>
    <colspec colnum="2" colwidth="30%"/>
    <colspec colnum="3" colwidth="30%"/>
    <thead>
     <row>
      <entry><para>Features</para></entry>
      <entry><para>Hypervisors</para></entry>
      <entry><para>Containers</para></entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry><para>Technologies</para></entry>
      <entry><para>Emulation of a physical computing environment</para></entry>
      <entry><para>Use kernel host</para></entry>
     </row>
     <row>
      <entry><para>System layer level</para></entry>
      <entry><para>Managed by a virtualization layer (Hypervisor)</para></entry>
      <entry><para>Rely on kernel namespaces and cgroups</para></entry>
     </row>
     <row>
      <entry><para>Level (layer)</para></entry>
      <entry><para>Hardware level</para></entry>
      <entry><para>Software level</para></entry>
     </row>
     <row>
      <entry><para>Virtualization mode available</para></entry>
      <entry><para>FV or PV</para></entry>
      <entry><para>None, only userland</para></entry>
     </row>
     <row>
      <entry><para>Security</para></entry>
      <entry><para>Strong</para></entry>
      <entry><warning><para>Security is very low</para></warning></entry>
     </row>
     <row>
      <entry><para>Confinement</para></entry>
      <entry><para>Full isolation</para></entry>
      <entry><warning><para>Host kernel (OS must be compatible with kernel version)</para></warning></entry>
     </row>
     <row>
      <entry><para>Operating system</para></entry>
      <entry><para>Any operating system</para></entry>
      <entry><para>Only Linux (must be "kernel" compatible)</para></entry>
     </row>
     <row>
      <entry><para>Type of system</para></entry>
      <entry><para>Full OS needed</para></entry>
      <entry><para>Scope is an instance of Linux</para></entry>
     </row>
     <row>
      <entry><para>Boot time</para></entry>
      <entry><para>Slow to start (OS delay)</para></entry>
      <entry><para>Really quick start</para></entry>
     </row>
     <row>
      <entry><para>Overhead</para></entry>
      <entry><para>High</para></entry>
      <entry><para>Very low</para></entry>
     </row>
     <row>
      <entry><para>Efficiency</para></entry>
      <entry><para>Depends on OS</para></entry>
      <entry><para>Very efficient</para></entry>
     </row>
     <row>
      <entry><para>Sharing with host</para></entry>
      <entry><warning><para>Complex because of isolation</para></warning></entry>
      <entry><para>Sharing is easy (host sees everything; container
      sees its <remark>dublin 2015-11-04: 
      It would be better to add a noun after this. Is it ok to say this?:
      "own objects" (If "objects" is a suitable word in the context...)
      </remark>own)</para></entry>
     </row>
     <row>
      <entry><para>Migration</para></entry>
      <entry><para>Supports migration (live mode)</para></entry>
      <entry><warning><para>Not possible</para></warning></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <sect2>
   <title>Getting the Best of Both Worlds</title>
   <para>
    Even if the above table seems to indicate that running a single application
    in a very secure way is not possible, starting with &sls; 12 SP1
    <command>virt-sandbox</command> will allow running a single application
    in a &kvm; guest. <command>virt-sandbox</command> bootstraps any command
    within a Linux kernel with a minimal root file system.
   </para>
   <para>
    The guest root file system can either be the root file system mounted read-only
    or a disk image. The following steps will show how to set up a sandbox with
    qcow2 disk image as root file system.
   </para>
   <procedure>
    <step>
     <para>Create the disk image using <command>qemu-img</command>:</para>
     <screen># qemu-img create -f qcow2 rootfs.qcow2 6G</screen>
    </step>
    <step>
     <para>Format the disk image:</para>
     <screen># modprobe nbd<co xml:id="co.vsmkfs.modprobe"/>
# /usr/bin/qemu-nbd --format qcow2 -n -c /dev/nbd0 $PWD/test-base.qcow2<co xml:id="co.vsmkfs.qemu-nbd"/>
# mkfs.ext3 /dev/nbd0<co xml:id="co.vsmkfs.do"/></screen>
     <calloutlist>
      <callout arearefs="co.vsmkfs.modprobe">
       <para>
        Make sure the nbd module is loaded: it is not loaded by default and
       will only be used to format the qcow image.</para>
      </callout>
      <callout arearefs="co.vsmkfs.qemu-nbd">
       <para>
        Create an NBD device for the qcow2 image. This device will then behave like
        any other block device. The example uses <replaceable>/dev/nbd0</replaceable>
        but any other free NBD device will work.
       </para>
      </callout>
      <callout arearefs="co.vsmkfs.do">
       <para>
        Format the disk image directly. Note that no partition table has been
        created: <remark>dublin 2015-11-04: 
        Should this text be in bold and underlined?
        (As in the first paragraph of this section 10.1)
        </remark>virt-sandbox considers the image to be a partition, not a disk.
       </para>
       <para>
        The partition formats that can be used are limited: the Linux kernel
        bootstrapping the sandbox needs to have the corresponding features
	built in.
        The ext4 module is also available at the sandbox start-up time.
       </para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>
      Now populate the newly formatted image:
     </para>
     <screen># guestmount -a base.qcow2 -m /dev/sda:/ /mnt<co
     xml:id="co.vsfs.mount"/>

# zypper --root /mnt ar cd:///?devices=/dev/dvd SLES12_DVD
# zypper --root /mnt in -t pattern Minimal<co xml:id="co.vsfs.install"/>

# guestunmount /mnt<co xml:id="co.vsfs.unmount"/></screen>
     <calloutlist>
      <callout arearefs="co.vsfs.mount">
       <para>
        Mount the qcow2 image using the <command>guestfs</command> tools.
       </para>
      </callout>
      <callout arearefs="co.vsfs.install">
       <para>
        Use zypper with the <emphasis>--root</emphasis> parameter to add a &sls;
        repository and install the Minimal pattern in the disk image. Any additional
        package or configuration change should be done in this step.
       </para>
       <note>
        <title>Using backing chains</title>
        <para>
         To share the root file system between several sandboxes, create
         qcow2 images with a common disk image as backing chain as described in the
         <link xlink:href="#vt.best.overlay">Overlay Storage Image section</link>.
        </para>
       </note> 
      </callout>
      <callout arearefs="co.vsfs.unmount">
       <para>Unmount the qcow2 image.</para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>
      Run the sandbox, using <command>virt-sandbox</command>. This command has many
      interesting options, read its man page to discover them all. It can either be
      running as root or as an unprivileged user.
     </para>
     <screen># virt-sandbox -n <replaceable>name</replaceable> \
     -m host-image:/=$PWD/rootfs.qcow2 \ <co xml:id="co.vs.rootfs"/>
     -m host-bind:/srv/www=/guests/www \ <co xml:id="co.vs.bind"/>
     -m ram:/tmp=100MiB \
     -m ram:/run=100MiB \ <co xml:id="co.vs.tmpfs"/>
     -N source=default,address=192.168.122.12/24 \ <co xml:id="co.vs.net"/>
     -- \
     <replaceable>/bin/sh</replaceable></screen>
     <calloutlist>
      <callout arearefs="co.vs.rootfs">
       <para>
        Mount the created disk image as the root file system. Note that without any
        image being mounted as <filename>/</filename>, the host root file system is
        read-only mounted as the guest one.
       </para>
       <para>
        The host-image mount is not reserved for the root file system, it can be used
        to mount any disk image anywhere in the guest.
       </para>
      </callout>
      <callout arearefs="co.vs.bind">
       <para>
        The host-bind mount is pretty convenient for sharing files and folders between the
        host and the guest. In this example the host folder <filename>/guests/www</filename>
        is mounted as <filename>/srv/www</filename> in the sandbox.
       </para>
      </callout>
      <callout arearefs="co.vs.tmpfs">
       <para>
        The ram mounts are defining <emphasis>tmpfs</emphasis> mounts in the sandbox.
       </para>
      </callout>
      <callout arearefs="co.vs.net">
       <para>
        The network uses a network defined in libvirt. When running as an unprivileged user, the
        source can be omitted, and the &kvm; user networking feature will be used. Using this
        option requires <emphasis>dhcp-client</emphasis> and <emphasis>iproute2</emphasis>, which
        is the case with the &sls; Minimal pattern.
       </para>
      </callout>
     </calloutlist>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1>
  <title>External References</title>
  <para>
  </para>
  <itemizedlist>
   <listitem><para>
    <link xlink:href="kernel.org/doc/ols/2009/ols2009-pages-19-28.pdf">Increasing memory density using KSM</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="http://www.linux-kvm.org/page/KSM">linux-kvm.org KSM</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="https://www.kernel.org/doc/Documentation/vm/ksm.txt">KSM's kernel documentation</link></para>
   </listitem>
   <listitem>
    <para><link xlink:href="https://lwn.net/Articles/329123/">ksm - dynamic page sharing driver for linux v4</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="http://www.espenbraastad.no/post/memory-ballooning/">Memory Ballooning</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="http://wiki.libvirt.org/page/Virtio">libvirt virtio</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="https://www.kernel.org/doc/Documentation/block/cfq-iosched.txt">CFQ's kernel documentation</link></para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="https://www.kernel.org/doc/Documentation/sysctl/kernel.txt">Documentation for sysctl</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/525459/">LWN Random Number</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://wiki.mikejung.biz/KVM_/_Xen">KVM Xen tweaks</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="http://events.linuxfoundation.org/sites/events/files/slides/CloudOpen2013_Khoa_Huynh_v3.pdf">Dr. Khoa
     Huynh, IBM Linux Technology Center</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="https://www.kernel.org/doc/Documentation/kernel-parameters.txt">Kernel Parameters</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://lwn.net/Articles/374424/">Huge pages
     Administration (Mel Gorman)</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt">kernel hugetlbpage</link>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</article>
