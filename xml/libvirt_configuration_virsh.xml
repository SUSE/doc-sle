<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="libvirt_configuration_virsh.xml" version="5.0" xml:id="cha-libvirt-config-virsh">
 <title>Configuring virtual machines with &virsh;</title>
 <info>
  <abstract>
   <para>
    You can use &virsh; to configure virtual machines (VM) on the command line
    as an alternative to using the &vmm;. With &virsh;, you can control the
    state of a VM, edit the configuration of a VM or even migrate a VM to
    another host. The following sections describe how to manage VMs by using
    &virsh;.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec-libvirt-config-editing-virsh">
  <title>Editing the VM configuration</title>

  <para>
   The configuration of a VM is stored in an XML file in
   <filename>/etc/libvirt/qemu/</filename> and looks like this:
  </para>

  <example>
   <title>Example XML configuration file</title>
<screen>
&lt;domain type='kvm'&gt;
  &lt;name&gt;sles15&lt;/name&gt;
  &lt;uuid&gt;ab953e2f-9d16-4955-bb43-1178230ee625&lt;/uuid&gt;
  &lt;memory unit='KiB'&gt;2097152&lt;/memory&gt;
  &lt;currentMemory unit='KiB'&gt;2097152&lt;/currentMemory&gt;
  &lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;
  &lt;os&gt;
    &lt;type arch='x86_64' machine='pc-q35-2.0'&gt;hvm&lt;/type&gt;
  &lt;/os&gt;
  &lt;features&gt;...&lt;/features&gt;
  &lt;cpu mode='custom' match='exact' check='partial'&gt;
    &lt;model fallback='allow'&gt;Skylake-Client-IBRS&lt;/model&gt;
  &lt;/cpu&gt;
  &lt;clock&gt;...&lt;/clock&gt;
  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;
  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;
  &lt;on_crash&gt;destroy&lt;/on_crash&gt;
  &lt;pm&gt;
    &lt;suspend-to-mem enabled='no'/&gt;
    &lt;suspend-to-disk enabled='no'/&gt;
  &lt;/pm&gt;
  &lt;devices&gt;
    &lt;emulator&gt;/usr/bin/qemu-system-x86_64&lt;/emulator&gt;
    &lt;disk type='file' device='disk'&gt;...&lt;/disk&gt;
  &lt;/devices&gt;
  ...
&lt;/domain&gt;
</screen>
  </example>

  <para>
   If you want to edit the configuration of a &vmguest;, check if it is
   offline:
  </para>

<screen>&prompt.sudo;<command>virsh list --inactive</command></screen>

  <para>
   If your &vmguest; is in this list, you can safely edit its configuration:
  </para>

<screen>&prompt.sudo;<command>virsh edit <replaceable>NAME_OF_VM_GUEST</replaceable></command>
    </screen>

  <para>
   Before saving the changes, &virsh; validates your input against a RelaxNG
   schema.
  </para>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-machinetype-virsh">
  <title>Changing the machine type</title>

  <para>
   When installing with the <command>virt-install</command> tool, the machine
   type for a &vmguest; is <emphasis>pc-q35</emphasis> by default. The
   machine type is stored in the &vmguest;'s configuration file in the
   <tag>type</tag> element:
  </para>

<screen>&lt;type arch='x86_64' machine='pc-q35-2.3'&gt;hvm&lt;/type&gt;</screen>

  <para>
   As an example, the following procedure shows how to change this value to the
   machine type <literal>q35</literal>. The value <literal>q35</literal> is an
   Intel* chipset and includes <xref linkend="gloss-vt-acronym-pcie"/>,
   supports up to 12 USB ports, and has support for
   <xref linkend="gloss-vt-acronym-sata"/> and
   <xref linkend="gloss-vt-acronym-iommu"/>.
   <!-- IRQ routing has also
    been improved. -->
  </para>

  <procedure>
   <title>Changing machine type</title>
   <step>
    <para>
     Check whether your &vmguest; is inactive:
    </para>
<screen>&prompt.sudo;<command>virsh list --inactive</command>
Id    Name                           State
----------------------------------------------------
-     sles15                         shut off</screen>
   </step>
   <step>
    <para>
     Edit the configuration for this &vmguest;:
    </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
   </step>
   <step>
    <para>
     Replace the value of the <tag class="attribute">machine</tag> attribute
     with <tag class="attvalue">pc-q35-2.0</tag> :
    </para>
<screen>&lt;type arch='x86_64' machine='pc-q35-2.0'&gt;hvm&lt;/type&gt;</screen>
   </step>
   <step>
    <para>
     Restart the &vmguest;:
    </para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
   </step>
   <step>
    <para>
     Check if the machine type has changed. Log in to the &vmguest; and run the
     following command:
    </para>
<screen>&prompt.sudo;<command>dmidecode | grep Product</command>
Product Name: Standard PC (Q35 + ICH9, 2009)</screen>
   </step>
  </procedure>

  <tip>
   <title>Machine type update recommendations</title>
   <para>
    Whenever the QEMU version on the host system is upgraded (for example, when
    upgrading the &vmhost; to a new service pack), upgrade the machine type of
    the &vmguest;s to the latest available version. To check, use the command
    <command>qemu-system-x86_64 -M help</command> on the &vmhost;.
   </para>
   <para>
    The default machine type <literal>pc-i440fx</literal>, for example, is
    regularly updated. If your &vmguest; still runs with a machine type of
    <literal>pc-i440fx-1.<replaceable>X</replaceable></literal>, we strongly
    recommend an update to
    <literal>pc-i440fx-2.<replaceable>X</replaceable></literal>. This allows
    taking advantage of the most recent updates and corrections in machine
    definitions, and ensures better future compatibility.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="sec-libvirt-hypervisor-features-virsh">
  <title>Configuring hypervisor features</title>

  <para>
   <command>libvirt</command> automatically enables a default set of hypervisor
   features that are sufficient in most circumstances, but also allows enabling
   and disabling features as needed. As an example, Xen does not support
   enabling PCI pass-through by default. It must be enabled with the
   <literal>passthrough</literal> setting. Hypervisor features can be
   configured with &virsh;. Look for the <tag>&lt;features&gt;</tag> element in
   the &vmguest;'s configuration file and adjust the various features as
   required. Continuing with the &xen; pass-through example:
  </para>

<screen>&prompt.sudo;virsh edit sle15sp1
 &lt;features&gt;
    &lt;xen&gt;
      &lt;passthrough/&gt;
    &lt;/xen&gt;
 &lt;/features&gt;
</screen>

  <para>
   Save your changes and restart the &vmguest;.
  </para>

  <para>
   See the <citetitle>Hypervisor features</citetitle> section of the libvirt
   <citetitle>Domain XML format</citetitle> manual at
   <link xlink:href="https://libvirt.org/formatdomain.html#elementsFeatures"/>
   for more information.
  </para>
 </sect1>
 <sect1 xml:id="libvirt-cpu-virsh">
  <title>Configuring CPU</title>

  <para>
   Many aspects of the virtual CPUs presented to &vmguest;s are configurable
   with &virsh;. The number of current and maximum CPUs allocated to a &vmguest;
   can be changed, as well as the model of the CPU and its feature set. The
   following subsections describe how to change the common CPU settings of a
   &vmguest;.
  </para>

  <sect2 xml:id="sec-libvirt-cpu-num-virsh">
   <title>Configuring the number of CPUs</title>
   <para>
    The number of allocated CPUs is stored in the &vmguest;'s XML configuration
    file in <filename>/etc/libvirt/qemu/</filename> in the
    <tag class="attribute">vcpu</tag> element:
   </para>
<screen>&lt;vcpu placement='static'&gt;1&lt;/vcpu&gt;</screen>

   <para>
    In this example, the &vmguest; has only one allocated CPU. The following
    procedure shows how to change the number of allocated CPUs for the
    &vmguest;:
   </para>

   <procedure>
    <step>
     <para>
      Check whether your &vmguest; is inactive:
     </para>
<screen>&prompt.sudo;<command>virsh list --inactive</command>
Id    Name                           State
----------------------------------------------------
-     sles15                         shut off</screen>
    </step>
    <step>
     <para>
      Edit the configuration for an existing &vmguest;:
     </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
    </step>
    <step>
     <para>
      Change the number of allocated CPUs:
     </para>
<screen>&lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;</screen>
    </step>
    <step>
     <para>
      Restart the &vmguest;:
     </para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
    </step>
    <step>
     <para>
      Check if the number of CPUs in the VM has changed.
     </para>
<screen>
&prompt.sudo;<command>virsh vcpuinfo sled15</command>
VCPU:           0
CPU:            N/A
State:          N/A
CPU time        N/A
CPU Affinity:   yy

VCPU:           1
CPU:            N/A
State:          N/A
CPU time        N/A
CPU Affinity:   yy
</screen>
    </step>
   </procedure>
   <para>
    You can also change the number of CPUs while the &vmguest; is running.
    CPUs can be hotplugged until the maximum number configured at &vmguest;
    start is reached. Likewise they can be hot-unplugged until the lower
    limit of 1 is reached. The following example demonstrates changing the
    active CPU count from 2 to pre-defined maximum of 4.
   </para>
   <procedure>
    <step>
     <para>
      Check the current live vcpu count:
     </para>
<screen>&prompt.sudo;<command>virsh vcpucount sles15 | grep live</command>
maximum      live           4
current      live           2
</screen>
    </step>
    <step>
     <para>
      Change the current, or active, number of CPUs to 4:
     </para>
<screen>&prompt.sudo;<command>virsh setvcpus sles15 --count 4 --live</command></screen>
    </step>
    <step>
     <para>
      Check that the current live vcpu count is now 4:
     </para>
<screen>&prompt.sudo;<command>virsh vcpucount sles15 | grep live</command>
maximum      live           4
current      live           4
</screen>
    </step>
   </procedure>
   <important>
     <title>Exceeding 255 CPUs</title>
     <para>
      With &kvm; it is possible to define a &vmguest; with more than 255
      CPUs, however additional configuration is necessary to start and
      run the &vmguest;. The <literal>ioapic</literal> feature needs to be tuned and an IOMMU
      device needs to be added to the &vmguest;. Below is an example
      configuration for 288 CPUs.
     </para>
<screen>
&lt;domain&gt;
 &lt;vcpu placement='static'&gt;288&lt;/vcpu&gt;
 &lt;features&gt;
  &lt;ioapic driver='qemu'/&gt;
 &lt;/features&gt;
 &lt;devices&gt;
  &lt;iommu model='intel'&gt;
   &lt;driver intremap='on' eim='on'/&gt;
  &lt;/iommu&gt;
 &lt;/devices&gt;
&lt;/domain&gt;
</screen>
   </important>
  </sect2>
  <sect2 xml:id="sec-libvirt-cpu-model-virsh">
   <title>Configuring the CPU model</title>
   <para>
    The CPU model exposed to a &vmguest; can often influence the workload
    running within it. The default CPU model is derived from a CPU mode
    known as <literal>host-model</literal>.
   </para>
<screen>&lt;cpu mode='host-model'/&gt;</screen>
   <para>
    When starting a &vmguest; with CPU mode <literal>host-model</literal>, &libvirt; will
    copy its model of the host CPU into the &vmguest; definition. The host
    CPU model and features copied to the &vmguest; definition can be
    observed in the output of the <command>virsh capabilities</command>.
   </para>
   <para>
    Another interesting CPU mode is <literal>host-passthrough</literal>.
   </para>
<screen>&lt;cpu mode='host-passthrough'/&gt;</screen>
   <para>
    When starting a &vmguest; with CPU mode <literal>host-passthrough</literal>, it is presented
    with a CPU that is exactly the same as the &vmhost; CPU. This can be
    useful when the &vmguest; workload requires CPU features not available in
    &libvirt;'s simplified <literal>host-model</literal> CPU. The <literal>host-passthrough</literal> CPU mode is
    also required in some cases, for example, when running &vmguest;s with more than
    4TB of memory. The <literal>host-passthrough</literal> CPU mode comes with the disadvantage of
    reduced migration flexibility. A &vmguest; with <literal>host-passthrough</literal> CPU mode
    can only be migrated to a &vmhost; with identical hardware.
   </para>
   <para>
    When using the <literal>host-passthrough</literal> CPU mode, it is still possible to
    disable undesirable features. The following configuration will present
    the &vmguest; with a CPU that is exactly the same as the host CPU but
    with the <literal>vmx</literal> feature disabled.
   </para>
<screen>
&lt;cpu mode='host-passthrough'&gt;
  &lt;feature policy='disable' name='vmx'/&gt;
  &lt;/cpu&gt;
</screen>
   <para>
    The <literal>custom</literal> CPU mode is another common mode used to define a normalized
    CPU that can be migrated throughout dissimilar hosts in a cluster. For
    example, in a cluster with hosts containing Nehalem, IvyBridge, and
    SandyBridge CPUs, the &vmguest; can be configured with a <literal>custom</literal> CPU
    mode that contains a Nehalem CPU model.
   </para>
<screen>
&lt;cpu mode='custom' match='exact'&gt;
  &lt;model fallback='allow'&gt;Nehalem&lt;/model&gt;
  &lt;feature policy='require' name='vme'/&gt;
  &lt;feature policy='require' name='ds'/&gt;
  &lt;feature policy='require' name='acpi'/&gt;
  &lt;feature policy='require' name='ss'/&gt;
  &lt;feature policy='require' name='ht'/&gt;
  &lt;feature policy='require' name='tm'/&gt;
  &lt;feature policy='require' name='pbe'/&gt;
  &lt;feature policy='require' name='dtes64'/&gt;
  &lt;feature policy='require' name='monitor'/&gt;
  &lt;feature policy='require' name='ds_cpl'/&gt;
  &lt;feature policy='require' name='vmx'/&gt;
  &lt;feature policy='require' name='est'/&gt;
  &lt;feature policy='require' name='tm2'/&gt;
  &lt;feature policy='require' name='xtpr'/&gt;
  &lt;feature policy='require' name='pdcm'/&gt;
  &lt;feature policy='require' name='dca'/&gt;
  &lt;feature policy='require' name='rdtscp'/&gt;
  &lt;feature policy='require' name='invtsc'/&gt;
  &lt;/cpu&gt;
</screen>
   <para>
    For more information on &libvirt;'s CPU model and topology options, see
    the <citetitle>CPU model and topology</citetitle> documentation
    at <link xlink:href="https://libvirt.org/formatdomain.html#cpu-model-and-topology"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-boot-menu-virsh">
  <title>Changing boot options</title>

  <para>
   The boot menu of the &vmguest; can be found in the <tag>os</tag> element and
   usually looks like this:
  </para>

<screen>&lt;os&gt;
  &lt;type&gt;hvm&lt;/type&gt;
  &lt;loader&gt;readonly='yes' secure='no' type='rom'/&gt;/usr/lib/xen/boot/hvmloader&lt;/loader&gt;
  &lt;nvram template='/usr/share/OVMF/OVMF_VARS.fd'/&gt;/var/lib/libvirt/nvram/guest_VARS.fd&lt;/nvram&gt;
  &lt;boot dev='hd'/&gt;
  &lt;boot dev='cdrom'/&gt;
  &lt;bootmenu enable='yes' timeout='3000'/&gt;
  &lt;smbios mode='sysinfo'/&gt;
  &lt;bios useserial='yes' rebootTimeout='0'/&gt;
  &lt;/os&gt;</screen>

  <para>
   In this example, two devices are available, <tag class="attvalue">hd</tag>
   and <tag class="attvalue">cdrom</tag> . The configuration also reflects the
   actual boot order, so the <tag class="attvalue">hd</tag> comes before the
   <tag class="attvalue">cdrom</tag> .
  </para>

  <sect2 xml:id="sec-libvirt-config-bootorder-virsh">
   <title>Changing boot order</title>
   <para>
    The &vmguest;'s boot order is represented through the order of devices in
    the XML configuration file. As the devices are interchangeable, it is
    possible to change the boot order of the &vmguest;.
   </para>
   <procedure>
    <step>
     <para>
      Open the &vmguest;'s XML configuration.
     </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
    </step>
    <step>
     <para>
      Change the sequence of the bootable devices.
     </para>
<screen>
...
&lt;boot dev='cdrom'/&gt;
&lt;boot dev='hd'/&gt;
...
      </screen>
    </step>
    <step>
     <para>
      Check if the boot order was changed successfully by looking at the boot
      menu in the BIOS of the &vmguest;.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-libvirt-config-directkernel-virsh">
   <title>Using direct kernel boot</title>
   <para>
    Direct Kernel Boot allows you to boot from a kernel and initrd stored on
    the host. Set the path to both files in the <tag>kernel</tag> and
    <tag>initrd</tag> elements:
   </para>
<screen>&lt;os&gt;
    ...
  &lt;kernel&gt;/root/f8-i386-vmlinuz&lt;/kernel&gt;
  &lt;initrd&gt;/root/f8-i386-initrd&lt;/initrd&gt;
    ...
&lt;os&gt;</screen>
   <para>
    To enable Direct Kernel Boot:
   </para>
   <procedure>
    <step>
     <para>
      Open the &vmguest;'s XML configuration:
     </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
    </step>
    <step>
     <para>
      Inside the <tag>os</tag> element, add a <tag>kernel</tag> element and the
      path to the kernel file on the host:
     </para>
<screen>...
&lt;kernel&gt;/root/f8-i386-vmlinuz&lt;/kernel&gt;
...</screen>
    </step>
    <step>
     <para>
      Add an <tag>initrd</tag> element and the path to the initrd file on the
      host:
     </para>
<screen>...
&lt;initrd&gt;/root/f8-i386-initrd&lt;/initrd&gt;
...</screen>
    </step>
    <step>
     <para>
      Start your VM to boot from the new kernel:
     </para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-memory-virsh">
  <title>Configuring memory allocation</title>

  <para>
   The amount of memory allocated for the &vmguest; can also be configured with
   &virsh;. It is stored in the <tag>memory</tag> element and defines the
   maximum allocation of memory for the &vmguest; at boot time. The optional
   <tag>currentMemory</tag> element defines the actual memory allocated to
   the &vmguest;. <tag>currentMemory</tag> can be less than <tag>memory</tag>,
   allowing for increasing (or <emphasis>ballooning</emphasis>) the memory while the &vmguest; is
   running. If <tag>currentMemory</tag> is omitted, it defaults to the same
   value as the <tag>memory</tag> element.
  </para>
  <para>
   You can adjust memory settings by editing the &vmguest; configuration,
   but be aware that changes do not take place until the next boot. The
   following steps demonstrate changing a &vmguest; to boot with 4G of
   memory, but allow later expansion to 8G:
  </para>

  <procedure>
   <step>
    <para>
     Open the &vmguest;'s XML configuration:
    </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
   </step>
   <step>
    <para>
     Search for the <tag>memory</tag> element and set to 8G:
    </para>
<screen>...
&lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
...</screen>
   </step>
   <step>
    <para>
     If the <tag>currentMemory</tag> element does not exist, add it below
     the <tag>memory</tag> element, or change its value to 4G:
    </para>
<screen>
[...]
&lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
&lt;currentMemory unit='KiB'&gt;4194304&lt;/currentMemory&gt;
[...]</screen>
   </step>
  </procedure>
  <para>
   Changing the memory allocation while the &vmguest; is running can be done
   with the <command>setmem</command> subcommand. The following example shows increasing the
   memory allocation to 8G:
  </para>

   <procedure>
   <step>
    <para>
     Check &vmguest; existing memory settings:
    </para>
<screen>&prompt.sudo;<command>virsh dominfo sles15 | grep memory</command>
Max memory:     8388608 KiB
Used memory:    4194608 KiB
</screen>
   </step>
   <step>
    <para>
     Change the used memory to 8G:
    </para>
<screen>&prompt.sudo;<command>virsh setmem sles15 8388608</command></screen>
   </step>
   <step>
    <para>
     Check the updated memory settings:
    </para>
<screen>&prompt.sudo;<command>virsh dominfo sles15 | grep memory</command>
Max memory:     8388608 KiB
Used memory:    8388608 KiB
</screen>
   </step>
  </procedure>
  <important>
   <title>Large memory &vmguest;s</title>
   <para>
    &vmguest;s with memory requirements of 4TB or more must currently
    use the <literal>host-passthrough</literal> CPU model.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-pci-virsh">
  <title>Adding a PCI device</title>

  <para>
   To assign a PCI device to &vmguest; with &virsh;, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Identify the host PCI device to assign to the &vmguest;. In the following
     example, we are assigning a DEC network card to the guest:
    </para>
<screen>&prompt.sudo;<command>lspci -nn</command>
[...]
<emphasis role="bold">03:07.0</emphasis> Ethernet controller [0200]: Digital Equipment Corporation DECchip \
21140 [FasterNet] [1011:0009] (rev 22)
[...]</screen>
    <para>
     Write down the device ID (<literal>03:07.0</literal> in this case).
    </para>
   </step>
   <step>
    <para>
     Gather detailed information about the device using <command>virsh
     nodedev-dumpxml <replaceable>ID</replaceable></command>. To get the
     <replaceable>ID</replaceable>, replace the colon and the period in the
     device ID (<literal>03:07.0</literal>) with underscores. Prefix the result
     with <quote>pci_0000_</quote>: <literal>pci_0000_03_07_0</literal>.
    </para>
<screen>&prompt.sudo;<command>virsh nodedev-dumpxml pci_0000_03_07_0</command>
&lt;device&gt;
  &lt;name&gt;pci_0000_03_07_0&lt;/name&gt;
  &lt;path&gt;/sys/devices/pci0000:00/0000:00:14.4/0000:03:07.0&lt;/path&gt;
  &lt;parent&gt;pci_0000_00_14_4&lt;/parent&gt;
  &lt;driver&gt;
    &lt;name&gt;tulip&lt;/name&gt;
  &lt;/driver&gt;
  &lt;capability type='pci'&gt;
    <emphasis role="bold">&lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;3&lt;/bus&gt;
    &lt;slot&gt;7&lt;/slot&gt;
    &lt;function&gt;0&lt;/function&gt;</emphasis>
    &lt;product id='0x0009'&gt;DECchip 21140 [FasterNet]&lt;/product&gt;
    &lt;vendor id='0x1011'&gt;Digital Equipment Corporation&lt;/vendor&gt;
    &lt;numa node='0'/&gt;
  &lt;/capability&gt;
&lt;/device&gt;</screen>
    <para>
     Write down the values for domain, bus, and function (see the previous XML
     code printed in bold).
    </para>
   </step>
   <step>
    <para>
     Detach the device from the host system prior to attaching it to the
     &vmguest;:
    </para>
<screen>&prompt.sudo;<command>virsh nodedev-detach pci_0000_03_07_0</command>
  Device pci_0000_03_07_0 detached</screen>
    <tip>
     <title>Multi-function PCI devices</title>
     <para>
      When using a multi-function PCI device that does not support FLR
      (function level reset) or PM (power management) reset, you need to detach
      all its functions from the &vmhost;. The whole device must be reset for
      security reasons. <systemitem>libvirt</systemitem> will refuse to assign
      the device if one of its functions is still in use by the &vmhost; or
      another &vmguest;.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Convert the domain, bus, slot, and function value from decimal to
     hexadecimal. In our example, domain = 0, bus = 3, slot = 7, and function =
     0. Ensure that the values are inserted in the right order:
    </para>
<screen>&prompt.user;<command>printf "&lt;address domain='0x%x' bus='0x%x' slot='0x%x' function='0x%x'/&gt;\n" 0 3 7 0</command></screen>
    <para>
     This results in:
    </para>
<screen>&lt;address domain='0x0' bus='0x3' slot='0x7' function='0x0'/&gt;</screen>
   </step>
   <step>
    <para>
     Run <command>virsh edit</command> on your domain, and add the following
     device entry in the <literal>&lt;devices&gt;</literal> section using the
     result from the previous step:
    </para>
<screen>&lt;hostdev mode='subsystem' type='pci' managed='yes'&gt;
  &lt;source&gt;
    &lt;address domain='0x0' bus='0x03' slot='0x07' function='0x0'/&gt;
  &lt;/source&gt;
&lt;/hostdev&gt;</screen>
    <tip xml:id="tip-libvirt-config-pci-virsh-managed">
     <title><literal>managed</literal> compared to <literal>unmanaged</literal></title>
     <para>
      <systemitem>libvirt</systemitem> recognizes two modes for handling PCI
      devices: they can be either <literal>managed</literal> or
      <literal>unmanaged</literal>. In the managed case,
      <systemitem>libvirt</systemitem> handles all details of unbinding the
      device from the existing driver if needed, resetting the device, binding
      it to <systemitem>vfio-pci</systemitem> before starting the domain, etc.
      When the domain is terminated or the device is removed from the domain,
      <systemitem>libvirt</systemitem> unbinds from
      <systemitem>vfio-pci</systemitem> and rebinds to the original driver in
      the case of a managed device. If the device is unmanaged, the user must
      ensure all of these management aspects of the device are done before
      assigning it to a domain, and after the device is no longer used by the
      domain.
     </para>
    </tip>
    <para>
     In the example above, the <literal>managed='yes'</literal> option means
     that the device is managed. To switch the device mode to unmanaged, set
     <literal>managed='no'</literal> in the listing above. If you do so, you
     need to take care of the related driver with the <command>virsh
     nodedev-detach</command> and <command>virsh nodedev-reattach</command>
     commands. Prior to starting the &vmguest; you need to detach the device
     from the host by running <command>virsh nodedev-detach
     pci_0000_03_07_0</command>. In case the &vmguest; is not running, you can
     make the device available for the host by running <command>virsh
     nodedev-reattach pci_0000_03_07_0</command>.
    </para>
   </step>
   <step>
    <para>
     Shut down the &vmguest; and disable &selnx; if it is running on the host.
    </para>
<screen>&prompt.sudo;<command>setsebool -P virt_use_sysfs 1</command></screen>
   </step>
   <step>
    <para>
     Start your &vmguest; to make the assigned PCI device available:
    </para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
   </step>
  </procedure>

  <important>
   <title>&slsa;11 SP4 &kvm; guests</title>
   <para>
    On a newer &qemu; machine type (pc-i440fx-2.0 or higher) with &slsa;11 SP4
    &kvm; guests, the <systemitem class="resource">acpiphp</systemitem> module
    is not loaded by default in the guest. This module must be loaded to enable
    hotplugging of disk and network devices. To load the module manually, use
    the command <command>modprobe acpiphp</command>. It is also possible to
    autoload the module by adding <literal>install acpiphp /bin/true</literal>
    to the <filename>/etc/modprobe.conf.local</filename> file.
   </para>
  </important>

  <important>
   <title>&kvm; guests using &qemu; Q35 machine type</title>
   <para>
    &kvm; guests using the &qemu; Q35 machine type have a PCI topology that
    includes a <literal>pcie-root</literal> controller and seven
    <literal>pcie-root-port</literal> controllers. The
    <literal>pcie-root</literal> controller does not support hotplugging. Each
    <literal>pcie-root-port</literal> controller supports hotplugging a single
    PCIe device. PCI controllers cannot be hotplugged, so plan accordingly and
    add more <literal>pcie-root-port</literal>s if more than seven PCIe devices
    will be hotplugged. A <literal>pcie-to-pci-bridge</literal> controller can
    be added to support hotplugging legacy PCI devices. See
    <link xlink:href="https://libvirt.org/pci-hotplug.html"/> for more
    information about PCI topology between &qemu; machine types.
   </para>
  </important>

  <sect2 xml:id="tip-libvirt-config-zpci">
   <title>&pciback; for &zseries;</title>
   <para>
    In order to support &zseries;, &qemu; extended PCI representation by
    allowing to configure extra attributes. Two more
    attributes&mdash;<option>uid</option> and <option>fid</option>&mdash;were
    added to the <literal>&lt;zpci/&gt;</literal> &libvirt; specification.
    <option>uid</option> represents user-defined identifier, while
    <option>fid</option> represents PCI function identifier. These attributes
    are optional and if you do not specify them, they are automatically
    generated with non-conflicting values.
   </para>
   <para>
    To include zPCI attribute in your domain specification, use the following
    example definition:
   </para>
<screen>
&lt;controller type='pci' index='0' model='pci-root'/&gt;
&lt;controller type='pci' index='1' model='pci-bridge'&gt;
  &lt;model name='pci-bridge'/&gt;
  &lt;target chassisNr='1'/&gt;
  &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x0'&gt;
    &lt;zpci uid='0x0001' fid='0x00000000'/&gt;
  &lt;/address&gt;
&lt;/controller&gt;
&lt;interface type='bridge'&gt;
  &lt;source bridge='virbr0'/&gt;
  &lt;model type='virtio'/&gt;
  &lt;address type='pci' domain='0x0000' bus='0x01' slot='0x01' function='0x0'&gt;
    &lt;zpci uid='0x0007' fid='0x00000003'/&gt;
  &lt;/address&gt;
&lt;/interface&gt;
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-usb-virsh">
  <title>Adding a USB device</title>

  <para>
   To assign a USB device to &vmguest; using &virsh;, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Identify the host USB device to assign to the &vmguest;:
    </para>
<screen>&prompt.sudo;<command>lsusb</command>
[...]
Bus 001 Device 003: ID <emphasis role="bold">0557:2221</emphasis> ATEN International Co., Ltd Winbond Hermon
[...]</screen>
    <para>
     Write down the vendor and product IDs. In our example, the vendor ID is
     <literal>0557</literal> and the product ID is <literal>2221</literal>.
    </para>
   </step>
   <step>
    <para>
     Run <command>virsh edit</command> on your domain, and add the following
     device entry in the <literal>&lt;devices&gt;</literal> section using the
     values from the previous step:
    </para>
<screen>&lt;hostdev mode='subsystem' type='usb'&gt;
  &lt;source startupPolicy='optional'&gt;
   <emphasis role="bold">&lt;vendor id='0557'/&gt;
   &lt;product id='2221'/&gt;</emphasis>
  &lt;/source&gt;
&lt;/hostdev&gt;</screen>
    <tip>
     <title>Vendor/product or device's address</title>
     <para>
      Instead of defining the host device with
      <tag class="emptytag">vendor</tag> and
      <tag class="emptytag">product</tag> IDs, you can use the
      <tag class="emptytag">address</tag> element as described for host PCI
      devices in <xref linkend="sec-libvirt-config-pci-virsh"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Shut down the &vmguest; and disable &selnx; if it is running on the host:
    </para>
<screen>&prompt.sudo;<command>setsebool -P virt_use_sysfs 1</command></screen>
   </step>
   <step>
    <para>
     Start your &vmguest; to make the assigned PCI device available:
    </para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-io">
  <title>Adding SR-IOV devices</title>

  <para>
   Single Root I/O Virtualization (<xref linkend="vt-io-sriov"/>) capable
   <xref linkend="gloss-vt-acronym-pcie"/> devices can replicate their
   resources, so they appear to be multiple devices. Each of these
   <quote>pseudo-devices</quote> can be assigned to a &vmguest;.
  </para>

  <para>
   <xref linkend="vt-io-sriov"/> is an industry specification that was created
   by the Peripheral Component Interconnect Special Interest Group (PCI-SIG)
   consortium. It introduces physical functions (PF) and virtual functions
   (VF). PFs are full <xref linkend="gloss-vt-acronym-pcie"/> functions used to
   manage and configure the device. PFs also can move data. VFs lack the
   configuration and management part—they only can move data and a reduced set
   of configuration functions. As VFs do not have all
   <xref linkend="gloss-vt-acronym-pcie"/> functions, the host operating system
   or the <xref linkend="gloss-vt-hypervisor"/> must support
   <xref linkend="vt-io-sriov"/> to be able to access and initialize VFs. The
   theoretical maximum for VFs is 256 per device (consequently the maximum for
   a dual-port Ethernet card would be 512). In practice this maximum is much
   lower, since each VF consumes resources.
  </para>

  <sect2 xml:id="sec-libvirt-config-io-requirements">
   <title>Requirements</title>
   <para>
    The following requirements must be met to use
    <xref linkend="vt-io-sriov"/>:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      An <xref linkend="vt-io-sriov"/>-capable network card (as of
      <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise
      Server</phrase></phrase>
      <phrase role="productnumber"><phrase os="sles;sled">15</phrase></phrase>,
      only network cards support <xref linkend="vt-io-sriov"/>)
     </para>
    </listitem>
    <listitem>
     <para>
      An AMD64/Intel 64 host supporting hardware virtualization (AMD-V or Intel
      VT-x)<phrase os="sles;sled">, see
      <xref linkend="sec-kvm-requires-hardware"/> for more information</phrase>
     </para>
    </listitem>
    <listitem>
     <para>
      A chipset that supports device assignment (AMD-Vi or Intel
      <xref linkend="gloss-vt-acronym-vtd"/>)
     </para>
    </listitem>
    <listitem>
     <para>
      &libvirt; 0.9.10 or better
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="vt-io-sriov"/> drivers must be loaded and configured on
      the host system
     </para>
    </listitem>
    <listitem>
     <para>
      A host configuration that meets the requirements listed at
      <xref linkend="ann-vt-io-require"/>
     </para>
    </listitem>
    <listitem>
     <para>
      A list of the PCI addresses of the VF(s) that will be assigned to
      &vmguest;s
     </para>
    </listitem>
   </itemizedlist>
   <tip>
    <title>Checking if a device is SR-IOV-capable</title>
    <para>
     The information whether a device is SR-IOV-capable can be obtained from
     its PCI descriptor by running <command>lspci</command>. A device that
     supports <xref linkend="vt-io-sriov"/> reports a capability similar to the
     following:
    </para>
<screen>Capabilities: [160 v1] Single Root I/O Virtualization (<xref linkend="vt-io-sriov"/>)</screen>
   </tip>
   <note>
    <title>Adding an SR-IOV device at &vmguest; creation</title>
    <para>
     Before adding an SR-IOV device to a &vmguest; when initially setting it
     up, the &vmhost; already needs to be configured as described in
     <xref linkend="sec-libvirt-config-io-config"/>.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="sec-libvirt-config-io-config">
   <title>Loading and configuring the SR-IOV host drivers</title>
   <para>
    To access and initialize VFs, an SR-IOV-capable driver needs to be loaded
    on the host system.
   </para>
   <procedure>
    <step>
     <para>
      Before loading the driver, make sure the card is properly detected by
      running <command>lspci</command>. The following example shows the
      <command>lspci</command> output for the dual-port Intel 82576NS network
      card:
     </para>
<screen>&prompt.sudo;<command>/sbin/lspci | grep 82576</command>
01:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)</screen>
     <para>
      In case the card is not detected, it is likely that the hardware
      virtualization support in the BIOS/EFI has not been enabled. To check if
      hardware virtualization support is enabled, look at the settings in the
      host's BIOS.
     </para>
    </step>
    <step>
     <para>
      Check whether the <xref linkend="vt-io-sriov"/> driver is already loaded
      by running <command>lsmod</command>. In the following example a check for
      the igb driver (for the Intel 82576NS network card) returns a result.
      That means the driver is already loaded. If the command returns nothing,
      the driver is not loaded.
     </para>
<screen>&prompt.sudo;<command>/sbin/lsmod | egrep "^igb "</command>
igb                   185649  0</screen>
    </step>
    <step>
     <para>
      Skip the following step if the driver is already loaded. If the
      <xref linkend="vt-io-sriov"/> driver is not yet loaded, the
      non-<xref linkend="vt-io-sriov"/> driver needs to be removed first,
      before loading the new driver. Use <command>rmmod</command> to unload a
      driver. The following example unloads the
      non-<xref linkend="vt-io-sriov"/> driver for the Intel 82576NS network
      card:
     </para>
<screen>&prompt.sudo;<command>/sbin/rmmod igbvf</command></screen>
    </step>
    <step>
     <para>
      Load the <xref linkend="vt-io-sriov"/> driver subsequently using the
      <command>modprobe</command> command—the VF parameter
      (<literal>max_vfs</literal>) is mandatory:
     </para>
<screen>&prompt.sudo;<command>/sbin/modprobe igb max_vfs=8</command></screen>
    </step>
   </procedure>
   <remark>Unsure if the following procedure is really needed.</remark>
   <para>
    As an alternative, you can also load the driver via SYSFS:
   </para>
   <procedure>
    <step>
     <para>
      Find the PCI ID of the physical NIC by listing Ethernet devices:
     </para>
<screen>&prompt.sudo;<command>lspci | grep Eth</command>
06:00.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:00.1 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)</screen>
    </step>
    <step>
     <para>
      To enable VFs, echo the number of desired VFs to load to the
      <literal>sriov_numvfs</literal> parameter:
     </para>
<screen>&prompt.sudo;<command>echo 1 &gt; /sys/bus/pci/devices/0000:06:00.1/sriov_numvfs</command></screen>
    </step>
    <step>
     <para>
      Verify that the VF NIC was loaded:
     </para>
<screen>&prompt.sudo;<command>lspci | grep Eth</command>
06:00.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:00.1 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:08.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)</screen>
    </step>
    <step>
     <para>
      Obtain the maximum number of VFs available:
     </para>
<screen>&prompt.sudo;<command>lspci -vvv -s 06:00.1 | grep 'Initial VFs'</command>
                       Initial VFs: 32, Total VFs: 32, Number of VFs: 0,
Function Dependency Link: 01</screen>
    </step>
    <step>
     <para>
      Create a <filename>/etc/systemd/system/before.service</filename> file
      which loads VF via SYSFS on boot:
     </para>
<screen>[Unit]
Before=
[Service]
Type=oneshot
RemainAfterExit=true
ExecStart=/bin/bash -c "echo 1 &gt; /sys/bus/pci/devices/0000:06:00.1/sriov_numvfs"
# beware, executable is run directly, not through a shell, check the man pages
# systemd.service and systemd.unit for full syntax
[Install]
# target in which to start the service
WantedBy=multi-user.target
#WantedBy=graphical.target</screen>
    </step>
    <step>
     <para>
      Prior to starting the VM, it is required to create another service file
      (<filename>after-local.service</filename>) pointing to the
      <filename>/etc/init.d/after.local</filename> script that detaches the
      NIC. Otherwise the VM would fail to start:
     </para>
<screen>[Unit]
Description=/etc/init.d/after.local Compatibility
After=libvirtd.service
Requires=libvirtd.service
[Service]
Type=oneshot
ExecStart=/etc/init.d/after.local
RemainAfterExit=true

[Install]
WantedBy=multi-user.target</screen>
    </step>
    <step>
     <para>
      Copy it to <filename>/etc/systemd/system</filename>.
     </para>
<screen>#! /bin/sh
# ...
virsh nodedev-detach pci_0000_06_08_0</screen>
     <para>
      Save it as <filename>/etc/init.d/after.local</filename>.
     </para>
    </step>
    <step>
     <para>
      Reboot the machine and check if the SR-IOV driver is loaded by re-running
      the <command>lspci</command> command from the first step of this
      procedure. If the SR-IOV driver was loaded successfully you should see
      additional lines for the VFs:
     </para>
<screen>01:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
[...]
04:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
04:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
04:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
[...]</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-libvirt-config-io-attach">
   <title>Adding a VF network device to a &vmguest;</title>
   <para>
    When the <xref linkend="vt-io-sriov"/> hardware is properly set up on the
    &vmhost;, you can add VFs to &vmguest;s. To do so, you need to collect some
    data first.
   </para>
   <procedure>
    <title>Adding a VF network device to an existing &vmguest;</title>
    <para>
     The following procedure uses example data. Make sure to replace it by
     appropriate data from your setup.
    </para>
    <step>
     <para>
      Use the <command>virsh nodedev-list</command> command to get the PCI
      address of the VF you want to assign and its corresponding PF. Numerical
      values from the <command>lspci</command> output shown in
      <xref linkend="sec-libvirt-config-io-config"/> (for example
      <literal>01:00.0</literal> or <literal>04:00.1</literal>) are transformed
      by adding the prefix "pci_0000_" and by replacing colons and dots with
      underscores. So a PCI ID listed as "04:00.0" by <command>lspci</command>
      is listed as "pci_0000_04_00_0" by virsh. The following example lists the
      PCI IDs for the second port of the Intel 82576NS network card:
     </para>
<screen>&prompt.sudo;<command>virsh nodedev-list | grep 0000_04_</command>
<emphasis role="bold">pci_0000_04_00_0</emphasis>
<emphasis role="bold">pci_0000_04_00_1</emphasis>
pci_0000_04_10_0
pci_0000_04_10_1
pci_0000_04_10_2
pci_0000_04_10_3
pci_0000_04_10_4
pci_0000_04_10_5
pci_0000_04_10_6
pci_0000_04_10_7
pci_0000_04_11_0
pci_0000_04_11_1
pci_0000_04_11_2
pci_0000_04_11_3
pci_0000_04_11_4
pci_0000_04_11_5</screen>
     <para>
      The first two entries represent the <emphasis role="bold">PFs</emphasis>,
      whereas the other entries represent the VFs.
     </para>
    </step>
    <step>
     <para>
      Run the following <command>virsh nodedev-dumpxml</command> command on the
      PCI ID of the VF you want to add:
     </para>
<screen>&prompt.sudo;<command>virsh nodedev-dumpxml pci_0000_04_10_0</command>
&lt;device&gt;
  &lt;name&gt;pci_0000_04_10_0&lt;/name&gt;
  &lt;parent&gt;pci_0000_00_02_0&lt;/parent&gt;
  &lt;capability type='pci'&gt;
    <emphasis role="bold">&lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;4&lt;/bus&gt;
    &lt;slot&gt;16&lt;/slot&gt;
    &lt;function&gt;0&lt;/function&gt;</emphasis>
    &lt;product id='0x10ca'&gt;82576 Virtual Function&lt;/product&gt;
    &lt;vendor id='0x8086'&gt;Intel Corporation&lt;/vendor&gt;
    &lt;capability type='phys_function'&gt;
      &lt;address domain='0x0000' bus='0x04' slot='0x00' function='0x0'/&gt;
    &lt;/capability&gt;
  &lt;/capability&gt;
&lt;/device&gt;</screen>
     <para>
      The following data is needed for the next step:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        <literal>&lt;domain&gt;0&lt;/domain&gt;</literal>
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>&lt;bus&gt;4&lt;/bus&gt;</literal>
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>&lt;slot&gt;16&lt;/slot&gt;</literal>
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>&lt;function&gt;0&lt;/function&gt;</literal>
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      Create a temporary XML file (for example
      <filename>/tmp/vf-interface.xml</filename> containing the data necessary
      to add a VF network device to an existing &vmguest;. The minimal content
      of the file needs to look like the following:
     </para>
<screen>&lt;interface type='hostdev'&gt;<co xml:id="sriov-iface"/>
 &lt;source&gt;
  &lt;address type='pci' domain='0' bus='11' slot='16' function='0'2/&gt;<co xml:id="sriov-data"/>
 &lt;/source&gt;
&lt;/interface&gt;</screen>
     <calloutlist>
      <callout arearefs="sriov-iface">
       <para>
        VFs do not get a fixed MAC address; it changes every time the host
        reboots. When adding network devices the <quote>traditional</quote> way
        with <tag class="attribute">hostdev</tag>, it would require to
        reconfigure the &vmguest;'s network device after each reboot of the
        host, because of the MAC address change. To avoid this kind of problem,
        &libvirt; introduced the <tag class="attvalue">hostdev</tag> value,
        which sets up network-specific data <emphasis>before</emphasis>
        assigning the device.
       </para>
      </callout>
      <callout arearefs="sriov-data">
       <para>
        Specify the data you acquired in the previous step here.
       </para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>
      In case a device is already attached to the host, it cannot be attached
      to a &vmguest;. To make it available for guests, detach it from the host
      first:
     </para>
<screen>&prompt.sudo;<command>virsh nodedev-detach pci_0000_04_10_0</command></screen>
    </step>
    <step>
     <para>
      Add the VF interface to an existing &vmguest;:
     </para>
<screen>&prompt.sudo;<command>virsh attach-device <replaceable>GUEST</replaceable> /tmp/vf-interface.xml --<replaceable>OPTION</replaceable></command></screen>
     <para>
      <replaceable>GUEST</replaceable> needs to be replaced by the domain name,
      ID or UUID of the &vmguest;. --<replaceable>OPTION</replaceable> can be
      one of the following:
     </para>
     <variablelist>
      <varlistentry>
       <term><option>--persistent</option></term>
       <listitem>
        <para>
         This option will always add the device to the domain's persistent XML.
         In addition, if the domain is running, it will be hotplugged.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>--config</option></term>
       <listitem>
        <para>
         This option will only affect the persistent XML, even if the domain is
         running. The device will only show up in the &vmguest; on next boot.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>--live</option></term>
       <listitem>
        <para>
         This option will only affect a running domain. If the domain is
         inactive, the operation will fail. The device is not persisted in the
         XML and will not be available in the &vmguest; on next boot.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>--current</option></term>
       <listitem>
        <para>
         This option affects the current state of the domain. If the domain is
         inactive, the device is added to the persistent XML and will be
         available on next boot. If the domain is active, the device is
         hotplugged but not added to the persistent XML.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </step>
    <step>
     <para>
      To detach a VF interface, use the <command>virsh detach-device</command>
      command, which also takes the options listed above.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="libvirt-config-io-pool">
   <title>Dynamic allocation of VFs from a pool</title>
   <para>
    If you define the PCI address of a VF into a &vmguest;'s configuration
    statically as described in <xref linkend="sec-libvirt-config-io-attach"/>,
    it is hard to migrate such guest to another host. The host must have
    identical hardware in the same location on the PCI bus, or the &vmguest;
    configuration must be modified prior to each start.
   </para>
   <para>
    Another approach is to create a &libvirt; network with a device pool that
    contains all the VFs of an <xref linkend="vt-io-sriov"/> device. The
    &vmguest; then references this network, and each time it is started, a
    single VF is dynamically allocated to it. When the &vmguest; is stopped,
    the VF is returned to the pool, available for another guest.
   </para>
   <sect3 xml:id="libvirt-config-io-pool-host">
    <title>Defining network with pool of VFs on &vmhost;</title>
    <para>
     The following example of network definition creates a pool of all VFs for
     the <xref linkend="vt-io-sriov"/> device with its physical function (PF)
     at the network interface <literal>eth0</literal> on the host:
    </para>
<screen>&lt;network&gt;
  &lt;name&gt;passthrough&lt;/name&gt;
    &lt;forward mode='hostdev' managed='yes'&gt;
      &lt;pf dev='eth0'/&gt;
    &lt;/forward&gt;
  &lt;/network&gt;</screen>
    <para>
     To use this network on the host, save the above code to a file, for
     example <filename>/tmp/passthrough.xml</filename>, and execute the
     following commands. Remember to replace <literal>eth0</literal> with the
     real network interface name of your <xref linkend="vt-io-sriov"/> device's
     PF:
    </para>
<screen>&prompt.sudo;<command>virsh net-define /tmp/passthrough.xml</command>
&prompt.sudo;<command>virsh net-autostart passthrough</command>
&prompt.sudo;<command>virsh net-start passthrough</command></screen>
   </sect3>
   <sect3 xml:id="libvirt-config-io-pool-guest">
    <title>Configuring &vmguest;s to use VF from the pool</title>
    <para>
     The following example of &vmguest; device interface definition uses a VF
     of the <xref linkend="vt-io-sriov"/> device from the pool created in
     <xref linkend="libvirt-config-io-pool-host"/>. &libvirt; automatically
     derives the list of all VFs associated with that PF the first time the
     guest is started.
    </para>
<screen>&lt;interface type='network'&gt;
  &lt;source network='passthrough'&gt;
&lt;/interface&gt;</screen>
    <para>
     After the first &vmguest; starts that uses the network with the pool of
     VFs, verify the list of associated VFs. Do so by running <command>virsh
     net-dumpxml passthrough</command> on the host.
    </para>
<screen>&lt;network connections='1'&gt;
  &lt;name&gt;passthrough&lt;/name&gt;
  &lt;uuid&gt;a6a26429-d483-d4ed-3465-4436ac786437&lt;/uuid&gt;
  &lt;forward mode='hostdev' managed='yes'&gt;
    &lt;pf dev='eth0'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x1'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x3'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x5'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x7'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x1'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x3'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x5'/&gt;
  &lt;/forward&gt;
  &lt;/network&gt;</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="libvirt-config-listing-host-devs">
  <title>Listing attached devices</title>

  <para>
   Although there is no mechanism in &virsh; to list all &vmhost;'s devices
   that have already been attached to its &vmguest;s, you can list all devices
   attached to a specific &vmguest; by running the following command:
  </para>

<screen>virsh dumpxml <replaceable>VMGUEST_NAME</replaceable> | xpath -e /domain/devices/hostdev</screen>

  <para>
   For example:
  </para>

<screen>
&prompt.sudo;virsh dumpxml sles12 | -e xpath /domain/devices/hostdev
Found 2 nodes:
-- NODE --
&lt;hostdev mode="subsystem" type="pci" managed="yes">
  &lt;driver name="xen" />
  &lt;source>
    &lt;address domain="0x0000" bus="0x0a" slot="0x10" function="0x1" />
  &lt;/source>
  &lt;address type="pci" domain="0x0000" bus="0x00" slot="0x0a" function="0x0" />
  &lt;/hostdev>
-- NODE --
&lt;hostdev mode="subsystem" type="pci" managed="yes">
  &lt;driver name="xen" />
  &lt;source>
    &lt;address domain="0x0000" bus="0x0a" slot="0x10" function="0x2" />
  &lt;/source>
  &lt;address type="pci" domain="0x0000" bus="0x00" slot="0x0b" function="0x0" />
&lt;/hostdev>
</screen>

  <tip xml:id="libvirt-config-listing-host-devs-sriov">
   <title>Listing SR-IOV devices attached via <literal>&lt;interface type='hostdev'&gt;</literal></title>
   <para>
    For SR-IOV devices that are attached to the &vmhost; by means of
    <literal>&lt;interface type='hostdev'&gt;</literal>, you need to use a
    different XPath query:
   </para>
<screen>virsh dumpxml <replaceable>VMGUEST_NAME</replaceable> | xpath -e /domain/devices/interface/@type</screen>
  </tip>
 </sect1>
 <sect1 xml:id="libvirt-config-storage-virsh">
  <title>Configuring storage devices</title>

  <para>
   Storage devices are defined within the <tag>disk</tag> element. The usual
   <tag>disk</tag> element supports several attributes. The following two
   attributes are the most important:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The <tag class="attribute">type</tag> attribute describes the source of
     the virtual disk device. Valid values are <tag class="attvalue">file</tag>
     , <tag class="attvalue">block</tag> , <tag class="attvalue">dir</tag> ,
     <tag class="attvalue">network</tag> , or
     <tag class="attvalue">volume</tag> .
    </para>
   </listitem>
   <listitem>
    <para>
     The <tag class="attribute">device</tag> attribute indicates how the disk
     is exposed to the &vmguest; OS. As an example, possible values can include
     <tag
      class="attvalue">floppy</tag> ,
     <tag class="attvalue">disk</tag> , <tag class="attvalue">cdrom</tag> , and
     others.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The following child elements are the most important:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <tag>driver</tag> contains the driver and the bus. These are used by the
     &vmguest; to work with the new disk device.
    </para>
   </listitem>
   <listitem>
    <para>
     The <tag>target</tag> element contains the device name under which the new
     disk is shown in the &vmguest;. It also contains the optional bus
     attribute, which defines the type of bus on which the new disk should
     operate.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The following procedure shows how to add storage devices to the &vmguest;:
  </para>

  <procedure>
   <step>
    <para>
     Edit the configuration for an existing &vmguest;:
    </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
   </step>
   <step>
    <para>
     Add a <tag>disk</tag> element inside the <tag>disk</tag> element together
     with the attributes <tag class="attvalue">type</tag> and
     <tag class="attvalue">device</tag>:
    </para>
<screen>&lt;disk type='file' device='disk'&gt;</screen>
   </step>
   <step>
    <para>
     Specify a <tag>driver</tag> element and use the default values:
    </para>
<screen>&lt;driver name='qemu' type='qcow2'/&gt;</screen>
   </step>
   <step>
    <para>
     Create a disk image, which will be used as a source for the new virtual
     disk device:
    </para>
<screen>&prompt.sudo;<command>qemu-img create -f qcow2 /var/lib/libvirt/images/sles15.qcow2 32G</command></screen>
   </step>
   <step>
    <para>
     Add the path for the disk source:
    </para>
<screen>&lt;source file='/var/lib/libvirt/images/sles15.qcow2'/&gt;</screen>
   </step>
   <step>
    <para>
     Define the target device name in the &vmguest; and the bus on which the
     disk should work:
    </para>
<screen>&lt;target dev='vda' bus='virtio'/&gt;</screen>
   </step>
   <step>
    <para>
     Restart your VM:
    </para>
<screen>&prompt.sudo;<command>virsh start sles15</command></screen>
   </step>
  </procedure>

  <para>
   Your new storage device should be available in the &vmguest; OS.
  </para>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-controllers-virsh">
  <title>Configuring controller devices</title>

  <para>
   <command>libvirt</command> generally manages controllers automatically based
   on the type of virtual devices used by the &vmguest;. If the &vmguest;
   contains PCI and SCSI devices, PCI and SCSI controllers will be created and
   managed automatically. <command>libvirt</command> will also model
   controllers that are hypervisor-specific, for example, a
   <literal>virtio-serial</literal> controller for KVM &vmguest;s or a
   <literal>xenbus</literal> controller for Xen &vmguest;s. Although the
   default controllers and their configuration are generally fine, there may be
   use cases where controllers or their attributes need to be adjusted
   manually. For example, a virtio-serial controller may need more ports, or a
   xenbus controller may need more memory or more virtual interrupts.
  </para>

  <para>
   The xenbus controller is unique in that it serves as the controller for all
   Xen paravirtual devices. If a &vmguest; has many disk and/or network
   devices, the controller may need more memory. Xen's
   <literal>max_grant_frames</literal> attribute sets how many grant frames, or
   blocks of shared memory, are allocated to the <literal>xenbus</literal>
   controller for each &vmguest;.
  </para>

  <para>
   The default of 32 is enough in most circumstances, but a &vmguest; with a
   large number of I/O devices and an I/O-intensive workload may experience
   performance issues because of grant frame exhaustion. The
   <command>xen-diag</command> can be used to check the current and maximum
   <literal>max_grant_frames</literal> values for dom0 and your &vmguest;s. The
   &vmguest;s must be running:
  </para>

<screen>&prompt.sudo;virsh list
 Id   Name             State
--------------------------------
 0    Domain-0         running
 3    sle15sp1         running

 &prompt.sudo;xen-diag gnttab_query_size 0
domid=0: nr_frames=1, max_nr_frames=256

&prompt.sudo;xen-diag gnttab_query_size 3
domid=3: nr_frames=3, max_nr_frames=32
</screen>

  <para>
   The <literal>sle15sp1</literal> guest is using only three frames out of 32.
   If you are seeing performance issues, and log entries that point to
   insufficient frames, increase the value with &virsh;. Look for the
   <literal>&lt;controller type='xenbus'&gt;</literal> line in the guest's
   configuration file, and add the <literal>maxGrantFrames</literal> control
   element:
  </para>

<screen>&prompt.sudo;virsh edit sle15sp1
 &lt;controller type='xenbus' index='0' maxGrantFrames='40'/>
</screen>

  <para>
   Save your changes and restart the guest. Now it should show your change:
  </para>

<screen>&prompt.sudo;xen-diag gnttab_query_size 3
domid=3: nr_frames=3, max_nr_frames=40
</screen>

  <para>
   Similar to maxGrantFrames, the xenbus controller also supports
   <option>maxEventChannels</option>. Event channels are like paravirtual
   interrupts, and in conjunction with grant frames, form a data transfer
   mechanism for paravirtual drivers. They are also used for inter-processor
   interrupts. &vmguest;s with a large number of vCPUs and/or many paravirtual
   devices may need to increase the maximum default value of 1023.
   maxEventChannels can be changed similarly to maxGrantFrames:
  </para>

<screen>&prompt.sudo;virsh edit sle15sp1
 &lt;controller type='xenbus' index='0' maxGrantFrames='128' maxEventChannels='2047'/>
</screen>

  <para>
   See the <citetitle>Controllers</citetitle> section of the libvirt
   <citetitle>Domain XML format</citetitle> manual at
   <link xlink:href="https://libvirt.org/formatdomain.html#elementsControllers"/>
   for more information.
  </para>
 </sect1>
 <sect1 xml:id="libvirt-video-virsh">
  <title>Configuring video devices</title>

  <para>
   When using the Virtual Machine Manager, only the Video device model can be
   defined. The amount of allocated VRAM or 2D/3D acceleration can only be
   changed in the XML configuration.
  </para>

  <sect2 xml:id="libvirt-video-vram-virsh">
   <title>Changing the amount of allocated VRAM</title>
   <procedure>
    <step>
     <para>
      Edit the configuration for an existing &vmguest;:
     </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
    </step>
    <step>
     <para>
      Change the size of the allocated VRAM:
     </para>
<screen>&lt;video&gt;
&lt;model type='vga' vram='65535' heads='1'&gt;
...
&lt;/model&gt;
&lt;/video&gt;</screen>
    </step>
    <step>
     <para>
      Check if the amount of VRAM in the VM has changed by looking at the
      amount in the Virtual Machine Manager.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="libvirt-video-accel-virsh">
   <title>Changing the state of 2D/3D acceleration</title>
   <procedure>
    <step>
     <para>
      Edit the configuration for an existing &vmguest;:
     </para>
<screen>&prompt.sudo;<command>virsh edit sles15</command></screen>
    </step>
    <step>
     <para>
      To enable/disable 2D/3D acceleration, change the value of
      <literal>accel3d</literal> and <literal>accel2d</literal> accordingly:
     </para>
<screen>
&lt;video&gt;
 &lt;model&gt;
  &lt;acceleration accel3d='yes' accel2d='no'&gt;
 &lt;/model&gt;
&lt;/video&gt;</screen>
    </step>
   </procedure>
   <tip>
    <title>Enabling 2D/3D acceleration</title>
    <para>
     Only <literal>virtio</literal> and <literal>vbox</literal> video devices
     are capable of 2D/3D acceleration. You cannot enable it on other video
     devices.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="virsh-network-devices">
  <title>Configuring network devices</title>

  <para>
   This section describes how to configure specific aspects of virtual network
   devices by using &virsh;.
  </para>

  <para>
   Find more details about &libvirt; network interface specification in
   <link xlink:href="https://libvirt.org/formatdomain.html#elementsDriverBackendOptions"/>.
  </para>

  <sect2 xml:id="virsh-multiqueue">
   <title>Scaling network performance with multiqueue virtio-net</title>
   <para>
    The multiqueue virtio-net feature scales the network performance by
    allowing the &vmguest;'s virtual CPUs to transfer packets in parallel.
    Refer to <xref linkend="kvm-qemu-multiqueue"/> for more general
    information.
   </para>
   <para>
    To enable multiqueue virtio-net for a specific &vmguest;, edit its XML
    configuration as described in
    <xref linkend="sec-libvirt-config-editing-virsh"/> and modify its network
    interface as follows:
   </para>
<screen>
&lt;interface type='network'>
 [...]
 &lt;model type='virtio'/>
 &lt;driver name='vhost' queues='<replaceable>NUMBER_OF_QUEUES</replaceable>'/>
&lt;/interface>
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-direct">
  <title>Using macvtap to share &vmhost; network interfaces</title>

  <para>
   Macvtap provides direct attachment of a &vmguest; virtual interface to a
   host network interface. The macvtap-based interface extends the &vmhost;
   network interface and has its own MAC address on the same Ethernet segment.
   Typically, this is used to make both the &vmguest; and the &vmhost; show up
   directly on the switch that the &vmhost; is connected to.
  </para>

  <note>
   <title>Macvtap cannot be used with a Linux bridge</title>
   <para>
    Macvtap cannot be used with network interfaces already connected to a Linux
    bridge. Before attempting to create the macvtap interface, remove the
    interface from the bridge.
   </para>
  </note>

  <note>
   <title>&vmguest; to &vmhost; communication with macvtap</title>
   <para>
    When using macvtap, a &vmguest; can communicate with other &vmguest;s, and
    with other external hosts on the network. But it cannot communicate with
    the &vmhost; on which the &vmguest; runs. This is the defined behavior of
    macvtap, because of the way the &vmhost;'s physical Ethernet is attached to
    the macvtap bridge. Traffic from the &vmguest; into that bridge that is
    forwarded to the physical interface cannot be bounced back up to the
    &vmhost;'s IP stack. Similarly, traffic from the &vmhost;'s IP stack that
    is sent to the physical interface cannot be bounced back up to the macvtap
    bridge for forwarding to the &vmguest;.
   </para>
  </note>

  <para>
   Virtual network interfaces based on macvtap are supported by libvirt by
   specifying an interface type of <literal>direct</literal>. For example:
  </para>

<screen>&lt;interface type='direct'&gt;
   &lt;mac address='aa:bb:cc:dd:ee:ff'/&gt;
   &lt;source dev='eth0' mode='bridge'/&gt;
   &lt;model type='virtio'/&gt;
   &lt;/interface&gt;</screen>

  <para>
   The operation mode of the macvtap device can be controlled with the
   <literal>mode</literal> attribute. The following list show its possible
   values and a description for each:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>vepa</literal>: All &vmguest; packets are sent to an external
     bridge. Packets whose destination is a &vmguest; on the same &vmhost; as
     where the packet originates from are sent back to the &vmhost; by the VEPA
     capable bridge (today's bridges are typically not VEPA capable).
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>bridge</literal>: Packets whose destination is on the same
     &vmhost; as where they originate from are directly delivered to the target
     macvtap device. Both origin and destination devices need to be in
     <literal>bridge</literal> mode for direct delivery. If either one of them
     is in <literal>vepa</literal> mode, a VEPA capable bridge is required.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>private</literal>: All packets are sent to the external bridge
     and will only be delivered to a target &vmguest; on the same &vmhost; if
     they are sent through an external router or gateway and that device sends
     them back to the &vmhost;. This procedure is followed if either the source
     or destination device is in private mode.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>passthrough</literal>: A special mode that gives more power to
     the network interface. All packets will be forwarded to the interface,
     allowing virtio &vmguest;s to change the MAC address or set promiscuous
     mode to bridge the interface or create VLAN interfaces on top of it. Note
     that a network interface is not shareable in
     <literal>passthrough</literal> mode. Assigning an interface to a &vmguest;
     will disconnect it from the &vmhost;. For this reason SR-IOV virtual
     functions are often assigned to the &vmguest; in
     <literal>passthrough</literal> mode.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec-libvirt-config-disable-virtio-mellon">
  <title>Disabling a memory balloon device</title>

  <para>
   Memory Balloon has become a default option for KVM. The device will be added
   to the &vmguest; explicitly, so you do not need to add this element in the
   &vmguest;'s XML configuration. However, if you want to disable Memory
   Balloon in the &vmguest; for any reason, you need to set
   <literal>model='none'</literal> as shown below:
  </para>

<screen>&lt;devices&gt;
   &lt;memballoon model='none'/&gt;
&lt;/device&gt;</screen>
 </sect1>
 <sect1 xml:id="virsh-video-dual-head">
  <title>Configuring multiple monitors (dual head)</title>

  <para>
   &libvirt; supports a dual head configuration to display the video output of
   the &vmguest; on multiple monitors.
  </para>

  <important>
   <title>No support for &xen;</title>
   <para>
    The &xen; hypervisor does not support dual head configuration.
   </para>
  </important>

  <procedure>
   <title>Configuring dual head</title>
   <step>
    <para>
     While the virtual machine is running, verify that the
     <package>xf86-video-qxl</package> package is installed in the &vmguest;:
    </para>
<screen>&prompt.user;rpm -q xf86-video-qxl</screen>
   </step>
   <step>
    <para>
     Shut down the &vmguest; and start editing its configuration XML as
     described in <xref linkend="sec-libvirt-config-editing-virsh"/>.
    </para>
   </step>
   <step>
    <para>
     Verify that the model of the virtual graphics card is 'qxl':
    </para>
<screen>
&lt;video>
 &lt;model type='qxl' ... />
</screen>
   </step>
   <step>
    <para>
     Increase the <option>heads</option> parameter in the graphics card model
     specification from the default <literal>1</literal> to
     <literal>2</literal>, for example:
    </para>
<screen>
&lt;video>
 &lt;model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='2' primary='yes'/>
 &lt;alias name='video0'/>
 &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x0'/>
&lt;/video>
</screen>
   </step>
   <step>
    <para>
     Configure the virtual machine to use the Spice display instead of VNC:
    </para>
<screen>
&lt;graphics type='spice' port='5916' autoport='yes' listen='0.0.0.0'>
 &lt;listen type='address' address='0.0.0.0'/>
&lt;/graphics>
</screen>
   </step>
   <step>
    <para>
     Start the virtual machine and connect to its display with
     <command>virt-viewer</command>, for example:
    </para>
<screen>&prompt.user;virt-viewer --connect qemu+ssh://<replaceable>USER@VM_HOST</replaceable>/system</screen>
   </step>
   <step>
    <para>
     From the list of VMs, select the one whose configuration you have modified
     and confirm with <guimenu>Connect</guimenu>.
    </para>
   </step>
   <step>
    <para>
     After the graphical subsystem (Xorg) loads in the &vmguest;, select
     <menuchoice><guimenu>View</guimenu><guimenu>Displays</guimenu><guimenu>Display
     2</guimenu></menuchoice> to open a new window with the second monitor's
     output.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="virsh-kvm-zseries-crypto">
  <title>Crypto adapter pass-through to &kvm; guests on &zseries;</title>

  <sect2 xml:id="virsh-kvm-zseries-crypto-intro">
   <title>Introduction</title>
   <para>
    &zseries; machines include cryptographic hardware with useful functions
    such as random number generation, digital signature generation, or
    encryption. &kvm; allows dedicating these crypto adapters to guests as
    pass-through devices. The means that the hypervisor cannot observe
    communications between the guest and the device.
   </para>
  </sect2>

  <sect2 xml:id="virsh-kvm-zseries-crypto-cover">
   <title>What is covered</title>
   <para>
    You will learn how to dedicate a crypto adapter and domains on an &zseries;
    host to a &kvm; guest. The procedure includes the following basic steps:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Mask the crypto adapter and domains from the default driver on the host.
     </para>
    </listitem>
    <listitem>
     <para>
      Load the <literal>vfio-ap</literal> driver.
     </para>
    </listitem>
    <listitem>
     <para>
      Assign the crypto adapter and domains to the <literal>vfio-ap</literal>
      driver.
     </para>
    </listitem>
    <listitem>
     <para>
      Configure the guest to use the crypto adapter.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="virsh-kvm-zseries-crypto-reqs">
   <title>Requirements</title>
   <itemizedlist>
    <listitem>
     <para>
      You need to have the &qemu; / &libvirt; virtualization environment
      correctly installed and functional.
     </para>
    </listitem>
    <listitem>
     <para>
      The <literal>vfio_ap</literal> and <literal>vfio_mdev</literal> modules
      for the running kernel need to be available on the host operating system.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="virsh-kvm-zseries-crypto-proc">
   <title>Dedicate a crypto adapter to a &kvm; host</title>
   <procedure>
    <step>
     <para>
      Verify that the <literal>vfio_ap</literal> and
      <literal>vfio_mdev</literal> kernel modules are loaded on the host:
     </para>
<screen>&prompt.user;lsmod | grep vfio_</screen>
     <para>
      If any of them is not listed, load it manually, for example:
     </para>
<screen>&prompt.sudo;modprobe vfio_mdev</screen>
    </step>
    <step>
     <para>
      Create a new MDEV device on the host and verify that it was added:
     </para>
<screen>
uuid=$(uuidgen)
$ echo ${uuid} | sudo tee /sys/devices/vfio_ap/matrix/mdev_supported_types/vfio_ap-passthrough/create
dmesg | tail
[...]
[272197.818811] iommu: Adding device 24f952b3-03d1-4df2-9967-0d5f7d63d5f2 to group 0
[272197.818815] vfio_mdev 24f952b3-03d1-4df2-9967-0d5f7d63d5f2: MDEV: group_id = 0
</screen>
    </step>
    <step>
     <para>
      Identify the device on the host's logical partition that you intend to
      dedicate to a &kvm; guest:
     </para>
<screen>&prompt.user;ls -l /sys/bus/ap/devices/
[...]
lrwxrwxrwx 1 root root 0 Nov 23 03:29 00.0016 -> ../../../devices/ap/card00/00.0016/
lrwxrwxrwx 1 root root 0 Nov 23 03:29 card00 -> ../../../devices/ap/card00/
</screen>
     <para>
      In this example, it is card <literal>0</literal> queue
      <literal>16</literal>. To match the Hardware Management Console (HMC)
      configuration, you need to convert from <literal>16</literal> hexadecimal
      to <literal>22</literal> decimal.
     </para>
    </step>
    <step>
     <para>
      Mask the adapter from the <literal>zcrypt</literal> use:
     </para>
<screen>
&prompt.user;lszcrypt
CARD.DOMAIN TYPE MODE STATUS REQUEST_CNT
-------------------------------------------------
00 CEX5C CCA-Coproc online 5
00.0016 CEX5C CCA-Coproc online 5
</screen>
     <para>
      Mask the adapter:
     </para>
<screen>
&prompt.user;cat /sys/bus/ap/apmask
0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
echo -0x0 | sudo tee /sys/bus/ap/apmask
0x7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
</screen>
     <para>
      Mask the domain:
     </para>
<screen>
&prompt.user;cat /sys/bus/ap/aqmask
0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
echo -0x0 | sudo tee /sys/bus/ap/aqmask
0xfffffdffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
</screen>
    </step>
    <step>
     <para>
      Assign adapter 0 and domain 16 (22 decimal) to
      <literal>vfio-ap</literal>:
     </para>
<screen>
&prompt.sudo;echo +0x0 > /sys/devices/vfio_ap/matrix/${uuid}/assign_adapter
&prompt.user;echo +0x16 | sudo tee /sys/devices/vfio_ap/matrix/${uuid}/assign_domain
&prompt.user;echo +0x16 | sudo tee /sys/devices/vfio_ap/matrix/${uuid}/assign_control_domain
</screen>
    </step>
    <step>
     <para>
      Verify the matrix that you have configured:
     </para>
<screen>
&prompt.user;cat /sys/devices/vfio_ap/matrix/${uuid}/matrix
00.0016
</screen>
    </step>
    <step>
     <para>
      Either create a new VM (refer to <xref linkend="cha-kvm-inst"/>) and wait
      until it is initialized, or use an existing VM. In both cases, make sure
      the VM is shut down.
     </para>
    </step>
    <step>
     <para>
      Change its configuration to use the MDEV device:
     </para>
<screen>
&prompt.sudo;virsh edit <replaceable>VM_NAME</replaceable>
[...]
&lt;hostdev mode='subsystem' type='mdev' model='vfio-ap'>
 &lt;source>
  &lt;address uuid='24f952b3-03d1-4df2-9967-0d5f7d63d5f2'/>
 &lt;/source>
&lt;/hostdev>
[...]
</screen>
    </step>
    <step>
     <para>
      Restart the VM:
     </para>
<screen>&prompt.sudo;virsh reboot <replaceable>VM_NAME</replaceable></screen>
    </step>
    <step>
     <para>
      Log in to the guest and verify that the adapter is present:
     </para>
<screen>
&prompt.user;lszcrypt
CARD.DOMAIN TYPE MODE STATUS REQUEST_CNT
-------------------------------------------------
00 CEX5C CCA-Coproc online 1
00.0016 CEX5C CCA-Coproc online 1
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="virsh-kvm-zseries-crypto-moreinfo">
   <title>Further reading</title>
   <itemizedlist>
    <listitem>
     <para>
      The installation of virtualization components is detailed in
      <xref linkend="cha-vt-installation"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      The <literal>vfio_ap</literal> architecture is detailed in
      <link xlink:href="https://www.kernel.org/doc/Documentation/s390/vfio-ap.txt"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      A general outline together with a detailed procedure is described in
      <link xlink:href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1787405"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      The architecture of VFIO Mediated devices (MDEVs) is detailed in
      <link xlink:href="https://www.kernel.org/doc/html/latest/driver-api/vfio-mediated-device.html"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
</chapter>
