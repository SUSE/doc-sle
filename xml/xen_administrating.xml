<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-xen-admin">
 <title>Administrative tasks</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
          </dm:bugtracker>
      </dm:docmanager>
    </info>
    <para/>
 <sect1 xml:id="sec-xen-config-bootloader">
  <title>The boot loader program</title>

  <para>
   The boot loader controls how the virtualization software boots and runs.
   You can modify the boot loader properties by using &yast;, or by
   directly editing the boot loader configuration file.
  </para>

  <para>
   The &yast; boot loader program is located at <menuchoice>
   <guimenu>&yast;</guimenu> <guimenu>System</guimenu> <guimenu>Boot
   Loader</guimenu> </menuchoice>. Click the <guimenu>Bootloader
   Options</guimenu> tab and select the line containing the &xen; kernel
   as the <guimenu>Default Boot Section</guimenu>.
  </para>

  <figure>
   <title>Boot loader settings</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="xen_bootloader.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="xen_bootloader.png" width="80%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Confirm with <guimenu>OK</guimenu>. Next time you boot the host, it will
   be ready to provide the &xen; virtualization environment.
  </para>

  <para>
   You can use the Boot Loader program to specify functionality, such as:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Pass kernel command line parameters.
    </para>
   </listitem>
   <listitem>
    <para>
     Specify the kernel image and initial RAM disk.
    </para>
   </listitem>
   <listitem>
    <para>
     Select a specific hypervisor.
    </para>
   </listitem>
   <listitem>
    <para>
     Pass additional parameters to the hypervisor. See
     <link xlink:href="http://xenbits.xen.org/docs/unstable/misc/xen-command-line.html"/>
     for their complete list.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   You can customize your virtualization environment by editing the
   <filename>/etc/default/grub</filename> file. Add the following line to
   this file:
   <literal>GRUB_CMDLINE_XEN="&lt;boot_parameters&gt;"</literal>. Do not
   forget to run <command>grub2-mkconfig -o /boot/grub2/grub.cfg</command>
   after editing the file.
  </para>
 </sect1>
 <sect1 xml:id="sec-xen-config-sparse">
  <title>Sparse image files and disk space</title>

  <para>
   If the host’s physical disk reaches a state where it has no available
   space, a virtual machine using a virtual disk based on a sparse image
   file cannot write to its disk. Consequently, it reports I/O errors.
  </para>

  <para>
   If this situation
   occurs, you should free up available space on the physical disk, remount
   the virtual machine’s file system, and set the file system back to
   read-write.
  </para>

  <para>
   To check the actual disk requirements of a sparse image file, use the
   command <command>du -h &lt;image file&gt;</command>.
  </para>

  <para>
   To increase the available space of a sparse image file, first increase
   the file size and then the file system.
  </para>

  <warning>
   <title>Back up before resizing</title>
   <para>
    Touching the sizes of partitions or sparse files always bears the risk
    of data failure. Do not work without a backup.
   </para>
  </warning>

  <para>
   The resizing of the image file can be done online, while the &vmguest;
   is running. Increase the size of a sparse image file with:
  </para>

<screen>&prompt.sudo;dd if=/dev/zero of=&lt;image file&gt; count=0 bs=1M seek=&lt;new size in MB&gt;</screen>

  <para>
   For example, to increase the file
   <filename>/var/lib/xen/images/sles/disk0</filename> to a size of 16GB,
   use the command:
  </para>

<screen>&prompt.sudo;dd if=/dev/zero of=/var/lib/xen/images/sles/disk0 count=0 bs=1M seek=16000</screen>

  <note>
   <title>Increasing non-sparse images</title>
   <para>
    It is also possible to increase the image files of devices that are not
    sparse files. However, you must know exactly where the previous image
    ends. Use the seek parameter to point to the end of the image file and
    use a command similar to the following:
   </para>
<screen>&prompt.sudo;dd if=/dev/zero of=/var/lib/xen/images/sles/disk0 seek=8000 bs=1M count=2000</screen>
  </note>

  <para>
   Be sure to use the right seek, else data loss may happen.
  </para>

  <para>
   If the &vmguest; is running during the resize operation, also resize
   the loop device that provides the image file to the &vmguest;. First
   detect the correct loop device with the command:
  </para>

<screen>&prompt.sudo;losetup -j /var/lib/xen/images/sles/disk0</screen>

  <para>
   Then resize the loop device, for example <filename>/dev/loop0</filename>,
   with the following command:
  </para>

<screen>&prompt.sudo;losetup -c /dev/loop0</screen>

  <para>
   Finally check the size of the block device inside the guest system with
   the command <command>fdisk -l /dev/xvdb</command>. The device name
   depends on the actually increased device.
  </para>

  <para>
   The resizing of the file system inside the sparse file involves tools that
   are depending on the actual file system. <phrase os="sles">This is
   described in detail in the <xref linkend="book-storage"/>.</phrase>
  </para>
 </sect1>
 <sect1 xml:id="sec-xen-manage-migrate">
  <title>Migrating &xen; &vmguest; systems</title>

  <para>
   With &xen; it is possible to migrate a &vmguest; system from one
   &vmhost; to another with almost no service interruption. This could be
   used for example to move a busy &vmguest; to a &vmhost; that has
   stronger hardware or is not yet loaded. Or, if a service of a &vmhost;
   is required, all &vmguest; systems running on this machine can be
   migrated to other machines to avoid interruption of service.
   These are only two examples&mdash;many more reasons may apply to your
   personal situation.
  </para>

  <para>
   Before starting, some preliminary considerations regarding the
   &vmhost; should be taken into account:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     All &vmhost; systems should use a similar CPU. The frequency is not so
     important, but they should be using the same CPU family. To get more
     information about the used CPU, use <command>cat /proc/cpuinfo</command>.
     Find more details about comparing host CPU features in <xref
      linkend="sec-xen-manage-migrate-cpu"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     All resources that are used by a specific guest system must be
     available on all involved &vmhost; systems&mdash;for example all used block devices
     must exist on both &vmhost; systems.
    </para>
   </listitem>
   <listitem>
    <para>
     If the hosts included in the migration process run in different subnets,
     make sure that either DHCP relay is available to the guests,
     or for guests with static network configuration, set up the network manually.
    </para>
   </listitem>
   <listitem>
    <para>
     Using special features like <literal>&pciback;</literal> may be
     problematic. Do not implement these when deploying for an environment
     that should migrate &vmguest; systems between different &vmhost;
     systems.
    </para>
   </listitem>
   <listitem>
    <para>
     For fast migrations, a fast network is mandatory. If possible, use GB
     Ethernet and fast switches. Deploying VLAN might also help avoid
     collisions.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="sec-xen-manage-migrate-cpu">
   <title>Detecting CPU features</title>
   <para>
    By using the <systemitem>cpuid</systemitem> and
    <systemitem>xen_maskcalc.py</systemitem> tools, you can compare features of
    a CPU on the host from where you are migrating the source &vmguest; with
    the features of CPUs on the target hosts. This way you can better predict
    if the guest migrations will be successful.
   </para>
   <procedure>
    <step>
     <para>
      Run the <command>cpuid -1r</command> command on each &dom0; that is
      supposed to run or receive the migrated &vmguest; and capture the output
      in text files, for example:
     </para>
<screen>
<prompt>tux@vm_host1 &gt; </prompt>sudo cpuid -1r > vm_host1.txt
<prompt>tux@vm_host2 &gt; </prompt>sudo cpuid -1r > vm_host2.txt
<prompt>tux@vm_host3 &gt; </prompt>sudo cpuid -1r > vm_host3.txt
</screen>
    </step>
    <step>
     <para>
      Copy all the output text files on a host with the
      <command>xen_maskcalc.py</command> script installed.
     </para>
    </step>
    <step>
     <para>
      Run the <command>xen_maskcalc.py</command> script on all output text
      files:
     </para>
<screen>
&prompt.sudo;xen_maskcalc.py vm_host1.txt vm_host2.txt vm_host3.txt
cpuid = [
    "0x00000001:ecx=x00xxxxxx0xxxxxxxxx00xxxxxxxxxxx",
    "0x00000007,0x00:ebx=xxxxxxxxxxxxxxxxxx00x0000x0x0x00"
]
</screen>
    </step>
    <step>
     <para>
      Copy the output <literal>cpuid=[...]</literal> configuration snipped into
      the <command>xl</command> configuration of the migrated
      guest<filename>domU.cfg</filename> or alternatively to its &libvirt;'s
      XML configuration.
     </para>
    </step>
    <step>
     <para>
      Start the source guest with the <emphasis>trimmed</emphasis> CPU
      configuration. The guest can now only use CPU features which are present
      on each of the hosts.
     </para>
    </step>
   </procedure>
   <tip>
    <para>
     &libvirt; also supports calculating a baseline CPU for migration. For more details, refer to <link
xlink:href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-vt-best-practices/" />.
    </para>
   </tip>
   <sect3 xml:id="sec-xen-manage-migrate-cpu-info">
    <title>For more information</title>
    <para>
     You can find more details about <systemitem>cpuid</systemitem> at
     <link xlink:href="http://etallen.com/cpuid.html"/>.
    </para>
    <para>
     You can download the latest version of the CPU mask calculator from
     <link xlink:href="https://github.com/twizted/xen_maskcalc"/>.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-xen-manage-migrate-block">
   <title>Preparing block devices for migrations</title>
   <para>
    The block devices needed by the &vmguest; system must be available on
    all involved &vmhost; systems. This is done by implementing some kind
    of shared storage that serves as container for the root file system of
    the migrated &vmguest; system. Common possibilities include:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <literal>iSCSI</literal> can be set up to give access to the same block
      devices from different systems at the same time. <phrase os="sles">For
      more information about iSCSI, see <xref linkend="cha-iscsi"/>.</phrase>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>NFS</literal> is a widely used root file system that can
      easily be accessed from different locations. For more information, see
      <xref linkend="cha-nfs"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>DRBD</literal> can be used if only two &vmhost; systems
      are involved. This gives some extra data security, because the used
      data is mirrored over the network. <phrase os="sles;sled">For more information, see the
      <citetitle>SUSE Linux Enterprise High Availability Extension
      &productnumber;</citetitle> documentation at
      <link xlink:href="https://documentation.suse.com/sle-ha/15-SP2/"/></phrase>.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>SCSI</literal> can also be used if the available hardware
      permits shared access to the same disks.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>NPIV</literal> is a special mode to use Fibre channel disks.
      However, in this case all migration hosts must be attached to the same
      Fibre channel switch. For more information about NPIV, see
      <xref linkend="sec-xen-config-disk"/>. Commonly, this works if the
      Fibre channel environment supports 4 Gbit or faster connections.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec-xen-manage-migrate-xl">
   <title>Migrating &vmguest; systems</title>
   <para>
    The actual migration of the &vmguest; system is done with the
    command:
   </para>
<screen>&prompt.sudo;xl migrate &lt;domain_name&gt; &lt;host&gt;</screen>
   <para>
    The speed of the migration depends on how fast the memory print can be
    saved to disk, sent to the new &vmhost; and loaded there. This means
    that small &vmguest; systems can be migrated faster than big systems
    with a lot of memory.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-xen-monitor">
  <title>Monitoring &xen;</title>

  <para>
   For a regular operation of many virtual guests, having a possibility to
   check the sanity of all the different &vmguest; systems is
   indispensable. &xen; offers several tools besides the system tools to
   gather information about the system.
  </para>

  <tip>
   <title>Monitoring the &vmhost;</title>
   <para>
    Basic monitoring of the &vmhost; (I/O and CPU) is available via the
    &vmm;. Refer to
    <xref linkend="cha-libvirt-admin-monitor-virt-manager"/> for details.
   </para>
  </tip>

  <sect2 xml:id="sec-xen-monitor-xentop">
   <title>Monitor &xen; with <command>xentop</command></title>
   <para>
    The preferred terminal application to gather information about &xen;
    virtual environment is <command>xentop</command>. Unfortunately, this
    tool needs a rather broad terminal, else it inserts line breaks into the
    display.
   </para>
   <para>
    <command>xentop</command> has several command keys that can give you
    more information about the system that is monitored. Some of the more
    important are:
   </para>
   <variablelist>
    <varlistentry>
     <term>D</term>
     <listitem>
      <para>
       Change the delay between the refreshes of the screen.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>N</term>
     <listitem>
      <para>
       Also display network statistics. Note, that only standard
       configurations will be displayed. If you use a special configuration
       like a routed network, no network will be displayed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>B</term>
     <listitem>
      <para>
       Display the respective block devices and their cumulated usage count.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For more information about <command>xentop</command> see the manual page
    <command>man 1 xentop</command>.
   </para>

   <tip>
    <title><command>virt-top</command></title>
    <para>
     libvirt offers the hypervisor-agnostic tool <command>virt-top</command>,
     which is recommended for monitoring &vmguest;s. See <xref
     linkend="cha-libvirt-admin-monitor-virt-top"/> for details.
    </para>
   </tip>

  </sect2>

  <sect2 xml:id="sec-xen-monitor-tools">
   <title>Additional tools</title>
   <para>
    There are many system tools that also help monitoring or debugging a
    running <phrase os="sles;sled">&sle;</phrase><phrase
    os="osuse">&opensuse;</phrase> system. Many of these are covered in
    <xref linkend="cha-util"/>. Especially useful for monitoring a
    virtualization environment are the following tools:
   </para>
   <variablelist>
    <varlistentry>
     <term>ip</term>
     <listitem>
      <para>
       The command line utility <command>ip</command> may be used to monitor
       arbitrary network interfaces. This is especially useful if you have
       set up a network that is routed or applied a masqueraded network. To
       monitor a network interface with the name
       <literal>&xenguest;.0</literal>, run the following command:
      </para>
<screen>&prompt.user;watch ip -s link show &xenguest;.0</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bridge</term>
     <listitem>
      <para>
       In a standard setup, all the &xen; &vmguest; systems are
       attached to a virtual network bridge. <command>bridge</command> allows
       you to determine the connection between the bridge and the virtual
       network adapter in the &vmguest; system. For example, the output
       of <command>bridge link</command> may look like the following:
      </para>
<screen>
2: eth0 state DOWN : &lt;NO-CARRIER, ...,UP> mtu 1500 master br0
8: vnet0 state UNKNOWN : &lt;BROADCAST, ...,LOWER_UP> mtu 1500 master virbr0 \
  state forwarding priority 32 cost 100
</screen>
      <para>
       This shows that there are two virtual bridges defined on the system.  One
       is connected to the physical Ethernet device <literal>eth0</literal>, the
       other one is connected to a VLAN interface <literal>vnet0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>iptables-save</term>
     <listitem>
      <para>
       Especially when using masquerade networks, or if several Ethernet
       interfaces are set up together with a firewall setup, it may be
       helpful to check the current firewall rules.
      </para>
      <para>
       The command <command>iptables</command> may be used to check all the
       different firewall settings. To list all the rules of a chain, or
       even of the complete setup, you may use the commands
       <command>iptables-save</command> or <command>iptables -S</command>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-xen-admin-vhostmd">
  <title>Providing host information for &vmguest; systems</title>

  <para>
   In a standard &xen; environment, the &vmguest; systems have only
   very limited information about the &vmhost; system they are running
   on. If a guest should know more about the &vmhost; it runs on,
   <systemitem>vhostmd</systemitem> can provide more information to selected
   guests. To set up your system to run <systemitem>vhostmd</systemitem>,
   proceed as follows:
  </para>

  <procedure>
   <step>
    <para>
     Install the package vhostmd on the &vmhost;.
    </para>
   </step>
   <step>
    <para>
     To add or remove <literal>metric</literal> sections from the
     configuration, edit the file
     <filename>/etc/vhostmd/vhostmd.conf</filename>. However, the default works
     well.
    </para>
   </step>
   <step>
    <para>
     Check the validity of the <filename>vhostmd.conf</filename>
     configuration file with the command:
    </para>
<screen>&prompt.user;cd /etc/vhostmd
&prompt.user;xmllint --postvalid --noout vhostmd.conf
    </screen>
   </step>
   <step>
    <para>
     Start the vhostmd daemon with the command <command>sudo systemctl start
     vhostmd</command>.
    </para>
    <para>
     If vhostmd should be started automatically during start-up of the
     system, run the command:
    </para>
<screen>&prompt.sudo;systemctl enable vhostmd</screen>
   </step>
   <step>
    <para>
     Attach the image file <filename>/dev/shm/vhostmd0</filename> to the
     &vmguest; system named &xenguest; with the command:
    </para>
<screen>&prompt.user;xl block-attach opensuse /dev/shm/vhostmd0,,xvdb,ro</screen>
   </step>
   <step>
    <para>
     Log on the &vmguest; system.
    </para>
   </step>
   <step>
    <para>
     Install the client package <systemitem>vm-dump-metrics</systemitem>.
    </para>
   </step>
   <step>
    <para>
     Run the command <command>vm-dump-metrics</command>. To save the result to
     a file, use the option <systemitem>-d &lt;filename&gt;</systemitem>.
    </para>
   </step>
  </procedure>

  <para>
   The result of the <systemitem>vm-dump-metrics</systemitem> is an XML
   output. The respective metric entries follow the DTD
   <filename>/etc/vhostmd/metric.dtd</filename>.
  </para>

  <para>
   For more information, see the manual pages <command>man 8
   vhostmd</command> and <filename>/usr/share/doc/vhostmd/README</filename>
   on the &vmhost; system. On the guest, see the manual page <command>man
   1 vm-dump-metrics</command>.
  </para>
 </sect1>
</chapter>
