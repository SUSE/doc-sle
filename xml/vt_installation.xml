<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-vt-installation">
  <title>Installation of virtualization components</title>
  <info>
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
      <dm:bugtracker></dm:bugtracker>
      <dm:translation>yes</dm:translation>
    </dm:docmanager>
  </info>
  <sect1 xml:id="introduction-install-virtualization-components">
    <title>Introduction</title>

    <para>
      To run a virtualization server (&vmhost;) that can host one or more guest
      systems (&vmguest;s), you need to install required virtualization
      components on the server. These components vary depending on which
      virtualization technology you want to use.
    </para>
  </sect1>
  <sect1 xml:id="install-virtualization-components">
    <title>Installing virtualization components</title>

    <para>
      You can install the virtualization tools required to run a &vmhost; in
      one of the following ways:
    </para>

    <itemizedlist>
      <listitem>
        <para>
          By selecting a specific system role during &productname; installation
          on the &vmhost;
        </para>
      </listitem>
      <listitem>
        <para>
          By running the <emphasis>&yast; Virtualization</emphasis> module on
          an already installed and running &productname;.
        </para>
      </listitem>
      <listitem>
        <para>
          By installing specific installation patterns on an already installed
          and running &productname;.
        </para>
      </listitem>
    </itemizedlist>

    <sect2 xml:id="install-virtualization-components-system-role">
      <title>Specifying a system role</title>
      <para>
        You can install all the tools required for virtualization during the
        installation of &productname; on the &vmhost;. During the installation,
        you are presented with the <guimenu>System Role</guimenu> screen.
      </para>
      <figure>
        <title>System Role screen</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="virt-system-roles.png" width="75%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="virt-system-roles.png" width="75%"/>
          </imageobject>
        </mediaobject>
      </figure>
      <para>
        Here you can select either the <guimenu>KVM Virtualization
        Host</guimenu> or the <guimenu>Xen Virtualization Host</guimenu> roles.
        The appropriate software selection and setup is automatically performed
        during &productname; installation.
      </para>
      <tip>
        <para>
          Both virtualization system roles create a dedicated
          <filename>/var/lib/libvirt</filename> partition, and enable the
          &firewalld; and &kdump; services.
        </para>
      </tip>
    </sect2>

    <sect2 xml:id="install-virtualization-components-yast">
      <title>Running the <emphasis>&yast; Virtualization</emphasis> module</title>
      <para>
        Depending on the scope of &productname; installation on the &vmhost;,
        none of the virtualization tools may be installed on your system. They
        are automatically installed when configuring the hypervisor with the
        <emphasis>&yast; Virtualization</emphasis> module.
      </para>
      <tip>
        <para>
          The <emphasis>&yast; Virtualization</emphasis> module is included in
          the <package>yast2-vm</package> package. Verify it is installed on
          the &vmhost; before installing virtualization components.
        </para>
      </tip>
      <procedure>
        <title>Installing the &kvm; environment</title>
        <para>
          To install the &kvm; virtualization environment and related tools,
          proceed as follows:
        </para>
        <step>
          <para>
            Start &yast; and select
            <menuchoice><guimenu>Virtualization</guimenu> <guimenu>Install
            Hypervisor and Tools</guimenu></menuchoice>.
          </para>
        </step>
        <step>
          <para>
            Select <guimenu>&kvm; server</guimenu> for a minimal installation
            of &qemu; and &kvm; environment. Select <guimenu>&kvm;
            tools</guimenu> if you want to use the &libvirt;-based management
            stack as well. Confirm with <guimenu>Accept</guimenu>.
          </para>
        </step>
        <step>
          <para>
            &yast; offers to automatically configure a network bridge on the
            &vmhost;. It ensures proper networking capabilities of the
            &vmguest;. Agree to do so by selecting <guimenu>Yes</guimenu>,
            otherwise choose <guimenu>No</guimenu>.
          </para>
        </step>
        <step>
          <para>
            After the setup has been finished, you can start creating and
            configuring &vmguest;s. Rebooting the &vmhost; is not required.
          </para>
        </step>
      </procedure>
      <procedure>
        <title>Installing the &xen; environment</title>
        <para>
          To install the &xen; virtualization environment, proceed as follows:
        </para>
        <step>
          <para>
            Start &yast; and select<menuchoice>
            <guimenu>Virtualization</guimenu> <guimenu>Install Hypervisor and
            Tools</guimenu></menuchoice>.
          </para>
        </step>
        <step>
          <para>
            Select <guimenu>&xen; server</guimenu> for a minimal installation
            of &xen; environment. Select <guimenu>&xen; tools</guimenu> to use
            the &libvirt;-based management stack as well. Confirm with
            <guimenu>Accept</guimenu>.
          </para>
        </step>
        <step>
          <para>
            &yast; offers to automatically configure a network bridge on the
            &vmhost;. It ensures proper networking capabilities of the
            &vmguest;. Agree to do so by selecting <guimenu>Yes</guimenu>,
            otherwise choose <guimenu>No</guimenu>.
          </para>
        </step>
        <step>
          <para>
            After the setup has been finished, you need to reboot the machine
            with the <emphasis>&xen; kernel</emphasis>.
          </para>
          <tip>
            <title>Default boot kernel</title>
            <para>
              If everything works as expected, change the default boot kernel
              with &yast; and make the &xen;-enabled kernel the default. For
              more information about changing the default kernel, see
              <link xlink:href="https://documentation.suse.com/sles/15-SP4/html/SLES-all/cha-grub2.html#sec-grub2-yast2-config"/>.
            </para>
          </tip>
        </step>
      </procedure>
    </sect2>

    <sect2 xml:id="install-virtualization-components-pattern">
      <title>Installing specific installation patterns</title>
      <para>
        Related software packages from &productname; software repositories are
        organized into <emphasis>installation patterns</emphasis>. You can use
        these patterns to install specific virtualization components on an
        already running &productname;. Use <command>zypper</command> to install
        them:
      </para>
<screen>zypper install -t pattern <replaceable>PATTERN_NAME</replaceable></screen>
      <para>
        To install the &kvm; environment, consider the following patterns:
      </para>
      <variablelist>
        <varlistentry>
          <term><literal>kvm_server</literal></term>
          <listitem>
            <para>
              Installs basic &vmhost; with the &kvm; and &qemu; environments.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>kvm_tools</literal></term>
          <listitem>
            <para>
              Installs &libvirt; tools for managing and monitoring &vmguest;s
              in &kvm; environment.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
      <para>
        To install the &xen; environment, consider the following patterns:
      </para>
      <variablelist>
        <varlistentry>
          <term><literal>xen_server</literal></term>
          <listitem>
            <para>
              Installs a basic &xen; &vmhost;.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>xen_tools</literal></term>
          <listitem>
            <para>
              Installs &libvirt; tools for managing and monitoring &vmguest;s
              in &xen; environment.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </sect2>
  </sect1>
  <sect1 xml:id="sec-vt-installation-ovmf">
    <title>Installing UEFI support</title>

    <note>
      <para>
        We support &uefisecboot; on &x86-64; guests only. &kvm; guests support
        &uefisecboot; by using the OVMF firmware. &xen; HVM guests support
        booting from the OVMF firmware as well, but they do not support
        &uefisecboot;.
      </para>
    </note>

    <para>
      UEFI support is provided by <emphasis>OVMF</emphasis> (<emphasis>Open
      Virtual Machine Firmware</emphasis>). To enable UEFI boot, first install
      the <package>qemu-ovmf-x86_64</package> or
      <package>qemu-uefi-aarch64</package> package depending on the
      architecture of the guest.
    </para>

    <para>
      The firmware used by virtual machines is auto-selected. The
      auto-selection is based on the JSON files in the firmware package
      described above. The &libvirt; &qemu; driver parses those files during
      loading to know the capabilities of multiple types of firmware. Then when
      the user selects the type of firmware and any desired features (for
      example, support for &uefisecboot;), &libvirt; can find a firmware file
      that satisfies the user's requirements.
    </para>

    <para>
      For example, to specify EFI with &uefisecboot;, use the following
      configuration:
    </para>

<screen>
&lt;os firmware='efi'>
 &lt;loader secure='yes'/>
&lt;/os>
</screen>

    <para>
      The <package>qemu-ovmf-x86_64</package> package contains the following
      important UEFI firmware images. They provide &uefisecboot; capability for
      &vmguest;s:
    </para>

<screen>
&prompt.root;<command>rpm -ql qemu-ovmf-x86_64</command>
[...]
/usr/share/qemu/ovmf-x86_64-smm-ms-code.bin
/usr/share/qemu/ovmf-x86_64-smm-ms-vars.bin
/usr/share/qemu/ovmf-x86_64-smm-opensuse-code.bin
/usr/share/qemu/ovmf-x86_64-smm-opensuse-vars.bin
/usr/share/qemu/ovmf-x86_64-smm-suse-code.bin
/usr/share/qemu/ovmf-x86_64-smm-suse-vars.bin
[...]
</screen>

    <itemizedlist>
      <listitem>
        <para>
          To use &uefisecboot; for &sle; guests, use the
          <filename>ovmf-x86_64-smm-suse-code.bin</filename> firmware.
        </para>
      </listitem>
      <listitem>
        <para>
          To use &uefisecboot; for &opensuse; guests, use the
          <filename>ovmf-x86_64-smm-opensuse-code.bin</filename> firmware.
        </para>
      </listitem>
      <listitem>
        <para>
          To use &uefisecboot; for &mswin; guests, use the
          <filename>ovmf-x86_64-smm-ms-code.bin</filename> firmware.
        </para>
      </listitem>
    </itemizedlist>

    <para>
      For the &aarch64; architecture, the package is named
      <package>qemu-uefi-aarch32</package>:
    </para>

<screen>
&prompt.root;<command>rpm -ql qemu-uefi-aarch32</command>
[...]
/usr/share/qemu/aavmf-aarch32-code.bin
/usr/share/qemu/aavmf-aarch32-vars.bin
/usr/share/qemu/firmware
/usr/share/qemu/firmware/60-aavmf-aarch32.json
/usr/share/qemu/qemu-uefi-aarch32.bin
</screen>

    <para>
      The <filename>*-code.bin</filename> files are the UEFI firmware files.
      The <filename>*-vars.bin</filename> files are corresponding variable
      store images that can be used as a template for a per-VM non-volatile
      store. &libvirt; copies the specified <literal>vars</literal> template to
      a per-VM path under <filename>/var/lib/libvirt/qemu/nvram/</filename>
      when first creating the VM. Files without <literal>code</literal> or
      <literal>vars</literal> in the name can be used as a single UEFI image.
      They are not as useful since no UEFI variables persist across power
      cycles of the VM.
    </para>

    <para>
      The <filename>*-ms*.bin</filename> files contain UEFI CA keys as found on
      real hardware. Therefore, they are configured as the default in
      &libvirt;. Likewise, the <filename>*-suse*.bin</filename> files contain
      preinstalled &suse; keys. There is also a set of files with no
      preinstalled keys.
    </para>

    <para>
      For details, see <xref linkend="vle-libvirt-inst-virt-install-ovmf"
   />
      and
      <link xlink:href=
   "http://www.linux-kvm.org/downloads/lersek/ovmf-whitepaper-c770f8c.txt"
   />.
    </para>
  </sect1>
  <sect1 xml:id="sec-vt-installation-nested-vms">
    <title>Enable nested virtualization in &kvm;</title>

    <important>
      <title>Technology preview</title>
      <para>
        &kvm;'s nested virtualization is still a technology preview. It is
        provided for testing purposes and is not supported.
      </para>
    </important>

    <para>
      Nested guests are &kvm; guests run in a &kvm; guest. When describing
      nested guests, we use the following virtualization layers:
    </para>

    <variablelist>
      <varlistentry>
        <term>L0</term>
        <listitem>
          <para>
            A bare metal host running &kvm;.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>L1</term>
        <listitem>
          <para>
            A virtual machine running on L0. Because it can run another &kvm;,
            it is called a <emphasis>guest hypervisor</emphasis>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>L2</term>
        <listitem>
          <para>
            A virtual machine running on L1. It is called a <emphasis>nested
            guest</emphasis>.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>

    <para>
      Nested virtualization has many advantages. You can benefit from it in the
      following scenarios:
    </para>

    <itemizedlist>
      <listitem>
        <para>
          Manage your own virtual machines directly with your hypervisor of
          choice in cloud environments.
        </para>
      </listitem>
      <listitem>
        <para>
          Enable the live migration of hypervisors and their guest virtual
          machines as a single entity.
        </para>
        <note>
          <para>
            Live migration of a nested &vmguest; is not supported.
          </para>
        </note>
      </listitem>
      <listitem>
        <para>
          Use it for software development and testing.
        </para>
      </listitem>
    </itemizedlist>

    <para>
      To enable nesting temporarily, remove the module and reload it with the
      <option>nested</option> &kvm; module parameter:
    </para>

    <itemizedlist>
      <listitem>
        <para>
          For Intel CPUs, run:
        </para>
<screen>
&prompt.sudo;modprobe -r kvm_intel &amp;&amp; modprobe kvm_intel nested=1
</screen>
      </listitem>
      <listitem>
        <para>
          For AMD CPUs, run:
        </para>
<screen>
&prompt.sudo;modprobe -r kvm_amd &amp;&amp; modprobe kvm_amd nested=1
</screen>
      </listitem>
    </itemizedlist>

    <para>
      To enable nesting permanently, enable the <option>nested</option> &kvm;
      module parameter in the <filename>/etc/modprobe.d/kvm_*.conf</filename>
      file, depending on your CPU:
    </para>

    <itemizedlist>
      <listitem>
        <para>
          For Intel CPUs, edit
          <filename>/etc/modprobe.d/kvm_intel.conf</filename> and add the
          following line:
        </para>
<screen>options kvm_intel nested=1</screen>
      </listitem>
      <listitem>
        <para>
          For AMD CPUs, edit <filename>/etc/modprobe.d/kvm_amd.conf</filename>
          and add the following line:
        </para>
<screen>options kvm_amd nested=1</screen>
      </listitem>
    </itemizedlist>

    <para>
      When your L0 host is capable of nesting, you can start an L1 guest in one
      of the following ways:
    </para>

    <itemizedlist>
      <listitem>
        <para>
          Use the <option>-cpu host</option> &qemu; command line option.
        </para>
      </listitem>
      <listitem>
        <para>
          Add the <literal>vmx</literal> (for Intel CPUs) or the
          <literal>svm</literal> (for AMD CPUs) CPU feature to the
          <option>-cpu</option> &qemu; command line option, which enables
          virtualization for the virtual CPU.
        </para>
      </listitem>
    </itemizedlist>

    <sect2 xml:id="sec-vt-installation-nested-vms-esxi">
      <title>&esx; as a guest hypervisor</title>
      <para>
        If you use &esx; as a guest hypervisor on top of a &kvm; bare metal
        hypervisor, you may experience unstable network communication. This
        problem occurs especially between nested &kvm; guests and the &kvm;
        bare metal hypervisor or external network. The following default CPU
        configuration of the nested &kvm; guest is causing the problem:
      </para>
<screen>&lt;cpu mode='host-model' check='partial'/></screen>
      <para>
        To fix it, modify the CPU configuration as follow:
      </para>
<screen>
[...]
&lt;cpu mode='host-passthrough' check='none'>
 &lt;cache mode='passthrough'/>
&lt;/cpu>
[...]
</screen>
    </sect2>
  </sect1>
</chapter>
