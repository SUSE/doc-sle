<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE sect1 PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!--
   fs 02/28/2006: 
     Do not change the following words/phrases (except in URls, 
     packagenames etc.)in all zSeries documents, because they are trademarks:
      - Redbook, Redpiece, Redpaper
      - developerWorks
      - ESCON
      - HiperSockets
-->
<sect1 id="sec.zseries.rescue">
 <title>IBM &zseries;: Using initrd as a Rescue System</title>

 <para>
  If the kernel of the &slsreg; for IBM &zseries; is upgraded or modified,
  it is possible to reboot the system accidentally in an inconsistent state,
  so standard procedures of IPLing the installed system fail. This most
  commonly occurs if a new or updated &sls; kernel has been installed and
  the zipl program has not been run to update the IPL record. In this case,
  use the standard installation package as a rescue system from which the
  zipl program can be executed to update the IPL record.
 </para><indexterm>

 <primary>rescue system</primary></indexterm>

 <sect2 id="sec.zseries.rescue.ipling">
  <title>IPLing the Rescue System</title>
  <important>
   <title>Making the Installation Data Available</title>
   <para>
    For this method to work, the &sls; for IBM &zseries; installation data
    must be available. For details, refer to
    <xref
     linkend="sec.prep.makeavail"/>. Additionally, you need the
    channel number of the device and the partition number within the device
    that contains the root file system of the &sls; installation.
   </para>
  </important>
  <para>
   First, IPL the &sls; for IBM &zseries; installation system as described
   in <xref linkend="sec.zseries.prep"/>. A list of choices for the network
   adapter to use is then presented.
  </para>
  <para>
   Select <guimenu>Start Installation or System</guimenu> then
   <guimenu>Start Rescue System</guimenu> to start the rescue system.
   Depending on the installation environment, you now must specify the
   parameters for the network adapter and the installation source. The
   rescue system is loaded and the following login prompt is shown at the
   end:
  </para>
<screen>Skipped services in runlevel 3:  nfs nfsboot

Rescue login:</screen>
  <para>
   You can now log in as &rootuser; without a password.
  </para>
 </sect2>

 <sect2 id="sec.zseries.rescue.configure_disks">
  <title>Configuring Disks</title>
  <para>
   In this state, no disks are configured. You need to configure them before
   you can proceed.
  </para>
  <procedure>
   <title>Configuring DASDs</title>
   <step>
    <para>
     Configure DASDs with the following command:
    </para>
<screen>dasd_configure 0.0.0150 1 0</screen>
    <para>
     0.0.0150 is the channel to which the DASD is connected. The
     <literal>1</literal> means activate the disk (a <literal>0</literal> at
     this place would deactivate the disk). The <literal>0</literal> stands
     for <quote>no DIAG mode</quote> for the disk (a <literal>1</literal>
     here would enable DAIG access to the disk).
    </para>
   </step>
   <step>
    <para>
     Now the DASD is online (check with <command>cat
     /proc/partitions</command>) and can used for subsequent commands.
    </para>
   </step>
  </procedure>
  <procedure>
   <title>Configuring a zFCP Disk</title>
   <step>
    <para>
     To configure a zFCP disk, it is necessary to first configure the zFCP
     adapter. Do this with the following command:
    </para>
<screen>zfcp_host_configure 0.0.4000 1</screen>
    <para>
     <literal>0.0.4000</literal> is the channel to which the adapter is
     attached and <literal>1</literal> stands for activate (a
     <literal>0</literal> here would deactivate the adapter).
    </para>
   </step>
   <step>
    <para>
     After the adapter is activated, a disk can be configured. Do this with
     the following command:
    </para>
<screen>zfcp_disk_configure 0.0.4000 1234567887654321 8765432100000000  1</screen>
    <para>
     <literal>0.0.4000</literal> is the previously-used channel ID,
     <literal>1234567887654321</literal> is the WWPN (World wide Port
     Number), and <literal>8765432100000000</literal> is the LUN (logical
     unit number). The <literal>1</literal> stands for activating the disk
     (a <literal>0</literal> here would deactivate the disk).
    </para>
   </step>
   <step>
    <para>
     Now the zFCP disk is online (check with <command>cat
     /proc/partitions</command>) and can used for subsequent commands.
    </para>
   </step>
  </procedure>
 </sect2>

 <sect2 id="sec.zseries.rescue.mount">
  <title>Mounting the Root Device</title>
  <para>
   If all needed disks are online, you should now be able to mount the root
   device. Assuming that the root device is on the second partition of the
   DASD device (<literal>/dev/dasda2</literal>), the corresponding command
   is <command>mount /dev/dasda2 /mnt</command>.
  </para>
  <important>
   <title>File System Consistency</title>
   <para>
    If the installed system has not been shut down properly, it may be
    advisable to check the file system consistency prior to mounting. This
    prevents any accidental loss of data. Using this example, issue the
    command <command>fsck</command>&nbsp;<option>/dev/dasda2</option> to
    ensure that the file system is in a consistent state.
   </para>
  </important>
  <para>
   By just issuing the command <command>mount</command>, it is possible to
   check whether the file system could be mounted correctly.
  </para>
  <example id="ex.zseries.rescue.mount.mount_output">
   <title>Output of the Mount Command</title>
<screen>SuSE Instsys suse:/ # mount
shmfs on /newroot type shm (rw,nr_inodes=10240)
devpts on /dev/pts type devpts (rw)
virtual-proc-filesystem on /proc type proc (rw)
/dev/dasda2 on /mnt type reiserfs (rw)</screen>
  </example>
 </sect2>

 <sect2 id="sec.zseries.rescue.chroot">
  <title>Changing to the Mounted File System</title>
  <para>
   For the <literal>zipl</literal> command to read the configuration file
   from the root device of the installed system and not from the rescue
   system, change the root device to the installed system with the
   <command>chroot</command> command:
  </para>
  <example id="ex.zseries.rescue.chroot">
   <title>chroot to the Mounted File System</title>
<screen>SuSE Instsys suse:/ # cd /mnt 
SuSE Instsys suse:/mnt # chroot /mnt</screen>
  </example>
 </sect2>

 <sect2 id="sec.zseries.rescue.zipl">
  <title>Executing zipl</title>
  <para>
   Now execute <command>zipl</command> to rewrite the IPL record with the
   correct values:
  </para>
  <example id="ex.zseries.rescue.zipl">
   <title>Installing the IPL Record with zipl</title>
<screen>sh-2.05b# zipl 
building bootmap : /boot/zipl/bootmap 
adding Kernel Image : /boot/kernel/image located at 0x00010000 
adding Ramdisk : /boot/initrd located at 0x00800000 
adding Parmline : /boot/zipl/parmfile located at 0x00001000 
Bootloader for ECKD type devices with z/OS compatible layout installed. 
Syncing disks.... 
...done</screen>
  </example>
 </sect2>

 <sect2 id="sec.zseries.rescue.exit">
  <title>Exiting the Rescue System</title>
  <para>
   To exit the rescue system, first leave the shell opened by the
   <command>chroot</command> command with <command>exit</command>. To
   prevent any loss of data, flush all unwritten buffers to disk with the
   <command>sync</command> command. Now change to the root directory of the
   rescue system and unmount the root device of &sls; for IBM &zseries;
   installation.
  </para>
  <example id="ex.zseries.rescue.exit.umount">
   <title>Unmounting the File System</title>
<screen>SuSE Instsys suse:/mnt # cd / 
SuSE Instsys suse:/ # umount /mnt</screen>
  </example>
  <para>
   Finally, halt the rescue system with the <command>halt</command> command.
   The &sls; system can now be IPLed as described in
   <xref linkend="sec.s390.inst.reipl"/>.
  </para>
 </sect2>
</sect1>
