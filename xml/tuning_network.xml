<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.tuning.network">
<!-- ibm 33:
      offload depends on the adapater's features.
      ibm 34:
     Bonding: Documentation/networking/bonding.txt.
     link aggregation and load balancing
     Check, whether this is already describe somewhere else!
     Obviously in ha and xen guides...
 -->
<!-- tuning NFS performance:
      http://blogs.techrepublic.com.com/opensource/?p=64&tag=rbxccnbtr1
 -->
<!-- apache is similar -->
 <title>Tuning the Network</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
          </dm:bugtracker>
      </dm:docmanager>
    </info>
    <para>
  The network subsystem is rather complex and its tuning highly depends on
  the system use scenario and also on external factors such as software
  clients or hardware components (switches, routers, or gateways) in your
  network. The Linux kernel aims more at reliability and low latency than
  low overhead and high throughput. Other settings can mean less security,
  but better performance.
 </para>
 <sect1 xml:id="sec.tuning.network.buffers">
  <title>Configurable Kernel Socket Buffers</title>

  <para>
   Networking is largely based on the TCP/IP protocol and a socket interface
   for communication; for more information about TCP/IP, see
   <xref linkend="cha.basicnet"/>. The Linux kernel handles data it receives
   or sends via the socket interface in socket buffers. These kernel socket
   buffers are tunable.
  </para>

<!-- http://www.psc.edu/networking/projects/tcptune/#Linux :-->

  <important>
   <title>TCP Autotuning</title>
   <para>
    Since kernel version 2.6.17 full autotuning with 4 MB maximum buffer
    size exists. This means that manual tuning usually will not
    improve networking performance considerably. It is often the best not to
    touch the following variables, or, at least, to check the outcome of
    tuning efforts carefully.
   </para>
   <para>
    If you update from an older kernel, it is recommended to remove manual
    TCP tunings in favor of the autotuning feature.
   </para>
  </important>

  <para>
   The special files in the <filename>/proc</filename> file system can
   modify the size and behavior of kernel socket buffers; for general
   information about the <filename>/proc</filename> file system, see
   <xref linkend="sec.util.proc"/>. Find networking related files in:
  </para>

<screen>/proc/sys/net/core
/proc/sys/net/ipv4
/proc/sys/net/ipv6</screen>

  <para>
   General <systemitem>net</systemitem> variables are explained in the
   kernel documentation
   (<filename>linux/Documentation/sysctl/net.txt</filename>). Special
   <systemitem>ipv4</systemitem> variables are explained in
   <filename>linux/Documentation/networking/ip-sysctl.txt</filename> and
   <filename>linux/Documentation/networking/ipvs-sysctl.txt</filename>.
  </para>

  <para>
   In the <filename>/proc</filename> file system, for example, it is
   possible to either set the Maximum Socket Receive Buffer and Maximum
   Socket Send Buffer for all protocols, or both these options for the TCP
   protocol only (in <filename>ipv4</filename>) and thus overriding the
   setting for all protocols (in <filename>core</filename>).
  </para>

<!-- http://www.psc.edu/networking/projects/tcptune/#Linux :
       CHECKIT / FIXME
       setting xxx variable, will disable autotuning completely!
-->

  <variablelist>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_moderate_rcvbuf</filename>
    </term>
    <listitem>
     <para>
      If <filename>/proc/sys/net/ipv4/tcp_moderate_rcvbuf</filename> is set
      to <literal>1</literal>, autotuning is active and buffer size is
      adjusted dynamically.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_rmem</filename>
    </term>
    <listitem>
     <para>
      The three values setting the minimum, initial, and maximum size of the
      Memory Receive Buffer per connection. They define the actual memory
      usage, not only TCP window size.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_wmem</filename>
    </term>
    <listitem>
     <para>
      The same as <filename>tcp_rmem</filename>, but for Memory Send Buffer
      per connection.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/core/rmem_max</filename>
    </term>
    <listitem>
     <para>
      Set to limit the maximum receive buffer size that applications can
      request.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/core/wmem_max</filename>
    </term>
    <listitem>
     <para>
      Set to limit the maximum send buffer size that applications can
      request.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Via <filename>/proc</filename> it is possible to disable TCP features
   that you do not need (all TCP features are switched on by default). For
   example, check the following files:
  </para>

  <variablelist>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_timestamps</filename>
    </term>
    <listitem>
     <para>
      TCP time stamps are defined in RFC1323.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_window_scaling</filename>
    </term>
    <listitem>
     <para>
      TCP window scaling is also defined in RFC1323.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><filename>/proc/sys/net/ipv4/tcp_sack</filename>
    </term>
    <listitem>
     <para>
      Select acknowledgments (SACKS).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Use <command>sysctl</command> to read or write variables of the
   <filename>/proc</filename> file system. <command>sysctl</command> is
   preferable to <command>cat</command> (for reading) and
   <command>echo</command> (for writing), because it also reads settings
   from <filename>/etc/sysctl.conf</filename> and, thus, those settings
   survive reboots reliably. With <command>sysctl</command> you can read all
   variables and their values easily; as &rootuser; use the following
   command to list TCP related settings:
  </para>

<screen>sysctl -a | grep tcp</screen>

<!--
  cf. id="sec.tuning.taskscheduler.cfs.tuning" -->

  <note>
   <title>Side-Effects of Tuning Network Variables</title>
   <para>
    Tuning network variables can affect other system resources such as CPU
    or memory use.
<!-- (p.124)-->
<!--
          Also see "Tuning TCP behavior", ibm p. 130
      -->
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec.tuning.network.analyzing">
  <title>Detecting Network Bottlenecks and Analyzing Network Traffic</title>

  <para>
   Before starting with network tuning, it is important to isolate network
   bottlenecks and network traffic patterns. There are some tools that can
   help you with detecting those bottlenecks.
  </para>

  <para>
   The following tools can help analyzing your network traffic:
   <command>netstat</command>, <command>tcpdump</command>, and
   <command>wireshark</command>. Wireshark is a network traffic analyzer.
  </para>
 </sect1>
<!-- ibm 33:
      offload depends on the adapater's features.
      ibm 34:
     Bonding: Documentation/networking/bonding.txt.
     link aggregation and load balancing
     Check, whether this is already describe somewhere else!
     Obviously in ha and xen guides...
 -->
 <sect1 xml:id="sec.tuning.network.netfilter">
  <title>Netfilter</title>

  <para>
   The Linux firewall and masquerading features are provided by the
   Netfilter kernel modules. This is a highly configurable rule based
   framework. If a rule matches a packet, Netfilter accepts or denies it or
   takes special action (<quote>target</quote>) as defined by rules such as
   address translation.
  </para>

  <para>
   There are quite a lot of properties Netfilter can take into account.
   Thus, the more rules are defined, the longer packet processing may last.
   Also advanced connection tracking could be rather expensive and, thus,
   slowing down overall networking.
  </para>

<!-- : security vs. performance -->

  <para>
   When the kernel queue becomes full, all new packets are dropped, causing
   existing connections to fail. The 'fail-open' feature, available since
   &sls; 11 SP3, allows a user to temporarily disable the packet
   inspection and maintain the connectivity under heavy network traffic. For
   reference, see
   <link xlink:href="https://home.regit.org/netfilter-en/using-nfqueue-and-libnetfilter_queue/"/>.
  </para>

  <para>
   For more information, see the home page of the Netfilter and iptables
   project, <link xlink:href="http://www.netfilter.org"/>
  </para>
 </sect1>
 <sect1 xml:id="sec.tuning.network.rps">
  <title>Improving the Network Performance with Receive Packet Steering (RPS)</title>

  <para>
   Modern network interface devices can move so many packets that the host
   can become the limiting factor for achieving maximum performance. In
   order to keep up, the system must be able to distribute the work across
   multiple CPU cores.
  </para>

  <para>
   Some modern network interfaces can help distribute the work to multiple
   CPU cores through the implementation of multiple transmission and
   multiple receive queues in hardware. However, others are only equipped
   with a single queue and the driver must deal with all incoming packets in
   a single, serialized stream. To work around this issue, the operating
   system must "parallelize" the stream to distribute the work across
   multiple CPUs. On &productname; this is done via Receive Packet
   Steering (RPS). RPS can also be used in virtual environments.
  </para>

  <para>
   RPS creates a unique hash for each data stream using IP addresses and
   port numbers. The use of this hash ensures that packets for the same data
   stream are sent to the same CPU, which helps to increase performance.
  </para>

  <para>
   RPS is configured per network device receive queue and interface. The
   configuration file names match the following scheme:
  </para>

<screen>/sys/class/net/<replaceable>&lt;device&gt;</replaceable>/queues/<replaceable>&lt;rx-queue&gt;</replaceable>/rps_cpus</screen>

  <para>
   <replaceable>&lt;device&gt;</replaceable> stands for the network
   device, such as <literal>eth0</literal>, <literal>eth1</literal>.
   <replaceable>&lt;rx-queue&gt;</replaceable> stands for the receive queue,
   such as <literal>rx-0</literal>, <literal>rx-1</literal>.
  </para>

  <para>
   If the network interface hardware only supports a single receive queue,
   only <literal>rx-0</literal> will exist. If it supports multiple receive
   queues, there will be an rx-<replaceable>N</replaceable> directory for
   each receive queue.
  </para>

  <para>
   These configuration files contain a comma-delimited list of CPU bitmaps.
   By default, all bits are set to <literal>0</literal>. With this setting
   RPS is disabled and therefore the CPU that handles the interrupt will
   also process the packet queue.
  </para>

  <para>
   To enable RPS and enable specific CPUs to process packets for the receive
   queue of the interface, set the value of their positions in the bitmap to
   <literal>1</literal>. For example, to enable CPUs 0-3 to process packets
   for the first receive queue for eth0, set the bit positions
   0-3 to 1 in binary: <literal>00001111</literal>. This representation then
   needs to be converted to hex&mdash;which results in <literal>F</literal> in
   this case. Set this hex value with the following command:
  </para>

<screen>echo "f" &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus</screen>

  <para>
   If you wanted to enable CPUs 8-15:
  </para>

<screen>1111 1111 0000 0000 (binary)
15     15    0    0 (decimal)
F       F    0    0 (hex)</screen>

  <para>
   The command to set the hex value of <literal>ff00</literal> would be:
  </para>

<screen>echo "ff00" &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus</screen>

  <para>
   On NUMA machines, best performance can be achieved by configuring RPS to
   use the CPUs on the same NUMA node as the interrupt for the interface's
   receive queue.
  </para>

  <para>
   On non-NUMA machines, all CPUs can be used. If the interrupt rate is very
   high, excluding the CPU handling the network interface can boost
   performance. The CPU being used for the network interface can be
   determined from <filename>/proc/interrupts</filename>. For example:
  </para>

<screen>&prompt.root;cat /proc/interrupts
            CPU0       CPU1       CPU2       CPU3
...
  51:  113915241          0          0          0      Phys-fasteoi   eth0
...</screen>

  <para>
   In this case, <literal>CPU 0</literal> is the only CPU processing
   interrupts for <literal>eth0</literal>, since only
   <literal>CPU0</literal> contains a non-zero value.
  </para>

  <para>
   On &x86; and &x86-64; platforms, <command>irqbalance</command> can be used
   to distribute hardware interrupts across CPUs. See <command>man 1
   irqbalance</command> for more details.
  </para>
 </sect1>
<!-- tuning NFS performance:
      http://blogs.techrepublic.com.com/opensource/?p=64&tag=rbxccnbtr1
 -->
<!-- apache is similar -->
 <sect1 xml:id="sec.tuning.network.info">
  <title>For More Information</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Eduardo Ciliendo, Takechika Kunimasa: <quote>Linux Performance and
     Tuning Guidelines</quote> (2007), esp. sections 1.5, 3.5, and 4.7:
     <link xlink:href="http://www.redbooks.ibm.com/redpapers/abstracts/redp4285.html"/>
    </para>
   </listitem>
   <listitem>
    <para>
     John Heffner, Matt Mathis: <quote>Tuning TCP for Linux 2.4 and
     2.6</quote> (2006):
     <link xlink:href="http://www.psc.edu/networking/projects/tcptune/#Linux"/>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
