<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-nvmeof" xml:lang="en">
 <title>&nvmeof;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
  <abstract>
   <para>
    This chapter describes how to set up an &nvmeof; host and target.
   </para>
  </abstract>
 </info>
 <sect1 xml:id="sec-nvmeof-overview">
  <title>Overview</title>

  <para>
   <emphasis>NVM Express</emphasis> (<emphasis>&nvme;</emphasis>) is an
   interface standard for accessing non-volatile storage, commonly SSD disks.
   &nvme; supports much higher speeds and has a lower latency than SATA.
  </para>

  <para>
   <emphasis>&nvmeof;</emphasis> is an architecture to access &nvme; storage
   over different networking fabrics&mdash;for example,
   <emphasis>RDMA</emphasis>, <emphasis>TCP</emphasis>, or <emphasis>&nvme;
   over Fibre Channel</emphasis> (<emphasis>&fcnvme;</emphasis>). The role of
   &nvmeof; is similar to iSCSI. To increase the fault-tolerance, &nvmeof; has
   a built-in support for multipathing. The &nvmeof; multipathing is not based
   on the traditional &dmmpio;.
  </para>

  <para>
   The <emphasis>&nvme; host</emphasis> is the machine that connects to an
   &nvme; target. The <emphasis>&nvme; target</emphasis> is the machine that
   shares its &nvme; block devices.
  </para>

  <para>
   &nvme; is supported on &sls; &productnumber;. There are Kernel modules
   available for the &nvme; block storage and &nvmeof; target and host.
  </para>

  <para>
   To see if your hardware requires any special consideration, refer to
   <xref linkend="sec-nvmeof-hardware" />.
  </para>
 </sect1>
 <sect1 xml:id="sec-nvmeof-host-configuration">
  <title>Setting up an &nvmeof; host</title>

  <para>
   To use &nvmeof;, a target must be available with one of the supported
   networking methods. Supported are &nvme; over Fibre Channel, TCP, and RDMA.
   The following sections describe how to connect a host to an &nvme; target.
  </para>

  <sect2 xml:id="sec-nvmeof-host-configuration-cli">
   <title>Installing command line client</title>
   <para>
    To use &nvmeof;, you need the <command>nvme</command> command line tool.
    Install it with <command>zypper</command>:
   </para>
<screen>&prompt.sudo;<command>zypper in nvme-cli</command></screen>
   <para>
    Use <command>nvme --help</command> to list all available subcommands. Man
    pages are available for <command>nvme</command> subcommands. Consult them
    by executing <command>man
    nvme-<replaceable>SUBCOMMAND</replaceable></command>. For example, to view
    the man page for the <option>discover</option> subcommand, execute
    <command>man nvme-discover</command>.
   </para>
  </sect2>

  <sect2 xml:id="sec-nvmeof-host-configuration-target-discovery">
   <title>Discovering &nvmeof; targets</title>
   <para>
    To list available &nvme; subsystems on the &nvmeof; target, you need the
    discovery controller address and service ID.
   </para>
<screen>&prompt.sudo;<command>nvme discover -t <replaceable>TRANSPORT</replaceable> -a <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> -s <replaceable>SERVICE_ID</replaceable></command></screen>
   <para>
    Replace <replaceable>TRANSPORT</replaceable> with the underlying transport
    medium: <option>loop</option>, <option>rdma</option>, <option>tcp</option>,
    or <option>fc</option>. Replace
    <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> with the address of
    the discovery controller. For RDMA and TCP, this should be an IPv4 address.
    Replace <replaceable>SERVICE_ID</replaceable> with the transport service
    ID. If the service is IP based, like RDMA or TCP, service ID specifies the
    port number. For Fibre Channel, the service ID is not required.
   </para>
   <para>
    The &nvme; hosts only see the subsystems they are allowed to connect to.
   </para>
   <para>
    Example:
   </para>
<screen>&prompt.sudo;nvme discover -t tcp -a 10.0.0.3 -s 4420</screen>
<para>
    For the FC, the example looks as follows:
    </para>
    <screen>&prompt.sudo;nvme discover --transport=fc \
                --traddr=nn-0x201700a09890f5bf:pn-0x201900a09890f5bf \
                --host-traddr=nn-0x200000109b579ef6:pn-0x100000109b579ef6</screen>
   <para>
    For more details, see <command>man nvme-discover</command>.
   </para>
  </sect2>

  <sect2 xml:id="sec-nvmeof-host-configuration-connect-target">
   <title>Connecting to &nvmeof; targets</title>
   <para>
    After you have identified the &nvme; subsystem, you can connect it with the
    <command>nvme connect</command> command.
   </para>
<screen>&prompt.sudo;<command>nvme connect -t <replaceable>transport</replaceable> -a <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> -s <replaceable>SERVICE_ID</replaceable> -n <replaceable>SUBSYSTEM_NQN</replaceable></command></screen>
   <para>
    Replace <replaceable>TRANSPORT</replaceable> with the underlying transport
    medium: <option>loop</option>, <option>rdma</option>, <option>tcp</option>
    or <option>fc</option>. Replace
    <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> with the address of
    the discovery controller. For RDMA and TCP this should be an IPv4 address.
    Replace <replaceable>SERVICE_ID</replaceable> with the transport service
    ID. If the service is IP based, like RDMA or TCP, this specifies the port
    number. Replace <replaceable>SUBSYSTEM_NQN</replaceable> with the NVMe
    qualified name of the desired subsystem as found by the discovery command.
    <emphasis>NQN</emphasis> is the abbreviation for <emphasis> &nvme;
    Qualified Name</emphasis>. The NQN must be unique.
   </para>
   <para>
    Example:
   </para>
<screen>&prompt.sudo;<command>nvme connect -t tcp -a 10.0.0.3 -s 4420 -n nqn.2014-08.com.example:nvme:nvm-subsystem-sn-d78432</command></screen>
<para>
    For the FC, the example looks as follows:
    </para>
    <screen>&prompt.sudo;nvme connect --transport=fc \
             --traddr=nn-0x201700a09890f5bf:pn-0x201900a09890f5bf \
             --host-traddr=nn-0x200000109b579ef6:pn-0x100000109b579ef6 \
             --nqn=nqn.2014-08.org.nvmexpress:uuid:1a9e23dd-466e-45ca-9f43-a29aaf47cb21</screen>
   <para>
    Alternatively, use <command>nvme connect-all</command> to connect to all
    discovered namespaces. For advanced usage, see <command>man
    nvme-connect</command> and <command>man nvme-connect-all</command>.
   </para>
   <para>
    In case of a path loss, the &nvme; subsystem tries to reconnect for a time
    period, defined by the <literal>ctrl-loss-tmo</literal> option of the
    <command>nvme connect</command> command. After this time (default value is
    600s), the path is removed and the upper layers of the block layer (file
    system) are notified. By default, the file system is then mounted
    read-only, which usually is not the expected behaviour. Therefore, it is
    recommended to set the <literal>ctrl-loss-tmo</literal> option so that the
    &nvme; subsystem keeps trying to reconnect without a limit. To do so, run
    the following command:
   </para>
<screen>&prompt.sudo;nvme connect --ctrl-loss-tmo=-1</screen>
   <para>
    To make an NVMe over Fabrics subsystem available at boot, create a
    <filename>/etc/nvme/discovery.conf</filename> file on the host with the
    parameters passed to the <command>discover</command> command (as described
    in <xref linkend="sec-nvmeof-host-configuration-target-discovery"/>. For
    example, if you use the <command>discover</command> command as follows:
   </para>
<screen>&prompt.sudo;nvme discover -t tcp -a 10.0.0.3 -s 4420</screen>
   <para>
    Add the parameters of the <command>discover</command> command to the
    <filename>/etc/nvme/discovery.conf</filename> file:
   </para>
<screen>echo "-t tcp -a 10.0.0.3 -s 4420" | sudo tee -a /etc/nvme/discovery.conf</screen>
   <para>
    Then enable the <guimenu>nvmf-autoconnect</guimenu> service:
   </para>
<screen>&prompt.sudo;systemctl enable nvmf-autoconnect.service</screen>
  </sect2>

  <sect2 xml:id="sec-nvmeof-host-configuration-multipathing">
   <title>Multipathing</title>
   <para>
    &nvme; native multipathing is enabled by default. If the
    <option>CMIC</option> option in the controller identity settings is set,
    the NVMe stack recognizes an NVME drive as a multipathed device by default.
   </para>
   <para>
    To manage the multipathing, you can use the following:
   </para>
   <variablelist>
    <title>Managing multipathing</title>
    <varlistentry>
     <term><command>nvme list-subsys</command></term>
     <listitem>
      <para>
       Prints the layout of the multipath devices.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>multipath -ll</command></term>
     <listitem>
      <para>
       The command has a compatibility mode and displays &nvme; multipath
       devices. Bear in mind that you need to enable the 
       <literal>enable_foreign</literal> option to use the command.
       For details, refer to <xref linkend="sec-multipath-conf-misc"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>nvme-core.multipath=N</option></term>
     <listitem>
      <para>
       When the option is added as a boot parameter, the NVMe native
       multipathing will be disabled.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <note>
    <title>Using <command>iostat</command> in multipath setups</title>
    <para>
     The <command>iostat</command> command might not show all
     controllers that are listed by <command>nvme list-subsys</command>.
     By default, <command>iostat</command> filters out all block devices
     with no I/O. To make <command>iostat</command> show
     <emphasis>all</emphasis> devices, use:
    </para>
    <screen>iostat -p ALL</screen>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-nvmeof-target-configuration">
  <title>Setting up an &nvmeof; target</title>

  <sect2 xml:id="sec-nvmeof-target-configuration-cli">
   <title>Installing command line client</title>
   <para>
    To configure an &nvmeof; target, you need the <command>nvmetcli</command>
    command line tool. Install it with <command>zypper</command>:
   </para>
<screen>&prompt.sudo;<command>zypper in nvmetcli</command></screen>
   <para>
    The current documentation for <command>nvmetcli</command> is available at
    <link xlink:href="http://git.infradead.org/users/hch/nvmetcli.git/blob_plain/HEAD:/Documentation/nvmetcli.txt"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec-nvmeof-target-configuration-steps">
   <title>Configuration steps</title>
   <para>
    The following procedure provides an example of how to set up an &nvmeof;
    target.
   </para>
   <para>
    The configuration is stored in a tree structure. Use the command
    <command>cd</command> to navigate. Use <command>ls</command> to list
    objects. You can create new objects with <command>create</command>.
   </para>
   <procedure>
    <step>
     <para>
      Start the <command>nvmetcli</command> interactive shell:
     </para>
<screen>&prompt.sudo;<command>nvmetcli</command></screen>
    </step>
    <step>
     <para>
      Create a new port:
     </para>
<screen>&prompt.nvmetcli;<command>cd ports</command>
&prompt.nvmetcli;<command>create 1</command>
&prompt.nvmetcli;<command>ls 1/</command>
o- 1
  o- referrals
  o- subsystems</screen>
    </step>
    <step>
     <para>
      Create an &nvme; subsystem:
     </para>
<screen>&prompt.nvmetcli;<command>cd /subsystems</command>
&prompt.nvmetcli;<command>create nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82</command>
&prompt.nvmetcli;<command>cd nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82/</command>
&prompt.nvmetcli;<command>ls</command>
o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
  o- allowed_hosts
  o- namespaces</screen>
    </step>
    <step>
     <para>
      Create a new namespace and set an &nvme; device to it:
     </para>
<screen>&prompt.nvmetcli;<command>cd namespaces</command>
&prompt.nvmetcli;<command>create 1</command>
&prompt.nvmetcli;<command>cd 1</command>
&prompt.nvmetcli;<command>set device path=/dev/nvme0n1</command>
Parameter path is now '/dev/nvme0n1'.</screen>
    </step>
    <step>
     <para>
      Enable the previously created namespace:
     </para>
<screen>&prompt.nvmetcli;<command>cd ..</command>
&prompt.nvmetcli;<command>enable</command>
The Namespace has been enabled.</screen>
    </step>
    <step>
     <para>
      Display the created namespace:
     </para>
<screen>&prompt.nvmetcli;<command>cd ..</command>
&prompt.nvmetcli;<command>ls</command>
o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
  o- allowed_hosts
  o- namespaces
    o- 1</screen>
    </step>
    <step>
     <para>
      Allow all hosts to use the subsystem. Only do this in secure
      environments.
     </para>
<screen>&prompt.nvmetcli;<command>set attr allow_any_host=1</command>
Parameter allow_any_host is now '1'.</screen>
     <para>
      Alternatively, you can allow only specific hosts to connect:
     </para>
<screen>&prompt.nvmetcli;<command>cd nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82/allowed_hosts/</command>
&prompt.nvmetcli;<command>create hostnqn</command></screen>
    </step>
    <step>
     <para>
      List all created objects:
     </para>
<screen>&prompt.nvmetcli;<command>cd /</command>
&prompt.nvmetcli;<command>ls</command>
o- /
  o- hosts
  o- ports
  | o- 1
  |   o- referrals
  |   o- subsystems
  o- subsystems
    o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
      o- allowed_hosts
      o- namespaces
        o- 1</screen>
    </step>
    <step>
     <para>
      Make the target available via TCP. Use <literal>trtype=rdma</literal> for
      RDMA:
     </para>
<screen>&prompt.nvmetcli;<command>cd ports/1/</command>
&prompt.nvmetcli;<command>set addr adrfam=ipv4 trtype=tcp traddr=10.0.0.3 trsvcid=4420</command>
Parameter trtype is now 'tcp'.
Parameter adrfam is now 'ipv4'.
Parameter trsvcid is now '4420'.
Parameter traddr is now '10.0.0.3'.</screen>
     <para>
      Alternatively, you can make it available with Fibre Channel:
     </para>
<screen>&prompt.nvmetcli;<command>cd ports/1/</command>
&prompt.nvmetcli;<command>set addr adrfam=fc trtype=fc traddr=nn-0x1000000044001123:pn-0x2000000055001123 trsvcid=none</command></screen>
    </step>
    <step>
     <para>
      Link the subsystem to the port:
     </para>
<screen>&prompt.nvmetcli;<command>cd /ports/1/subsystems</command>
&prompt.nvmetcli;<command>create nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82</command>      
      </screen>
     <para>
      Now you can verify that the port is enabled using
      <command>dmesg</command>:
     </para>
<screen>&prompt.root;dmesg
        ...
[  257.872084] nvmet_tcp: enabling port 1 (10.0.0.3:4420)
      </screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-nvmeof-target-configuration-backup-configuration">
   <title>Back up and restore target configuration</title>
   <para>
    You can save the target configuration in a JSON file with the following
    commands:
   </para>
<screen>&prompt.sudo;<command>nvmetcli</command>
&prompt.nvmetcli;<command>saveconfig nvme-target-backup.json</command></screen>
   <para>
    To restore the configuration, use:
   </para>
<screen>&prompt.nvmetcli;<command>restore nvme-target-backup.json</command></screen>
   <para>
    You can also wipe the current configuration:
   </para>
<screen>&prompt.nvmetcli;<command>clear</command></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-nvmeof-hardware">
  <title>Special hardware configuration</title>

  <sect2 xml:id="sec-nvmeof-hardware-overview">
   <title>Overview</title>
   <para>
    Some hardware needs special configuration to work correctly. Skim the
    titles of the following sections to see if you are using any of the
    mentioned devices or vendors.
   </para>
  </sect2>

  <sect2 xml:id="sec-nvmeof-hardware-broadcom">
   <title>Broadcom</title>
   <para>
    If you are using the <emphasis>Broadcom Emulex LightPulse Fibre Channel
    SCSI</emphasis> driver, add a Kernel configuration parameter on the target
    and host for the <literal>lpfc</literal> module:
   </para>
<screen>&prompt.sudo;<command>echo "options lpfc lpfc_enable_fc4_type=3" > /etc/modprobe.d/lpfc.conf</command></screen>
   <para>
    Make sure that the Broadcom adapter firmware has at least version
    11.4.204.33. Also make sure that you have the current versions of
    <package>nvme-cli</package>, <package>nvmetcli</package> and the Kernel
    installed.
   </para>
   <para>
    To enable a Fibre Channel port as an &nvme; target, an additional module
    parameter needs to be configured: <option>lpfc_enable_nvmet=<replaceable>
    COMMA_SEPARATED_WWPNS</replaceable></option>. Enter the WWPN with a leading
    <literal>0x</literal>, for example
    <option>lpfc_enable_nvmet=0x2000000055001122,0x2000000055003344</option>.
    Only listed WWPNs will be configured for target mode. A Fibre Channel port
    can either be configured as target <emphasis>or</emphasis> as initiator.
   </para>
  </sect2>

  <sect2 xml:id="sec-nvmeof-hardware-marvell">
   <title>Marvell</title>
   <para>
    FC-&nvme; is supported on QLE269x and QLE27xx adapters. FC-&nvme; support
    is enabled by default in the Marvell® QLogic® QLA2xxx Fibre Channel driver.
   </para>
   <para>
    To confirm &nvme; is enabled, run the following command:
   </para>
<screen>&prompt.user;cat /sys/module/qla2xxx/parameters/ql2xnvmeenable</screen>
   <para>
    A resulting <literal>1</literal> means &nvme; is enabled, a
    <literal>0</literal> indicates it is disabled.
   </para>
   <para>
    Next, ensure that the Marvell adapter firmware is at least version 8.08.204
    by checking the output of the following command:
   </para>
<screen>&prompt.user;cat /sys/class/scsi_host/host0/fw_version</screen>
   <para>
    Last, ensure that the latest versions available for &productname; of
    <package>nvme-cli</package>, <package>QConvergeConsoleCLI</package>, and
    the Kernel are installed. You may, for example, run
   </para>
<screen>&prompt.root;zypper lu &amp;&amp; zypper pchk</screen>
   <para>
    to check for updates and patches.
   </para>
   <para>
    For more details on installation, please refer to the FC-&nvme; sections in
    the following Marvell user guides:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <link xlink:href="http://driverdownloads.qlogic.com/QLogicDriverDownloads_UI/ShowEula.aspx?resourceid=32769&amp;docid=96728&amp;ProductCategory=39&amp;Product=1259&amp;Os=126"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="http://driverdownloads.qlogic.com/QLogicDriverDownloads_UI/ShowEula.aspx?resourceid=32761&amp;docid=96726&amp;ProductCategory=39&amp;Product=1261&amp;Os=126"/>
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-nvmeof-more-information">
  <title>More information</title>

  <para>
   For more details about the abilities of the <command>nvme</command> command,
   refer to <command>nvme nvme-help</command>.
  </para>

  <para>
   The following links provide a basic introduction to &nvme; and &nvmeof;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <link xlink:href="http://nvmexpress.org/"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://www.nvmexpress.org/wp-content/uploads/NVMe_Over_Fabrics.pdf"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://storpool.com/blog/demystifying-what-is-nvmeof"/>
    </para>
   </listitem>   
  </itemizedlist>
 </sect1>
</chapter>
