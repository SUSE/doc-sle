<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-vt-installation">
 <title>Installation of virtualization components</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="introduction-install-virtualization-components">
  <title>Introduction</title>

  <para>
   In order to run a virtualization server (&vmhost;) that can host one or more
   guest systems (&vmguest;s), you need to install required virtualization
   components on the server. These components vary depending on which
   virtualization technology you want to use.
  </para>
 </sect1>
 <sect1 xml:id="install-virtualization-components">
  <title>Installing virtualization components</title>

  <para>
   You can install the virtualization tools required to run a &vmhost; in one
   of the following ways:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     By selecting a specific system role during &productname; installation on
     the &vmhost;
    </para>
   </listitem>
   <listitem>
    <para>
     By running the <emphasis>&yast; Virtualization</emphasis> module on an
     already installed and running &productname;.
    </para>
   </listitem>
   <listitem>
    <para>
     By installing specific installation patterns on an already installed and
     running &productname;.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="install-virtualization-components-system-role">
   <title>Specifying a system role</title>
   <para>
    You can install all the tools required for virtualization during the
    installation of &productname; on the &vmhost;. During the installation you
    will be presented with the <guimenu>System Role</guimenu> screen.
   </para>
   <figure>
    <title>System Role screen</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="virt-system-roles.png" width="75%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="virt-system-roles.png" width="75%"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Here you can select either the <guimenu>KVM Virtualization Host</guimenu>
    or the <guimenu>Xen Virtualization Host</guimenu> roles. The appropriate
    software selection and setup will be automatically performed during
    &productname; installation.
   </para>
   <tip>
    <para>
     Both virtualization system roles will create a dedicated
     <filename>/var/lib/libvirt</filename> partition, and enable the
     &firewalld; and &kdump; services.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="install-virtualization-components-yast">
   <title>Running the <emphasis>&yast; Virtualization</emphasis> module</title>
   <para>
    Depending on the scope of &productname; installation on the &vmhost;, none
    of the virtualization tools may be installed on your system. They will be
    automatically installed when configuring the hypervisor with the
    <emphasis>&yast; Virtualization</emphasis> module.
   </para>
   <tip>
    <para>
     The <emphasis>&yast; Virtualization</emphasis> module is included in the
     <package>yast2-vm</package> package. Verify it is installed on the
     &vmhost; before installing virtualization components.
    </para>
   </tip>
   <procedure>
    <title>Installing the &kvm; environment</title>
    <para>
     To install the &kvm; virtualization environment and related tools, proceed
     as follows:
    </para>
    <step>
     <para>
      Start &yast; and select <menuchoice><guimenu>Virtualization</guimenu>
      <guimenu>Install Hypervisor and Tools</guimenu></menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Select <guimenu>&kvm; server</guimenu> for a minimal installation of
      &qemu; and &kvm; environment. Select <guimenu>&kvm; tools</guimenu> if
      you want to use the &libvirt;-based management stack as well. Confirm
      with <guimenu>Accept</guimenu>.
     </para>
    </step>
    <step>
     <para>
      &yast; offers to automatically configure a network bridge on the
      &vmhost;. It ensures proper networking capabilities of the &vmguest;.
      Agree to do so by selecting <guimenu>Yes</guimenu>, otherwise choose
      <guimenu>No</guimenu>.
     </para>
    </step>
    <step>
     <para>
      After the setup has been finished, you can start creating and configuring
      &vmguest;s. Rebooting the &vmhost; is not required.
     </para>
    </step>
   </procedure>
   <procedure>
    <title>Installing the &xen; environment</title>
    <para>
     To install the &xen; virtualization environment, proceed as follows:
    </para>
    <step>
     <para>
      Start &yast; and select<menuchoice> <guimenu>Virtualization</guimenu>
      <guimenu>Install Hypervisor and Tools</guimenu></menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Select <guimenu>&xen; server</guimenu> for a minimal installation of
      &xen; environment. Select <guimenu>&xen; tools</guimenu> if you want to
      use the &libvirt;-based management stack as well. Confirm with
      <guimenu>Accept</guimenu>.
     </para>
    </step>
    <step>
     <para>
      &yast; offers to automatically configure a network bridge on the
      &vmhost;. It ensures proper networking capabilities of the &vmguest;.
      Agree to do so by selecting <guimenu>Yes</guimenu>, otherwise choose
      <guimenu>No</guimenu>.
     </para>
    </step>
    <step>
     <para>
      After the setup has been finished, you need to reboot the machine with
      the <emphasis>&xen; kernel</emphasis>.
     </para>
     <tip>
      <title>Default boot kernel</title>
      <para>
       If everything works as expected, change the default boot kernel with
       &yast; and make the &xen;-enabled kernel the default. For more
       information about changing the default kernel, see
       <link xlink:href="https://documentation.suse.com/sles/15-SP4/html/SLES-all/cha-grub2.html#sec-grub2-yast2-config"/>.
      </para>
     </tip>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="install-virtualization-components-pattern">
   <title>Installing specific installation patterns</title>
   <para>
    Related software packages from &productname; software repositories are
    organized into <emphasis>installation patterns</emphasis>. You can use
    these patterns to install specific virtualization components on an already
    running &productname;. Use <command>zypper</command> to install them:
   </para>
<screen>zypper install -t pattern <replaceable>PATTERN_NAME</replaceable></screen>
   <para>
    To install the &kvm; environment, consider the following patterns:
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>kvm_server</literal></term>
     <listitem>
      <para>
       Installs basic &vmhost; with the &kvm; and &qemu; environments.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>kvm_tools</literal></term>
     <listitem>
      <para>
       Installs &libvirt; tools for managing and monitoring &vmguest;s in &kvm;
       environment.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    To install the &xen; environment, consider the following patterns:
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>xen_server</literal></term>
     <listitem>
      <para>
       Installs a basic &xen; &vmhost;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>xen_tools</literal></term>
     <listitem>
      <para>
       Installs &libvirt; tools for managing and monitoring &vmguest;s in &xen;
       environment.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-vt-installation-ovmf">
  <title>Installing UEFI support</title>

  <note>
   <para>
    We support &uefisecboot; on &x86-64; guests only.  &kvm; guests support
    &uefisecboot; by using the OVMF firmware. &xen; HVM guests support booting
    from the OVMF firmware as well, but they do not support &uefisecboot;.
   </para>
  </note>

  <para>
   UEFI support is provided by <emphasis>OVMF</emphasis> (<emphasis>Open
   Virtual Machine Firmware</emphasis>). To enable UEFI boot, first install the
   <package>qemu-ovmf-x86_64</package> or <package>qemu-uefi-aarch64</package>
   package depending on the architecture of the guest.
  </para>

  <para>
   The firmware used by virtual machines is auto-selected. The auto-selection
   is based on the JSON files in the firmware package described above. The
   &libvirt; &qemu; driver parses those files when loading so it knows the
   capabilities of the various types of firmware. Then when the user selects
   the type of firmware and any desired features (for example, support for
   &uefisecboot;), &libvirt; will be able to find a firmware file that
   satisfies the user's requirements.
  </para>

  <para>
   For example, to specify EFI with &uefisecboot;, use the following
   configuration:
  </para>

<screen>
&lt;os firmware='efi'>
 &lt;loader secure='yes'/>
&lt;/os>
</screen>

  <para>
   The <package>qemu-ovmf-x86_64</package> package contains the following
   important UEFI firmware images. They provide &uefisecboot; capability for
   various &vmguest;s:
  </para>

<screen>
&prompt.root;<command>rpm -ql qemu-ovmf-x86_64</command>
[...]
/usr/share/qemu/ovmf-x86_64-smm-ms-code.bin
/usr/share/qemu/ovmf-x86_64-smm-ms-vars.bin
/usr/share/qemu/ovmf-x86_64-smm-opensuse-code.bin
/usr/share/qemu/ovmf-x86_64-smm-opensuse-vars.bin
/usr/share/qemu/ovmf-x86_64-smm-suse-code.bin
/usr/share/qemu/ovmf-x86_64-smm-suse-vars.bin
[...]
</screen>

  <itemizedlist>
   <listitem>
    <para>
     To use &uefisecboot; for &sle; guests, use the
     <filename>ovmf-x86_64-smm-suse-code.bin</filename> firmware.
    </para>
   </listitem>
   <listitem>
    <para>
     To use &uefisecboot; for &opensuse; guests, use the
     <filename>ovmf-x86_64-smm-opensuse-code.bin</filename> firmware.
    </para>
   </listitem>
   <listitem>
    <para>
     To use &uefisecboot; for &mswin; guests, use the
     <filename>ovmf-x86_64-smm-ms-code.bin</filename> firmware.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   For the &aarch64; architecture, the package is named
   <package>qemu-uefi-aarch32</package>:
  </para>

<screen>
&prompt.root;<command>rpm -ql qemu-uefi-aarch32</command>
[...]
/usr/share/qemu/aavmf-aarch32-code.bin
/usr/share/qemu/aavmf-aarch32-vars.bin
/usr/share/qemu/firmware
/usr/share/qemu/firmware/60-aavmf-aarch32.json
/usr/share/qemu/qemu-uefi-aarch32.bin
</screen>

  <para>
   The <filename>*-code.bin</filename> files are the UEFI firmware files. The
   <filename>*-vars.bin</filename> files are corresponding variable store
   images that can be used as a template for a per-VM non-volatile store.
   &libvirt; copies the specified <literal>vars</literal> template to a per-VM
   path under <filename>/var/lib/libvirt/qemu/nvram/</filename> when first
   creating the VM. Files without <literal>code</literal> or
   <literal>vars</literal> in the name can be used as a single UEFI image. They
   are not as useful since no UEFI variables persist across power cycles of the
   VM.
  </para>

  <para>
   The <filename>*-ms*.bin</filename> files contain UEFI CA keys as found on
   real hardware. Therefore, they are configured as the default in &libvirt;.
   Likewise, the <filename>*-suse*.bin</filename> files contain preinstalled
   &suse; keys. There is also a set of files with no preinstalled keys.
  </para>

  <para>
   For details, see <xref linkend="vle-libvirt-inst-virt-install-ovmf"
   />
   and
   <link xlink:href=
   "http://www.linux-kvm.org/downloads/lersek/ovmf-whitepaper-c770f8c.txt"
   />.
  </para>
 </sect1>
 <sect1 xml:id="sec-vt-installation-nested-vms">
  <title>Enable nested virtualization in &kvm;</title>

  <important>
   <title>Technology preview</title>
   <para>
    &kvm;'s nested virtualization is still a technology preview. It is provided
    for testing purposes and is not supported.
   </para>
  </important>

  <para>
   Nested guests are &kvm; guests run in a &kvm; guest. When describing nested
   guests, we will use the following virtualization layers:
  </para>

  <variablelist>
   <varlistentry>
    <term>L0</term>
    <listitem>
     <para>
      A bare metal host running &kvm;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>L1</term>
    <listitem>
     <para>
      A virtual machine running on L0. Because it can run another &kvm;, it is
      called a <emphasis>guest hypervisor</emphasis>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>L2</term>
    <listitem>
     <para>
      A virtual machine running on L1. It is called a <emphasis>nested
      guest</emphasis>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Nested virtualization has many advantages. You can benefit from it in the
   following scenarios:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Manage your own virtual machines directly with your hypervisor of choice
     in cloud environments.
    </para>
   </listitem>
   <listitem>
    <para>
     Enable the live migration of hypervisors and their guest virtual machines
     as a single entity.
    </para>
    <note>
     <para>
      Live migration of a nested &vmguest; is not supported.
     </para>
    </note>
   </listitem>
   <listitem>
    <para>
     Use it for software development and testing.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   To enable nesting temporarily, remove the module and reload it with the
   <option>nested</option> &kvm; module parameter:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     For Intel CPUs, run:
    </para>
<screen>
&prompt.sudo;modprobe -r kvm_intel &amp;&amp; modprobe kvm_intel nested=1
</screen>
   </listitem>
   <listitem>
    <para>
     For AMD CPUs, run:
    </para>
<screen>
&prompt.sudo;modprobe -r kvm_amd &amp;&amp; modprobe kvm_amd nested=1
</screen>
   </listitem>
  </itemizedlist>

  <para>
   To enable nesting permanently, enable the <option>nested</option> &kvm;
   module parameter in the <filename>/etc/modprobe.d/kvm_*.conf</filename>
   file, depending on your CPU:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     For Intel CPUs, edit <filename>/etc/modprobe.d/kvm_intel.conf</filename>
     and add the following line:
    </para>
<screen>options kvm_intel nested=1</screen>
   </listitem>
   <listitem>
    <para>
     For AMD CPUs, edit <filename>/etc/modprobe.d/kvm_amd.conf</filename> and
     add the following line:
    </para>
<screen>options kvm_amd nested=1</screen>
   </listitem>
  </itemizedlist>

  <para>
   When your L0 host is capable of nesting, you will be able to start an L1
   guest in one of the following ways:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Use the <option>-cpu host</option> &qemu; command line option.
    </para>
   </listitem>
   <listitem>
    <para>
     Add the <literal>vmx</literal> (for Intel CPUs) or the
     <literal>svm</literal> (for AMD CPUs) CPU feature to the
     <option>-cpu</option> &qemu; command line option, which enables
     virtualization for the virtual CPU.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="sec-vt-installation-nested-vms-esxi">
   <title>&esx; as a guest hypervisor</title>
   <para>
    If you use &esx; as a guest hypervisor on top of a &kvm; bare metal
    hypervisor, you may experience unstable network communication. This problem
    occurs especially between nested &kvm; guests and the &kvm; bare metal
    hypervisor or external network. The following default CPU configuration of
    the nested &kvm; guest is causing the problem:
   </para>
<screen>&lt;cpu mode='host-model' check='partial'/></screen>
   <para>
    To fix it, modify the CPU configuration as follow:
   </para>
<screen>
[...]
&lt;cpu mode='host-passthrough' check='none'>
 &lt;cache mode='passthrough'/>
&lt;/cpu>
[...]
</screen>
  </sect2>
 </sect1>
</chapter>
