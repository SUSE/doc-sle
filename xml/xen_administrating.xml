<?xml version="1.0"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.xen.admin">
 <title>Administration Tasks</title>
 <para></para>
 <sect1 id="sec.xen.config.bootloader">
  <title>The Boot Loader Program</title>

  <remark condition="clarity">
    2014-02-10 - fs: FIXME: These entries needs to be adjusted to GRUB 2.
   </remark>

  <para>
   The boot loader controls how the virtualization software boots and runs.
   You can modify the boot loader properties by using &yast;, or by directly
   editing the boot loader configuration file.
  </para>

  <para>
   The &yast; boot loader program is located at <menuchoice>
   <guimenu>&yast;</guimenu> <guimenu>System</guimenu> <guimenu>Boot
   Loader</guimenu> </menuchoice>. Click the <guimenu>Bootloader
   Options</guimenu> tab and select the line containing the &xen; kernel as
   the <guimenu>Default Boot Section</guimenu>.
  </para>

  <figure>
   <title>Boot Loader Settings</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="xen_bootloader.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="xen_bootloader.png" width="80%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Confirm with <guimenu>OK</guimenu>. Next time you boot the host, it will
   be ready to provide the &xen; virtualization environment.
  </para>

  <para>
   You can use the Boot Loader program to specify functionality, such as:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Pass kernel command line parameters.
    </para>
   </listitem>
   <listitem>
    <para>
     Specify the kernel image and initial RAM disk.
    </para>
   </listitem>
   <listitem>
    <para>
     Select a specific hypervisor.
    </para>
   </listitem>
   <listitem>
    <para>
     Pass additional parameters to the hypervisor. See
     <ulink
     url="http://xenbits.xen.org/docs/unstable/misc/xen-command-line.html"/>
     for their complete list.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   You can customize your virtualization environment by editing the
   <filename>/etc/default/grub</filename> file. Add the following line to
   this file: <literal>GRUB_CMDLINE_XEN="&lt;boot_parameters>"</literal>. Do
   not forget to run <command>grub2-mkconfig -o
   /boot/grub2/grub.cfg</command> after editing the file.
  </para>
 </sect1>
 <sect1 id="sec.xen.config.sparse">
  <title>Sparse Image Files and Disk Space</title>

  <para>
   If the host’s physical disk reaches a state where it has no available
   space, a virtual machine using a virtual disk based on a sparse image
   file is unable to write to its disk. Consequently, it reports I/O errors.
  </para>

  <para>
   The Reiser file system, perceiving a corrupt disk environment,
   automatically sets the file system to read-only. If this situation
   happens, you should free up available space on the physical disk, remount
   the virtual machine’s file system, and set the file system back to
   read-write.
  </para>

  <para>
   To check the actual disk requirements of a sparse image file, use the
   command <command>du -h &lt;image file&gt;</command>.
  </para>

  <para>
   To increase the available space of a sparse image file, first increase
   the file size and then the file system.
  </para>

  <warning>
   <title>Back Up Before Resizing</title>
   <para>
    Touching the sizes of partitions or sparse files always bears the risk
    of data failure. Do not work without a backup.
   </para>
  </warning>

  <para>
   The resizing of the image file can be done online, while the &vmguest; is
   running. Increase the size of a sparse image file with:
  </para>

<screen>dd if=/dev/zero of=&lt;image file&gt; count=0 bs=1M seek=&lt;new size in MB&gt;</screen>

  <para>
   For example, to increase the file
   <filename>/var/lib/xen/images/sles/disk0</filename> to a size of 16GB,
   use the command:
  </para>

<screen>dd if=/dev/zero of=/var/lib/xen/images/sles/disk0 count=0 bs=1M seek=16000</screen>

  <note>
   <title>Increasing Non-Sparse Images</title>
   <para>
    It is also possible to increase the image files of devices that are not
    sparse files. However, you must know exactly where the previous image
    ends. Use the seek parameter to point to the end of the image file and
    use a command similar to the following:
   </para>
<screen>dd if=/dev/zero of=/var/lib/xen/images/sles/disk0 seek=8000 bs=1M count=2000</screen>
  </note>

  <para>
   Be sure to use the right seek, else data loss may happen.
  </para>

  <para>
   If the &vmguest; is running during the resize operation, also resize the
   loop device that provides the image file to the &vmguest;. First detect
   the correct loop device with the command:
  </para>

<screen>losetup -j /var/lib/xen/images/sles/disk0</screen>

  <para>
   Then resize the loop device, for example <filename>/dev/loop0</filename>,
   with the following command:
  </para>

<screen>losetup -c /dev/loop0</screen>

  <para>
   Finally check the size of the block device inside the guest system with
   the command <command>fdisk -l /dev/xvdb</command>. The device name
   depends on the actually increased device.
  </para>

  <para>
   The resizing of the file system inside the sparse file involves tools
   that are depending on the actual file system. This is described in detail
   in the <xref linkend="stor_admin"/>.
  </para>
 </sect1>
 <sect1 id="sec.xen.manage.migrate">
  <title>Migrating &xen; &vmguest; Systems</title>

  <para>
   With &xen; it is possible to migrate a &vmguest; system from one &vmhost;
   to another with almost no service interruption. This could be used for
   example to move a busy &vmguest; to a &vmhost; that has stronger hardware
   or is not yet loaded. Or, if a service of a &vmhost; is required, all
   &vmguest; systems running on this machine can be migrated to other
   machines in order to avoid interruption of service. These are only two
   examples&mdash;many more reasons may apply to your personal situation.
  </para>

  <para>
   Before starting, some preliminary considerations regarding the &vmhost;
   should be taken into account:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     All &vmhost; systems should use a similar CPU. The frequency is not so
     important, but they should be using the same CPU family. To get more
     information about the used CPU, see <command>cat
     /proc/cpuinfo</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     All resources that are used by a specific guest system must be
     available on all involved &vmhost; systems. This means the network
     bridges must be in the same subnet, and all used block devices must
     exist on both &vmhost; systems.
    </para>
   </listitem>
   <listitem>
    <para>
     Using special features like <literal>&pciback;</literal> may be
     problematic. Do not implement these when deploying for an environment
     that should migrate &vmguest; systems between different &vmhost;
     systems.
    </para>
   </listitem>
   <listitem>
    <para>
     For fast migrations, a fast network is mandatory. If possible, use GB
     Ethernet and fast switches. Deploying VLAN might also help avoid
     collisions.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 id="sec.xen.manage.migrate.block">
   <title>Preparing Block Devices for Migrations</title>
   <para>
    The block devices needed by the &vmguest; system must be available on
    all involved &vmhost; systems. This is done by implementing some kind of
    shared storage that serves as container for the root file system of the
    migrated &vmguest; system. Common possibilities include:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>iSCSI</literal> can be set up to give access to the same
      block devices from different systems at the same time. For more
      information about iSCSI, see
      <xref linkend="cha.iscsi"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>NFS</literal> is a widely used root file system that can
      easily be accessed from different locations. For more information, see
      <xref linkend="cha.nfs"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>DRBD</literal> can be used if only two &vmhost; systems are
      involved. This gives some extra data security, because the used data is
      mirrored over the network. For more information, see the <citetitle>SUSE
      Linux Enterprise High Availability Extension &productnumber;</citetitle>
      documentation at <ulink url="http://www.suse.com/doc/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>SCSI</literal> can also be used if the available hardware
      permits shared access to the same disks.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>NPIV</literal> is a special mode to use fibre channel disks.
      However, in this case all migration hosts must be attached to the same
      fibre channel switch. For more information about NPIV, see
      <xref
      linkend="sec.xen.config.disk"/>. Commonly, this works if
      the fibre channel environment supports 4 Gbit or faster connections.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 id="sec.xen.manage.migrate.xl">
   <title>Migrating &vmguest; Systems</title>
   <para>
    The actual migration of the &vmguest; system is done with the command:
   </para>
<screen>xl migrate &lt;domain_name&gt; &lt;host&gt;</screen>
   <para>
    The speed of the migration depends on how fast the memory print can be
    saved to disk, sent to the new &vmhost; and loaded there. This means
    that small &vmguest; systems can be migrated faster than big systems
    with a lot of memory.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.xen.monitor">
  <title>Monitoring &xen;</title>

  <para>
   For a regular operation of many virtual guests, having a possibility to
   check the sanity of all the different &vmguest; systems is indispensable.
   &xen; offers several tools besides the system tools to gather information
   about the system.
  </para>

  <tip>
   <title>Monitoring the &vmhost;</title>
   <para>
    Basic monitoring of the &vmhost; (I/O and CPU) is available via the
    &vmm;. Refer to <xref linkend="cha.libvirt.admin.monitor.virt-manager"/>
    for details.
   </para>
  </tip>

  <sect2 id="sec.xen.monitor.xentop">
   <title>Monitor &xen; with <command>xentop</command></title>
   <para>
    The preferred terminal application to gather information about &xen;
    virtual environment is <command>xentop</command>. Unfortunately, this
    tool needs a rather broad terminal, else it inserts line breaks into the
    display.
   </para>
   <para>
    <command>xentop</command> has several command keys that can give you
    more information about the system that is monitored. Some of the more
    important are:
   </para>
   <variablelist>
    <varlistentry>
     <term>D</term>
     <listitem>
      <para>
       Change the delay between the refreshes of the screen.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>N</term>
     <listitem>
      <para>
       Also display network statistics. Note, that only standard
       configurations will be displayed. If you use a special configuration
       like a routed network, no network will be displayed at all.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>B</term>
     <listitem>
      <para>
       Display the respective block devices and their cumulated usage count.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For more information about <command>xentop</command> see the manual page
    <command>man 1 xentop</command>.
   </para>
  </sect2>

  <sect2 id="sec.xen.monitor.tools">
   <title>More Helpful Tools</title>
   <para>
    There are many different system tools that also help monitoring or
    debugging a running &sle; system. Many of these are covered in the
    official &sle; documentation. Especially useful for monitoring a
    virtualization environment are the following tools:
   </para>
   <variablelist>
    <varlistentry>
     <term>ip</term>
     <listitem>
      <para>
       The command line utility <command>ip</command> may be used to monitor
       arbitrary network interfaces. This is especially useful if you have
       set up a network that is routed or applied a masqueraded network. To
       monitor a network interface with the name
       <literal>&xenguest;.0</literal>, run the following command:
      </para>
<screen>watch ip -s link show &xenguest;.0</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <variablelist>
    <varlistentry>
     <term>brctl</term>
     <listitem>
      <para>
       In a standard setup, all the &xen; &vmguest; systems are attached to
       a virtual network bridge. <command>brctl</command> allows you to
       determine the connection between the bridge and the virtual network
       adapter in the &vmguest; system. For example, the output of
       <command>brctl show</command> may look like the following:
      </para>
<screen>bridge name     bridge id               STP enabled     interfaces
br0             8000.000476f060cc       no              eth0
                                                        vif1.0
br1             8000.00001cb5a9e7       no              vlan22 </screen>
      <para>
       This shows that there are two virtual bridges defined on the system.
       One is connected to the physical Ethernet device
       <literal>eth0</literal>, the other one is connected to a VLAN
       interface <literal>vlan22</literal>.
      </para>
      <para>
       There is only one guest interface active in this setup,
       <literal>vif1.0</literal>. This means that the guest with ID 1 has
       an Ethernet interface <literal>eth0</literal> assigned, that is
       connected to <literal>br0</literal> in the &vmhost;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>iptables-save</term>
     <listitem>
      <para>
       Especially when using masquerade networks, or if several Ethernet
       interfaces are set up together with a firewall setup, it may be
       helpful to check the current firewall rules.
      </para>
      <para>
       The command <command>iptables</command> may be used to check all the
       different firewall settings. To list all the rules of a chain, or
       even of the complete setup, you may use the commands
       <command>iptables-save</command> or <command>iptables -S</command>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 id="sec.xen.admin.vhostmd">
  <title>Providing Host Information for &vmguest; Systems</title>

  <para>
   In a standard &xen; environment, the &vmguest; systems have only very
   limited information about the &vmhost; system they are running on. If a
   guest should know more about the &vmhost; it runs on,
   <systemitem>vhostmd</systemitem> can provide more information to selected
   guests. To set up your system to run <systemitem>vhostmd</systemitem>,
   proceed as follows:
  </para>

  <procedure>
   <step>
    <para>
     Install the package vhostmd on the &vmhost;.
    </para>
   </step>
   <step>
    <para>
     Edit the file <filename>/etc/vhostmd/vhostmd.conf</filename> if you
     want to add or remove <literal>metric</literal> sections from the
     configuration. However, the default works well.
    </para>
   </step>
   <step>
    <para>
     Check the validity of the <filename>vhostmd.conf</filename>
     configuration file with the command:
    </para>
<screen>cd /etc/vhostmd
xmllint --postvalid --noout vhostmd.conf    
    </screen>
   </step>
   <step>
    <para>
     Start the vhostmd daemon with the command <command>sudo systemctl start
     vhostmd.service</command>.
    </para>
    <para>
     If vhostmd should be started automatically during start-up of the
     system, run the command:
    </para>
<screen>sudo systemctl enable vhostmd.service</screen>
   </step>
   <step>
    <para>
     Attach the image file <filename>/dev/shm/vhostmd0</filename> to the
     &vmguest; system named &xenguest; with the command:
    </para>
<screen>xl block-attach opensuse /dev/shm/vhostmd0,,xvdb,ro</screen>
   </step>
   <step>
    <para>
     Log on the &vmguest; system.
    </para>
   </step>
   <step>
    <para>
     Install the client package <systemitem>vm-dump-metrics</systemitem>.
    </para>
   </step>
   <step>
    <para>
     Run the command <command>vm-dump-metrics</command>. If you would like
     to have the result in a file, use the option <systemitem>-d
     &lt;filename&gt;</systemitem>.
    </para>
   </step>
  </procedure>

  <para>
   The result of the <systemitem>vm-dump-metrics</systemitem> is an XML
   output. The respective metric entries follow the DTD
   <filename>/etc/vhostmd/metric.dtd</filename>.
  </para>

  <para>
   For more information, see the manual pages <command>man 8
   vhostmd</command> and <filename>/usr/share/doc/vhostmd/README</filename>
   on the &vmhost; system. On the guest, see the manual page <command>man 1
   vm-dump-metrics</command>.
  </para>
 </sect1>
</chapter>
