<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>

<chapter xmlns="http://docbook.org/ns/docbook"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         version="5.0" xml:id="cha-tuning-cgroups">

 <title>Kernel Control Groups</title>
 <info>
  <abstract>
   <para>
    Kernel Control Groups (<quote>cgroups</quote>) are a kernel feature
    that allows assigning and limiting hardware and system resources for processes.
    Processes can also be organized in a hierarchical tree structure.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker>
   </dm:bugtracker>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec-tuning-cgroups-overview">
  <title>Overview</title>
  <para>
   Every process is assigned exactly one administrative cgroup. cgroups are ordered
   in a hierarchical tree structure. You can set resource limitations, such as
   CPU, memory, disk I/O, or network bandwidth usage,for single processes or for 
   whole branches of the hierarchy tree.
  </para>
  <para>
   On &productname;, &systemd; uses cgroups to organize all
   processes in groups, which &systemd; calls slices. &systemd; also
   provides an interface for setting cgroup properties.
  </para>
  <para>
   The command <command>systemd-cgls</command> displays the hierarchy
   tree.
  </para>
  <para>
   This chapter is an overview. For more details, refer to the listed
   references.
  </para>
 </sect1>

 <sect1 xml:id="sec-tuning-cgroups-usage">
  <title>Setting Resource Limits</title>
  <note>
    <title>Implicit Resource Consumption</title>
    <para>
      Be aware that resource consumption implicitly depends on the environment
      where your workload executes (e.g. size of data structures in libraries/kernel,
      forking behavior of utilities, computational efficiency),
      hence it is recommended to (re)calibrate your limits should the environment change.
    </para>
  </note>
  <para>
   Limitations to <literal>cgroups</literal> can be set with the
   <command>systemctl set-property</command> command. The syntax is:
  </para>
  <screen>&prompt.root;<command>systemctl set-property [--runtime] <replaceable>NAME</replaceable> <replaceable>PROPERTY1</replaceable>=<replaceable>VALUE</replaceable> [<replaceable>PROPERTY2</replaceable>=<replaceable>VALUE</replaceable>]</command></screen>
  <para>
   Optionally, use the <option>--runtime</option> option. With this
   option, set limits do not persist after the next reboot.
  </para>
  <para>
   Replace <replaceable>NAME</replaceable> with a &systemd; service
   slice, scope, socket, mount, or swap name. Replace properties with
   one or more of the following:
  </para>
  <variablelist>
   <varlistentry>
    <term><literal>CPUAccounting=</literal><option>[yes|no]</option></term>
    <listitem>
     <para>
      Turns on CPU usage accounting. This property takes
      <literal>yes</literal> and <literal>no</literal> as arguments.
     </para>
     <para>
      Example:
     </para>
     <screen>&prompt.root;<command>systemctl set-property user.slice CPUAccounting=yes</command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>CPUQuota=</literal><replaceable>PERCENTAGE</replaceable></term>
    <listitem>
     <para>
      Assigns a CPU time to processes. The value is a percentage
      followed by a <literal>%</literal> as suffix. This implies
      <literal>CPUAccounting=yes</literal>.
     </para>
     <para>
      Example:
     </para>
     <screen>&prompt.root;<command>systemctl set-property user.slice CPUQuota=50%</command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>MemoryAccounting=</literal><option>[yes|no]</option></term>
    <listitem>
     <para>
      Turns on memory usage accounting. This property takes
      <literal>yes</literal> and <literal>no</literal> as arguments.
     </para>
     <para>
      Example:
     </para>
     <screen>&prompt.root;<command>systemctl set-property user.slice MemoryAccounting=yes</command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>MemoryLow=</literal><replaceable>BYTES</replaceable></term>
    <listitem>
     <para>
      Unused memory from processes below this limit will not be
      reclaimed for other use. Use suffixes K, M, G or T for
      <replaceable>BYTES</replaceable>. This implies
      <literal>MemoryAccounting=yes</literal>.
     </para>
     <para>
      Example:
     </para>
     <screen>&prompt.root;<command>systemctl set-property nginx.service MemoryLow=512M</command></screen>
     <note>
      <title>Unified Control Group Hierarchy</title>
      <para>
       This setting is available only if the unified control group hierarchy is
       used, and disables <option>MemoryLimit=</option>. To enable the unified
       control group hierarchy, append
       <option>systemd.unified_cgroup_hierarchy=1</option> as a kernel command
       line parameter to the &grub; boot loader. Refer to <xref
        linkend="cha-grub2"/> for more details about configuring &grub;.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>MemoryHigh=</literal><replaceable>BYTES</replaceable></term>
    <listitem>
     <para>
      If more memory above this limit is used, memory is aggressively
      taken away from the processes. Use suffixes K, M, G or T for
      <replaceable>BYTES</replaceable>. This implies
      <literal>MemoryAccounting=yes</literal>.
      For example:
     </para>
     <screen>&prompt.root;<command>systemctl set-property nginx.service MemoryHigh=2G</command></screen>
     <note>
      <title>Unified Control Group Hierarchy</title>
      <para>
       This setting is available only if the unified control group hierarchy is
       used, and disables <option>MemoryLimit=</option>. To enable the unified
       control group hierarchy, append
       <option>systemd.unified_cgroup_hierarchy=1</option> as a kernel command
       line parameter to the &grub; boot loader.
       For more details about configuring &grub;, see <xref linkend="cha-grub2"/>.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>MemoryMax=</literal><replaceable>BYTES</replaceable></term>
    <listitem>
     <para>
      Sets a maximum limit for used memory. Processes will be killed if
      they use more memory than allowed. Use suffixes K, M, G or T for
      <replaceable>BYTES</replaceable>. This implies
      <literal>MemoryAccounting=yes</literal>.
     </para>
     <para>
      Example:
     </para>
     <screen>&prompt.root;<command>systemctl set-property nginx.service MemoryMax=4G</command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>DeviceAllow=</literal></term>
    <listitem>
     <para>
      Allows read (<literal>r</literal>), write (<literal>w</literal>)
      and mknod (<literal>m</literal>) access. The command takes a
      device node specifier and a list of <literal>r</literal>, <literal>w</literal> or
      <literal>m</literal>, separated by a white space.
     </para>
     <para>
      Example:
     </para>
     <screen>&prompt.root;<command>systemctl set-property system.slice DeviceAllow="/dev/sdb1 r"</command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>DevicePolicy=</literal><option>[auto|closed|strict]</option></term>
    <listitem>
     <para>
      When set to <literal>strict</literal>, only access to devices
      that are listed in <literal>DeviceAllow</literal> is allowed.
      <literal>closed</literal> additionally allows access to standard
      pseudo devices including <filename>/dev/null</filename>,
      <filename>/dev/zero</filename>, <filename>/dev/full</filename>,
      <filename>/dev/random</filename>, and
      <filename>/dev/urandom</filename>.
      <literal>auto</literal> allows access to all devices if no
      specific rule is defined in <literal>DeviceAllow</literal>.
      <literal>auto</literal> is the default setting.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   For more details and a complete list of properties, see <command>man
   systemd.resource-control</command>.
  </para>
 </sect1>
 
 <sect1 xml:id="sec-tuning-cgroups-tasksmax">
  <title>Preventing Fork Bombs with TasksMax</title>
   <para>
    &systemd; 228 shipped with a <literal>DefaultTasksMax</literal>
    limit of 512. This limited the number of processes any system unit
    can create at one time to 512. Previous versions had no default
    limit. The goal was to improve security by preventing runaway
    processes from creating excessive forks, or spawning enough
    threads to exhaust system resources.
   </para> 
   <para>
    However, it soon became apparent that there is not a single
    default that applies to all use cases. 512 is not low enough
    to prevent a runaway process from crashing a system, especially
    when other resources such as CPU and RAM are not restricted,
    and not high enough for processes that create a lot of threads,
    such as databases. In &systemd; 234 the default was changed to 15%,
    which is 4915 tasks (15% of the kernel limit of 32768;
    see <command>cat /proc/sys/kernel/pid_max</command>). This default is
    compiled, and can be changed in configuration files. The compiled
    defaults are documented in
    <filename>/etc/systemd/system.conf</filename>. You can edit this file
    to override the defaults, though there are other methods we will
    show in the following sections.
   </para>

   <sect2 xml:id="sec-tasksmax-defaults">
    <title>Finding the Current Default TasksMax Values</title>
    <para>
     &productname; ships with two custom configurations that override the
     upstream defaults for system units and for user slices, and sets them
     both to <literal>infinity</literal>.
     <filename>/usr/lib/systemd/system.conf.d/20-suse-defaults.conf</filename>
     contains these lines:
    </para>
<screen>
[Manager]
DefaultTasksMax=infinity
</screen>
    <para>
     <filename>/usr/lib/systemd/system/user-.slice.d/20-suse-defaults.conf</filename>
     contains these lines:
    </para>
<screen>[Slice]
TasksMax=infinity
</screen>
    <para>
     <literal>infinity</literal> means having no limit. It is not a
     requirement to change the default, but setting some limits may help to
     prevent system crashes from runaway processes.
    </para>
  </sect2>

  <sect2 xml:id="sec-edit-taskmax-default">
   <title>Overriding the DefaultTasksMax Value</title>
   <para>
    Change the global <literal>DefaultTasksMax</literal> value by creating
    a new override file,
    <filename>/etc/systemd/system.conf.d/10-system-tasksmax.conf</filename>,
    and write the following lines to set new default limit of 256 tasks per
    system unit:
  </para>
<screen>
[Manager]
DefaultTasksMax=256
</screen>
  <para>
   Load the new setting, then verify that it changed:
  </para>
<screen>&prompt.sudo;systemctl daemon-reload
&prompt.user;systemctl show --property DefaultTasksMax
DefaultTasksMax=256
</screen>
  <para>
   Adjust this default value to suit your needs. You can set higher
   limits on individual services as needed. This example is for MariaDB.
   First check the current active value:
  </para>
<screen>
&prompt.user;systemctl status mariadb.service
  ● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset>
   Active: active (running) since Tue 2020-05-26 14:15:03 PDT; 27min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
 Main PID: 11845 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 256)
   CGroup: /system.slice/mariadb.service
           └─11845 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
</screen>
  <para>
   The Tasks line shows that MariaDB currently has 30 tasks running, and has
   an upper limit of the default 256, which is inadequate for a database.
   The following example demonstrates how to raise MariaDB's limit to 8192.
   Create a new override file with <command>systemctl edit</command>, and
   enter the new value:
   </para>
<screen>&prompt.sudo;systemctl edit mariadb.service

[Service]
TasksMax=8192

&prompt.user;systemctl status mariadb.service 
● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disab>
  Drop-In: /etc/systemd/system/mariadb.service.d
           └─override.conf
   Active: active (running) since Tue 2020-06-02 17:57:48 PDT; 7min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
  Process: 3446 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper upgrade (code=exited, sta>
  Process: 3440 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper install (code=exited, sta>
 Main PID: 3452 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 8192)
   CGroup: /system.slice/mariadb.service
           └─3452 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
</screen>
   <para>
    <command>systemctl edit</command> creates an override file,
    <filename>/etc/systemd/system/mariadb.service.d/override.conf</filename>,
    that contains only the changes you want to apply to the existing unit file.
    The value does not have to be 8192, but should be whatever limit is
    appropriate for your workloads.
   </para>
  </sect2>

  <sect2>
   <title>Default TasksMax Limit on Users</title>
   <para>
    The default limit on users should be fairly high, because user sessions
    need more resources.
    Set your own default for users by creating a new file, for example
    <filename>/etc/systemd/system/user-.slice.d/user-taskmask.conf</filename>.
    The following example sets a default of 16284:
   </para>
<screen>
[Slice]
TasksMax=16284
</screen>
   <para>
    Then reload systemd to load the new value, and verify the change:
   </para>
<screen>&prompt.sudo;systemctl daemon-reload

&prompt.user;systemctl show --property TasksMax user-.slice
TasksMax=16284

&prompt.user;systemctl show --property TasksMax user-1000.slice
TasksMax=16284
</screen>
   <para>
    How do you know what values to use? This varies according to your workloads,
    system resources, and other resource configurations. When your TasksMax
    value is too low, you will see error messages such as
    <emphasis>Failed to fork (Resources temporarily unavailable)</emphasis>,
    <emphasis>Can't create thread to handle new connection</emphasis>, and
    <emphasis>Error: Function call 'fork' failed with error code 11,
    'Resource temporarily unavailable'</emphasis>.
   </para>
   <para>
    For more information on configuring system resources in systemd, see
    <literal>systemd.resource-control (5)</literal>.
   </para>
  </sect2>
 </sect1>

 <sect1>
  <title>Controlling I/O with Proportional Weight Policy</title>

  <sect2>
   <title>Using cgroup-v1</title>

   <para>
    Assume you have a reader processes and you want to grant it a
    higher priority for I/O operations then other reader processes
    accessing the same disk. This can be managed with proportional
    weight policy files:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <filename>blkio.weight</filename> (only available in older
      kernels with legacy block layer and using the CFQ I/O scheduler)
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>blkio.bfq.weight</filename> (only available in newer
      kernels with blk-mq and using BFQ I/O scheduler)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Running a single reader process (reading from SSD) without
    controlling its I/O:
   </para>
<screen>
/mnt/tests/io-cgroup/> sudo su -c "sync; echo 3 > /proc/sys/vm/drop_caches"
/mnt/tests/io-cgroup/> echo $$; dd if=file2 of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 1.33049 s, 404 MB/s
</screen>
   <para>
    Now running a background process reading from the same disk.
   </para>
<screen>
[reader1] /mnt/tests/io-cgroup/> sudo su -c "sync; echo 3 > /proc/sys/vm/drop_caches"
[reader1] /mnt/tests/io-cgroup/> echo $$; dd if=file1 of=/dev/null bs=4k
5220
...
[reader2] /mnt/tests/io-cgroup/> echo $$; dd if=file2 of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 2.61592 s, 205 MB/s
</screen>
   <para>
    Both processes get an equal share for I/O, halving throughput for
    each process. Now we set up two control groups - one for each
    process - check that BFQ is used and set different weights:
   </para>
<screen>
[I/O controller] > cd /sys/fs/cgroup/blkio/
[I/O controller] /sys/fs/cgroup/blkio> sudo mkdir reader1
[I/O controller] /sys/fs/cgroup/blkio> sudo mkdir reader2
[I/O controller] /sys/fs/cgroup/blkio> sudo su -c "echo 5220 > reader1/cgroup.procs"
[I/O controller] /sys/fs/cgroup/blkio> sudo su -c "echo 5251 > reader2/cgroup.procs"
[I/O controller] /sys/fs/cgroup/blkio> cat /sys/block/sda/queue/scheduler
mq-deadline kyber [bfq] none
[I/O controller] /sys/fs/cgroup/blkio> cat reader1/blkio.bfq.weight
100
[I/O controller] /sys/fs/cgroup/blkio> sudo su -c "echo 200 >reader1/blkio.bfq.weight"
[I/O controller] /sys/fs/cgroup/blkio> cat reader2/blkio.bfq.weight
200
</screen>
   <para>
    With this settings and reader1 in background, reader2 should
    achieve higher throughput than before:
   </para>
<screen>
[reader1] /mnt/tests/io-cgroup/> sudo su -c "sync; echo 3 > /proc/sys/vm/drop_caches"
[reader1] /mnt/tests/io-cgroup/> echo $$; dd if=file1 of=/dev/null bs=4k
5220
...
[reader2] /mnt/tests/io-cgroup/> echo $$; dd if=file2 of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 2.06604 s, 260 MB/s
</screen>
   <para>
    The higher proportional weight resulted in higher throughput for reader2.
    Now doubling its weight again:
   </para>
<screen>
[I/O controller] /sys/fs/cgroup/blkio> cat reader1/blkio.bfq.weight
100
[I/O controller] /sys/fs/cgroup/blkio> sudo su -c "echo 400 >reader1/blkio.bfq.weight"
[I/O controller] /sys/fs/cgroup/blkio> cat reader2/blkio.bfq.weight
400
</screen>
   <para>
    This results in another increase in throughput for reader2:
   </para>
<screen>
[reader1] /mnt/tests/io-cgroup/> sudo su -c "sync; echo 3 > /proc/sys/vm/drop_caches"
[reader1] /mnt/tests/io-cgroup/> echo $$; dd if=file1 of=/dev/null bs=4k
5220
...
[reader2] /mnt/tests/io-cgroup/> echo $$; echo $$; dd if=file2 of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 1.69026 s, 318 MB/s
</screen>
  </sect2>
  <sect2>
   <title>Using cgroup-v2</title>
   <para>
    First we need to make sure that Block IO controller is not active
    under cgroup-v1. Thus we boot with kernel parameter
    <option>cgroup_no_v1=blkio</option>. Block IO controller is
    inactive in cgroup-v1 and IO controller shows up as an option for
    cgroup-v2.
   </para>
<screen>
$ cat /proc/cmdline
BOOT_IMAGE=... cgroup_no_v1=blkio ...
$ ls /sys/fs/cgroup/blkio/
$ cat /sys/fs/cgroup/unified/cgroup.controllers
io
</screen>
 <para>
   Next steps are: enable IO controller, create 2 control groups,
   ensure that BFQ is used, set weights.
   </para>
<screen>
[I/O controller] > cd /sys/fs/cgroup/unified/
[I/O controller] > sudo su -c "echo '+io' > cgroup.subtree_control"
[I/O controller] > cat cgroup.subtree_control
io
[I/O controller] /sys/fs/cgroup/unified> sudo mkdir reader1
[I/O controller] /sys/fs/cgroup/unified> sudo mkdir reader2
[I/O controller] /sys/fs/cgroup/unified> sudo su -c "echo 3712 >reader1/cgroup.procs"
[I/O controller] /sys/fs/cgroup/unified> sudo su -c "echo 3744 >reader2/cgroup.procs"
[I/O controller] /sys/fs/cgroup/unified> cat /sys/block/sda/queue/scheduler
mq-deadline kyber [bfq] none
[I/O controller] /sys/fs/cgroup/unified> cat reader1/io.bfq.weight
default 100
[I/O controller] /sys/fs/cgroup/unified> sudo su -c "echo 400 >reader2/io.bfq.weight"
[I/O controller] /sys/fs/cgroup/unified> cat reader2/io.bfq.weight
default 400
</screen>
   <para>
    With this setting throughput looks like:
   </para>
<screen>
[reader1] /mnt/tests/io-cgroup/> sudo su -c "sync; echo 3 > /proc/sys/vm/drop_caches"
[reader1] /mnt/tests/io-cgroup/> echo $$; dd if=file1 of=/dev/null bs=4k
3712
...
[reader2] /mnt/tests/io-cgroup/> echo $$; dd if=file2 of=/dev/null bs=4k count=131072
3744
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 1.75746 s, 305 MB/s
</screen>
  </sect2>
 </sect1>
 <sect1>
  <title>For More Information</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Kernel documentation (package <systemitem>kernel-source</systemitem>):
     files in <filename>/usr/src/linux/Documentation/cgroups-v1</filename>,
     file <filename>/usr/src/linux/Documentation/cgroups-v2.rst</filename>, and
     <filename>/usr/src/linux/Documentation/block/bfq-iosched.rst</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://lwn.net/Articles/604609/"/>&mdash;Brown,
     Neil: Control Groups Series (2014, 7 parts).
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://lwn.net/Articles/243795/"/>&mdash;Corbet,
     Jonathan: Controlling memory use in containers (2007).
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://lwn.net/Articles/236038/"/>&mdash;Corbet,
     Jonathan: Process containers (2007).
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
