<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.xen.vbd">
 <title>Block Devices in &xen;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
  </dm:docmanager>
 </info>
 <para/>
 <sect1 xml:id="sec.xen.config.disk">
  <title>Mapping Physical Storage to Virtual Disks</title>

  <para>
   The disk(s) specification for a &xen; domain in the domain configuration
   file is as straightforward as the following example:
  </para>

<screen>disk = [ 'format=raw,vdev=hdc,access=ro,devtype=cdrom,target=/root/image.iso' ]</screen>

  <para>
   It defines a disk block device based on the
   <filename>/root/image.iso</filename> disk image file. The disk will be seen
   as <literal>hdc</literal> by the guest, with read-only
   (<literal>ro</literal>) access. The type of the device is
   <literal>cdrom</literal> with <literal>raw</literal> format.
  </para>

  <para>
   The following example defines an identical device, but using simplified
   positional syntax:
  </para>

<screen>disk = [ '/root/image.iso,raw,hdc,ro,cdrom' ]</screen>

  <para>
   You can include more disk definitions in the same line, each one separated
   by a comma. If a parameter is not specified, then its default value is
   taken:
  </para>

<screen>disk = [ '/root/image.iso,raw,hdc,ro,cdrom','/dev/vg/guest-volume,,hda','...' ]</screen>

  <variablelist>
   <title>List of Parameters</title>
   <varlistentry>
    <term>target</term>
    <listitem>
     <para>
      Source block device or disk image file path.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>format</term>
    <listitem>
     <para>
      The format of the image file. Default is <literal>raw</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>vdev</term>
    <listitem>
     <para>
      Virtual device as seen by the guest. Supported values are hd[x], xvd[x],
      sd[x] etc. See
      <filename>/usr/share/doc/packages/xen/misc/vbd-interface.txt</filename>
      for more details. This parameter is mandatory.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>access</term>
    <listitem>
     <para>
      Whether the block device is provided to the guest in read-only or
      read-write mode. Supported values are <literal>ro</literal> or
      <literal>r</literal> for read-only, and <literal>rw</literal> or
      <literal>w</literal> for read/write access. Default is
      <literal>ro</literal> for <literal>devtype=cdrom</literal>, and
      <literal>rw</literal> for other device types.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>devtype</term>
    <listitem>
     <para>
      Qualifies virtual device type. Supported value is
      <literal>cdrom</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>backendtype</term>
    <listitem>
     <para>
      The back-end implementation to use. Supported values are
      <literal>phy</literal>, <literal>tap</literal>, and
      <literal>qdisk</literal>. Normally this option should not be specified as
      the back-end type is automatically determined.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>script</term>
    <listitem>
     <para>
      Specifies that <literal>target</literal> is not a normal host path, but
      rather information to be interpreted by the executable program. The
      specified script file is looked for in
      <filename>/etc/xen/scripts</filename> if it does not point to an absolute
      path. These scripts are normally called
      <literal>block-&lt;script_name&gt;</literal>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For more information about specifying virtual disks, see
   <filename>/usr/share/doc/packages/xen/misc/xl-disk-configuration.txt</filename>.
  </para>
 </sect1>
 <sect1 xml:id="sec.xen.config.netdisk">
  <title>Mapping Network Storage to Virtual Disk</title>

  <para>
   Similar to mapping a local disk image (see
   <xref linkend="sec.xen.config.disk"/>), you can map a network disk as a
   virtual disk as well.
  </para>

  <para>
   The following example shows mapping of an RBD (RADOS Block Device) disk with
   multiple Ceph monitors and cephx authentication enabled:
  </para>

<screen>disk = [ 'vdev=hdc, backendtype=qdisk, \
target=rbd:libvirt-pool/new-libvirt-image:\
id=libvirt:key=AQDsPWtW8JoXJBAAyLPQe7MhCC+JPkI3QuhaAw==:auth_supported=cephx;none:\
mon_host=137.65.135.205\\:6789;137.65.135.206\\:6789;137.65.135.207\\:6789' ]</screen>

  <para>
   Following is an example of an NBD (Network Block Device) disk mapping:
  </para>

<screen>disk = [ 'vdev=hdc, backendtype=qdisk, target=nbd:151.155.144.82:5555' ]</screen>
 </sect1>
 <sect1 xml:id="sec.xen.config.filedisk">
  <title>File-Backed Virtual Disks and Loopback Devices</title>

  <para>
   When a virtual machine is running, each of its file-backed virtual disks
   consumes a loopback device on the host. By default, the host allows up to 64
   loopback devices to be consumed.
  </para>

  <para>
   To simultaneously run more file-backed virtual disks on a host, you can
   increase the number of available loopback devices by adding the following
   option to the hostâ€™s <filename>/etc/modprobe.conf.local</filename> file.
  </para>

<screen>
options loop max_loop=x
</screen>

  <para>
   where <command>x</command> is the maximum number of loopback devices to
   create.
  </para>

  <para>
   Changes take effect after the module is reloaded.
  </para>

  <tip>
   <para>
    Enter <command>rmmod loop</command> and <command>modprobe loop</command> to
    unload and reload the module. In case <command>rmmod</command> does not
    work, unmount all existing loop devices or reboot the computer.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="sec.xen.block.resize">
  <title>Resizing Block Devices</title>

  <para>
   While it is always possible to add new block devices to a &vmguest; system,
   it is sometimes more desirable to increase the size of an existing block
   device. In case such a system modification is already planned during
   deployment of the &vmguest;, some basic considerations should be done:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Use a block device that may be increased in size. LVM devices and file
     system images are commonly used.
    </para>
   </listitem>
   <listitem>
    <para>
     Do not partition the device inside the &vmguest;, but use the main device
     directly to apply the file system. For example, use
     <filename>/dev/xvdb</filename> directly instead of adding partitions to
     <filename>/dev/xvdb</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure that the file system to be used can be resized. Sometimes, for
     example with Ext3, some features must be switched off to be able to resize
     the file system. A file system that can be resized online and mounted is
     <literal>XFS</literal>. Use the command <command>xfs_growfs</command> to
     resize that file system after the underlying block device has been
     increased in size. For more information about <literal>XFS</literal>, see
     <command>man 8 xfs_growfs</command>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   When resizing an LVM device that is assigned to a &vmguest;, the new size is
   automatically known to the &vmguest;. No further action is needed to inform
   the &vmguest; about the new size of the block device.
  </para>

  <para>
   When using file system images, a loop device is used to attach the image
   file to the guest. For more information about resizing that image and
   refreshing the size information for the &vmguest;, see
   <xref linkend="sec.xen.config.sparse"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec.xen.block.advstoragescripts">
  <title>Scripts for Managing Advanced Storage Scenarios</title>
  <!-- fate#319810: Xen + block-drbd + libvirt -->

  <para>
   There are scripts that can help with managing advanced storage scenarios
   such as disk environments provided by
   <command>dmmd</command> (<quote>device mapper&mdash;multi
   disk</quote>) including LVM environments built upon a software RAID set, or
   a software RAID set built upon an LVM environment. These scripts are part of
   the <package>xen-tools</package> package. After installation, they can be
   found in <filename>/etc/xen/scripts</filename>:
  </para>

  <itemizedlist>
   <listitem>
    <para>
      <command>block-dmmd</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>block-drbd-probe</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>block-npiv</command>
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The scripts allow for external commands to perform some action, or
   series of actions of the block devices prior to serving them up to a
   guest.
  </para>
  <para>
   These scripts could formerly only be used with <command>xl</command>
   or <command>libxl</command> using the disk configuration syntax
   <literal>script=</literal>.  They can now be used with libvirt by
   specifying the base name of the block script in the
   <literal>&lt;source&gt;</literal> element of the disk. For example:
  </para>

  <screen>&lt;source dev='dmmd:md;/dev/md0;lvm;/dev/vgxen/lv-vm01'/&gt;</screen>

 </sect1>
</chapter>
