<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-nvmeof" xml:lang="en">
  <title>&nvmeof;</title>
  <info>
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
      <dm:bugtracker></dm:bugtracker>
      <dm:translation>yes</dm:translation>
    </dm:docmanager>
    <abstract>
      <para>
        This chapter describes how to set up an &nvmeoff; host and target.
      </para>
    </abstract>
  </info>
  <sect1 xml:id="sec-nvmeof-overview">
    <title>Overview</title>

    <para>
      <emphasis><trademark class="registered">NVM
      Express</trademark></emphasis>
      (<emphasis><trademark class="registered">&nvme;</trademark></emphasis>)
      is an interface standard for accessing non-volatile storage, commonly SSD
      disks. &nvme; supports much higher speeds and has a lower latency than
      SATA.
    </para>

    <para>
      <emphasis><trademark class="trade">&nvmeof;</trademark></emphasis> is an
      architecture to access &nvme; storage over different networking
      fabrics&mdash;for example, <emphasis>RDMA</emphasis>,
      <emphasis>TCP</emphasis>, or <emphasis>&nvme; over Fibre
      Channel</emphasis> (<emphasis>&fcnvme;</emphasis>). The role of &nvmeof;
      is similar to iSCSI. To increase the fault-tolerance, &nvmeof; has a
      built-in support for multipathing. The &nvmeof; multipathing is not based
      on the traditional &dmmpio;.
    </para>

    <para>
      The <emphasis>&nvme; host</emphasis> is the machine that connects to an
      &nvme; target. The <emphasis>&nvme; target</emphasis> is the machine that
      shares its &nvme; block devices.
    </para>

    <para>
      &nvme; is supported on &sls; &productnumber;. There are Kernel modules
      available for the &nvme; block storage and &nvmeof; target and host.
    </para>

    <para>
      To see if your hardware requires any special consideration, refer to
      <xref linkend="sec-nvmeof-hardware" />.
    </para>
  </sect1>
  <sect1 xml:id="sec-nvmeof-host-configuration">
    <title>Setting up an &nvmeof; host</title>

    <para>
      To use &nvmeof;, a target must be available with one of the supported
      networking methods. Supported are &nvme; over Fibre Channel, TCP, and
      RDMA. The following sections describe how to connect a host to an &nvme;
      target.
    </para>

    <sect2 xml:id="sec-nvmeof-host-configuration-cli">
      <title>Installing command line client</title>
      <para>
        To use &nvmeof;, you need the <command>nvme</command> command line
        tool. Install it with <command>zypper</command>:
      </para>
<screen>&prompt.sudo;<command>zypper in nvme-cli</command></screen>
      <para>
        Use <command>nvme --help</command> to list all available subcommands.
        Man pages are available for <command>nvme</command> subcommands.
        Consult them by executing <command>man
        nvme-<replaceable>SUBCOMMAND</replaceable></command>. For example, to
        view the man page for the <option>discover</option> subcommand, execute
        <command>man nvme-discover</command>.
      </para>
    </sect2>

    <sect2 xml:id="sec-nvmeof-host-configuration-target-discovery">
      <title>Discovering &nvmeof; targets</title>
      <para>
        To list available &nvme; subsystems on the &nvmeof; target, you need
        the discovery controller address and service ID.
      </para>
<screen>&prompt.sudo;<command>nvme discover -t <replaceable>TRANSPORT</replaceable> -a <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> -s <replaceable>SERVICE_ID</replaceable></command></screen>
      <para>
        Replace <replaceable>TRANSPORT</replaceable> with the underlying
        transport medium: <option>loop</option>, <option>rdma</option>,
        <option>tcp</option>, or <option>fc</option>. Replace
        <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> with the
        address of the discovery controller. For RDMA and TCP, this should be
        an IPv4 address. Replace <replaceable>SERVICE_ID</replaceable> with the
        transport service ID. If the service is IP based, like RDMA or TCP,
        service ID specifies the port number. For Fibre Channel, the service ID
        is not required.
      </para>
      <para>
        The &nvme; hosts only see the subsystems they are allowed to connect
        to.
      </para>
      <para>
        Example:
      </para>
<screen>&prompt.sudo;nvme discover -t tcp -a 10.0.0.3 -s 4420</screen>
      <para>
        For the FC, the example looks as follows:
      </para>
<screen>&prompt.sudo;nvme discover --transport=fc \
                --traddr=nn-0x201700a09890f5bf:pn-0x201900a09890f5bf \
                --host-traddr=nn-0x200000109b579ef6:pn-0x100000109b579ef6</screen>
      <para>
        For more details, see <command>man nvme-discover</command>.
      </para>
    </sect2>

    <sect2 xml:id="sec-nvmeof-host-configuration-connect-target">
      <title>Connecting to &nvmeof; targets</title>
      <para>
        After you have identified the &nvme; subsystem, you can connect it with
        the <command>nvme connect</command> command.
      </para>
<screen>&prompt.sudo;<command>nvme connect -t <replaceable>transport</replaceable> -a <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> -s <replaceable>SERVICE_ID</replaceable> -n <replaceable>SUBSYSTEM_NQN</replaceable></command></screen>
      <para>
        Replace <replaceable>TRANSPORT</replaceable> with the underlying
        transport medium: <option>loop</option>, <option>rdma</option>,
        <option>tcp</option> or <option>fc</option>. Replace
        <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> with the
        address of the discovery controller. For RDMA and TCP this should be an
        IPv4 address. Replace <replaceable>SERVICE_ID</replaceable> with the
        transport service ID. If the service is IP based, like RDMA or TCP,
        this specifies the port number. Replace
        <replaceable>SUBSYSTEM_NQN</replaceable> with the NVMe qualified name
        of the desired subsystem as found by the discovery command.
        <emphasis>NQN</emphasis> is the abbreviation for <emphasis> &nvme;
        Qualified Name</emphasis>. The NQN must be unique.
      </para>
      <para>
        Example:
      </para>
<screen>&prompt.sudo;<command>nvme connect -t tcp -a 10.0.0.3 -s 4420 -n nqn.2014-08.com.example:nvme:nvm-subsystem-sn-d78432</command></screen>
      <para>
        For the FC, the example looks as follows:
      </para>
<screen>&prompt.sudo;nvme connect --transport=fc \
             --traddr=nn-0x201700a09890f5bf:pn-0x201900a09890f5bf \
             --host-traddr=nn-0x200000109b579ef6:pn-0x100000109b579ef6 \
             --nqn=nqn.2014-08.org.nvmexpress:uuid:1a9e23dd-466e-45ca-9f43-a29aaf47cb21</screen>
      <para>
        Alternatively, use <command>nvme connect-all</command> to connect to
        all discovered namespaces. For advanced usage, see <command>man
        nvme-connect</command> and <command>man nvme-connect-all</command>.
      </para>
      <para>
        In case of a path loss, the &nvme; subsystem tries to reconnect for a
        time period, defined by the <literal>ctrl-loss-tmo</literal> option of
        the <command>nvme connect</command> command. After this time (default
        value is 600s), the path is removed and the upper layers of the block
        layer (file system) are notified. By default, the file system is then
        mounted read-only, which usually is not the expected behavior.
        Therefore, it is recommended to set the
        <literal>ctrl-loss-tmo</literal> option so that the &nvme; subsystem
        keeps trying to reconnect without a limit. To do so, run the following
        command:
      </para>
<screen>&prompt.sudo;nvme connect --ctrl-loss-tmo=-1</screen>
      <para>
        To make an NVMe over Fabrics subsystem available at boot, create a
        <filename>/etc/nvme/discovery.conf</filename> file on the host with the
        parameters passed to the <command>discover</command> command (as
        described in
        <xref linkend="sec-nvmeof-host-configuration-target-discovery"/>. For
        example, if you use the <command>discover</command> command as follows:
      </para>
<screen>&prompt.sudo;nvme discover -t tcp -a 10.0.0.3 -s 4420</screen>
      <para>
        Add the parameters of the <command>discover</command> command to the
        <filename>/etc/nvme/discovery.conf</filename> file:
      </para>
<screen>echo "-t tcp -a 10.0.0.3 -s 4420" | sudo tee -a /etc/nvme/discovery.conf</screen>
      <para>
        Then enable the <guimenu>nvmf-autoconnect</guimenu> service:
      </para>
<screen>&prompt.sudo;systemctl enable nvmf-autoconnect.service</screen>
    </sect2>

    <sect2 xml:id="sec-nvmeof-host-configuration-multipathing">
      <title>Multipathing</title>
      <para>
        &nvme; native multipathing is enabled by default. If the
        <option>CMIC</option> option in the controller identity settings is
        set, the NVMe stack recognizes an NVME drive as a multipathed device by
        default.
      </para>
      <para>
        To manage the multipathing, you can use the following:
      </para>
      <variablelist>
        <title>Managing multipathing</title>
        <varlistentry>
          <term><command>nvme list-subsys</command></term>
          <listitem>
            <para>
              Prints the layout of the multipath devices.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><command>multipath -ll</command></term>
          <listitem>
            <para>
              The command has a compatibility mode and displays &nvme;
              multipath devices. Bear in mind that you need to enable the
              <literal>enable_foreign</literal> option to use the command. For
              details, refer to <xref linkend="sec-multipath-conf-misc"/>.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>nvme-core.multipath=N</option></term>
          <listitem>
            <para>
              When the option is added as a boot parameter, the NVMe native
              multipathing will be disabled.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
      <note>
        <title>Using <command>iostat</command> in multipath setups</title>
        <para>
          The <command>iostat</command> command might not show all controllers
          that are listed by <command>nvme list-subsys</command>. By default,
          <command>iostat</command> filters out all block devices with no I/O.
          To make <command>iostat</command> show <emphasis>all</emphasis>
          devices, use:
        </para>
<screen>iostat -p ALL</screen>
      </note>
    </sect2>
  </sect1>
  <sect1 xml:id="sec-nvmeof-target-configuration">
    <title>Setting up an &nvmeof; target</title>

    <sect2 xml:id="sec-nvmeof-target-configuration-cli">
      <title>Installing command line client</title>
      <para>
        To configure an &nvmeof; target, you need the
        <command>nvmetcli</command> command line tool. Install it with
        <command>zypper</command>:
      </para>
<screen>&prompt.sudo;<command>zypper in nvmetcli</command></screen>
      <para>
        The current documentation for <command>nvmetcli</command> is available
        at
        <link xlink:href="http://git.infradead.org/users/hch/nvmetcli.git/blob_plain/HEAD:/Documentation/nvmetcli.txt"/>.
      </para>
    </sect2>

    <sect2 xml:id="sec-nvmeof-target-configuration-steps">
      <title>Configuration steps</title>
      <para>
        The following procedure provides an example of how to set up an
        &nvmeof; target.
      </para>
      <para>
        The configuration is stored in a tree structure. Use the command
        <command>cd</command> to navigate. Use <command>ls</command> to list
        objects. You can create new objects with <command>create</command>.
      </para>
      <procedure>
        <step>
          <para>
            Start the <command>nvmetcli</command> interactive shell:
          </para>
<screen>&prompt.sudo;<command>nvmetcli</command></screen>
        </step>
        <step>
          <para>
            Create a new port:
          </para>
<screen>&prompt.nvmetcli;<command>cd ports</command>
&prompt.nvmetcli;<command>create 1</command>
&prompt.nvmetcli;<command>ls 1/</command>
o- 1
  o- referrals
  o- subsystems</screen>
        </step>
        <step>
          <para>
            Create an &nvme; subsystem:
          </para>
<screen>&prompt.nvmetcli;<command>cd /subsystems</command>
&prompt.nvmetcli;<command>create nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82</command>
&prompt.nvmetcli;<command>cd nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82/</command>
&prompt.nvmetcli;<command>ls</command>
o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
  o- allowed_hosts
  o- namespaces</screen>
        </step>
        <step>
          <para>
            Create a new namespace and set an &nvme; device to it:
          </para>
<screen>&prompt.nvmetcli;<command>cd namespaces</command>
&prompt.nvmetcli;<command>create 1</command>
&prompt.nvmetcli;<command>cd 1</command>
&prompt.nvmetcli;<command>set device path=/dev/nvme0n1</command>
Parameter path is now '/dev/nvme0n1'.</screen>
        </step>
        <step>
          <para>
            Enable the previously created namespace:
          </para>
<screen>&prompt.nvmetcli;<command>cd ..</command>
&prompt.nvmetcli;<command>enable</command>
The Namespace has been enabled.</screen>
        </step>
        <step>
          <para>
            Display the created namespace:
          </para>
<screen>&prompt.nvmetcli;<command>cd ..</command>
&prompt.nvmetcli;<command>ls</command>
o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
  o- allowed_hosts
  o- namespaces
    o- 1</screen>
        </step>
        <step>
          <para>
            Allow all hosts to use the subsystem. Only do this in secure
            environments.
          </para>
<screen>&prompt.nvmetcli;<command>set attr allow_any_host=1</command>
Parameter allow_any_host is now '1'.</screen>
          <para>
            Alternatively, you can allow only specific hosts to connect:
          </para>
<screen>&prompt.nvmetcli;<command>cd nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82/allowed_hosts/</command>
&prompt.nvmetcli;<command>create hostnqn</command></screen>
        </step>
        <step>
          <para>
            List all created objects:
          </para>
<screen>&prompt.nvmetcli;<command>cd /</command>
&prompt.nvmetcli;<command>ls</command>
o- /
  o- hosts
  o- ports
  | o- 1
  |   o- referrals
  |   o- subsystems
  o- subsystems
    o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
      o- allowed_hosts
      o- namespaces
        o- 1</screen>
        </step>
        <step>
          <para>
            Make the target available via TCP. Use
            <literal>trtype=rdma</literal> for RDMA:
          </para>
<screen>&prompt.nvmetcli;<command>cd ports/1/</command>
&prompt.nvmetcli;<command>set addr adrfam=ipv4 trtype=tcp traddr=10.0.0.3 trsvcid=4420</command>
Parameter trtype is now 'tcp'.
Parameter adrfam is now 'ipv4'.
Parameter trsvcid is now '4420'.
Parameter traddr is now '10.0.0.3'.</screen>
          <para>
            Alternatively, you can make it available with Fibre Channel:
          </para>
<screen>&prompt.nvmetcli;<command>cd ports/1/</command>
&prompt.nvmetcli;<command>set addr adrfam=fc trtype=fc traddr=nn-0x1000000044001123:pn-0x2000000055001123 trsvcid=none</command></screen>
        </step>
        <step>
          <para>
            Link the subsystem to the port:
          </para>
<screen>&prompt.nvmetcli;<command>cd /ports/1/subsystems</command>
&prompt.nvmetcli;<command>create nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82</command>      
      </screen>
          <para>
            Now you can verify that the port is enabled using
            <command>dmesg</command>:
          </para>
<screen>&prompt.root;dmesg
        ...
[  257.872084] nvmet_tcp: enabling port 1 (10.0.0.3:4420)
      </screen>
        </step>
      </procedure>
    </sect2>

    <sect2 xml:id="sec-nvmeof-target-configuration-backup-configuration">
      <title>Back up and restore target configuration</title>
      <para>
        You can save the target configuration in a JSON file with the following
        commands:
      </para>
<screen>&prompt.sudo;<command>nvmetcli</command>
&prompt.nvmetcli;<command>saveconfig nvme-target-backup.json</command></screen>
      <para>
        To restore the configuration, use:
      </para>
<screen>&prompt.nvmetcli;<command>restore nvme-target-backup.json</command></screen>
      <para>
        You can also wipe the current configuration:
      </para>
<screen>&prompt.nvmetcli;<command>clear</command></screen>
    </sect2>
  </sect1>
  <sect1 xml:id="sec-nvmeof-hardware">
    <title>Special hardware configuration</title>

    <sect2 xml:id="sec-nvmeof-hardware-overview">
      <title>Overview</title>
      <para>
        Some hardware needs special configuration to work correctly. Skim the
        titles of the following sections to see if you are using any of the
        mentioned devices or vendors.
      </para>
    </sect2>

    <sect2 xml:id="sec-nvmeof-hardware-broadcom">
      <title>Broadcom</title>
      <para>
        If you are using the <emphasis>Broadcom Emulex LightPulse Fibre Channel
        SCSI</emphasis> driver, add a Kernel configuration parameter on the
        target and host for the <literal>lpfc</literal> module:
      </para>
<screen>&prompt.sudo;<command>echo "options lpfc lpfc_enable_fc4_type=3" > /etc/modprobe.d/lpfc.conf</command></screen>
      <para>
        Make sure that the Broadcom adapter firmware has at least version
        11.4.204.33. Also make sure that you have the current versions of
        <package>nvme-cli</package>, <package>nvmetcli</package> and the Kernel
        installed.
      </para>
      <para>
        To enable a Fibre Channel port as an &nvme; target, an additional
        module parameter needs to be configured:
        <option>lpfc_enable_nvmet=<replaceable>
        COMMA_SEPARATED_WWPNS</replaceable></option>. Enter the WWPN with a
        leading <literal>0x</literal>, for example
        <option>lpfc_enable_nvmet=0x2000000055001122,0x2000000055003344</option>.
        Only listed WWPNs will be configured for target mode. A Fibre Channel
        port can either be configured as target <emphasis>or</emphasis> as
        initiator.
      </para>
    </sect2>

    <sect2 xml:id="sec-nvmeof-hardware-marvell">
      <title>Marvell</title>
      <para>
        FC-&nvme; is supported on QLE269x and QLE27xx adapters. FC-&nvme;
        support is enabled by default in the Marvell® QLogic® QLA2xxx Fibre
        Channel driver.
      </para>
      <para>
        To confirm &nvme; is enabled, run the following command:
      </para>
<screen>&prompt.user;cat /sys/module/qla2xxx/parameters/ql2xnvmeenable</screen>
      <para>
        A resulting <literal>1</literal> means &nvme; is enabled, a
        <literal>0</literal> indicates it is disabled.
      </para>
      <para>
        Next, ensure that the Marvell adapter firmware is at least version
        8.08.204 by checking the output of the following command:
      </para>
<screen>&prompt.user;cat /sys/class/scsi_host/host0/fw_version</screen>
      <para>
        Last, ensure that the latest versions available for &productname; of
        <package>nvme-cli</package>, <package>QConvergeConsoleCLI</package>,
        and the Kernel are installed. You may, for example, run
      </para>
<screen>&prompt.root;zypper lu &amp;&amp; zypper pchk</screen>
      <para>
        to check for updates and patches.
      </para>
      <para>
        For more details on installation, please refer to the FC-&nvme;
        sections in the following Marvell user guides:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            <link xlink:href="http://driverdownloads.qlogic.com/QLogicDriverDownloads_UI/ShowEula.aspx?resourceid=32769&amp;docid=96728&amp;ProductCategory=39&amp;Product=1259&amp;Os=126"/>
          </para>
        </listitem>
        <listitem>
          <para>
            <link xlink:href="http://driverdownloads.qlogic.com/QLogicDriverDownloads_UI/ShowEula.aspx?resourceid=32761&amp;docid=96726&amp;ProductCategory=39&amp;Product=1261&amp;Os=126"/>
          </para>
        </listitem>
      </itemizedlist>
    </sect2>
  </sect1>
  <sect1 xml:id="sec-nvmeof-boot">
    <title>Booting from &nvmeof; over TCP</title>

    <para>
      &productnameshort; supports booting from &nvmeof; over TCP according to
      the
      <link xlink:href="https://nvmexpress.org/wp-content/uploads/NVM-Express-Boot-Specification-2022.11.15-Ratified.pdf">NVM
      Express® Boot Specification 1.0</link>.
    </para>

    <para>
      The UEFI pre-boot environment can be configured to attempt &nvmeof; over
      TCP connections to remote storage servers and use these for booting. The
      pre-boot environment creates an ACPI table&mdash;NVMe Boot Firmware Table
      (NBFT) to store information about the &nvmeof; configuration used for
      booting. The operating system uses this table at a later boot stage to
      set up networking and &nvmeof; connections to access the root file
      system.
    </para>

    <sect2 xml:id="sec-nvmeof-tcp-requirements">
      <title>System requirements</title>
      <para>
        To boot the system from &nvmeof; over TCP, the following requirements
        must be met:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            &productnameshort;&productnumber; or later.
          </para>
        </listitem>
        <listitem>
          <para>
            A SAN storage array supporting &nvmeof; over TCP
          </para>
        </listitem>
        <listitem>
          <para>
            A host system with a BIOS that supports booting from &nvmeof; over
            TCP. Contact your hardware vendor for information about support for
            this feature. Booting from &nvmeof; over TCP is currently only
            supported on UEFI platforms.
          </para>
        </listitem>
      </itemizedlist>
    </sect2>

    <sect2 xml:id="sec-install-nvme-tcp">
      <title>Installation</title>
      <para>
        To install &productnameshort; from &nvmeof; over TCP, proceed as
        follows:
      </para>
      <procedure>
        <step>
          <para>
            Use the host system's UEFI setup menus to configure &nvmeof;
            connections to be established at boot time. Typically, you need to
            configure both networking (local IP addresses, gateways, etc.) and
            NVMe-oF targets (remote IP address, subsystem NQN or discovery
            NQN). Refer to the hardware documentaion for the configuration
            description. Your hardware vendor may provide means to manage the
            BIOS configuration centrally and remotely. Please contact your
            hardware vendor for additional information.
          </para>
        </step>
        <step>
          <para>
            Prepare the installation as described in
            <xref linkend="book-deployment"/>.
          </para>
        </step>
        <step>
          <para>
            Start the system installation using any supported installation
            method. You do not need to use any specific boot parameters to
            enable installation on &nvmeof; over TCP.
          </para>
        </step>
        <step>
          <para>
            If the BIOS has been configured correctly, the disk partitioning
            dialog in &yast; will show &nvme; namespaces exported by the
            subsystems configured in the BIOS. They will be displayed as NVMe
            devices, where the <literal>tcp</literal> string indicates that the
            devices are connected via the TCP transport. Install the operating
            system (in particular the EFI boot partition and the root file
            system) on these namespaces.
          </para>
        </step>
        <step>
          <para>
            Complete the installation.
          </para>
        </step>
      </procedure>
      <para>
        After installation, the system should boot from &nvmeof; over TCP
        automatically. If it does not, check if the boot priority is set
        correctly in the BIOS setup.
      </para>
      <para>
        The network interfaces used for booting are named
        <literal>nbft0</literal>, <literal>nbft1</literal> and so on. To get
        information about the &nvmeof; boot, run the command:
      </para>
<screen>&prompt.root;<command>nvme nbft show</command></screen>
    </sect2>
  </sect1>
  <sect1 xml:id="sec-nvmeof-more-information">
    <title>More information</title>

    <para>
      For more details about the abilities of the <command>nvme</command>
      command, refer to <command>nvme nvme-help</command>.
    </para>

    <para>
      The following links provide a basic introduction to &nvme; and &nvmeof;:
    </para>

    <itemizedlist>
      <listitem>
        <para>
          <link xlink:href="http://nvmexpress.org/"/>
        </para>
      </listitem>
      <listitem>
        <para>
          <link xlink:href="http://www.nvmexpress.org/wp-content/uploads/NVMe_Over_Fabrics.pdf"/>
        </para>
      </listitem>
      <listitem>
        <para>
          <link xlink:href="https://storpool.com/blog/demystifying-what-is-nvmeof"/>
        </para>
      </listitem>
    </itemizedlist>
  </sect1>
</chapter>
