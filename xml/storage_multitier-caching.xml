<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.multitiercache" xml:lang="en">
 <title>Multi-tier Caching for Block Device Operations</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  A multi-tier cache is a replicated/distributed cache that consists of at
  least two tiers: one is represented by a slower but cheaper rotational
  block devices (HDD), while the other is more expensive but performs faster
  data operations (for example SSD flash disks).
 </para>
 <para>
  &sls; implements two different solutions for caching between flash and
  rotational devices: &bcache; and &lvmcache;.
 </para>
 <sect1 xml:id="terminology">
  <title>General Terminology</title>

  <para>
   This section explains several terms often used when describing cache
   related features:
  </para>

  <variablelist>
   <varlistentry>
    <term>Migration</term>
    <listitem>
     <para>
      Movement of the primary copy of a logical block from one device to the
      other.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Promotion</term>
    <listitem>
     <para>
      Migration from the slow device to the fast device.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Demotion</term>
    <listitem>
     <para>
      Migration from the fast device to the slow device.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Origin device</term>
    <listitem>
     <para>
      The big and slower block device. It always contains a copy of the
      logical block, which may be out of date or kept in sync with the copy
      on the cache device (depending on policy).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cache device</term>
    <listitem>
     <para>
      The small and faster block device.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Metadata device</term>
    <listitem>
     <para>
      A small device that records which blocks are in the cache, which are
      dirty, and extra hints for use by the policy object. This information
      could be put on the cache device as well, but having it separate
      allows the volume manager to configure it differently. For example as
      a mirror for extra robustness. The metadata device may only be used by
      a single cache device.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Dirty block</term>
    <listitem>
     <para>
      If some process writes to a block of data which is placed in the
      cache, the cached block is marked as <emphasis>dirty</emphasis>
      because it was overwritten in the cache and needs to be written back
      to the original device.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cache miss</term>
    <listitem>
     <para>
      A request for I/O operations is pointed to the cached device's cache
      first. If it cannot find the requested values, it looks into the
      device itself, which is slow. This is called a <emphasis>cache
      miss</emphasis>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cache hit</term>
    <listitem>
     <para>
      When a requested value is found in the cached device's cache, it is
      served fast. This is called a <emphasis>cache hit</emphasis>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cold cache</term>
    <listitem>
     <para>
      Cache that holds no values (is empty) and causes <emphasis>cache
      misses</emphasis>. As the cached block device operations progress, it
      gets filled with data and becomes <emphasis>warm</emphasis>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Warm cache</term>
    <listitem>
     <para>
      Cache that already holds some values and is probable to cause
      <emphasis>cache hits</emphasis> .
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="multitiercache.caching_modes">
  <title>Caching Modes</title>

  <para>
   Following are the basic caching modes that multi-tier caches use:
   <emphasis>write-back</emphasis>, <emphasis>write-through</emphasis>, and
   <emphasis>pass-through</emphasis>.
  </para>

  <variablelist>
   <varlistentry>
    <term>write-back</term>
    <listitem>
     <para>
      Data written to a block that is cached go to the cache only, and the
      block is marked dirty. This is the default caching mode.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>write-through</term>
    <listitem>
     <para>
      Writing to a cached block will not complete until it has hit both the
      origin and cache devices. Clean blocks remain clean with
      <emphasis>write-through</emphasis> cache.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>write-around</term>
    <listitem>
     <para>
      A similar technique to write-through cache, but write I/O is written
      directly to a permanent storage, bypassing the cache. This can reduce
      the cache being flooded with write I/O that will not subsequently be
      re-read, but the disadvantage is that a read request for recently
      written data will create a 'cache miss' and have to be read from
      slower bulk storage and experience higher latency.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>pass-through</term>
    <listitem>
     <para>
      To enable the <emphasis>pass-through</emphasis> mode, the cache needs
      to be clean. Reading is served from the origin device bypassing the
      cache. Writing is forwarded to the origin device and 'invalidates' the
      cache block. <emphasis>Pass-through</emphasis> allows a cache device
      activation without having to care about data coherency, which is
      maintained. The cache will gradually become cold as writing takes
      place. If you can verify the coherency of the cache later, or
      establish it by using the 'invalidate_cblocks' message, you can switch
      the cache device to <emphasis>write-through</emphasis> or
      <emphasis>write-back</emphasis> mode while it is still warm.
      Otherwise, you can discard the cache contents before switching to the
      desired caching mode.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="bcache">
  <title>&bcache;</title>

<!-- https://bcache.evilpiepirate.org/ -->

  <para>
   &bcache; is a Linux kernel block layer cache. It allows one or more fast
   disk drives (such as SSDs) to act as a cache for one or more slower hard
   disk drives. &bcache; supports write-through and write-back, and is
   independent of the file system used. By default it caches random reads
   and writes only, where SSDs excel at. It is suitable for both desktops,
   servers, and high end storage arrays as well.
  </para>

  <sect2 xml:id="bcache.features">
   <title>Main Features</title>
   <itemizedlist>
    <listitem>
     <para>
      A single cache device can be used to cache an arbitrary number of
      backing devices. Backing devices can be attached and detached at
      runtime, while mounted and in use.
     </para>
    </listitem>
    <listitem>
     <para>
      Recovers from unclean shutdowns&mdash;writes are not completed until
      the cache is consistent with respect to the backing device.
     </para>
    </listitem>
    <listitem>
     <para>
      Throttles traffic to the SSD if it becomes congested.
     </para>
    </listitem>
    <listitem>
     <para>
      Highly efficient write-back implementation. Dirty data is always
      written out in sorted order.
     </para>
    </listitem>
    <listitem>
     <para>
      Stable and reliable&mdash;in production use.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

<!-- https://wiki.archlinux.org/index.php/Bcache#Setting_up_a_bcache_device_on_an_existing_system -->

  <sect2 xml:id="bcache.setting_bcache_device">
   <title>Setting Up a &bcache; Device</title>
   <para>
    This section describes steps to set up and manage a &bcache; device.
   </para>
   <procedure>
    <step>
     <para>
      Install the <systemitem>bcache-tools</systemitem> package:
     </para>
<screen>sudo zypper in bcache-tools</screen>
    </step>
    <step>
     <para>
      Create a backing device (typically a mechanical drive). The backing
      device can be a whole device, a partition, or any other standard block
      device.
     </para>
<screen>sudo make-bcache -B /dev/sdb</screen>
    </step>
    <step>
     <para>
      Create a cache device (typically an SSD disk).
     </para>
<screen>sudo make-bcache -C /dev/sdc</screen>
     <para>
      In this example, the default block and bucket sizes of 512B and 128KB
      are used. The block size should match the backing devices sector size
      which will usually be either 512 or 4k. The bucket size should match
      the erase block size of the caching device with the intent of reducing
      write amplification. For example, using a HDD with 4k sectors and an
      SSD with an erase block size of 2MB this command would look as
      follows:
     </para>
<screen>sudo make-bcache --block 4k --bucket 2M -C /dev/sdc</screen>
     <tip>
      <para>
       <command>make-bcache</command> can prepare and register multiple
       backing devices and a cache device at the same time. In this case you
       do not need to manually attach the cache device to the backing device
       afterwards:
      </para>
<screen>sudo make-bcache -B /dev/sda /dev/sdb -C /dev/sdc</screen>
     </tip>
    </step>
    <step>
     <para>
      &bcache; devices show up as
     </para>
<screen>/dev/bcache<replaceable>N</replaceable></screen>
     <para>
      as well as
     </para>
<screen>/dev/bcache/by-uuid/<replaceable>uuid</replaceable>
/dev/bcache/by-label/<replaceable>label</replaceable></screen>
     <para>
      You can normally format and mount &bcache; devices as usual:
     </para>
<screen>mkfs.ext4 /dev/bcache0
mount /dev/bcache0 /mnt</screen>
     <para>
      You can control &bcache; devices through
      <systemitem>sysfs</systemitem> at
      <filename>/sys/block/bcache<replaceable>N</replaceable>/bcache</filename>.
     </para>
    </step>
    <step>
     <para>
      After both the cache and backing devices are registered, you need to
      attach the backing device to the related cache set to enable caching:
     </para>
<screen>echo <replaceable>cache_set_uuid</replaceable> > /sys/block/bcache0/bcache/attach</screen>
     <para>
      where <replaceable>cache_set_uuid</replaceable> is found in
      <filename>/sys/fs/bcache</filename>.
     </para>
    </step>
    <step>
     <para>
      By default &bcache; uses a pass-through caching mode. To change it to
      for example write-back, run
     </para>
<screen>echo writeback > /sys/block/bcache0/bcache/cache_mode</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="bcache.sysfs">
   <title>&bcache; Configuration using <systemitem>sysfs</systemitem></title>
   <para>
    &bcache; devices use the <systemitem>sysfs</systemitem> interface to
    store their runtime configuration values. This way you can change
    &bcache; backing and cache disks behavior or see their usage statistics.
   </para>
   <para>
    For the complete list of &bcache; <systemitem>sysfs</systemitem>
    parameters, see the contents of the
    <filename>/usr/src/linux/Documentation/bcache.txt</filename> file,
    mainly the <literal>SYSFS - BACKING DEVICE</literal>, <literal>SYSFS -
    BACKING DEVICE STATS</literal>, and <literal>SYSFS - CACHE
    DEVICE</literal> sections.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="lvmcache">
  <title>&lvmcache;</title>

  <para>
   &lvmcache; is a caching mechanism consisting of logical volumes (LVs). It
   makes use of the <literal>dm-cache</literal> kernel driver and supports
   write-through (default) and write-back caching modes. &lvmcache; improves
   performance of a large and slow LV by dynamically migrating some of its
   data from to a faster and smaller LV. For more information on LVM, see
   <xref linkend="part.lvm"/>
  </para>

  <para>
   LVM refers to the small fast LV as a <emphasis>cache pool LV</emphasis> .
   The large slow LV is called the <emphasis>origin LV</emphasis> . Due to
   requirements from dm-cache, LVM further splits the cache pool LV into two
   devices&mdash;the <emphasis>cache data LV</emphasis> and <emphasis>cache
   metadata LV</emphasis> . The cache data LV is where copies of data blocks
   are kept from the origin LV to increase speed. The cache metadata LV
   holds the accounting information that specifies where data blocks are
   stored.
  </para>

  <sect2 xml:id="lvmcache.configure">
   <title>Configuring &lvmcache;</title>
   <para>
    This section describes steps to create and configure LVM based caching.
   </para>
   <procedure>
    <step>
     <para>
      <emphasis>Create the origin LV.</emphasis> Create a new LV or use an
      existing LV to become the origin LV:
     </para>
<screen>lvcreate -n <replaceable>origin_lv</replaceable> -L 100G vg <replaceable>/dev/slow_dev</replaceable></screen>
    </step>
    <step>
     <para>
      <emphasis>Create the cache data LV.</emphasis> This LV will hold data
      blocks from the origin LV. The size of this LV is the size of the
      cache and will be reported as the size of the cache pool LV.
     </para>
<screen>lvcreate -n <replaceable>cache_data_lv</replaceable> -L 10G vg <replaceable>/dev/fast</replaceable></screen>
    </step>
    <step>
     <para>
      <emphasis>Create the cache metadata LV.</emphasis> This LV will hold
      cache pool metadata. The size of this LV should be approximately 1000
      times smaller than the cache data LV, with a minimum size of 8MB.
     </para>
<screen>lvcreate -n <replaceable>cache_metadata_lv</replaceable> -L 12M vg <replaceable>/dev/fast</replaceable></screen>
     <para>
      List the volumes you have created so far:
     </para>
<screen>lvs -a vg
LV                VG   Attr        LSize   Pool Origin
cache_data_lv     vg   -wi-a-----  10.00g
cache_metadata_lv vg   -wi-a-----  12.00m
origin_lv         vg   -wi-a----- 100.00g</screen>
    </step>
    <step>
     <para>
      <emphasis>Create a cache pool LV.</emphasis> Combine the data and
      metadata LVs into a cache pool LV. You can set the cache pool LV's
      behavior at the same time.
     </para>
     <para>
      <replaceable>cache_pool_lv</replaceable> takes the name of
      <replaceable>cache_data_lv</replaceable>.
     </para>
     <para>
      <replaceable>cache_data_lv</replaceable> is renamed to
      <replaceable>cache_data_lv</replaceable>_cdata and becomes hidden.
     </para>
     <para>
      <replaceable>cache_meta_lv</replaceable> is renamed to
      <replaceable>cache_data_lv</replaceable>_cmeta and becomes hidden.
     </para>
<screen>lvconvert --type cache-pool \
 --poolmetadata vg/cache_metadata_lv vg/cache_data_lv</screen>
<screen>lvs -a vg
LV                     VG   Attr       LSize   Pool Origin
cache_data_lv          vg   Cwi---C---  10.00g
[cache_data_lv_cdata]  vg   Cwi-------  10.00g
[cache_data_lv_cmeta]  vg   ewi-------  12.00m
origin_lv              vg   -wi-a----- 100.00g</screen>
    </step>
    <step>
     <para>
      <emphasis>Create a cache LV.</emphasis> Create a cache LV by linking
      the cache pool LV to the origin LV.
     </para>
     <para>
      The user accessible cache LV takes the name of the origin LV, while
      the origin LV becomes a hidden LV renamed to
      <replaceable>origin_lv</replaceable>_corig.
     </para>
     <para>
      CacheLV takes the name of <replaceable>origin_lv</replaceable>.
     </para>
     <para>
      <replaceable>origin_lv</replaceable> is renamed to
      <replaceable>origin_lv</replaceable>_corig and becomes hidden.
     </para>
<screen>lvconvert --type cache --cachepool vg/cache_data_lv vg/origin_lv</screen>
<screen>lvs -a vg
LV              VG   Attr       LSize   Pool   Origin
cache_data_lv          vg   Cwi---C---  10.00g
[cache_data_lv_cdata]  vg   Cwi-ao----  10.00g
[cache_data_lv_cmeta]  vg   ewi-ao----  12.00m
origin_lv              vg   Cwi-a-C--- 100.00g cache_data_lv [origin_lv_corig]
[origin_lv_corig]      vg   -wi-ao---- 100.00g</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="lvmcache.remove">
   <title>Removing a Cache Pool</title>
   <para>
    There are several ways to turn off the LV cache.
   </para>
   <sect3>
    <title>Detach a Cache Pool LV from a Cache LV</title>
    <para>
     You can disconnect a cache pool LV from a cache LV, leaving an unused
     cache pool LV and an uncached origin LV. Data are written back from the
     cache pool to the origin LV when necessary.
    </para>
<screen>lvconvert --splitcache vg/origin_lv</screen>
   </sect3>
   <sect3>
    <title>Removing a Cache Pool LV without Removing its Origin LV</title>
    <para>
     This writes back data from the cache pool to the origin LV when
     necessary, then removes the cache pool LV, leaving the uncached origin
     LV.
    </para>
<screen>lvremove vg/cache_data_lv</screen>
    <para>
     An alternative command that also disconnects the cache pool from the
     cache LV, and deletes the cache pool:
    </para>
<screen>lvconvert --uncache vg/origin_lv</screen>
   </sect3>
   <sect3>
    <title>Removing Both the Origin LV and the Cache Pool LV</title>
    <para>
     Removing a cache LV removes both the origin LV and the linked cache
     pool LV.
    </para>
<screen>lvremove vg/origin_lv</screen>
   </sect3>
   <sect3>
    <title>For More Information</title>
    <para>
     You can find more &lvmcache; related topics, such as supported cache
     modes, redundant sub-logical volumes, cache policy, or converting
     existing LVs to cache types, in the &lvmcache; manual page
     (<command>man 7 lvmcache</command>).
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
