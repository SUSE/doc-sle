<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.libvirt.storage">
 <title>Managing Storage</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
  </dm:docmanager>
 </info>
 <para>
  When managing a &vmguest; on the &vmhost; itself, you can access the complete
  file system of the &vmhost; to attach or create virtual hard disks or to
  attach existing images to the &vmguest;. However, this is not possible when
  managing &vmguest;s from a remote host. For this reason, &libvirt; supports
  so called <quote>Storage Pools</quote>, which can be accessed from remote
  machines.
 </para>
 <tip>
  <title>CD/DVD ISO images</title>
  <para>
   To be able to access CD/DVD ISO images on the &vmhost; from remote, they
   also need to be placed in a storage pool.
  </para>
 </tip>
 <para>
  &libvirt; knows two different types of storage: volumes and pools.
 </para>
 <variablelist>
  <varlistentry>
   <term>Storage Volume</term>
   <listitem>
    <para>
     A storage volume is a storage device that can be assigned to a
     guest&mdash;a virtual disk or a CD/DVD/floppy image. Physically (on the
     &vmhost;) it can be a block device (a partition, a logical volume, etc.)
     or a file.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>Storage Pool</term>
   <listitem>
    <para>
     A storage pool is a storage resource on the &vmhost; that can be used for
     storing volumes, similar to network storage for a desktop machine.
     Physically it can be one of the following types:
    </para>
    <variablelist>
     <varlistentry>
      <term>File System Directory (<guimenu>dir</guimenu>)</term>
      <listitem>
       <para>
        A directory for hosting image files. The files can be either one of the
        supported disk formats (raw, qcow2, or qed), or ISO images.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Physical Disk Device (<guimenu>disk</guimenu>)</term>
      <listitem>
       <para>
        Use a complete physical disk as storage. A partition is created for
        each volume that is added to the pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Pre-Formatted Block Device (<guimenu>fs</guimenu>)</term>
      <listitem>
       <para>
        Specify a partition to be used in the same way as a file system
        directory pool (a directory for hosting image files). The only
        difference to using a file system directory is that &libvirt; takes
        care of mounting the device.
       </para>
      </listitem>
     </varlistentry>
<!-- <varlistentry>
      <term>GlusterFS device (<guimenu>gluster</guimenu>)</term>
      <listitem>
       <para>
        ...
       </para>
      </listitem>
     </varlistentry> -->
     <varlistentry>
      <term>iSCSI Target (iscsi)</term>
      <listitem>
       <para>
        Set up a pool on an iSCSI target. You need to have been logged in to
        the volume once before, to use it with &libvirt;. Use the &yast;
        <guimenu>iSCSI Initiator</guimenu> to detect and log in to a
        volume<phrase os="sles">, see <xref linkend="stor_admin"/> for
        details</phrase>. Volume creation on iSCSI pools is not supported,
        instead each existing Logical Unit Number (LUN) represents a volume.
        Each volume/LUN also needs a valid (empty) partition table or disk
        label before you can use it. If missing, use <command>fdisk</command>
        to add it:
       </para>
<screen>~ # fdisk -cu /dev/disk/by-path/ip-&wsIip;:3260-iscsi-iqn.2010-10.com.example:[...]-lun-2
Device contains neither a valid DOS partition table, nor Sun, SGI
or OSF disklabel
Building a new DOS disklabel with disk identifier 0xc15cdc4e.
Changes will remain in memory only, until you decide to write them.
After that, of course, the previous content won't be recoverable.

Warning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite)

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>LVM Volume Group (logical)</term>
      <listitem>
       <para>
        Use an LVM volume group as a pool. You may either use a predefined
        volume group, or create a group by specifying the devices to use.
        Storage volumes are created as partitions on the volume.
       </para>
       <warning>
        <title>Deleting the LVM-Based Pool</title>
        <para>
         When the LVM-based pool is deleted in the Storage Manager, the volume
         group is deleted as well. This results in a non-recoverable loss of
         all data stored on the pool!
        </para>
       </warning>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Multipath Devices (<guimenu>mpath</guimenu>)</term>
      <listitem>
       <para>
        At the moment, multipathing support is limited to assigning existing
        devices to the guests. Volume creation or configuring multipathing from
        within &libvirt; is not supported.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Network Exported Directory (<guimenu>netfs</guimenu>)</term>
      <listitem>
       <para>
        Specify a network directory to be used in the same way as a file system
        directory pool (a directory for hosting image files). The only
        difference to using a file system directory is that &libvirt; takes
        care of mounting the directory. Supported protocols are NFS and
        GlusterFS.
       </para>
      </listitem>
     </varlistentry>
<!-- <varlistentry>
      <term>Rados Block Device (<guimenu>rbd</guimenu>)</term>
      <listitem>
       <para>
        ...
       </para>
      </listitem>
     </varlistentry> -->
<!-- <varlistentry>
      <term>Sheepdog distributed storage system (<guimenu>sheepdog</guimenu>)</term>
      <listitem>
       <para>
        ...
       </para>
      </listitem>
     </varlistentry> -->
     <varlistentry>
      <term>SCSI Host Adapter (<guimenu>scsi</guimenu>)</term>
      <listitem>
       <para>
        Use an SCSI host adapter in almost the same way as an iSCSI target. We
        recommend to use a device name from
        <filename>/dev/disk/by-*</filename> rather than
        <filename>/dev/sd<replaceable>X</replaceable></filename>. The
        latter can change (for example, when adding or removing hard disks).
        Volume creation on iSCSI pools is not supported. Instead, each existing
        LUN (Logical Unit Number) represents a volume.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </listitem>
  </varlistentry>
 </variablelist>
 <warning>
  <title>Security Considerations</title>
  <para>
   To avoid data loss or data corruption, do not attempt to use resources such
   as LVM volume groups, iSCSI targets, etc., that are also used to build
   storage pools on the &vmhost;. There is no need to connect to these
   resources from the &vmhost; or to mount them on the &vmhost;&mdash;&libvirt;
   takes care of this.
  </para>
  <para>
   Do not mount partitions on the &vmhost; by label. Under certain
   circumstances it is possible that a partition is labeled from within a
   &vmguest; with a name already existing on the &vmhost;.
  </para>
 </warning>
 <sect1 xml:id="sec.libvirt.storage.vmm">
  <title>Managing Storage with &vmm;</title>

  <para>
   The &vmm; provides a graphical interface&mdash;the Storage Manager&mdash;to
   manage storage volumes and pools. To access it, either right-click a
   connection and choose <guimenu>Details</guimenu>, or highlight a connection
   and choose <menuchoice> <guimenu>Edit</guimenu> <guimenu>Connection
   Details</guimenu> </menuchoice>. Select the <guimenu>Storage</guimenu> tab.
  </para>

  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="virt_virt-manager_storage.png" width="60%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="virt_virt-manager_storage.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <sect2 xml:id="sec.libvirt.storage.vmm.addpool">
   <title>Adding a Storage Pool</title>
   <para>
    To add a storage pool, proceed as follows:
   </para>
   <procedure>
    <step>
     <para>
      Click <guimenu>Add</guimenu> in the bottom left corner. The dialog
      <guimenu>Add a New Storage Pool</guimenu> appears.
     </para>
    </step>
    <step>
     <para>
      Provide a <guimenu>Name</guimenu> for the pool (consisting of
      alphanumeric characters and <literal>_-.</literal>) and select a
      <guimenu>Type</guimenu>. Proceed with <guimenu>Forward</guimenu>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="virt_virt-manager_storage_add.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="virt_virt-manager_storage_add.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Specify the required details in the following window. The data that needs
      to be entered depends on the type of pool you are creating:
     </para>
     <variablelist>
      <varlistentry>
       <term><emphasis role="bold">Type</emphasis><guimenu>dir</guimenu>
       </term>
       <listitem>
        <itemizedlist mark="bullet" spacing="normal">
         <listitem>
          <para>
           <guimenu>Target Path</guimenu>: Specify an existing directory.
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><emphasis role="bold">Type</emphasis><guimenu>disk</guimenu>
       </term>
       <listitem>
        <itemizedlist mark="bullet" spacing="normal">
         <listitem>
          <para>
           <guimenu>Target Path</guimenu>: The directory that hosts the
           devices. The default value <filename>/dev</filename> should usually
           fit.
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Format</guimenu>: Format of the device's partition table.
           Using <guimenu>auto</guimenu> should usually work. If not, get the
           required format by running the command <command>parted</command>
           <option>-l</option> on the &vmhost;.
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Path</guimenu>: Path to the device. It is
           recommended to use a device name from
           <filename>/dev/disk/by-*</filename> rather than the simple
           <filename>/dev/sd<replaceable>X</replaceable></filename>, since the
           latter can change (for example, when adding or removing hard disks).
           You need to specify the path that resembles the whole disk, not a
           partition on the disk (if existing).
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Build Pool</guimenu>: Activating this option formats the
           device. Use with care&mdash;all data on the device will be lost!
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><emphasis role="bold">Type</emphasis><guimenu>fs</guimenu>
       </term>
       <listitem>
        <itemizedlist mark="bullet" spacing="normal">
         <listitem>
          <para>
           <guimenu>Target Path</guimenu>: Mount point on the &vmhost; file
           system.
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Format: </guimenu> File system format of the device. The
           default value <literal>auto</literal> should work.
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Path</guimenu>: Path to the device file. It is
           recommended to use a device name from
           <filename>/dev/disk/by-*</filename> rather than
           <filename>/dev/sd<replaceable>X</replaceable></filename>, because
           the latter can change (for example, when adding or removing hard
           disks).
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>
<!-- <varlistentry>
       <term><emphasis role="bold">Type</emphasis> <guimenu>gluster</guimenu></term>
       <listitem>
        <itemizedlist>
         <listitem>
          <para>
           <guimenu>Host Name</guimenu>: ...
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Name</guimenu>: ...
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Path</guimenu>: ...
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>-->
      <varlistentry>
       <term><emphasis role="bold">Type</emphasis><guimenu>iscsi</guimenu>
       </term>
       <listitem>
        <para>
         Get the necessary data by running the following command on the
         &vmhost;:
        </para>
<screen>iscsiadm --mode node</screen>
        <para>
         It will return a list of iSCSI volumes with the following format. The
         elements in bold text are required:
        </para>
<screen><emphasis role="bold">IP_ADDRESS</emphasis>:PORT,TPGT <emphasis role="bold">TARGET_NAME_(IQN)</emphasis></screen>
        <itemizedlist mark="bullet" spacing="normal">
         <listitem>
          <para>
           <guimenu>Target Path</guimenu>: The directory containing the device
           file. Use <literal>/dev/disk/by-path</literal> (default) or
           <literal>/dev/disk/by-id</literal>.
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Host Name</guimenu>: Host name or IP address of the iSCSI
           server.
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Path</guimenu>: The iSCSI target name (IQN).
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><emphasis role="bold">Type</emphasis><guimenu>logical</guimenu>
       </term>
       <listitem>
        <itemizedlist mark="bullet" spacing="normal">
         <listitem>
          <para>
           <guimenu>Target Path</guimenu>: In case you use an existing volume
           group, specify the existing device path. When building a new LVM
           volume group, specify a device name in the <filename>/dev</filename>
           directory that does not already exist.
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Path</guimenu>: Leave empty when using an existing
           volume group. When creating a new one, specify its devices here.
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Build Pool</guimenu>: Only activate when creating a new
           volume group.
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><emphasis role="bold">Type</emphasis><guimenu>mpath</guimenu>
       </term>
       <listitem>
        <itemizedlist mark="bullet" spacing="normal">
         <listitem>
          <para>
           <guimenu>Target Path</guimenu>: Support for multipathing is
           currently limited to making all multipath devices available.
           Therefore, specify an arbitrary string here that will then be
           ignored. The path is required, otherwise the XML parser will fail.
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><emphasis role="bold">Type</emphasis><guimenu>netfs</guimenu>
       </term>
       <listitem>
        <itemizedlist mark="bullet" spacing="normal">
         <listitem>
          <para>
           <guimenu>Target Path</guimenu>: Mount point on the &vmhost; file
           system.
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Host Name</guimenu>: IP address or host name of the server
           exporting the network file system.
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Path</guimenu>: Directory on the server that is
           being exported.
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>
<!-- <varlistentry>
       <term><emphasis role="bold">Type</emphasis> <guimenu>rbd</guimenu></term>
       <listitem>
        <itemizedlist>
         <listitem>
          <para>
           <guimenu>Host Name</guimenu>: ...
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Name</guimenu>: ...
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>-->
      <varlistentry>
       <term><emphasis role="bold">Type</emphasis><guimenu>scsi</guimenu>
       </term>
       <listitem>
        <itemizedlist mark="bullet" spacing="normal">
         <listitem>
          <para>
           <guimenu>Target Path</guimenu>: The directory containing the device
           file. Use <literal>/dev/disk/by-path</literal> (default) or
           <literal>/dev/disk/by-id</literal>.
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Path</guimenu>: Name of the SCSI adapter.
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>
<!-- <varlistentry>
       <term><emphasis role="bold">Type</emphasis> <guimenu>rbd</guimenu></term>
       <listitem>
        <itemizedlist>
         <listitem>
          <para>
           <guimenu>Host Name</guimenu>: ...
          </para>
         </listitem>
         <listitem>
          <para>
           <guimenu>Source Name</guimenu>: ...
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>-->
     </variablelist>
     <note>
      <title>File Browsing</title>
      <para>
       Using the file browser by clicking <guimenu>Browse</guimenu> is not
       possible when operating from remote.
      </para>
     </note>
    </step>
    <step>
     <para>
      Click <guimenu>Finish</guimenu> to add the storage pool.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.libvirt.storage.vmm.manage">
   <title>Managing Storage Pools</title>
   <para>
    &vmm;'s Storage Manager lets you create or delete volumes in a pool. You
    may also temporarily deactivate or permanently delete existing storage
    pools. Changing the basic configuration of a pool is currently not
    supported by &suse;.
   </para>
   <sect3 xml:id="sec.libvirt.storage.vmm.manage.pool">
    <title>Starting, Stopping and Deleting Pools</title>
    <para>
     The purpose of storage pools is to provide block devices located on the
     &vmhost; that can be added to a &vmguest; when managing it from remote. To
     make a pool temporarily inaccessible from remote, click
     <guimenu>Stop</guimenu> in the bottom left corner of the Storage Manager.
     Stopped pools are marked with <guimenu>State: Inactive</guimenu> and are
     grayed out in the list pane. By default, a newly created pool will be
     automatically started <guimenu>On Boot</guimenu> of the &vmhost;.
    </para>
    <para>
     To start an inactive pool and make it available from remote again, click
     <guimenu>Start</guimenu> in the bottom left corner of the Storage Manager.
    </para>
    <note>
     <title>A Pool's State Does not Affect Attached Volumes</title>
     <para>
      Volumes from a pool attached to &vmguest;s are always available,
      regardless of the pool's state (<guimenu>Active</guimenu> (stopped) or
      <guimenu>Inactive</guimenu> (started)). The state of the pool solely
      affects the ability to attach volumes to a &vmguest; via remote
      management.
     </para>
    </note>
    <para>
     To permanently make a pool inaccessible, click <guimenu>Delete</guimenu>
     in the bottom left corner of the Storage Manager. You may only delete
     inactive pools. Deleting a pool does not physically erase its contents on
     &vmhost;&mdash;it only deletes the pool configuration. However, you need
     to be extra careful when deleting pools, especially when deleting LVM
     volume group-based tools:
    </para>
    <warning xml:id="deleting.storage.pools">
     <title>Deleting Storage Pools</title>
     <para>
      Deleting storage pools based on <emphasis>local</emphasis> file system
      directories, local partitions or disks has no effect on the availability
      of volumes from these pools currently attached to &vmguest;s.
     </para>
     <para>
      Volumes located in pools of type iSCSI, SCSI, LVM group or Network
      Exported Directory will become inaccessible from the &vmguest; if the
      pool is deleted. Although the volumes themselves will not be deleted, the
      &vmhost; will no longer have access to the resources.
     </para>
     <para>
      Volumes on iSCSI/SCSI targets or Network Exported Directory will become
      accessible again when creating an adequate new pool or when
      mounting/accessing these resources directly from the host system.
     </para>
     <para>
      When deleting an LVM group-based storage pool, the LVM group definition
      will be erased and the LVM group will no longer exist on the host system.
      The configuration is not recoverable and all volumes from this pool are
      lost.
     </para>
    </warning>
   </sect3>
   <sect3 xml:id="sec.libvirt.storage.vmm.manage.volume_add">
    <title>Adding Volumes to a Storage Pool</title>
    <para>
     &vmm; lets you create volumes in all storage pools, except in pools of
     types Multipath, iSCSI, or SCSI. A volume in these pools is equivalent to
     a LUN and cannot be changed from within &libvirt;.
    </para>
    <procedure>
     <step>
      <para>
       A new volume can either be created using the Storage Manager or while
       adding a new storage device to a &vmguest;. In either case, select a
       storage pool from the left panel, then click <guimenu>Create new
       volume</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Specify a <guimenu>Name</guimenu> for the image and choose an image
       format.
      </para>
      <para>
       Note that &suse; currently only supports <literal>raw</literal>,
       <literal>qcow2</literal>, or <literal>qed</literal> images. The latter
       option is not available on LVM group-based pools.
      </para>
      <para>
       Next to <guimenu>Max Capacity</guimenu>, specify the amount maximum size
       that the disk image is allowed to reach. Unless you are working with a
       <literal>qcow2</literal> image, you can also set an amount for
       <guimenu>Allocation</guimenu> that should be allocated initially. If
       both values differ, a sparse image file will be created which grows on
       demand.
      </para>
      <para>
       For <literal>qcow2</literal> images, you can use a <guimenu>Backing
       Store</guimenu> (also called <quote>backing file</quote>) which
       constitutes a base image. The newly created <literal>qcow2</literal>
       image will then only record the changes that are made to the base image.
      </para>
     </step>
     <step>
      <para>
       Start the volume creation by clicking <guimenu>Finish</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec.libvirt.storage.vmm.manage.volume_delete">
    <title>Deleting Volumes From a Storage Pool</title>
    <para>
     Deleting a volume can only be done from the Storage Manager, by selecting
     a volume and clicking <guimenu>Delete Volume</guimenu>. Confirm with
     <guimenu>Yes</guimenu>.
    </para>
    <warning>
     <title>Volumes Can Be Deleted Even While in Use</title>
     <para>
      Volumes can be deleted even if they are currently used in an active or
      inactive &vmguest;. There is no way to recover a deleted volume.
     </para>
     <para>
      Whether a volume is used by a &vmguest; is indicated in the <guimenu>Used
      By</guimenu> column in the Storage Manager.
     </para>
    </warning>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.libvirt.storage.virsh">
  <title>Managing Storage with <command>virsh</command></title>

  <para>
   Managing storage from the command line is also possible by using
   <command>virsh</command>. However, creating storage pools is currently not
   supported by &suse;. Therefore, this section is restricted to documenting
   functions like starting, stopping and deleting pools and volume management.
  </para>

  <para>
   A list of all <command>virsh</command> subcommands for managing pools and
   volumes is available by running <command>virsh help pool</command> and
   <command>virsh help volume</command>, respectively.
  </para>

  <sect2 xml:id="sec.libvirt.storage.virsh.list_pools">
   <title>Listing Pools and Volumes</title>
   <para>
    List all pools currently active by executing the following command. To also
    list inactive pools, add the option <option>--all</option>:
   </para>
<screen>virsh pool-list --details</screen>
   <para>
    Details about a specific pool can be obtained with the
    <literal>pool-info</literal> subcommand:
   </para>
<screen>virsh pool-info <replaceable>POOL</replaceable></screen>
   <para>
    Volumes can only be listed per pool by default. To list all volumes from a
    pool, enter the following command.
   </para>
<screen>virsh vol-list --details <replaceable>POOL</replaceable></screen>
   <para>
    At the moment <command>virsh</command> offers no tools to show whether a
    volume is used by a guest or not. The following procedure describes a way
    to list volumes from all pools that are currently used by a &vmguest;.
   </para>
   <procedure xml:id="pro.libvirt.storage.virsh.list_vols">
    <title>Listing all Storage Volumes Currently Used on a &vmhost;</title>
    <step>
     <para>
      Create an XSLT style sheet by saving the following content to a file, for
      example, ~/libvirt/guest_storage_list.xsl:
     </para>
<screen>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;xsl:stylesheet version="1.0"
  xmlns:xsl="http://www.w3.org/1999/XSL/Transform"&gt;
  &lt;xsl:output method="text"/&gt;
  &lt;xsl:template match="text()"/&gt;
  &lt;xsl:strip-space elements="*"/&gt;
  &lt;xsl:template match="disk"&gt;
    &lt;xsl:text&gt;  &lt;/xsl:text&gt;
    &lt;xsl:value-of select="(source/@file|source/@dev|source/@dir)[1]"/&gt;
    &lt;xsl:text&gt;&amp;#10;&lt;/xsl:text&gt;
  &lt;/xsl:template&gt;
&lt;/xsl:stylesheet&gt;</screen>
    </step>
    <step>
     <para>
      Run the following commands in a shell. It is assumed that the guest's XML
      definitions are all stored in the default location
      (<filename>/etc/libvirt/qemu</filename>). <command>xsltproc</command> is
      provided by the package
      <systemitem class="resource">libxslt</systemitem>.
     </para>
<screen>SSHEET="$HOME/libvirt/guest_storage_list.xsl"
cd /etc/libvirt/qemu
for FILE in *.xml; do
  basename $FILE .xml
  xsltproc $SSHEET $FILE
done</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.libvirt.storage.virsh.start_pools">
   <title>Starting, Stopping and Deleting Pools</title>
   <para>
    Use the <command>virsh</command> pool subcommands to start, stop or delete
    a pool. Replace <replaceable>POOL</replaceable> with the pool's name or its
    UUID in the following examples:
   </para>
   <variablelist>
    <varlistentry>
     <term>Stopping a Pool</term>
     <listitem>
<screen>virsh pool-destroy <replaceable>POOL</replaceable></screen>
      <note>
       <title>A Pool's State Does not Affect Attached Volumes</title>
       <para>
        Volumes from a pool attached to &vmguest;s are always available,
        regardless of the pool's state (<guimenu>Active</guimenu> (stopped) or
        <guimenu>Inactive</guimenu> (started)). The state of the pool solely
        affects the ability to attach volumes to a &vmguest; via remote
        management.
       </para>
      </note>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Deleting a Pool</term>
     <listitem>
<screen>virsh pool-delete <replaceable>POOL</replaceable></screen>
      <warning>
       <title>Deleting Storage Pools</title>
       <para>
        See <xref linkend="deleting.storage.pools"/>
       </para>
      </warning>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Starting a Pool</term>
     <listitem>
<screen>virsh pool-start <replaceable>POOL</replaceable></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Enable Autostarting a Pool</term>
     <listitem>
<screen>virsh pool-autostart <replaceable>POOL</replaceable></screen>
      <para>
       Only pools that are marked to autostart will automatically be started if
       the &vmhost; reboots.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Disable Autostarting a Pool</term>
     <listitem>
<screen>virsh pool-autostart <replaceable>POOL</replaceable> --disable</screen>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec.libvirt.storage.virsh.add_volumes">
   <title>Adding Volumes to a Storage Pool</title>
   <para>
    <command>virsh</command> offers two ways to add volumes to storage pools:
    either from an XML definition with <literal>vol-create</literal> and
    <literal>vol-create-from</literal> or via command line arguments with
    <literal>vol-create-as</literal>. The first two methods are currently not
    supported by &suse;, therefore this section focuses on the subcommand
    <literal>vol-create-as</literal>.
   </para>
   <para>
    To add a volume to an existing pool, enter the following command:
   </para>
<screen>virsh vol-create-as <replaceable>POOL</replaceable><co xml:id="co.vol-create-as.pool"/><replaceable>NAME</replaceable><co xml:id="co.vol-create-as.name"/> 12G --format<co xml:id="co.vol-create-as.capacity"/><replaceable>raw|qcow2|qed</replaceable><co xml:id="co.vol-create-as.format"/> --allocation 4G<co xml:id="co.vol-create-as.alloc"/></screen>
   <calloutlist>
    <callout arearefs="co.vol-create-as.pool">
     <para>
      Name of the pool to which the volume should be added
     </para>
    </callout>
    <callout arearefs="co.vol-create-as.name">
     <para>
      Name of the volume
     </para>
    </callout>
    <callout arearefs="co.vol-create-as.capacity">
     <para>
      Size of the image, in this example 12 gigabytes. Use the suffixes k, M,
      G, T for kilobyte, megabyte, gigabyte, and terabyte, respectively.
     </para>
    </callout>
    <callout arearefs="co.vol-create-as.format">
     <para>
      Format of the volume. &suse; currently supports <literal>raw</literal>,
      <literal>qcow2</literal>, and <literal>qed</literal>.
     </para>
    </callout>
    <callout arearefs="co.vol-create-as.alloc">
     <para>
      Optional parameter. By default <command>virsh </command>creates a sparse
      image file that grows on demand. Specify the amount of space that should
      be allocated with this parameter (4 gigabytes in this example). Use the
      suffixes k, M, G, T for kilobyte, megabyte, gigabyte, and terabyte,
      respectively.
     </para>
     <para>
      When not specifying this parameter, a sparse image file with no
      allocation will be generated. To create a non-sparse volume, specify the
      whole image size with this parameter (would be <literal>12G</literal> in
      this example).
     </para>
    </callout>
   </calloutlist>
   <sect3 xml:id="sec.libvirt.storage.virsh.add_volumes.clone">
    <title>Cloning Existing Volumes</title>
    <para>
     Another way to add volumes to a pool is to clone an existing volume. The
     new instance is always created in the same pool as the original.
    </para>
<screen>virsh vol-clone <replaceable>NAME_EXISTING_VOLUME</replaceable><co xml:id="co.vol-clone.existing"/><replaceable>NAME_NEW_VOLUME</replaceable><co xml:id="co.vol-clone.new"/> --pool <replaceable>POOL</replaceable><co xml:id="co.vol-clone.pool"/></screen>
    <calloutlist>
     <callout arearefs="co.vol-clone.existing">
      <para>
       Name of the existing volume that should be cloned
      </para>
     </callout>
     <callout arearefs="co.vol-clone.new">
      <para>
       Name of the new volume
      </para>
     </callout>
     <callout arearefs="co.vol-clone.pool">
      <para>
       Optional parameter. &libvirt; tries to locate the existing volume
       automatically. If that fails, specify this parameter.
      </para>
     </callout>
    </calloutlist>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.libvirt.storage.virsh.del_volumes">
   <title>Deleting Volumes from a Storage Pool</title>
   <para>
    To permanently delete a volume from a pool, use the subcommand
    <literal>vol-delete</literal>:
   </para>
<screen>virsh vol-delete <replaceable>NAME</replaceable> --pool <replaceable>POOL</replaceable></screen>
   <para>
    <option>--pool</option> is optional. &libvirt; tries to locate the volume
    automatically. If that fails, specify this parameter.
   </para>
   <warning>
    <title>No Checks Upon Volume Deletion</title>
    <para>
     A volume will be deleted in any case, regardless of whether it is
     currently used in an active or inactive &vmguest;. There is no way to
     recover a deleted volume.
    </para>
    <para>
     Whether a volume is used by a &vmguest; can only be detected by using by
     the method described in
     <xref linkend="pro.libvirt.storage.virsh.list_vols"/>.
    </para>
   </warning>
  </sect2>

  <sect2 xml:id="libvirt.storage.virsh.attach_volumes">
   <title>Attaching Volumes to a &vmguest;</title>
   <para>
    After you create a volume as described in
    <xref
     linkend="sec.libvirt.storage.virsh.add_volumes"/>, you can
    attach it to a virtual machine and use it as a hard disk:
   </para>
<screen>virsh attach-disk <replaceable>DOMAIN</replaceable> <replaceable>SOURCE_IMAGE_FILE</replaceable> <replaceable>TARGET_DISK_DEVICE</replaceable></screen>
   <para>
    For example:
   </para>
<screen>virsh attach-disk sles12sp3 /virt/images/example_disk.qcow2 sda2</screen>
   <para>
    To check if the new disk is attached, inspect the result of the
    <command>virsh dumpxml</command> command:
   </para>
<screen>&prompt.root;virsh dumpxml sles12sp3
[...]
&lt;disk type='file' device='disk'>
 &lt;driver name='qemu' type='raw'/>
 &lt;source file='/virt/images/example_disk.qcow2'/>
 &lt;backingStore/>
 &lt;target dev='sda2' bus='scsi'/>
 &lt;alias name='scsi0-0-0'/>
 &lt;address type='drive' controller='0' bus='0' target='0' unit='0'/>
&lt;/disk>
[...]</screen>
   <sect3>
    <title>Hotplug or Persistent Change</title>
    <para>
     You can attach disks to both active and inactive domains. The attachment
     is controlled by the <option>--live</option> and <option>--config</option>
     options:
    </para>
    <variablelist>
     <varlistentry>
      <term><option>--live</option>
      </term>
      <listitem>
       <para>
        Hotplugs the disk to an active domain. The attachment is not saved in
        the domain configuration. Using <option>--live</option> on an inactive
        domain is an error.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--config</option>
      </term>
      <listitem>
       <para>
        Changes the domain configuration persistently. The attached disk is
        then available after the next domain start.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--live</option><option>--config</option>
      </term>
      <listitem>
       <para>
        Hotplugs the disk and adds it to the persistent domain configuration.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <tip>
     <title><command>virsh attach-device</command></title>
     <para>
      <command>virsh attach-device</command> is the more generic form of
      <command>virsh attach-disk</command>. You can use it to attach other
      types of devices to a domain.
     </para>
    </tip>
   </sect3>
  </sect2>

  <sect2 xml:id="libvirt.storage.virsh.detach_volumes">
   <title>Detaching Volumes from a &vmguest;</title>
   <para>
    To detach a disk from a domain, use <command>virsh detach-disk</command>:
   </para>
<screen>&prompt.root;virsh detach-disk <replaceable>DOMAIN</replaceable> <replaceable>TARGET_DISK_DEVICE</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.root;virsh detach-disk sles12sp3 sda2</screen>
   <para>
    You can control the attachment with the <option>--live</option> and
    <option>--config</option> options as described in
    <xref
     linkend="libvirt.storage.virsh.attach_volumes"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.libvirt.storage.locking">
  <title>Locking Disk Files and Block Devices with <systemitem class="daemon">virtlockd</systemitem></title>

  <para>
   Locking block devices and disk files prevents concurrent writes to these
   resources from different VM Guests. It provides protection against starting
   the same &vmguest; twice, or adding the same disk to two different virtual
   machines. This will reduce the risk of a virtual machine's disk image
   becoming corrupted because of a wrong configuration.
  </para>

  <para>
   The locking is controlled by a daemon called
   <systemitem
   class="daemon">virtlockd</systemitem>. Since it operates
   independently from the &libvirtd; daemon, locks will endure a crash or a
   restart of &libvirtd;. Locks will even persist in the case of an update of
   the <systemitem class="daemon">virtlockd</systemitem> itself, since it can
   re-execute itself. This ensures that &vmguest;s do <emphasis>not</emphasis>
   need to be restarted upon a
   <systemitem
   class="daemon">virtlockd</systemitem> update.
   <systemitem
   class="daemon">virtlockd</systemitem> is supported for &kvm;,
   &qemu;, and &xen;.
  </para>

  <sect2 xml:id="sec.libvirt.storage.locking.enable">
   <title>Enable Locking</title>
   <para>
    Locking virtual disks is not enabled by default on &productname;. To enable
    and automatically start it upon rebooting, perform the following steps:
   </para>
   <procedure>
    <step>
     <para>
      Edit <filename>/etc/libvirt/qemu.conf</filename> and set
     </para>
<screen>lock_manager = "lockd"</screen>
    </step>
    <step>
     <para>
      Start the <systemitem class="daemon">virtlockd</systemitem> daemon with
      the following command:
     </para>
<screen>systemctl start virtlockd</screen>
    </step>
    <step>
     <para>
      Restart the &libvirtd; daemon with:
     </para>
<screen>systemctl restart libvirtd</screen>
    </step>
    <step>
     <para>
      Make sure <systemitem class="daemon">virtlockd</systemitem> is
      automatically started when booting the system:
     </para>
<screen>systemctl enable virtlockd</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.libvirt.storage.locking.configure">
   <title>Configure Locking</title>
   <para>
    By default <systemitem class="daemon">virtlockd</systemitem> is configured
    to automatically lock all disks configured for your &vmguest;s. The default
    setting uses a "direct" lockspace, where the locks are acquired against the
    actual file paths associated with the VM Guest &lt;disk&gt; devices. For
    example, <literal>flock(2)</literal> will be called directly on
    <filename>/var/lib/libvirt/images/my-server/disk0.raw</filename> when the
    &vmguest; contains the following &lt;disk&gt; device:
   </para>
<screen>&lt;disk type='file' device='disk'&gt;
 &lt;driver name='qemu' type='raw'/&gt;
 &lt;source file='/var/lib/libvirt/images/my-server/disk0.raw'/&gt;
 &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</screen>
   <para>
    The <systemitem class="daemon">virtlockd</systemitem> configuration can be
    changed by editing the file
    <filename>/etc/libvirt/qemu-lockd.conf</filename>. It also contains
    detailed comments with further information. Make sure to activate
    configuration changes by reloading
    <systemitem class="daemon">virtlockd</systemitem>:
   </para>
<screen>systemctl reload virtlockd</screen>
   <note>
    <title>Locking Currently Only Available for All Disks</title>
    <para>
<!-- fs 2014-08-05: FIXME Check if tru &sle; 11 SP3 -->
     Currently, locking can only be activated globally, so that all virtual
     disks are locked. Support for locking selected disks is planned for future
     releases.
    </para>
   </note>
<!--

   <sect3 id="sec.libvirt.storage.locking.configure.noauto">
    <title>Manually Managing Locks</title>
    <para>
     As stated above, all virtual disks are locked by default. To
     restrict locking to selected disks, you need to turn off auto locking by
     setting
    </para>
    <screen>auto_disk_leases = 0</screen>
    <para>
     If auto locking is turned off you need to add &lt;lease> statements to
     the &lt;devices> section of the &vmguest;s XML definitions by editing
     them with the <command>virsh edit</command> command. A sample entry will
     look like the following example:
    </para>
    see https://www.redhat.com/archives/libvir-list/2013-April/msg01714.html
    <screen>TBD</screen>
    <para>

     Device leases
When using a lock manager, it may be desirable to record device leases against a VM. The lock manager will ensure the VM won't start unless the leases can be acquired.

  ...
  <devices>
    ...
    <lease>
      <lockspace>somearea</lockspace>
      <key>somekey</key>
      <target path='/some/lease/path' offset='1024'/>
    </lease>
    ...
  </devices>
  ...
lockspace
This is an arbitrary string, identifying the lockspace within which the key is held. Lock managers may impose extra restrictions on the format, or length of the lockspace name.
key
This is an arbitrary string, uniquely identifying the lease to be acquired. Lock managers may impose extra restrictions on the format, or length of the key.
target
This is the fully qualified path of the file associated with the
lockspace. The offset specifies where the lease is stored within the file. If
the lock manager does not require a offset, just pass 0.
    </para>
    <important>
     <title>Starting &vmguest;s without Locking</title>
     <para>
      If auto locking is disabled, <systemitem
      class="daemon">virtlockd</systemitem> automatically prevents all
      &vmguest;s that have no proper <quote>lease-configuration</quote> from
      being started. This ensures that only &vmguest;s with disks that are
      locked can be started.
     </para>
     <para>
      Disable this behavior by setting
     </para>
     <screen>require_lease_for_disks = 0</screen>
    </important>
   </sect3>
-->
   <sect3 xml:id="sec.libvirt.storage.locking.configure.shared_fs">
    <title>Enabling an Indirect Lockspace</title>
    <para>
     The default configuration of
     <systemitem class="daemon">virtlockd</systemitem> uses a
     <quote>direct</quote> lockspace. This means that the locks are acquired
     against the actual file paths associated with the &lt;disk&gt; devices.
    </para>
    <para>
     If the disk file paths are not accessible to all hosts,
     <systemitem class="daemon">virtlockd</systemitem> can be configured to
     allow an <quote>indirect</quote> lockspace. This means that a hash of the
     disk file path is used to create a file in the indirect lockspace
     directory. The locks are then held on these hash files instead of the
     actual disk file paths. Indirect lockspace is also useful if the file
     system containing the disk files does not support
     <literal>fcntl()</literal> locks. An indirect lockspace is specified with
     the <option>file_lockspace_dir</option> setting:
    </para>
<screen>file_lockspace_dir = "<replaceable>/MY_LOCKSPACE_DIRECTORY</replaceable>"</screen>
   </sect3>
   <sect3 xml:id="sec.libvirt.storage.locking.configure.lvm_iscsi">
    <title>Enable Locking on LVM or iSCSI Volumes</title>
    <para>
     When wanting to lock virtual disks placed on LVM or iSCSI volumes shared
     by several hosts, locking needs to be done by UUID rather than by path
     (which is used by default). Furthermore, the lockspace directory needs to
     be placed on a shared file system accessible by all hosts sharing the
     volume. Set the following options for LVM and/or iSCSI:
    </para>
<screen>lvm_lockspace_dir = "<replaceable>/MY_LOCKSPACE_DIRECTORY</replaceable>"
iscsi_lockspace_dir = "<replaceable>/MY_LOCKSPACE_DIRECTORY</replaceable>"</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.libvirt.storage.resize">
  <title>Online Resizing of Guest Block Devices</title>

  <para>
   Sometimes you need to change&mdash;extend or shrink&mdash;the size of the
   block device used by your guest system. For example, when the disk space
   originally allocated is no longer enough, it is time to increase its size.
   If the guest disk resides on a <emphasis>logical volume</emphasis>, you can
   resize it while the guest system is running. This is a big advantage over an
   offline disk resizing (see the <command>virt-resize</command> command from
   the <xref linkend="sec.guestfs.tools"/> package) as the service provided by
   the guest is not interrupted by the resizing process. To resize a &vmguest;
   disk, follow these steps:
  </para>

  <procedure>
   <title>Online Resizing of Guest Disk</title>
   <step>
    <para>
     Inside the guest system, check the current size of the disk (for example
     <filename>/dev/vda</filename>).
    </para>
<screen>&prompt.root;fdisk -l /dev/vda
Disk /dev/sda: 160.0 GB, 160041885696 bytes, 312581808 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes</screen>
   </step>
   <step>
    <para>
     On the host, resize the logical volume holding the
     <filename>/dev/vda</filename> disk of the guest to the required size, for
     example 200 GB.
    </para>
<screen>&prompt.root;lvresize -L 2048M /dev/mapper/vg00-home
Extending logical volume home to 2.00 GiB
Logical volume home successfully resized</screen>
   </step>
   <step>
    <para>
     On the host, resize the block device related to the disk
     <filename>/dev/mapper/vg00-home</filename> of the guest. Note that you can
     find the <replaceable>DOMAIN_ID</replaceable> with <command>virsh
     list</command>.
    </para>
<screen>&prompt.root;virsh blockresize  --path /dev/vg00/home --size 2048M <replaceable>DOMAIN_ID</replaceable>
Block device '/dev/vg00/home' is resized</screen>
   </step>
   <step>
    <para>
     Check that the new disk size is accepted by the guest.
    </para>
<screen>&prompt.root;fdisk -l /dev/vda
Disk /dev/sda: 200.0 GB, 200052357120 bytes, 390727260 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.libvirt.storage.share">
  <title>Sharing Directories between Host and Guests (File System Pass-Through)</title>

  <para>
   libvirt allows to share directories between host and guests using &qemu;'s
   file system pass-through (also called VirtFS) feature. Such a directory can
   be also be accessed by several &vmguest;s at once and therefore be used to
   exchange files between &vmguest;s.
  </para>

  <note>
   <title>Windows Guests and File System Pass-Through</title>
   <para>
    Note that sharing directories between &vmhost; and Windows guests via File
    System Pass-Through does not work, because Windows lacks the drivers
    required to mount the shared directory.
   </para>
  </note>

  <para>
   To make a shared directory available on a &vmguest;, proceed as follows:
  </para>

  <procedure>
   <step>
    <para>
     Open the guest's console in &vmm; and either choose
     <menuchoice><guimenu>View</guimenu>
     <guimenu>Details</guimenu></menuchoice> from the menu or click
     <guimenu>Show virtual hardware details</guimenu> in the toolbar. Choose
     <menuchoice> <guimenu>Add Hardware</guimenu> <guimenu>Filesystem</guimenu>
     </menuchoice> to open the <guimenu>Filesystem Passthrough</guimenu>
     dialog.
    </para>
   </step>
   <step>
    <para>
     <guimenu>Driver</guimenu> allows you to choose between a
     <guimenu>Handle</guimenu> or <guimenu>Path</guimenu> base driver. The
     default setting is <guimenu>Path</guimenu>. <guimenu>Mode</guimenu> lets
     you choose the security model, which influences the way file permissions
     are set on the host. Three options are available:
    </para>
    <variablelist>
     <varlistentry>
      <term><guimenu>Passthrough</guimenu> (Default)</term>
      <listitem>
       <para>
        Files on the file system are directly created with the client-user's
        credentials. This is very similar to what NFSv3 is using.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>Squash</guimenu>
      </term>
      <listitem>
       <para>
        Same as <guimenu>Passthrough</guimenu>, but failure of privileged
        operations like <command>chown</command> are ignored. This is required
        when &kvm; is not run with
        <systemitem
        class="username">root</systemitem> privileges.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>Mapped</guimenu>
      </term>
      <listitem>
       <para>
        Files are created with the file server's credentials
        (<literal>qemu.qemu</literal>). The user credentials and the
        client-user's credentials are saved in extended attributes. This model
        is recommended when host and guest domains should be kept completely
        isolated.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </step>
   <step>
    <para>
     Specify the path to the directory on the &vmhost; with <guimenu>Source
     Path</guimenu>. Enter a string at <guimenu>Target Path</guimenu> that will
     be used as a tag to mount the shared directory. Note that the string of
     this field is a tag only, not a path on the &vmguest;.
    </para>
   </step>
   <step>
    <para>
     <guimenu>Apply</guimenu> the setting. If the &vmguest; is currently
     running, you need to shut it down to apply the new setting (rebooting the
     guest is not sufficient).
    </para>
   </step>
   <step>
    <para>
     Boot the &vmguest;. To mount the shared directory, enter the following
     command:
    </para>
<screen>sudo mount -t 9p -o trans=virtio,version=9p2000.L,rw <replaceable>TAG</replaceable> /<replaceable>MOUNT_POINT</replaceable></screen>
    <para>
     To make the shared directory permanently available, add the following line
     to the <filename>/etc/fstab</filename> file:
    </para>
<screen><replaceable>TAG</replaceable>   /<replaceable>MOUNT_POINT</replaceable>    9p  trans=virtio,version=9p2000.L,rw    0   0</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="libvirt.storage.rbd">
  <title>Using RADOS Block Devices with &libvirt;</title>
  <para>
   RADOS Block Devices (RBD) store data in a Ceph cluster. They allow snapshotting,
   replication, and data consistency. You can use an RBD from your
   &libvirt;-managed &vmguest;s similarly you use other block devices.
  </para>
  <para>
   Refer to <link
    xlink:href="https://www.suse.com/documentation/ses-4/book_storage_admin/data/cha_ceph_libvirt.html">SUSE
    Enterprise Storage documentation</link> for more details.
  </para>
 </sect1>
</chapter>
