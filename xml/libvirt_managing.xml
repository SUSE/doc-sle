<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-libvirt-managing">
 <title>Basic &vmguest; Management</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
  </dm:docmanager>
 </info>
 <para>
  Most management tasks, such as starting or stopping a &vmguest;, can either
  be done using the graphical application &vmm; or on the command line using
  <command>virsh</command>. Connecting to the graphical console via VNC is only
  possible from a graphical user interface.
 </para>
 <note>
  <title>Managing &vmguest;s on a Remote &vmhost;</title>
  <para>
   If started on a &vmhost;, the &libvirt; tools &vmm;,
   <command>virsh</command>, and <command>virt-viewer</command> can be used to
   manage &vmguest;s on the host. However, it is also possible to manage
   &vmguest;s on a remote &vmhost;. This requires configuring remote access for
   &libvirt; on the host. For instructions, see
   <xref linkend="cha-libvirt-connect"/>.
  </para>
  <para>
   To connect to such a remote host with &vmm;, you need to set up a connection
   as explained in <xref linkend="sec-libvirt-connect-connecting-vmm"/>. If
   connecting to a remote host using <command>virsh</command> or
   <command>virt-viewer</command>, you need to specify a connection URI with
   the parameter <option>-c</option> (for example, <command>virsh -c
   qemu+tls://&wsIIIname;/system</command> or <command>virsh -c
   xen+ssh://</command>). The form of connection URI depends on the connection
   type and the hypervisor&mdash;see
   <xref linkend="sec-libvirt-connect-connecting"/> for details.
  </para>
  <para>
   Examples in this chapter are all listed without a connection URI.
  </para>
 </note>
 <sect1 xml:id="sec-libvirt-managing-list">
  <title>Listing &vmguest;s</title>

  <para>
   The &vmguest; listing shows all &vmguest;s managed by &libvirt; on a
   &vmhost;.
  </para>

  <sect2 xml:id="sec-libvirt-managing-list-vmm">
   <title>Listing &vmguest;s with &vmm;</title>
   <para>
    The main window of the &vmm; lists all &vmguest;s for each &vmhost; it is
    connected to. Each &vmguest; entry contains the machine's name, its status
    (<guimenu>Running</guimenu>, <guimenu>Paused</guimenu>, or
    <guimenu>Shutoff</guimenu>) displayed as an icon and literally, and a CPU
    usage bar.
   </para>
  </sect2>

  <sect2 xml:id="sec-libvirt-managing-list-virsh">
   <title>Listing &vmguest;s with <command>virsh</command></title>
   <para>
    Use the command <command>virsh</command> <option>list</option> to get a
    list of &vmguest;s:
   </para>
   <variablelist>
    <varlistentry>
     <term>List all running guests
     </term>
     <listitem>
<screen>&prompt.user;virsh list</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>List all running and inactive guests</term>
     <listitem>
<screen>&prompt.user;virsh list --all</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For more information and further options, see <command>virsh help
    list</command> or <command>man 1 virsh</command>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-managing-console">
  <title>Accessing the &vmguest; via Console</title>

  <para>
   &vmguest;s can be accessed via a VNC connection (graphical console) or, if
   supported by the guest operating system, via a serial console.
  </para>

  <sect2 xml:id="sec-libvirt-managing-console-vnc">
   <title>Opening a Graphical Console</title>
   <para>
    Opening a graphical console to a &vmguest; lets you interact with the
    machine like a physical host via a VNC connection. If accessing the VNC
    server requires authentication, you are prompted to enter a user name (if
    applicable) and a password.
   </para>
   <para>
    When you click into the VNC console, the cursor is <quote>grabbed</quote>
    and cannot be used outside the console anymore. To release it, press
    <keycombo> <keycap function="alt"/> <keycap function="control"/>
    </keycombo>.
   </para>
   <tip>
    <title>Seamless (Absolute) Cursor Movement</title>
    <para>
     To prevent the console from grabbing the cursor and to enable seamless
     cursor movement, add a tablet input device to the &vmguest;. See
     <xref linkend="sec-libvirt-config-input"/> for more information.
    </para>
   </tip>
   <para>
    Certain key combinations such as <keycombo> <keycap function="control"/>
    <keycap function="alt"/> <keycap function="delete"/> </keycombo> are
    interpreted by the host system and are not passed to the &vmguest;. To pass
    such key combinations to a &vmguest;, open the <guimenu>Send Key</guimenu>
    menu from the VNC window and choose the desired key combination entry. The
    <guimenu>Send Key</guimenu> menu is only available when using &vmm; and
    <command>virt-viewer</command>. With &vmm;, you can alternatively use the
    <quote>sticky key</quote> feature as explained in
    <xref linkend="tip-libvirt-inst-vmm-sticky"/>.
   </para>
   <note>
    <title>Supported VNC Viewers</title>
    <para>
     Principally all VNC viewers can connect to the console of a &vmguest;.
     However, if you are using SASL authentication and/or TLS/SSL connection to
     access the guest, the options are limited. Common VNC viewers such as
     <command>tightvnc</command> or <command>tigervnc</command> support neither
     SASL authentication nor TLS/SSL. The only supported alternative to &vmm;
     and <command>virt-viewer</command> is Remmina (refer to
     <xref linkend="vnc-remmina"/>).
    </para>
   </note>
   <sect3 xml:id="sec-libvirt-managing-console-vnc-vmm">
    <title>Opening a Graphical Console with &vmm;</title>
    <procedure>
     <step>
      <para>
       In the &vmm;, right-click a &vmguest; entry.
      </para>
     </step>
     <step>
      <para>
       Choose <guimenu>Open</guimenu> from the pop-up menu.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec-libvirt-managing-vnc-viewer">
    <title>Opening a Graphical Console with <command>virt-viewer</command></title>
    <para>
     <command>virt-viewer</command> is a simple VNC viewer with added
     functionality for displaying &vmguest; consoles. For example, it can be
     started in <quote>wait</quote> mode, where it waits for a &vmguest; to
     start before it connects. It also supports automatically reconnecting to a
     &vmguest; that is rebooted.
    </para>
    <para>
     <command>virt-viewer</command> addresses &vmguest;s by name, by ID or by
     UUID. Use <command>virsh</command> <option>list --all</option> to get this
     data.
    </para>
    <para>
     To connect to a guest that is running or paused, use either the ID, UUID,
     or name. &vmguest;s that are shut off do not have an ID&mdash;you can only
     connect to them by UUID or name.
    </para>
    <variablelist>
     <varlistentry>
      <term>Connect to guest with the ID <literal>8</literal>
      </term>
      <listitem>
<screen>&prompt.user;virt-viewer 8</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Connect to the inactive guest named <literal>sles12</literal>; the
      connection window will open once the guest starts</term>
      <listitem>
<screen>&prompt.user;virt-viewer --wait sles12</screen>
       <para>
        With the <option>--wait</option> option, the connection will be upheld
        even if the &vmguest; is not running at the moment. When the guest
        starts, the viewer will be launched.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     For more information, see <command>virt-viewer</command>
     <option>--help</option> or <command>man 1 virt-viewer</command>.
    </para>
    <note>
     <title>Password Input on Remote Connections with SSH</title>
     <para>
      When using <command>virt-viewer</command> to open a connection to a
      remote host via SSH, the SSH password needs to be entered twice. The
      first time for authenticating with &libvirt;, the second time for
      authenticating with the VNC server. The second password needs to be
      provided on the command line where virt-viewer was started.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-libvirt-managing-console-serial">
   <title>Opening a Serial Console</title>
   <para>
    Accessing the graphical console of a virtual machine requires a graphical
    environment on the client accessing the &vmguest;. As an alternative,
    virtual machines managed with libvirt can also be accessed from the shell
    via the serial console and <command>virsh</command>. To open a serial
    console to a &vmguest; named <quote>sles12</quote>, run the following
    command:
   </para>
<screen>&prompt.user;virsh console sles12</screen>
   <para>
    <command>virsh console</command> takes two optional flags:
    <option>--safe</option> ensures exclusive access to the console,
    <option>--force</option> disconnects any existing sessions before
    connecting. Both features need to be supported by the guest operating
    system.
   </para>
   <para>
    Being able to connect to a &vmguest; via serial console requires that the
    guest operating system supports serial console access and is properly
    supported. Refer to the guest operating system manual for more information.
   </para>
   <tip>
    <title>Enabling Serial Console Access for &sle; and &opensuse; Guests</title>
    <para>
     Serial console access in &sle; and &opensuse; is disabled by default. To
     enable it, proceed as follows:
    </para>
    <variablelist>
     <varlistentry>
      <term>&slsa; 12 / &opensuse;</term>
      <listitem>
       <para>
        Launch the &yast; Boot Loader module and switch to the <guimenu>Kernel
        Parameters</guimenu> tab. Add <literal>console=ttyS0</literal> to the
        field <guimenu>Optional Kernel Command Line Parameter</guimenu>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>&slsa; 11</term>
      <listitem>
       <para>
        Launch the &yast; Boot Loader module and select the boot entry for
        which to activate serial console access. Choose <guimenu>Edit</guimenu>
        and add <literal>console=ttyS0</literal> to the field <guimenu>Optional
        Kernel Command Line Parameter</guimenu>. Additionally, edit
        <filename>/etc/inittab</filename> and uncomment the line with the
        following content:
       </para>
<screen>#S0:12345:respawn:/sbin/agetty -L 9600 ttyS0 vt102</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-managing-status">
  <title>Changing a &vmguest;'s State: Start, Stop, Pause</title>

  <para>
   Starting, stopping or pausing a &vmguest; can be done with either &vmm; or
   <command>virsh</command>. You can also configure a &vmguest; to be
   automatically started when booting the &vmhost;.
  </para>

  <para>
   When shutting down a &vmguest;, you may either shut it down gracefully, or
   force the shutdown. The latter is equivalent to pulling the power plug on a
   physical host and is only recommended if there are no alternatives. Forcing
   a shutdown may cause file system corruption and loss of data on the
   &vmguest;.
  </para>

  <tip>
   <title>Graceful Shutdown</title>
   <para>
    To be able to perform a graceful shutdown, the &vmguest; must be configured
    to support <xref linkend="gloss-vt-acpi"/>. If you have created the guest
    with the &vmm;, ACPI should be available in the &vmguest;.
   </para>
   <para>
    Depending on the guest operating system, availability of ACPI may not be
    sufficient to perform a graceful shutdown. It is strongly recommended to
    test shutting down and rebooting a guest before using it in production.
    &opensuse; or &sled;, for example, can require &pk; authorization for
    shutdown and reboot. Make sure this policy is turned off on all &vmguest;s.
   </para>
   <para>
    If ACPI was enabled during a Windows XP/Windows Server 2003 guest
    installation, turning it on in the &vmguest; configuration only is not
    sufficient. For more information, see:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <link xlink:href="https://support.microsoft.com/en-us/kb/314088"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://support.microsoft.com/en-us/kb/309283"/>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Regardless of the &vmguest;'s configuration, a graceful shutdown is always
    possible from within the guest operating system.
   </para>
  </tip>

  <sect2 xml:id="sec-libvirt-managing-status-vmm">
   <title>Changing a &vmguest;'s State with &vmm;</title>
   <para>
    Changing a &vmguest;'s state can be done either from &vmm;'s main window,
    or from a VNC window.
   </para>
   <procedure>
    <title>State Change from the &vmm; Window</title>
    <step>
     <para>
      Right-click a &vmguest; entry.
     </para>
    </step>
    <step>
     <para>
      Choose <guimenu>Run</guimenu>, <guimenu>Pause</guimenu>, or one of the
      <guimenu>Shutdown options</guimenu> from the pop-up menu.
     </para>
    </step>
   </procedure>
   <procedure>
    <title>State change from the VNC Window</title>
    <step>
     <para>
      Open a VNC Window as described in
      <xref linkend="sec-libvirt-managing-console-vnc-vmm"/>.
     </para>
    </step>
    <step>
     <para>
      Choose <guimenu>Run</guimenu>, <guimenu>Pause</guimenu>, or one of the
      <guimenu>Shut Down</guimenu> options either from the toolbar or from the
      <guimenu>Virtual Machine</guimenu> menu.
     </para>
    </step>
   </procedure>
   <sect3 xml:id="sec-libvirt-managing-status-vmm-autostart">
    <title>Automatically Starting a &vmguest;</title>
    <para>
     You can automatically start a guest when the &vmhost; boots. This feature
     is not enabled by default and needs to be enabled for each &vmguest;
     individually. There is no way to activate it globally.
    </para>
    <procedure>
     <step>
      <para>
       Double-click the &vmguest; entry in &vmm; to open its console.
      </para>
     </step>
     <step>
      <para>
       Choose <menuchoice> <guimenu>View</guimenu>
       <guimenu>Details</guimenu></menuchoice> to open the &vmguest;
       configuration window.
      </para>
     </step>
     <step>
      <para>
       Choose <guimenu>Boot Options</guimenu> and check <guimenu>Start virtual
       machine on host boot up</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Save the new configuration with <guimenu>Apply</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-libvirt-managing-status-virsh">
   <title>Changing a &vmguest;'s State with <command>virsh</command></title>
   <para>
    In the following examples, the state of a &vmguest; named
    <quote>sles12</quote> is changed.
   </para>
   <variablelist>
    <varlistentry>
     <term>Start</term>
     <listitem>
<screen>&prompt.user;virsh start sles12</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Pause</term>
     <listitem>
<screen>&prompt.user;virsh suspend sles12</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Resume (a Suspended  &vmguest;)</term>
     <listitem>
<screen>&prompt.user;virsh resume sles12</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Reboot</term>
     <listitem>
<screen>&prompt.user;virsh reboot sles12</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Graceful shutdown</term>
     <listitem>
<screen>&prompt.user;virsh shutdown sles12</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Force shutdown</term>
     <listitem>
<screen>&prompt.user;virsh destroy sles12</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Turn on automatic start</term>
     <listitem>
<screen>&prompt.user;virsh autostart sles12</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Turn off automatic start</term>
     <listitem>
<screen>&prompt.user;virsh autostart --disable sles12</screen>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-manage-save">
  <title>Saving and Restoring the State of a &vmguest;</title>

  <para>
   Saving a &vmguest; preserves the exact state of the guest’s memory. The
   operation is similar to <emphasis>hibernating</emphasis> a computer. A saved
   &vmguest; can be quickly restored to its previously saved running condition.
  </para>

  <para>
   When saved, the &vmguest; is paused, its current memory state is saved to
   disk, and then the guest is stopped. The operation does not make a copy of
   any portion of the &vmguest;’s virtual disk. The amount of time taken to
   save the virtual machine depends on the amount of memory allocated. When
   saved, a &vmguest;’s memory is returned to the pool of memory available on
   the &vmhost;.
  </para>

  <para>
   The restore operation loads a &vmguest;’s previously saved memory state
   file and starts it. The guest is not booted but instead resumed at the point
   where it was previously saved. The operation is similar to coming out of
   hibernation.
  </para>

  <para>
   The &vmguest; is saved to a state file. Make sure there is enough space on
   the partition you are going to save to. For an estimation of the file size
   in megabytes to be expected, issue the following command on the guest:
  </para>

<screen>&prompt.user;free -mh | awk '/^Mem:/ {print $3}'</screen>

  <warning xml:id="adm-vm-restore">
   <title>Always Restore Saved Guests</title>
   <para>
    After using the save operation, do not boot or start the saved &vmguest;.
    Doing so would cause the machine's virtual disk and the saved memory state
    to get out of synchronization. This can result in critical errors when
    restoring the guest.
   </para>
   <para>
    To be able to work with a saved &vmguest; again, use the restore operation.
    If you used <command>virsh</command> to save a &vmguest;, you cannot
    restore it using &vmm;. In this case, make sure to restore using
    <command>virsh</command>.
   </para>
  </warning>

  <important>
   <title>Only for &vmguest;s with Disk Types <literal>raw</literal>, <literal>qcow2</literal></title>
   <para>
    Saving and restoring &vmguest;s is only possible if the &vmguest; is using
    a virtual disk of the type <literal>raw</literal>
    (<filename>.img</filename>), or <emphasis>qcow2</emphasis>.
   </para>
  </important>

  <sect2 xml:id="sec-libvirt-manage-save-vmm">
   <title>Saving/Restoring with &vmm;</title>
   <procedure>
    <title>Saving a &vmguest;</title>
    <step>
     <para>
      Open a VNC connection window to a &vmguest;. Make sure the guest is
      running.
     </para>
    </step>
    <step>
     <para>
      Choose <menuchoice> <guimenu>Virtual Machine</guimenu>
      <guimenu>Shutdown</guimenu> <guimenu>Save</guimenu> </menuchoice>.
     </para>
    </step>
   </procedure>
   <procedure>
    <title>Restoring a &vmguest;</title>
    <step>
     <para>
      Open a VNC connection window to a &vmguest;. Make sure the guest is not
      running.
     </para>
    </step>
    <step>
     <para>
      Choose <menuchoice> <guimenu>Virtual Machine</guimenu>
      <guimenu>Restore</guimenu> </menuchoice>.
     </para>
     <para>
      If the &vmguest; was previously saved using &vmm;, you will not be
      offered an option to <guimenu>Run</guimenu> the guest. However, note the
      caveats on machines saved with <command>virsh</command> outlined in
      <xref linkend="adm-vm-restore"/>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-libvirt-manage-save-virsh">
   <title>Saving and Restoring with <command>virsh</command></title>
   <para>
    Save a running &vmguest; with the command <command>virsh</command>
    <option>save</option> and specify the file which it is saved to.
   </para>
   <variablelist>
    <varlistentry>
     <term>Save the guest named <literal>opensuse13</literal>
     </term>
     <listitem>
<screen>&prompt.user;virsh save opensuse13 /virtual/saves/opensuse13.vmsav</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Save the guest with the ID <literal>37</literal>
     </term>
     <listitem>
<screen>&prompt.user;virsh save 37 /virtual/saves/opensuse13.vmsave</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    To restore a &vmguest;, use <command>virsh</command>
    <option>restore</option>:
   </para>
<screen>&prompt.user;virsh restore /virtual/saves/opensuse13.vmsave</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-managing-snapshots">
  <title>Creating and Managing Snapshots</title>

  <para>
   &vmguest; snapshots are snapshots of the complete virtual machine including
   the state of CPU, RAM, devices, and the content of all writable disks. To
   use virtual machine snapshots, all the attached hard disks need to use the
   qcow2 disk image format, and at least one of them needs to be writable.
  </para>

  <para>
   Snapshots let you restore the state of the machine at a particular point in
   time. This is useful when undoing a faulty configuration or the installation
   of a lot of packages. After starting a snapshot that was created while the
   &vmguest; was shut off, you will need to boot it. Any changes written to the
   disk after that point in time will be lost when starting the snapshot.
  </para>

  <note>
   <para>
    Snapshots are supported on &kvm; &vmhost;s only.
   </para>
  </note>

  <sect2 xml:id="libvirt-snapshots-terminology">
   <title>Terminology</title>
   <para>
    There are several specific terms used to describe the types of snapshots:
   </para>
   <variablelist>
    <varlistentry>
     <term>Internal snapshots</term>
     <listitem>
      <para>
       Snapshots that are saved into the qcow2 file of the original &vmguest;.
       The file holds both the saved state of the snapshot and the changes made
       since the snapshot was taken. The main advantage of internal snapshots
       is that they are all stored in one file and therefore it is easy to copy
       or move them across multiple machines.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>External snapshots</term>
     <listitem>
      <para>
       When creating an external snapshot, the original qcow2 file is saved and
       made read-only, while a new qcow2 file is created to hold the changes.
       The original file is sometimes called a 'backing' or 'base' file, while
       the new file with all the changes is called an 'overlay' or 'derived'
       file. External snapshots are useful when performing backups of &vmguest;s.
       However, external snapshots are not supported by &vmm;, and
       cannot be deleted by <command>virsh</command> directly.
       For more information on external snapshots in &qemu;, refer
       to <xref linkend="cha-qemu-guest-inst-qemu-img-effect"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Live snapshots</term>
     <listitem>
      <para>
       Snapshots created when the original &vmguest; is running. Internal live
       snapshots support saving the devices, and memory and disk states, while
       external live snapshots with <command>virsh</command> support saving
       either the memory state, or the disk state, or both.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Offline snapshots</term>
     <listitem>
      <para>
       Snapshots created from a &vmguest; that is shut off. This ensures data
       integrity as all the guest's processes are stopped and no memory is in
       use.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec-libvirt-managing-snapshots-vmm">
   <title>Creating and Managing Snapshots with &vmm;</title>
   <important>
    <title>Internal Snapshots Only</title>
    <para>
     &vmm; supports only internal snapshots, either live or offline.
    </para>
   </important>
   <para>
    To open the snapshot management view in &vmm;, open the VNC window as
    described in <xref linkend="sec-libvirt-managing-console-vnc-vmm"/>. Now
    either choose <menuchoice> <guimenu>View</guimenu>
    <guimenu>Snapshots</guimenu> </menuchoice> or click <guimenu>Manage VM
    Snapshots</guimenu> in the toolbar.
   </para>
   <informalfigure>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="virt_vmm_snapshots_list.png" width="75%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="virt_vmm_snapshots_list.png" width="75%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </informalfigure>
   <para>
    The list of existing snapshots for the chosen &vmguest; is displayed in the
    left-hand part of the window. The snapshot that was last started is marked
    with a green tick. The right-hand part of the window shows details of the
    snapshot currently marked in the list. These details include the snapshot's
    title and time stamp, the state of the &vmguest; at the time the snapshot
    was taken and a description. Snapshots of running guests also include a
    screenshot. The <guimenu>Description</guimenu> can be changed directly from
    this view. Other snapshot data cannot be changed.
   </para>
   <sect3 xml:id="sec-libvirt-managing-snapshots-vmm-add">
    <title>Creating a Snapshot</title>
    <para>
     To take a new snapshot of a &vmguest;, proceed as follows:
    </para>
    <procedure>
     <step>
      <para>
       Optionally, shut down the &vmguest; if you want to create an offline
       snapshot.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Add</guimenu> in the bottom left corner of the VNC
       window.
      </para>
      <para>
       The window <guimenu>Create Snapshot</guimenu> opens.
      </para>
     </step>
     <step>
      <para>
       Provide a <guimenu>Name</guimenu> and, optionally, a description. The
       name cannot be changed after the snapshot has been taken. To be able to
       identify the snapshot later easily, use a <quote>speaking name</quote>.
      </para>
     </step>
     <step>
      <para>
       Confirm with <guimenu>Finish</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec-libvirt-managing-snapshots-vmm-delete">
    <title>Deleting a Snapshot</title>
    <para>
     To delete a snapshot of a &vmguest;, proceed as follows:
    </para>
    <procedure>
     <step>
      <para>
       Click <guimenu>Delete</guimenu> in the bottom left corner of the VNC
       window.
      </para>
     </step>
     <step>
      <para>
       Confirm the deletion with <guimenu>Yes</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec-libvirt-managing-snapshots-vmm-start">
    <title>Starting a Snapshot</title>
    <para>
     To start a snapshot, proceed as follows:
    </para>
    <procedure>
     <step>
      <para>
       Click <guimenu>Run</guimenu> in the bottom left corner of the VNC
       window.
      </para>
     </step>
     <step>
      <para>
       Confirm the start with <guimenu>Yes</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec-libvirt-managing-snapshots-virsh">
   <title>Creating and Managing Snapshots with <command>virsh</command></title>
   <para>
    To list all existing snapshots for a domain
    (<replaceable>admin_server</replaceable> in the following), run the
    <literal>snapshot-list</literal> command:
   </para>
<screen>&prompt.user;virsh snapshot-list --domain sle-ha-node1
 Name                 Creation Time             State
------------------------------------------------------------
 sleha_12_sp2_b2_two_node_cluster 2016-06-06 15:04:31 +0200 shutoff
 sleha_12_sp2_b3_two_node_cluster 2016-07-04 14:01:41 +0200 shutoff
 sleha_12_sp2_b4_two_node_cluster 2016-07-14 10:44:51 +0200 shutoff
 sleha_12_sp2_rc3_two_node_cluster 2016-10-10 09:40:12 +0200 shutoff
 sleha_12_sp2_gmc_two_node_cluster 2016-10-24 17:00:14 +0200 shutoff
 sleha_12_sp3_gm_two_node_cluster 2017-08-02 12:19:37 +0200 shutoff
 sleha_12_sp3_rc1_two_node_cluster 2017-06-13 13:34:19 +0200 shutoff
 sleha_12_sp3_rc2_two_node_cluster 2017-06-30 11:51:24 +0200 shutoff
 sleha_15_b6_two_node_cluster 2018-02-07 15:08:09 +0100 shutoff
 sleha_15_rc1_one-node 2018-03-09 16:32:38 +0100 shutoff</screen>
   <para>
    The snapshot that was last started is shown with the
    <literal>snapshot-current command:</literal>
   </para>
<screen>&prompt.user;virsh snapshot-current --domain admin_server
Basic installation incl. SMT for CLOUD4
</screen>
   <para>
    Details about a particular snapshot can be obtained by running the
    <literal>snapshot-info</literal> command:
   </para>
<screen>&prompt.user;virsh snapshot-info --domain admin_server \
   -name  "Basic installation incl. SMT for CLOUD4"
Name:           Basic installation incl. SMT for CLOUD4
Domain:         admin_server
Current:        yes
State:          shutoff
Location:       internal
Parent:         Basic installation incl. SMT for CLOUD3-HA
Children:       0
Descendants:    0
Metadata:       yes
</screen>
   <sect3 xml:id="sec-libvirt-managing-snapshots-virsh-add">
    <title>Creating Internal Snapshots</title>
    <para>
     To take an internal snapshot of a &vmguest;, either a live or offline, use
     the <literal>snapshot-create-as</literal> command as follows:
    </para>
<screen>&prompt.user;virsh snapshot-create-as --domain admin_server<co xml:id="virsh-snapshot-add-domain"/> --name "Snapshot 1"<co xml:id="virsh-snapshot-add-name"/> \
--description "First snapshot"<co xml:id="virsh-snapshot-add-description"/></screen>
    <calloutlist>
     <callout arearefs="virsh-snapshot-add-domain">
      <para>
       Domain name. Mandatory.
      </para>
     </callout>
     <callout arearefs="virsh-snapshot-add-name">
      <para>
       Name of the snapshot. It is recommended to use a <quote>speaking
       name</quote>, since that makes it easier to identify the snapshot.
       Mandatory.
      </para>
     </callout>
     <callout arearefs="virsh-snapshot-add-description">
      <para>
       Description for the snapshot. Optional.
      </para>
     </callout>
    </calloutlist>
   </sect3>
   <sect3 xml:id="sec-libvirt-managing-snapshots-virsh-add-ext">
    <title>Creating External Snapshots</title>
    <para>
     With <command>virsh</command>, you can take external snapshots of the
     guest's memory state, disk state, or both.
    </para>
    <para>
     To take both live and offline external snapshots of the guest's disk,
     specify the <option>--disk-only</option> option:
    </para>
<screen>&prompt.user;virsh snapshot-create-as --domain admin_server --name \
 "Offline external snapshot" --disk-only</screen>
    <para>
     You can specify the <option>--diskspec</option> option to control how the
     external files are created:
    </para>
<screen>&prompt.user;virsh snapshot-create-as --domain admin_server --name \
 "Offline external snapshot" \
 --disk-only --diskspec vda,snapshot=external,file=/path/to/snapshot_file</screen>
    <para>
     To take a live external snapshot of the guest's memory, specify the
     <option>--live</option> and <option>--memspec</option> options:
    </para>
<screen>&prompt.user;virsh snapshot-create-as --domain admin_server --name \
 "Offline external snapshot" --live \
 --memspec snapshot=external,file=/path/to/snapshot_file</screen>
    <para>
     To take a live external snapshot of both the guest's disk and memory
     states, combine the <option>--live</option>, <option>--diskspec</option>,
     and <option>--memspec</option> options:
    </para>
<screen>&prompt.user;virsh snapshot-create-as --domain admin_server --name \
 "Offline external snapshot" --live \
 --memspec snapshot=external,file=/path/to/snapshot_file
 --diskspec vda,snapshot=external,file=/path/to/snapshot_file</screen>
    <para>
     Refer to the <citetitle>SNAPSHOT COMMANDS</citetitle> section in
     <command>man 1 virsh</command> for more details.
    </para>
   </sect3>
   <sect3 xml:id="sec-libvirt-managing-snapshots-virsh-delete">
    <title>Deleting a Snapshot</title>
    <para>
     External snapshots cannot be deleted with <command>virsh</command>.
     To delete an internal snapshot of a &vmguest; and restore the disk space
     it occupies, use the <literal>snapshot-delete</literal> command:
    </para>
<screen>&prompt.user;virsh snapshot-delete --domain admin_server --snapshotname "Snapshot 2"</screen>
   </sect3>
   <sect3 xml:id="sec-libvirt-managing-snapshots-virsh-start">
    <title>Starting a Snapshot</title>
    <para>
     To start a snapshot, use the <literal>snapshot-revert</literal> command:
    </para>
<screen>&prompt.user;virsh snapshot-revert --domain admin_server --snapshotname "Snapshot 1"</screen>
    <para>
     To start the current snapshot (the one the &vmguest; was started off), it
     is sufficient to use <option>--current</option> rather than specifying the
     snapshot name:
    </para>
<screen>&prompt.user;virsh snapshot-revert --domain admin_server --current</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-managing-delete">
  <title>Deleting a &vmguest;</title>

  <para>
   By default, deleting a &vmguest; using <command>virsh</command> removes only
   its XML configuration. Since attached storage is not deleted by default, you
   can reuse it with another &vmguest;. With &vmm;, you can also delete a
   guest's storage files as well&mdash;this will completely erase the guest.
  </para>

  <sect2 xml:id="sec-libvirt-managing-delete-vmm">
   <title>Deleting a &vmguest; with &vmm;</title>
   <procedure>
    <step>
     <para>
      In the &vmm;, right-click a &vmguest; entry.
     </para>
    </step>
    <step>
     <para>
      From the context menu, choose <guimenu>Delete</guimenu>.
     </para>
    </step>
    <step>
     <para>
      A confirmation window opens. Clicking <guimenu>Delete</guimenu> will
      permanently erase the &vmguest;. The deletion is not recoverable.
     </para>
     <para>
      You can also permanently delete the guest's virtual disk by activating
      <guimenu>Delete Associated Storage Files</guimenu>. The deletion is not
      recoverable either.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-libvirt-managing-delete-virsh">
   <title>Deleting a &vmguest; with <command>virsh</command></title>
   <para>
    To delete a &vmguest;, it needs to be shut down first. It is not possible
    to delete a running guest. For information on shutting down, see
    <xref linkend="sec-libvirt-managing-status"/>.
   </para>
   <para>
    To delete a &vmguest; with <command>virsh</command>, run
    <command>virsh</command> <option>undefine</option>
    <replaceable>VM_NAME</replaceable>.
   </para>
<screen>&prompt.user;virsh undefine sles12</screen>
   <para>
    There is no option to automatically delete the attached storage files. If
    they are managed by libvirt, delete them as described in
    <xref linkend="sec-libvirt-storage-virsh-del-volumes"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-libvirt-admin-migrate">
  <title>Migrating &vmguest;s</title>

  <para>
   One of the major advantages of virtualization is that &vmguest;s are
   portable. When a &vmhost; needs to go down for maintenance, or when the host
   gets overloaded, the guests can easily be moved to another &vmhost;. &kvm;
   and &xen; even support <quote>live</quote> migrations during which the
   &vmguest; is constantly available.
  </para>

  <sect2 xml:id="libvirt-admin-live-migration-requirements">
   <title>Migration Requirements</title>
   <para>
    To successfully migrate a &vmguest; to another &vmhost;, the following
    requirements need to be met:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      It is recommended that the source and destination systems have the same
      architecture.
     </para>
    </listitem>
    <listitem>
     <para>
      Storage devices must be accessible from both machines (for example, via
      NFS or iSCSI) and must be configured as a storage pool on both machines.
      For more information, see <xref linkend="cha-libvirt-storage"/>.
     </para>
     <para>
      This is also true for CD-ROM or floppy images that are connected during
      the move. However, you can disconnect them prior to the move as described
      in <xref linkend="sec-libvirt-config-cdrom-media-change"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      &libvirtd; needs to run on both &vmhost;s and you must be able to open a
      remote &libvirt; connection between the target and the source host (or
      vice versa). Refer to <xref linkend="sec-libvirt-connect-remote"/> for
      details.
     </para>
    </listitem>
    <listitem>
     <para>
      If a firewall is running on the target host, ports need to be opened to
      allow the migration. If you do not specify a port during the migration
      process, &libvirt; chooses one from the range 49152:49215. Make sure that
      either this range (recommended) or a dedicated port of your choice is
      opened in the firewall on the <emphasis>target host</emphasis>.
     </para>
    </listitem>
    <listitem>
     <para>
      Host and target machine should be in the same subnet on the network,
      otherwise networking will not work after the migration.
     </para>
    </listitem>
    <listitem>
     <para>
      All &vmhost;s participating in migration must have the same UID for
      the qemu user and the same GIDs for the kvm, qemu, and libvirt groups.
     </para>
    </listitem>
    <listitem>
     <para>
      No running or paused &vmguest; with the same name must exist on the
      target host. If a shut down machine with the same name exists, its
      configuration will be overwritten.
     </para>
    </listitem>
    <listitem>
     <para>
      All CPU models except <emphasis>host cpu</emphasis> model are supported
      when migrating &vmguest;s.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="gloss-vt-acronym-sata"/> disk device type is not
      migratable.
     </para>
    </listitem>
    <listitem>
     <para>
      File system pass-through feature is incompatible with migration.
     </para>
    </listitem>
    <listitem>
     <para>
      The &vmhost; and &vmguest; need to have proper timekeeping installed. See
      <xref linkend="sec-kvm-managing-clock"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      No physical devices can be passed from host to guest. Live migration is
      currently not supported when using devices with PCI pass-through or
      <xref linkend="vt-io-sriov"/>. If live migration needs to be supported,
      you need to use software virtualization (paravirtualization or full
      virtualization).
     </para>
    </listitem>
    <listitem>
     <para>
      Cache mode setting is an important setting for migration. See:
      <xref linkend="sec-cache-mode-live-migration"/>.
     </para>
    </listitem>
    <listitem os="sles;sled">
     <para>
      Live migration of &vmguest;s from a host running one operating system to
      a host running a different operating system is fully supported for the
      following scenarios:
     </para>
     <itemizedlist>
      <listitem>
<!-- fate#324058 -->
       <para>
        &slsa; 12 SP2 to &slsa; 15
       </para>
      </listitem>
      <listitem>
       <para>
        &slsa; 12 SP3 to &slsa; 15
       </para>
      </listitem>
      <listitem>
       <para>
        &slsa; 12 SP4 to &slsa; 15 (when released)
       </para>
      </listitem>
     </itemizedlist>
<!-- fate#320425 -->
     <para os="sles;sled">
      Backward migration (for example from &slsa; 12 SP1 or SP2 to 12 or from
      &slsa; SP2 to SP1) is not supported.
     </para>
    </listitem>
    <listitem>
     <para>
      The image directory should be located in the same path on both hosts.
     </para>
    </listitem>
    <listitem>
     <para>
      All hosts should be on the same level of microcode (especially the spectre
      microcode updates). This can be achieved by installing the latest updates
      of &productname; on all hosts.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec-libvirt-admin-migrate-virtmanager">
   <title>Migrating with &vmm;</title>
   <para>
    When using the &vmm; to migrate &vmguest;s, it does not matter on which
    machine it is started. You can start &vmm; on the source or the target host
    or even on a third host. In the latter case you need to be able to open
    remote connections to both the target and the source host.
   </para>
   <procedure>
    <step>
     <para>
      Start &vmm; and establish a connection to the target or the source host.
      If the &vmm; was started neither on the target nor the source host,
      connections to both hosts need to be opened.
     </para>
    </step>
    <step>
     <para>
      Right-click the &vmguest; that you want to migrate and choose
      <guimenu>Migrate</guimenu>. Make sure the guest is running or
      paused&mdash;it is not possible to migrate guests that are shut down.
     </para>
     <tip>
      <title>Increasing the Speed of the Migration</title>
      <para>
       To increase the speed of the migration somewhat, pause the &vmguest;.
       This is the equivalent of the former so-called <quote>offline
       migration</quote> option of &vmm;.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Choose a <guimenu>New Host</guimenu> for the &vmguest;. If the desired
      target host does not show up, make sure that you are connected to the
      host.
     </para>
     <para>
      To change the default options for connecting to the remote host, under
      <guimenu>Connection</guimenu>, set the <guimenu>Mode</guimenu>, and the
      target host's <guimenu>Address</guimenu> (IP address or host name) and
      <guimenu>Port</guimenu>. If you specify a <guimenu>Port</guimenu>, you
      must also specify an <guimenu>Address</guimenu>.
     </para>
     <para>
      Under <guimenu>Advanced options</guimenu>, choose whether the move should
      be permanent (default) or temporary, using <guimenu>Temporary
      move</guimenu>.
     </para>
     <para>
      Additionally, there is the option <guimenu>Allow unsafe</guimenu>, which
      allows migrating without disabling the cache of the &vmhost;. This can
      speed up the migration but only works when the current configuration
      allows for a consistent view of the &vmguest; storage without using
      <literal>cache=&quot;none&quot;</literal>/<literal>0_DIRECT</literal>.
     </para>
     <note>
      <title>Bandwidth Option</title>
      <para>
       In recent versions of &vmm;, the option of setting a bandwidth for the
       migration has been removed. To set a specific bandwidth, use
       <command>virsh</command> instead.
      </para>
     </note>
    </step>
    <step>
     <para>
      To perform the migration, click <guimenu>Migrate</guimenu>.
     </para>
     <para>
      When the migration is complete, the <guimenu>Migrate</guimenu> window
      closes and the &vmguest; is now listed on the new host in the &vmm;
      window. The original &vmguest; will still be available on the target host
      (in shut down state).
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-libvirt-admin-migrate-virsh">
   <title>Migrating with <command>virsh</command></title>
   <para>
    To migrate a &vmguest; with <command>virsh</command>
    <option>migrate</option>, you need to have direct or remote shell access to
    the &vmhost;, because the command needs to be run on the host. The
    migration command looks like this:
   </para>
<screen>&prompt.user;virsh migrate [OPTIONS] <replaceable>VM_ID_or_NAME</replaceable> <replaceable>CONNECTION_URI</replaceable> [--migrateuri tcp://<replaceable>REMOTE_HOST:PORT</replaceable>]</screen>
   <para>
    The most important options are listed below. See <command>virsh help
    migrate</command> for a full list.
   </para>
   <variablelist>
    <varlistentry>
     <term><option>--live</option>
     </term>
     <listitem>
      <para>
       Does a live migration. If not specified, the guest will be paused during
       the migration (<quote>offline migration</quote>).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--suspend</option>
     </term>
     <listitem>
      <para>
       Does an offline migration and does not restart the &vmguest; on the
       target host.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--persistent</option>
     </term>
     <listitem>
      <para>
       By default a migrated &vmguest; will be migrated temporarily, so its
       configuration is automatically deleted on the target host if it is shut
       down. Use this switch to make the migration persistent.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--undefinesource</option>
     </term>
     <listitem>
      <para>
       When specified, the &vmguest; definition on the source host will be
       deleted after a successful migration (however, virtual disks attached to
       this guest will <emphasis>not</emphasis> be deleted).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The following examples use &wsIVname; as the source system and &wsIname; as
    the target system; the &vmguest;'s name is <literal>opensuse131</literal>
    with Id <literal>37</literal>.
   </para>
   <variablelist>
    <varlistentry>
     <term>Offline migration with default parameters</term>
     <listitem>
<screen>&prompt.user;virsh migrate 37 qemu+ssh://&exampleuser_plain;@&wsIname;/system</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Transient live migration with default parameters</term>
     <listitem>
<screen>&prompt.user;virsh migrate --live opensuse131 qemu+ssh://&exampleuser_plain;@&wsIname;/system</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Persistent live migration; delete VM definition on source</term>
     <listitem>
<screen>&prompt.user;virsh migrate --live --persistent --undefinesource 37 \
qemu+tls://&exampleuser_plain;@&wsIname;/system</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Offline migration using port 49152</term>
     <listitem>
<screen>&prompt.user;virsh migrate opensuse131 qemu+ssh://&exampleuser_plain;@&wsIname;/system \
--migrateuri tcp://@&wsIname;:49152</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <note>
    <title>Transient Compared to Persistent Migrations</title>
    <para>
     By default <command>virsh migrate</command> creates a temporary
     (transient) copy of the &vmguest; on the target host. A shut down version
     of the original guest description remains on the source host. A transient
     copy will be deleted from the server after it is shut down.
    </para>
    <para>
     To create a permanent copy of a guest on the target host, use the switch
     <option>--persistent</option>. A shut down version of the original guest
     description remains on the source host, too. Use the option
     <option>--undefinesource</option> together with
     <option>--persistent</option> for a <quote>real</quote> move where a
     permanent copy is created on the target host and the version on the source
     host is deleted.
    </para>
    <para>
     It is not recommended to use <option>--undefinesource</option> without the
     <option>--persistent</option> option, since this will result in the loss
     of both &vmguest; definitions when the guest is shut down on the target
     host.
    </para>
   </note>
  </sect2>

<!-- Step by step example -->

  <sect2 xml:id="sec-libvirt-migrate-stepbstep">
   <title>Step-by-Step Example</title>
   <para/>
   <sect3 xml:id="sec-migrate-stepbstep-export">
    <title>Exporting the Storage</title>
    <para>
     First you need to export the storage, to share the Guest image between
     host. This can be done by an NFS server. In the following example we want
     to share the <filename>/volume1/VM</filename> directory for all machines
     that are on the network 10.0.1.0/24. We will use a &sle; NFS server. As
     root user, edit the <filename>/etc/exports</filename> file and add:
    </para>
<screen>/volume1/VM 10.0.1.0/24  (rw,sync,no_root_squash)</screen>
    <para>
     You need to restart the NFS server:
    </para>
<screen>&prompt.sudo;systemctl restart nfsserver
&prompt.sudo;exportfs
/volume1/VM      10.0.1.0/24</screen>
   </sect3>
   <sect3 xml:id="sec-migrate-stepbstep-pool">
    <title>Defining the Pool on the Target Hosts</title>
    <para>
     On each host where you want to migrate the &vmguest;, the pool must be
     defined to be able to access the volume (that contains the Guest image).
     Our NFS server IP address is 10.0.1.99, its share is the
     <filename>/volume1/VM</filename> directory, and we want to get it mounted
     in the <filename>/var/lib/libvirt/images/VM</filename> directory. The pool
     name will be <emphasis>VM</emphasis>. To define this pool, create a
     <filename>VM.xml</filename> file with the following content:
    </para>
<screen>&lt;pool type='netfs'&gt;
  &lt;name&gt;VM&lt;/name&gt;
  &lt;source&gt;
    &lt;host name='10.0.1.99'/&gt;
    &lt;dir path='/volume1/VM'/&gt;
    &lt;format type='auto'/&gt;
  &lt;/source&gt;
  &lt;target&gt;
    &lt;path&gt;/var/lib/libvirt/images/VM&lt;/path&gt;
    &lt;permissions&gt;
      &lt;mode&gt;0755&lt;/mode&gt;
      &lt;owner&gt;-1&lt;/owner&gt;
      &lt;group&gt;-1&lt;/group&gt;
    &lt;/permissions&gt;
  &lt;/target&gt;
  &lt;/pool&gt;</screen>
    <para>
     Then load it into &libvirt; using the <command>pool-define</command>
     command:
    </para>
<screen>&prompt.root;virsh pool-define VM.xml</screen>
    <para>
     An alternative way to define this pool is to use the
     <command>virsh</command> command:
    </para>
<screen>&prompt.root;virsh pool-define-as VM --type netfs --source-host 10.0.1.99 \
     --source-path /volume1/VM --target /var/lib/libvirt/images/VM
Pool VM created</screen>
    <para>
     The following commands assume that you are in the interactive shell of
     <command>virsh</command> which can also be reached by using the command
     <command>virsh</command> without any arguments. Then the pool can be set
     to start automatically at host boot (autostart option):
    </para>
<screen><prompt>virsh # </prompt>pool-autostart VM
Pool VM marked as autostarted</screen>
    <para>
     If you want to disable the autostart:
    </para>
<screen><prompt>virsh # </prompt>pool-autostart VM --disable
Pool VM unmarked as autostarted</screen>
    <para>
     Check if the pool is present:
    </para>
<screen><prompt>virsh # </prompt>pool-list --all
 Name                 State      Autostart
-------------------------------------------
 default              active     yes
 VM                   active     yes

<prompt>virsh # </prompt>pool-info VM
Name:           VM
UUID:           42efe1b3-7eaa-4e24-a06a-ba7c9ee29741
State:          running
Persistent:     yes
Autostart:      yes
Capacity:       2,68 TiB
Allocation:     2,38 TiB
Available:      306,05 GiB</screen>
    <warning>
     <title>Pool Needs to Exist on All Target Hosts</title>
     <para>
      Remember: this pool must be defined on each host where you want to be
      able to migrate your &vmguest;.
     </para>
    </warning>
   </sect3>
   <sect3 xml:id="sec-migrate-stepbstep-volume">
    <title>Creating the Volume</title>
    <para>
     The pool has been defined&mdash;now we need a volume which will contain
     the disk image:
    </para>
<screen><prompt>virsh # </prompt>vol-create-as VM sled12.qcow12 8G --format qcow2
Vol sled12.qcow12 created</screen>
    <para>
     The volume names shown will be used later to install the guest with
     virt-install.
    </para>
   </sect3>
   <sect3 xml:id="sec-migrate-stepbstep-guest">
    <title>Creating the &vmguest;</title>
    <para>
     Let's create a &productname; &vmguest; with the
     <command>virt-install</command> command. The <emphasis>VM</emphasis> pool
     will be specified with the <command>--disk</command> option,
     <emphasis>cache=none</emphasis> is recommended if you do not want to use
     the <command>--unsafe</command> option while doing the migration.
    </para>
<screen>&prompt.root;virt-install --connect qemu:///system --virt-type kvm --name \
   sled12 --memory 1024 --disk vol=VM/sled12.qcow2,cache=none --cdrom \
   /mnt/install/ISO/SLE-12-Desktop-DVD-x86_64-Build0327-Media1.iso --graphics \
   vnc --os-variant sled12
Starting install...
Creating domain...</screen>
   </sect3>
   <sect3 xml:id="sec-migrate-stepbstep-migrate">
    <title>Migrate the &vmguest;</title>
    <para>
     Everything is ready to do the migration now. Run the
     <command>migrate</command> command on the &vmhost; that is currently
     hosting the &vmguest;, and choose the destination.
    </para>
<screen>virsh # migrate --live sled12 --verbose qemu+ssh://<replaceable>IP/Hostname</replaceable>/system
Password:
Migration: [ 12 %]</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-libvirt-admin-monitor">
  <title>Monitoring</title>

  <para/>

  <sect2 xml:id="cha-libvirt-admin-monitor-virt-manager">
   <title>Monitoring with &vmm;</title>
   <para>
    After starting &vmm; and connecting to the &vmhost;, a CPU usage graph of
    all the running guests is displayed.
   </para>
   <para>
    It is also possible to get information about disk and network usage with
    this tool, however, you must first activate this in
    <guimenu>Preferences</guimenu>:
   </para>
   <procedure>
    <step>
     <para>
      Run <command>virt-manager</command>.
     </para>
    </step>
    <step>
     <para>
      Select <menuchoice><guimenu>Edit</guimenu>
      <guimenu>Preferences</guimenu></menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Change the tab from <guimenu>General</guimenu> to
      <guimenu>Polling</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Activate the check boxes for the kind of activity you want to see:
      <guimenu>Poll Disk I/O</guimenu>, <guimenu>Poll Network I/O</guimenu>,
      and <guimenu>Poll Memory stats</guimenu>.
     </para>
    </step>
    <step>
     <para>
      If desired, also change the update interval using <guimenu>Update status
      every n seconds</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Close the <guimenu>Preferences</guimenu> dialog.
     </para>
    </step>
    <step>
     <para>
      Activate the graphs that should be displayed under <menuchoice>
      <guimenu>View</guimenu> <guimenu>Graph</guimenu> </menuchoice>.
     </para>
    </step>
   </procedure>
   <para>
    Afterward, the disk and network statistics are also displayed in the main
    window of the &vmm;.
   </para>
   <para>
    More precise data is available from the VNC window. Open a VNC window as
    described in <xref linkend="sec-libvirt-managing-console-vnc"/>. Choose
    <guimenu>Details</guimenu> from the toolbar or the <guimenu>View</guimenu>
    menu. The statistics are displayed from the <guimenu>Performance</guimenu>
    entry of the left-hand tree menu.
   </para>
  </sect2>

  <sect2 xml:id="cha-libvirt-admin-monitor-virt-top">
   <title>Monitoring with <command>virt-top</command></title>
   <para>
    <command>virt-top</command> is a command line tool similar to the
    well-known process monitoring tool <command>top</command>.
    <command>virt-top</command> uses libvirt and therefore is capable of
    showing statistics for &vmguest;s running on different hypervisors. It is
    recommended to use <command>virt-top</command> instead of
    hypervisor-specific tools like <command>xentop</command>.
   </para>
   <para>
    By default <command>virt-top</command> shows statistics for all running
    &vmguest;s. Among the data that is displayed is the percentage of memory
    used (<literal>%MEM</literal>) and CPU (<literal>%CPU</literal>) and the
    uptime of the guest (<literal>TIME</literal>). The data is updated
    regularly (every three seconds by default). The following shows the output
    on a &vmhost; with seven &vmguest;s, four of them inactive:
   </para>
<screen>virt-top 13:40:19 - x86_64 8/8CPU 1283MHz 16067MB 7.6% 0.5%
7 domains, 3 active, 3 running, 0 sleeping, 0 paused, 4 inactive D:0 O:0 X:0
CPU: 6.1%  Mem: 3072 MB (3072 MB by guests)

   ID S RDRQ WRRQ RXBY TXBY %CPU %MEM    TIME   NAME
    7 R  123    1  18K  196  5.8  6.0   0:24.35 sled12_sp1
    6 R    1    0  18K    0  0.2  6.0   0:42.51 sles12_sp1
    5 R    0    0  18K    0  0.1  6.0  85:45.67 opensuse_leap
    -                                           (Ubuntu_1410)
    -                                           (debian_780)
    -                                           (fedora_21)
    -                                           (sles11sp3)</screen>
   <para>
    By default the output is sorted by ID. Use the following key combinations
    to change the sort field:
   </para>
   <simplelist>
    <member><keycombo><keycap function="shift"/><keycap>P</keycap></keycombo>: CPU
     usage
    </member>
    <member><keycombo><keycap function="shift"/><keycap>M</keycap></keycombo>:
     Total memory allocated by the guest
    </member>
    <member><keycombo><keycap function="shift"/><keycap>T</keycap></keycombo>: Time
    </member>
    <member><keycombo><keycap function="shift"/><keycap>I</keycap></keycombo>: ID
    </member>
   </simplelist>
   <para>
    To use any other field for sorting, press <keycombo>
    <keycap
    function="shift"/> <keycap>F</keycap> </keycombo> and select a
    field from the list. To toggle the sort order, use <keycombo>
    <keycap
    function="shift"/> <keycap>R</keycap> </keycombo>.
   </para>
   <para>
    <command>virt-top</command> also supports different views on the &vmguest;s
    data, which can be changed on-the-fly by pressing the following keys:
   </para>
   <simplelist>
    <member><keycap>0</keycap>: default view</member>
    <member><keycap>1</keycap>: show physical CPUs</member>
    <member><keycap>2</keycap>: show network interfaces</member>
    <member><keycap>3</keycap>: show virtual disks</member>
   </simplelist>
   <para>
    <command>virt-top</command> supports more hot keys to change the view of
    the data and many command line switches that affect the behavior of the
    program. For more information, see <command>man 1 virt-top</command>.
   </para>
  </sect2>

  <sect2 xml:id="cha-libvirt-admin-monitor-kvm-stat">
   <title>Monitoring with <command>kvm_stat</command></title>
   <para>
    <command>kvm_stat</command> can be used to trace &kvm; performance events.
    It monitors <filename>/sys/kernel/debug/kvm</filename>, so it needs the
    debugfs to be mounted. On &productname; it should be mounted by default. In
    case it is not mounted, use the following command:
   </para>
<screen>&prompt.sudo;mount -t debugfs none /sys/kernel/debug</screen>
   <para>
    <command>kvm_stat</command> can be used in three different modes:
   </para>
<screen>kvm_stat                    # update in 1 second intervals
kvm_stat -1                 # 1 second snapshot
kvm_stat -l &gt; kvmstats.log  # update in 1 second intervals in log format
                            # can be imported to a spreadsheet</screen>
   <example>
    <title>Typical Output of <command>kvm_stat</command></title>
<screen>kvm statistics

 efer_reload                  0       0
 exits                 11378946  218130
 fpu_reload               62144     152
 halt_exits              414866     100
 halt_wakeup             260358      50
 host_state_reload       539650     249
 hypercalls                   0       0
 insn_emulation         6227331  173067
 insn_emulation_fail          0       0
 invlpg                  227281      47
 io_exits                113148      18
 irq_exits               168474     127
 irq_injections          482804     123
 irq_window               51270      18
 largepages                   0       0
 mmio_exits                6925       0
 mmu_cache_miss           71820      19
 mmu_flooded              35420       9
 mmu_pde_zapped           64763      20
 mmu_pte_updated              0       0
 mmu_pte_write           213782      29
 mmu_recycled                 0       0
 mmu_shadow_zapped       128690      17
 mmu_unsync                  46      -1
 nmi_injections               0       0
 nmi_window                   0       0
 pf_fixed               1553821     857
 pf_guest               1018832     562
 remote_tlb_flush        174007      37
 request_irq                  0       0
 signal_exits                 0       0
 tlb_flush               394182     148</screen>
   </example>
   <para>
    See
    <link xlink:href="http://clalance.blogspot.com/2009/01/kvm-performance-tools.html"/>
    for further information on how to interpret these values.
   </para>
  </sect2>
 </sect1>
</chapter>
