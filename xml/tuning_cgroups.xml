<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
  %entities;
  <!-- custom prompt entities for the 'Controlling I/O with Proportional Weight Policy' section --> 
  <!ENTITY reader1 "reader1:/io-cgroup #">
  <!ENTITY prompt.reader1 "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[reader1] host1:/io-cgroup # </prompt>">
  <!ENTITY prompt.reader2 "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[reader2] host1:/io-cgroup # </prompt>">
  <!ENTITY prompt.io-controller "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[io-controller] host1:/io-cgroup # </prompt>">
  <!ENTITY prompt.io-cgroup "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>host1:/io-cgroup # </prompt>">
  <!ENTITY prompt.plain-root "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>host1:~ # </prompt>">
  <!ENTITY prompt.blkio "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[io-controller] host1:/sys/fs/cgroup/blkio/ # </prompt>">
  <!ENTITY prompt.unified "<prompt role='root' xmlns='http://docbook.org/ns/docbook'>[io-controller] host1:/sys/fs/cgroup/unified # </prompt>">
]>
<chapter xmlns="http://docbook.org/ns/docbook"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         version="5.0" xml:id="cha-tuning-cgroups">
 <title>Kernel control groups</title>
 <info>
  <abstract>
   <para>
    Kernel Control Groups (<quote>cgroups</quote>) are a kernel feature for
    assigning and limiting hardware and system resources for processes.
    Processes can also be organized in a hierarchical tree structure.
   </para>
  </abstract>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec-tuning-cgroups-overview">
  <title>Overview</title>

  <para>
   Every process is assigned exactly one administrative cgroup. cgroups are
   ordered in a hierarchical tree structure. You can set resource limitations,
   such as CPU, memory, disk I/O, or network bandwidth usage, for single
   processes or for whole branches of the hierarchy tree.
  </para>

  <para>
   On &productname;, &systemd; uses cgroups to organize all processes in
   groups, which &systemd; calls slices. &systemd; also provides an interface
   for setting cgroup properties.
  </para>

  <para>
   The command <command>systemd-cgls</command> displays the hierarchy tree.
  </para>

  <para>
   There are two versions of cgroup APIs provided by the kernel. These differ
   in the cgroup attributes they provide, and in the organization of controller
   hierarchies. &systemd; attempts to abstract the differences away. By default
   &systemd; runs in the <emphasis>hybrid</emphasis> mode, which means
   controllers are used through the v1 API. cgroup v2 is only used for
   systemd's own tracking. There is also the <emphasis>unified</emphasis> mode
   when the controllers are used though v2 API. You may set only one mode.
  </para>

  <para>
   To enable the unified control group hierarchy, append
   <option>systemd.unified_cgroup_hierarchy=1</option> as a kernel command line
   parameter to the &grub; boot loader. (Refer to <xref linkend="cha-grub2"/>
   for more details about configuring &grub;.)
  </para>
 </sect1>
 <sect1 xml:id="sec-tuning-cgroups-accounting">
  <title>Resource accounting</title>

  <para>
   Organizing processes into different cgroups can be used to obtain per-cgroup
   resource consumption data.
  </para>

  <para>
   The accounting has relatively small but non-zero overhead, whose impact
   depends on the workload. Activating accounting for one unit will also
   implicitly activate it for all units in the same slice, and for all its
   parent slices, and the units contained in them.
  </para>

  <para>
   The accounting can be set on a per-unit basis with directives such as
   <literal>MemoryAccounting=</literal> or globally for all units in
   <filename>/etc/systemd/system.conf</filename> with the directive
   <literal>DefaultMemoryAccounting=</literal>. Refer to <command>man
   systemd.resource-control</command> for the exhaustive list of possible
   directives.
  </para>
 </sect1>
 <sect1 xml:id="sec-tuning-cgroups-usage">
  <title>Setting resource limits</title>

  <note>
   <title>Implicit resource consumption</title>
   <para>
    Be aware that resource consumption implicitly depends on the environment
    where your workload executes (for example, size of data structures in
    libraries/kernel, forking behavior of utilities, computational efficiency).
    Hence it is recommended to (re)calibrate your limits should the environment
    change.
   </para>
  </note>

  <para>
   Limitations to cgroups can be set with the <command>systemctl
   set-property</command> command. The syntax is:
  </para>

<screen>&prompt.root;<command>systemctl set-property [--runtime] <replaceable>NAME</replaceable> <replaceable>PROPERTY1</replaceable>=<replaceable>VALUE</replaceable> [<replaceable>PROPERTY2</replaceable>=<replaceable>VALUE</replaceable>]</command></screen>

  <para>
   The configured value is applied immediately. Optionally, use the
   <option>--runtime</option> option, so that the new values do not persist
   after reboot.
  </para>

  <para>
   Replace <replaceable>NAME</replaceable> with a &systemd; service, scope, or
   slice name.
  </para>

  <para>
   For a complete list of properties and more details, see <command>man
   systemd.resource-control</command>.
  </para>
 </sect1>
 <sect1 xml:id="sec-tuning-cgroups-tasksmax">
  <title>Preventing fork bombs with <literal>TasksMax</literal></title>

  <para>
   &systemd; supports configuring task count limits both for each individual
   leaf unit, or aggregated on slices. Upstream &systemd; ships with defaults
   that limit the number of tasks in each unit (15% of the kernel global limit,
   run <command>/usr/sbin/sysctl kernel.pid_max</command> to see the total
   limit). Each user's slice is limited to 33% of the kernel limit. However,
   this is different for &slea;.
  </para>

  <sect2 xml:id="sec-tasksmax-defaults">
   <title>Finding the current default <literal>TasksMax</literal> values</title>
   <para>
    It became apparent, in practice, that there is not a single default that
    applies to all use cases. &productname; ships with two custom
    configurations that override the upstream defaults for system units and for
    user slices, and sets them both to <literal>infinity</literal>.
    <filename>/usr/lib/systemd/system.conf.d/__25-defaults-SLE.conf </filename>
    contains these lines:
   </para>
<screen>
[Manager]
DefaultTasksMax=infinity
</screen>
   <para>
    <filename>/usr/lib/systemd/system/user-.slice.d/25-defaults-SLE.conf
    </filename> contains these lines:
   </para>
<screen>[Slice]
TasksMax=infinity
</screen>
   <para>
    Use <command>systemctl</command> to verify the DefaultTasksMax value:
   </para>
<screen>&prompt.user;<command>systemctl show --property DefaultTasksMax</command>
DefaultTasksMax=infinity</screen>
   <para>
    <literal>infinity</literal> means having no limit. It is not a requirement
    to change the default, but setting some limits may help to prevent system
    crashes from runaway processes.
   </para>
  </sect2>

  <sect2 xml:id="sec-edit-taskmax-default">
   <title>Overriding the <literal>DefaultTasksMax</literal> value</title>
   <para>
    Change the global <literal>DefaultTasksMax</literal> value by creating a
    new override file,
    <filename>/etc/systemd/system.conf.d/90-system-tasksmax.conf</filename>,
    and write the following lines to set a new default limit of 256 tasks per
    system unit:
   </para>
<screen>
[Manager]
DefaultTasksMax=256
</screen>
   <para>
    Load the new setting, then verify that it changed:
   </para>
<screen>&prompt.sudo;<command>systemctl daemon-reload</command>
&prompt.user;<command>systemctl show --property DefaultTasksMax</command>
DefaultTasksMax=256
</screen>
   <para>
    Adjust this default value to suit your needs. You can set different limits
    on individual services as needed. This example is for MariaDB. First check
    the current active value:
   </para>
<screen>
&prompt.user;<command>systemctl status mariadb.service</command>
  ● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset>
   Active: active (running) since Tue 2020-05-26 14:15:03 PDT; 27min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
 Main PID: 11845 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 256)
   CGroup: /system.slice/mariadb.service
           └─11845 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
</screen>
   <para>
    The Tasks line shows that MariaDB currently has 30 tasks running, and has
    an upper limit of the default 256, which is inadequate for a database. The
    following example demonstrates how to raise MariaDB's limit to 8192.
   </para>
<screen>&prompt.sudo;<command>systemctl set-property mariadb.service TasksMax=8192</command>
&prompt.user;<command>systemctl status mariadb.service</command> 
● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disab>
  Drop-In: /etc/systemd/system/mariadb.service.d
           └─50-TasksMax.conf
   Active: active (running) since Tue 2020-06-02 17:57:48 PDT; 7min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
  Process: 3446 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper upgrade (code=exited, sta>
  Process: 3440 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper install (code=exited, sta>
 Main PID: 3452 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 8192)
   CGroup: /system.slice/mariadb.service
           └─3452 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
</screen>
   <para>
    <command>systemctl set-property</command> applies the new limit and creates
    a drop-in file for persistence,
    <filename>/etc/systemd/system/mariadb.service.d/50-TasksMax.conf</filename>,
    that contains only the changes you want to apply to the existing unit file.
    The value does not have to be 8192, but should be whatever limit is
    appropriate for your workloads.
   </para>
  </sect2>

  <sect2>
   <title>Default <literal>TasksMax</literal> limit on users</title>
   <para>
    The default limit on users should be fairly high, because user sessions
    need more resources. Set your own default for any user by creating a new
    file, for example
    <filename>/etc/systemd/system/user-.slice.d/40-user-taskmask.conf</filename>.
    The following example sets a default of 16284:
   </para>
<screen>
[Slice]
TasksMax=16284
</screen>
   <note>
    <title>Numeric prefixes reference</title>
    <para>
     See
     <link xlink:href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-systemd.html#sec-boot-systemd-custom-drop-in"/>
     to learn what numeric prefixes are expected for drop-in files.
    </para>
   </note>
   <para>
    Then reload systemd to load the new value, and verify the change:
   </para>
<screen>&prompt.sudo;<command>systemctl daemon-reload</command>
&prompt.user;<command>systemctl show --property TasksMax user-1000.slice</command>
TasksMax=16284
</screen>
   <para>
    How do you know what values to use? This varies according to your
    workloads, system resources, and other resource configurations. When your
    <literal>TasksMax</literal> value is too low, you will see error messages
    such as <emphasis>Failed to fork (Resources temporarily
    unavailable)</emphasis>, <emphasis>Can't create thread to handle new
    connection</emphasis>, and <emphasis>Error: Function call 'fork' failed
    with error code 11, 'Resource temporarily unavailable'</emphasis>.
   </para>
   <para>
    For more information on configuring system resources in systemd, see
    <literal>systemd.resource-control (5)</literal>.
   </para>
  </sect2>
 </sect1>
 <sect1>
  <title>Controlling I/O with proportional weight policy</title>

  <para>
   This section introduces using the Linux kernel's block I/O controller to
   prioritize I/O operations. The cgroup blkio subsystem controls and monitors
   access to I/O on block devices. State objects that contain the subsystem
   parameters for a cgroup are represented as pseudo-files within the cgroup
   virtual file system, also called a pseudo-file system.
  </para>

  <para>
   The examples in this section show how writing values to some of these
   pseudo-files limits access or bandwidth, and reading values from some of
   these pseudo-files provides information on I/O operations. Examples are
   provided for both cgroup-v1 and cgroup-v2.
  </para>

  <para>
   You need a test directory containing two files for testing performance and
   changed settings. A quick way to create test files fully populated with text
   is using the <command>yes</command> command. The following example commands
   create a test directory, and then populate it with two 537 MB text files:
  </para>

<screen>&prompt.plain-root;mkdir /io-cgroup
&prompt.plain-root;cd /io-cgroup
&prompt.plain-root;yes this is a test file | head -c 537MB > file1.txt
&prompt.plain-root;yes this is a test file | head -c 537MB > file2.txt
</screen>

  <para>
   To run the examples open three command shells. Two shells are for reader
   processes, and one shell is for running the steps that control I/O. In the
   examples, each command prompt is labeled to indicate if it represents one of
   the reader processes, or I/O.
  </para>

  <sect2>
   <title>Using cgroup-v1</title>
   <para>
    The following proportional weight policy files can be used to grant a
    reader process a higher priority for I/O operations than other reader
    processes accessing the same disk.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <filename>blkio.bfq.weight</filename> (available in kernels starting with
      version 5.0 with blk-mq and when using the BFQ I/O scheduler)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    To test this, run a single reader process (in the examples, reading from an
    SSD) without controlling its I/O, using <filename>file2.txt</filename>:
   </para>
<screen> 
&prompt.io-controller;sync; echo 3 > /proc/sys/vm/drop_caches
&prompt.io-controller;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 1.33049 s, 404 MB/s
</screen>
   <para>
    Now run a background process reading from the same disk:
   </para>
<screen>
&prompt.reader1;sync; echo 3 > /proc/sys/vm/drop_caches
&prompt.reader1;echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
&prompt.reader2;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 2.61592 s, 205 MB/s
</screen>
   <para>
    Each process gets half of the throughput for I/O operations. Next, set up
    two control groups&mdash;one for each process&mdash;verify that BFQ is
    used, and set a different weight for reader2:
   </para>
<screen>
&prompt.io-controller;cd /sys/fs/cgroup/blkio/
&prompt.blkio;mkdir reader1
&prompt.blkio;mkdir reader2
&prompt.blkio;echo 5220 > reader1/cgroup.procs
&prompt.blkio;echo 5251 > reader2/cgroup.procs
&prompt.blkio;cat /sys/block/sda/queue/scheduler
mq-deadline kyber [bfq] none
&prompt.blkio;cat reader1/blkio.bfq.weight
100
&prompt.blkio;echo 200 > reader2/blkio.bfq.weight
&prompt.blkio;cat reader2/blkio.bfq.weight
200
</screen>
   <para>
    With these settings and reader1 in the background, reader2 should have
    higher throughput than previously:
   </para>
<screen>
&prompt.reader1;sync; echo 3 > /proc/sys/vm/drop_caches
&prompt.reader1;echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
&prompt.reader2;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 2.06604 s, 260 MB/s
</screen>
   <para>
    The higher proportional weight resulted in higher throughput for reader2.
    Now double its weight again:
   </para>
<screen>
&prompt.blkio;cat reader1/blkio.bfq.weight
100
&prompt.blkio;echo 400 > reader2/blkio.bfq.weight
&prompt.blkio;cat reader2/blkio.bfq.weight
400
</screen>
   <para>
    This results in another increase in throughput for reader2:
   </para>
<screen>
&prompt.reader1;sync; echo 3 > /proc/sys/vm/drop_caches
&prompt.reader1;echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
&prompt.reader2;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 1.69026 s, 318 MB/s
</screen>
  </sect2>

  <sect2>
   <title>Using cgroup-v2</title>
   <para>
    First set up your test environment as shown at the beginning of this
    chapter.
   </para>
   <para>
    Then make sure that the Block IO controller is not active, as that is for
    cgroup-v1. To do this, boot with kernel parameter
    <option>cgroup_no_v1=blkio</option>. Verify that this parameter was used,
    and that the IO controller (cgroup-v2) is available:
   </para>
<screen>
&prompt.io-controller;cat /proc/cmdline
BOOT_IMAGE=... cgroup_no_v1=blkio ...
&prompt.io-controller;cat /sys/fs/cgroup/unified/cgroup.controllers
io
</screen>
   <para>
    Next, enable the IO controller:
   </para>
<screen>
&prompt.io-controller;cd /sys/fs/cgroup/unified/
&prompt.unified;echo '+io' > cgroup.subtree_control
&prompt.unified;cat cgroup.subtree_control
io
</screen>
   <para>
    Now run all the test steps, similarly to the steps for cgroup-v1. Note that
    some of the directories are different. Run a single reader process (in the
    examples, reading from an SSD) without controlling its I/O, using
    file2.txt:
   </para>
<screen>
&prompt.unified;cd -
&prompt.io-controller;sync; echo 3 > /proc/sys/vm/drop_caches
&prompt.io-controller;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5633
[...]
</screen>
   <para>
    Run a background process reading from the same disk and note your
    throughput values:
   </para>
<screen>
&prompt.reader1;sync; echo 3 > /proc/sys/vm/drop_caches
&prompt.reader1;echo $$; dd if=file1.txt of=/dev/null bs=4k
5633
[...]
&prompt.reader2;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5703
[...]
</screen>
   <para>
    Each process gets half of the throughput for I/O operations. Set up two
    control groups&mdash;one for each process&mdash;verify that BFQ is the
    active scheduler, and set a different weight for reader2:
   </para>
<screen>
&prompt.io-controller;cd -
&prompt.unified;mkdir reader1
&prompt.unified;mkdir reader2
&prompt.unified;echo 5633 > reader1/cgroup.procs
&prompt.unified;echo 5703 > reader2/cgroup.procs
&prompt.unified;cat /sys/block/sda/queue/scheduler
mq-deadline kyber [bfq] none
&prompt.unified;cat reader1/io.bfq.weight
default 100
&prompt.unified;echo 200 > reader2/io.bfq.weight
&prompt.unified;cat reader2/io.bfq.weight
default 200
</screen>
   <para>
    Test your throughput with the new settings. reader2 should show an increase
    in throughput.
   </para>
<screen>
&prompt.reader1;sync; echo 3 > /proc/sys/vm/drop_caches
&prompt.reader1;echo $$; dd if=file1 of=/dev/null bs=4k
5633
[...]
&prompt.reader2;echo $$; dd if=file2 of=/dev/null bs=4k count=131072
5703
[...]
</screen>
   <para>
    Try doubling the weight again for reader2, and testing the new setting:
   </para>
<screen>
&prompt.reader2;echo 400 > reader1/blkio.bfq.weight
&prompt.reader2;cat reader2/blkio.bfq.weight
400
&prompt.reader1;sync; echo 3 > /proc/sys/vm/drop_caches
&prompt.reader1;echo $$; dd if=file1.txt of=/dev/null bs=4k
[...]
&prompt.reader2;echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
[...]
</screen>
  </sect2>
 </sect1>
 <sect1>
  <title>More information</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Kernel documentation (package <systemitem>kernel-source</systemitem>):
     files in
     <filename>/usr/src/linux/Documentation/admin-guide/cgroup-v1</filename>
     and file
     <filename>/usr/src/linux/Documentation/admin-guide/cgroup-v2.rst</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/604609/"/>&mdash;Brown, Neil:
     Control Groups Series (2014, 7 parts).
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/243795/"/>&mdash;Corbet,
     Jonathan: Controlling memory use in containers (2007).
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/236038/"/>&mdash;Corbet,
     Jonathan: Process containers (2007).
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>