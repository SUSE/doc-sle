<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>

<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-vt-installation">
 <title>Installation of Virtualization Components</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker>
   </dm:bugtracker>
  </dm:docmanager>
 </info>
 <para>
  Depending on the scope of the installation, none of the virtualization tools
  may be installed on your system. They will be automatically installed when
  configuring the hypervisor with the &yast; module <menuchoice>
  <guimenu>Virtualization</guimenu> <guimenu>Install Hypervisor and
  Tools</guimenu></menuchoice>. In case this module is not available in &yast;,
  install the package <package>yast2-vm</package>.
 </para>

 <sect1 xml:id="sec-vt-installation-kvm">
  <title>Installing &kvm;</title>

  <para>
   To install &kvm; and KVM tools, proceed as follows:
  </para>

  <procedure>
   <step>
    <para>
     Verify that the <package>yast2-vm</package> package is installed. This package
     is &yast;'s configuration tool that simplifies the installation of virtualization
     hypervisors.
    </para>
   </step>
   <step>
    <para>
     Start &yast; and choose <menuchoice>
     <guimenu>Virtualization</guimenu> <guimenu>Install Hypervisor and
     Tools</guimenu></menuchoice>.
    </para>
   </step>
   <step>
    <para>
     Select <guimenu>&kvm; server</guimenu> for a minimal installation of
     &qemu; tools. Select <guimenu>&kvm; tools</guimenu> if a
     &libvirt;-based management stack is also desired. Confirm with
     <guimenu>Accept</guimenu>.
    </para>
   </step>
   <step>
    <para>
     To enable normal networking for the &vmguest;, using a
     network bridge is recommended. &yast; offers to automatically
     configure a bridge on the &vmhost;. Agree to do so by choosing
     <guimenu>Yes</guimenu>, otherwise choose <guimenu>No</guimenu>.
    </para>
   </step>
   <step>
    <para>
     After the setup has been finished, you can start setting up
     &vmguest;s. Rebooting the &vmhost; is not required.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-vt-installation-xen">
  <title>Installing &xen;</title>

  <para>
   To install &xen; and &xen; tools, proceed as follows:
  </para>

  <procedure>
   <step>
    <para>
     Start &yast; and choose <menuchoice>
     <guimenu>Virtualization</guimenu> <guimenu>Install Hypervisor and
     Tools</guimenu></menuchoice>.
    </para>
   </step>
   <step>
    <para>
     Select <guimenu>&xen; server</guimenu> for a minimal installation of
     &xen; tools. Select <guimenu>&xen; tools</guimenu> if a
     &libvirt;-based management stack is also desired. Confirm with
     <guimenu>Accept</guimenu>.
    </para>
   </step>
   <step>
    <para>
     To enable normal networking for the &vmguest;, using a
     network bridge is recommended. &yast; offers to automatically
     configure a bridge on the &vmhost;. Agree to do so by choosing
     <guimenu>Yes</guimenu>, otherwise choose <guimenu>No</guimenu>.
    </para>
   </step>
   <step>
    <para>
     After the setup has been finished, you need to reboot the machine with
     the <emphasis>&xen; kernel</emphasis>.
    </para>
    <tip>
     <title>Default Boot Kernel</title>
     <para>
      If everything works as expected, change the default boot kernel with &yast; and make the &xen;-enabled kernel the default.
      For more information about changing the default kernel, see <xref linkend="sec-grub2-yast2-config"/>.
     </para>
    </tip>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-vt-installation-containers">
  <title>Installing Containers</title>

  <para>
   To install containers, proceed as follows:
  </para>

  <procedure>
   <step>
    <para>
     Start &yast; and choose <menuchoice>
     <guimenu>Virtualization</guimenu> <guimenu>Install Hypervisor and
     Tools</guimenu></menuchoice>.
    </para>
   </step>
   <step>
    <para>
     Select <guimenu>libvirt lxc daemon</guimenu> and confirm with
     <guimenu>Accept</guimenu>.
    </para>
   </step>
<!-- ========================================================= -->
<!-- TODO Either fix Yast to reload apparmor or describe it here -->
<!-- ========================================================= -->
  </procedure>
 </sect1>
 <sect1 xml:id="sec-vt-installation-patterns">
  <title>Patterns</title>

  <para>
   It is possible using Zypper and patterns to install virtualization
   packages. Run the command <command>zypper in -t pattern</command>
   <replaceable>PATTERN</replaceable>. Available patterns are:
  </para>

  <variablelist>
   <varlistentry>
    <term>KVM</term>
    <listitem>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        <systemitem class="resource">kvm_server</systemitem>: sets up the
        &kvm; &vmhost; with &qemu; tools for management
       </para>
      </listitem>
      <listitem>
       <para>
        <systemitem class="resource">kvm_tools</systemitem>: installs the
        &libvirt; tools for managing and monitoring &vmguest;s
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Xen</term>
    <listitem>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        <systemitem class="resource">xen_server</systemitem>: sets up the
        &xen; &vmhost; with &xen; tools for management
       </para>
      </listitem>
      <listitem>
       <para>
        <systemitem class="resource">xen_tools</systemitem>: installs the
        &libvirt; tools for managing and monitoring &vmguest;s
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Containers</term>
    <listitem>
     <para>
      There is no pattern for containers; install the
      <emphasis>libvirt-daemon-lxc</emphasis> package.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec-vt-installation-ovmf">
  <title>Installing UEFI Support</title>
  <note>
   <para>
    &kvm; guests support secure boot by using the OVMF firmware.
    &xen; HVM guests support booting from the OVMF firmware as well, but they do not support secure boot.
   </para>
  </note>
  <para>
   UEFI support is provided by <emphasis>OVMF</emphasis>
   (<emphasis>Open Virtual Machine Firmware</emphasis>). To enable UEFI
   boot, first install the <package>qemu-ovmf-x86_64</package> or
   <package>qemu-uefi-aarch64</package> package depending on the architecture of the guest.
  </para>
  <para>
   The firmware used by virtual machines is auto-selected. The auto-selection
   is based on the *.json files in the
   <package>qemu-ovmf-<replaceable>ARCH</replaceable></package> package. The
   &libvirt; &qemu; driver parses those files when loading so it knows the
   capabilities of the various types of firmware. Then when the user selects the type
   of firmware and any desired features (for example, support for secure boot),
   &libvirt; will be able to find a firmware that satisfies the user's
   requirements.
  </para>
  <para>
   For example, to specify EFI with secure boot, use the following
   configuration:
  </para>
<screen>
&lt;os firmware='efi'>
 &lt;loader secure='yes'/>
&lt;/os>
</screen>
  <para>
   The <package>qemu-ovmf-<replaceable>ARCH</replaceable></package> packages
   contain the following files:
  </para>
<screen>
&prompt.root;<command>rpm -ql qemu-ovmf-x86_64</command>
[...]
/usr/share/qemu/ovmf-x86_64-ms-code.bin
/usr/share/qemu/ovmf-x86_64-ms-vars.bin
/usr/ddshare/qemu/ovmf-x86_64-ms.bin
/usr/share/qemu/ovmf-x86_64-suse-code.bin
/usr/share/qemu/ovmf-x86_64-suse-vars.bin
/usr/share/qemu/ovmf-x86_64-suse.bin
[...]
</screen>
  <para>
   The <filename>*-code.bin</filename> files are the UEFI firmware files.
   The <filename>*-vars.bin</filename> files are corresponding variable
   store images that can be used as a template for a per-VM non-volatile
   store. &libvirt; copies the specified <literal>vars</literal>
   template to a per-VM path under
   <filename>/var/lib/libvirt/qemu/nvram/</filename> when first
   creating the VM. Files without <literal>code</literal> or
   <literal>vars</literal> in the name can be used as a single UEFI
   image. They are not as useful since no UEFI variables persist
   across power cycles of the VM.
  </para>
  <para>
   The <filename>*-ms*.bin</filename> files contain Microsoft keys as
   found on real hardware. Therefore, they are configured as the default in
   &libvirt;. Likewise, the <filename>*-suse*.bin</filename> files
   contain preinstalled &suse; keys. There is also a set
   of files with no preinstalled keys.
  </para>
  <para>
   For details, see <xref linkend="vle-libvirt-inst-virt-install-ovmf"
   /> and <link xlink:href=
   "http://www.linux-kvm.org/downloads/lersek/ovmf-whitepaper-c770f8c.txt"
   />.
  </para>
 </sect1>
 <sect1 xml:id="sec-vt-installation-nested-vms">
  <title>Enable Support for Nested Virtualization in &kvm;</title>

  <important>
   <title>Technology Preview</title>
   <para>
    &kvm;'s nested virtualization is still a technology preview. It is provided
    for testing purposes and is not supported.
   </para>
  </important>

  <para>
   Nested guests are &kvm; guests run in a &kvm; guest. When describing nested
   guests, we will use the following virtualization layers:
  </para>

  <variablelist>
   <varlistentry>
    <term>L0</term>
    <listitem>
     <para>
      A bare metal host running &kvm;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>L1</term>
    <listitem>
     <para>
      A virtual machine running on L0. Because it can run another &kvm;, it is
      called a <emphasis>guest hypervisor</emphasis>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>L2</term>
    <listitem>
     <para>
      A virtual machine running on L1. It is called a <emphasis>nested
       guest</emphasis>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Nested virtualization has many advantages. You can benefit from it in the
   following scenarios:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Manage your own virtual machines directly with your hypervisor of choice in
     cloud environments.
    </para>
   </listitem>
   <listitem>
    <para>
     Enable the live migration of hypervisors and their guest virtual machines
     as a single entity.
    </para>
   </listitem>
   <listitem>
    <para>
     Use it for software development and testing.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   To enable nesting temporarily, remove the module and reload it with the
   <option>nested</option> &kvm; module parameter:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     For Intel CPUs, run:
    </para>
<screen>
&prompt.sudo;modprobe -r kvm_intel &amp;&amp; modprobe kvm_intel nested=1
</screen>
   </listitem>
   <listitem>
    <para>
     For AMD CPUs, run:
    </para>
<screen>
&prompt.sudo;modprobe -r kvm_amd &amp;&amp; modprobe kvm_amd nested=1
</screen>
   </listitem>
  </itemizedlist>

  <para>
   To enable nesting permanently, enable the <option>nested</option> &kvm;
   module parameter in the <filename>/etc/modprobe.d/kvm_*.conf</filename> file,
   depending on your CPU:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     For Intel CPUs, edit <filename>/etc/modprobe.d/kvm_intel.conf</filename>
     and add the following line:
    </para>
    <screen>options kvm_intel nested=1</screen>
   </listitem>
   <listitem>
    <para>
     For AMD CPUs, edit <filename>/etc/modprobe.d/kvm_amd.conf</filename> and
     add the following line:
    </para>
    <screen>options kvm_amd nested=1</screen>
   </listitem>
  </itemizedlist>

  <para>
   When your L0 host is capable of nesting, you will be able to start an L1
   guest in one of the following ways:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Use the <option>-cpu host</option> &qemu; command line option.
    </para>
   </listitem>
   <listitem>
    <para>
     Add the <literal>vmx</literal> (for Intel CPUs) or the
     <literal>svm</literal> (for AMD CPUs) CPU feature to the
     <option>-cpu</option> &qemu; command line option, which enables
     virtualization for the virtual CPU.
    </para>
   </listitem>
  </itemizedlist>
  <sect2 xml:id="sec-vt-installation-nested-vms-esxi">
   <title>&esx; as a guest hypervisor</title>
   <para>
    If you use &esx; as a guest hypervisor on top of a &kvm; bare metal
    hypervisor, you may experience unstable network communication. This problem
    occurs especially between nested &kvm; guests and the &kvm; bare metal
    hypervisor or external network. The following default CPU configuration of
    the nested &kvm; guest is causing the problem:
   </para>
   <screen>&lt;cpu mode='host-model' check='partial'/></screen>
   <para>
    To fix it, modify the CPU configuration as follow:
   </para>
<screen>
[...]
&lt;cpu mode='host-passthrough' check='none'>
 &lt;cache mode='passthrough'/>
&lt;/cpu>
[...]
</screen>
  </sect2>
 </sect1>
</chapter>
