<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
  [
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.xen.vhost">
 <title>Setting Up a Virtual Machine Host</title>
 <para>
  This section documents how to set up and use &productname; &productnumber;
  as a virtual machine host.
 </para>
 <para>
  In most cases, the hardware requirements for the &dom0; are the same as
  those for the &productname; operating system, but additional CPU, disk,
  memory, and network resources should be added to accommodate the resource
  demands of all planned &vmguest; systems.
 </para>
 <tip>
  <para>
   Remember that &vmguest; systems, like physical machines, perform better
   when they run on faster processors and have access to more system memory.
  </para>
 </tip>
 <!-- hardware should be in RN now
 <para>
  The following table lists the minimum hardware requirements for running a
  typical virtualized environment. Additional requirements have to be added
  for the number and type of the respective guest systems.
 </para>
 <table id="tab.xen.vhost" frame="topbot" rowsep="1" pgwide="0">
  <title>Hardware Requirements</title>
  <tgroup cols="3">
   <colspec colnum="1" colname="1" colwidth="24*"/>
   <colspec colnum="2" colname="2" colwidth="76*"/>
   <thead>
    <row>
     <entry>
      <para>
       System Component
      </para>
     </entry>
     <entry>
      <para>
       Minimum Requirements
      </para>
     </entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry>
      <para>
       Computer
      </para>
     </entry>
     <entry>
      <para>
       Computer with Pentium II or AMD K7 450 MHz processor
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Memory
      </para>
     </entry>
     <entry>
      <para>
       512 MB of RAM for the host
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Free Disk Space
      </para>
     </entry>
     <entry>
      <para>
       7 GB of available disk space for the host.
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Optical Drive
      </para>
     </entry>
     <entry colname="2">
      <para>
       DVD-ROM Drive
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Hard Drive
      </para>
     </entry>
     <entry>
      <para>
       20 GB
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Network Device
      </para>
     </entry>
     <entry>
      <para>
       Ethernet 100 Mbps
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       IP Address
      </para>
     </entry>
     <entry>
      <itemizedlist>
       <listitem>
        <para>
         One IP address on a subnet for the host.
        </para>
       </listitem>
       <listitem>
        <para>
         One IP address on a subnet for each &vmguest;.
        </para>
       </listitem>
      </itemizedlist>
      <para/>
     </entry>
    </row>
   </tbody>
  </tgroup>
 </table>
-->
 <para>
  &xen; virtualization technology is available in &productname; products
  based on code path 10 and later. Code path 10 products include Open
  Enterprise Server 2 Linux, &productname; 10, &sled; 10, and &opensuse;
  10.x.
 </para>
 <para>
  The virtual machine host requires a number of software packages and their
  dependencies to be installed. To install all necessary packages, run
  &yast; <guimenu>Software Management</guimenu>, select
  <menuchoice><guimenu>View</guimenu>
  <guimenu>Patterns</guimenu></menuchoice> and choose <guimenu>&xen; Virtual
  Machine Host Server</guimenu> for installation. The installation can also
  be performed with &yast; using the module
  <menuchoice><guimenu>Virtualization</guimenu><guimenu>Install Hypervisor
  and Tools</guimenu></menuchoice>.
 </para>
 <para>
  After the &xen; software is installed, restart the computer.
 </para>
 <para>
  Updates are available through your update channel. To be sure to have the
  latest updates installed, run &yast; <guimenu>Online Update</guimenu>
  after the installation has finished.
 </para>
 <sect1 id="sec.xen.vhost.best">
  <title>Best Practices and Suggestions</title>

  <para>
   When installing and configuring the &sle; operating system on the host,
   be aware of the following best practices and suggestions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     If the host should always run as &xen; host, run &yast; <menuchoice>
     <guimenu>System</guimenu> <guimenu>Boot Loader</guimenu> </menuchoice>
     and activate the &xen; boot entry as default boot section.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       In &yast;, click <guimenu>System &gt; Boot Loader</guimenu>.
      </para>
     </listitem>
     <listitem>
      <para>
       Change the default boot to the <guimenu>&xen;</guimenu> label, then
       click <guimenu>Set as Default</guimenu>.
      </para>
     </listitem>
     <listitem>
      <para>
       Click <guimenu>Finish</guimenu>.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     For best performance, only the applications and processes required for
     virtualization should be installed on the virtual machine host.
    </para>
   </listitem>
   <listitem>
<!-- TODO: this is outdated!
      - document Xen with openAIS, outline with old heartbeat is
         http://www.novell.com/coolsolutions/feature/19571.html
    -->
    <para>
     When using both, iSCSI and OCFS2 to host &xen; images, the latency
     required for OCFS2 default timeouts in SP2 may not be met. To
     reconfigure this timeout, run <command>/etc/init.d/o2cb
     configure</command> or edit <literal>O2CB_HEARTBEAT_THRESHOLD</literal>
     in the system configuration.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.xen.vhost.memory">
  <title>Managing &dom0; Memory</title>

  <para>
   When the host is set up, a percentage of system memory is reserved for
   the hypervisor, and all remaining memory is automatically allocated to
   &dom0;.
  </para>

  <para>
   A better solution is to set a default amount of memory for &dom0;, so the
   memory can be allocated appropriately to the hypervisor. An adequate
   amount would be 20 percent of the total system memory up to 2 GB. An
   recommended minimum amount would be 512 MB.
  </para>

  <sect2 id="sec.xen.vhost.maxmem">
   <title>Setting a Maximum Amount of Memory</title>
   <procedure>
    <step>
     <para>
      Determine the amount of memory to set for &dom0;.
     </para>
    </step>
    <step>
     <para>
      At &dom0;, type <command>xl info</command> to view the amount of
      memory that is available on the machine. The memory that is currently
      allocated by &dom0; can be determined with the command <command>xl
      list</command>.
     </para>
    </step>
    <step>
     <para>
      Run <menuchoice> <guimenu>&yast;</guimenu> <guimenu>Boot
      Loader</guimenu> </menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Select the &xen; section.
     </para>
    </step>
    <step>
     <para>
      In <guimenu>Additional &xen; Hypervisor Parameters</guimenu>, add
      <command>dom0_mem=<replaceable>mem_amount</replaceable></command>
      where <replaceable>mem_amount</replaceable> is the maximum amount of
      memory to allocate to &dom0;. Add <command>K</command>,
      <command>M</command>, or <command>G</command>, to specify the size,
      for example, <command>dom0_mem=768M</command>.
     </para>
    </step>
    <step>
     <para>
      Restart the computer to apply the changes.
     </para>
    </step>
   </procedure>
   <warning>
    <title>&xen; &dom0; memory</title>
    <para>
     When using XL toolstack and <command>dom0_mem=</command> option for &xen; hypervisor in grub2
     you need to disable xl <emphasis>autoballoon</emphasis> in
     <filename>etc/xen/xl.conf</filename>, otherwise launching VMs
     will fail with errors about not being able to balloon down &dom0;. So add
     <emphasis>autoballoon=0</emphasis> to <filename>xl.conf</filename> 
     if you have <command>dom0_mem=</command> option specified for &xen;. Also
     see <ulink url="http://wiki.xen.org/wiki/Xen_Best_Practices#Xen_dom0_dedicated_memory_and_preventing_dom0_memory_ballooning">Xen dom0 memory</ulink> 
    </para>
   </warning>
  </sect2>

 </sect1>
 <sect1 id="sec.xen.vhost.netcard">
  <title>Network Card in Fully Virtualized Guests</title>

  <para>
   In a fully virtualized guest, the default network card is an emulated
   Realtek network card. However, it also possible to use the split network
   driver to run the communication between &dom0; and a &vmguest;. By
   default, both interfaces are presented to the &vmguest;, because the
   drivers of some operating systems require both to be present.
  </para>

  <para>
   When using &sle;, only the paravirtualized network cards are available
   for the &vmguest; by default. The following network options are
   available:
  </para>

  <variablelist>
   <varlistentry>
    <term>emulated</term>
    <listitem>
     <para>
      To use an emulated network interface like an emulated Realtek card,
      specify <literal>type=ioemu</literal> in the <literal>vif</literal> device section of the
      domain xl configuration. An example configuration would look like:
     </para>
<screen>vif = [ 'type=ioemu,mac=00:16:3e:5f:48:e4,bridge=br0' ]</screen>
     <para>
      Find more details about the xl configuration in the
      <filename>xl.conf</filename> manual page <command>man 5
      xl.conf</command>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>paravirtualized</term>
    <listitem>
     <para>
      When you specify <literal>type=vif</literal> and not specify a model
      or type, the paravirtualized network interface is used:
     </para>
<screen>vif = [ 'type=vif,mac=00:16:3e:5f:48:e4,bridge=br0,backen=0' ]</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>emulated and paravirtualized</term>
    <listitem>
     <para>
      If the administrator should be offered both options, simply specify
      both, type and model. The xl configuration would look like:
     </para>
<screen>vif = [ 'type=ioemu,mac=00:16:3e:5f:48:e4,model=rtl8139,bridge=br0' ]</screen>
     <para>
      In this case, one of the network interfaces should be disabled on the
      &vmguest;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.xen.vhost.start">
  <title>Starting the Virtual Machine Host</title>

  <para>
   If virtualization software is correctly installed, the computer boots to
   display the &grub; boot loader with a <citetitle>Xen</citetitle> option
   on the menu. Select this option to start the virtual machine host.
  </para>

  <note>
   <title>Xen and Kdump</title>
   <para>
    In &xen;, the hypervisor manages the memory resource. If you need to
    reserve system memory for a recovery kernel in &dom0;, this memory has
    to be reserved by the hypervisor. Thus, it is necessary to add the
    parameter <systemitem>crashkernel=size@offset</systemitem> to the
    <literal>kernel</literal> line instead of using the line with the other
    boot options.
   </para>
  </note>

  <para>
   If the <citetitle>&xen;</citetitle> option is not on the &grub; menu,
   review the steps for installation and verify that the &grub; boot loader
   has been updated. If the installation has been done without selecting the
   &xen; pattern, run the &yast; <guimenu>Software Management</guimenu>,
   select the filter <guimenu>Patterns</guimenu> and choose <guimenu>&xen;
   Virtual Machine Host Server</guimenu> for installation.
  </para>

  <para>
   After booting the hypervisor, the &dom0; virtual machine starts and
   displays its graphical desktop environment. If you did not install a
   graphical desktop, the command line environment appears.
  </para>

  <tip>
   <title>Graphics Problems</title>
   <para>
    Sometimes it may happen that the graphics system does not work properly.
    In this case, add <literal>vga=ask</literal> to the boot parameters. To
    activate permanent settings, use <literal>vga=mode-0x???</literal> where
    <literal>???</literal> is calculated as <literal>0x100</literal> + VESA
    mode from
    <ulink
     url="http://en.wikipedia.org/wiki/VESA_BIOS_Extensions"/>,
    e.g. <literal>vga=mode-0x361</literal>.
   </para>
  </tip>

  <para>
   Before starting to install virtual guests, make sure that the system time
   is correct. To do this, configure NTP (Network Time Protocol) on the
   controlling domain:
  </para>

  <procedure>
   <step>
    <para>
     In &yast; select <menuchoice> <guimenu>Network Services</guimenu>
     <guimenu>NTP Configuration</guimenu> </menuchoice>.
    </para>
   </step>
   <step>
    <para>
     Select the option to automatically start the NTP daemon during boot.
     Provide the IP address of an existing NTP time server, then click
     <guimenu>Finish</guimenu>.
    </para>
   </step>
  </procedure>

  <note>
   <title>Time Services on Virtual Guests</title>
   <para>
    Hardware clocks commonly are not very precise. All modern operating
    systems try to correct the system time compared to the hardware time by
    means of an additional time source. To get the correct time on all
    &vmguest; systems, also activate the network time services on each
    respective guest or make sure that the guest uses the system time of the
    host. For more about <literal>Independent Wallclocks</literal> in
    &productname; see <xref linkend="sec.xen.guests.suse.time"/>.
   </para>
  </note>

  <para>
   For more information about managing virtual machines, see
   <xref linkend="cha.xen.manage"/>.
  </para>
 </sect1>
 <sect1 id="sec.xen.vhost.pciback">
  <title>&pciback;</title>

  <para>
   To take full advantage of &vmguest; systems, it is sometimes necessary to
   assign specific PCI devices to a dedicated domain. When using fully
   virtualized guests, this functionality is only available if the chipset
   of the system supports this feature, and if it is activated from the
   BIOS.
  </para>

  <para>
   This feature is available from both, AMD* and Intel*. For AMD machines,
   the feature is called IOMMU, in Intel speak, this is VT-d. Note that
   Intel-VT technology is not sufficient to use this feature for fully
   virtualized guests. To make sure that your computer supports this
   feature, ask your supplier specifically to deliver a system that supports
   &pciback;.
  </para>

  <itemizedlist>
   <title>Limitations</title>
   <listitem>
    <para>
     Some graphics drivers use highly optimized ways to access DMA. This is
     not supported, and thus using graphics cards may be difficult.
    </para>
   </listitem>
   <listitem>
    <para>
     When accessing PCI devices behind a PCIe bridge, all of the PCI devices
     must be assigned to a single guest. This limitations does not apply to
     PCIe devices.
    </para>
   </listitem>
   <listitem>
    <para>
     Guests with dedicated PCI devices cannot be live migrated to a
     different host.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The configuration of &pciback; is twofold. First, the hypervisor must be
   informed at boot time that a PCI device should be available for
   reassigning. Second, the PCI device must be assigned to the &vmguest;.
  </para>

  <sect2 id="config.hypervisor.pciback">
   <title>Configuring the Hypervisor for &pciback;</title>
   <procedure>
    <step>
     <para>
      Select a device to reassign to a &vmguest;. To do this run
      <command>lspci</command> and read the device number. For example, if
      <command>lspci</command> contains the following line:
     </para>
<screen>06:01.0 Ethernet controller: Digital Equipment Corporation DECchip 21142/43 (rev 41)</screen>
     <para>
      In this case, the PCI number is <literal>(06:01.0)</literal>.
     </para>
    </step>
    <step>
     <para>
      Run <menuchoice><guimenu>&yast;</guimenu><guimenu>System</guimenu>
      <guimenu>Boot Loader</guimenu></menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Select the <literal>&xen;</literal> section and press
      <guimenu>Edit</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Add the PCI number to the <guimenu>Optional Kernel Command Line
      Parameter</guimenu> line:
     </para>
<screen>pciback.hide=(06:01.0)</screen>
    </step>
    <step>
     <para>
      Press <guimenu>OK</guimenu> and finish &yast;.
     </para>
    </step>
    <step>
     <para>
      Reboot the system.
     </para>
    </step>
    <step>
     <para>
      Check if the device is in the list of assignable devices with the
      command
     </para>
<screen>xl pci-assignable-list</screen>
    </step>
   </procedure>
   <sect3>
    <title>Dynamic assignment with xl</title>
    <para>
     If you want to avoid restarting the host system, you can use the 
     dynamic assignment with xl to use &pciback;.
    </para>
    <para>
      Begin by making sure that dom0 has the pciback module loaded:
    </para>
    <screen>modprobe pciback</screen>
    <para>
     Then make a device assignable by using <command>xl
     pci-assignable-add</command>. For example, if you wanted to
     make the device <emphasis>06:01.0</emphasis> available for guests, you
     should type the following:
    </para>
    <screen>xl pci-assignable-add 06:01.0</screen>
   </sect3>
  </sect2>
  <sect2>
   <title>Assigning PCI Devices to &vmguest; Systems</title>
   <para>
    There are several possibilities to dedicate a PCI device to a &vmguest;:
   </para>
   <variablelist>
    <varlistentry>
     <term>Adding the device while installing:</term>
     <listitem>
      <para>
       During installation, add the <literal>pci</literal> line to the
       configuration file:
      </para>
<screen>pci=['06:01.0']</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Hot adding PCI devices to &vmguest; systems</term>
     <listitem>
      <para>
       The command <literal>xl</literal> can be used to add or remove PCI
       devices on the fly. To Add the device with number
       <literal>06:01.0</literal> to a guest with name
       <literal>sles11</literal> use:
      </para>
<screen>xl pci-attach sles12 06:01.0</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Adding the PCI device to Xend</term>
     <listitem>
      <para>
       To add the device to the guest permanently, add the following snippet
       the guest configuration file:
      </para>
<screen>pci = [ '06:01.0,power_mgmt=1,permissive=1' ]</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    After assigning the PCI device to the &vmguest;, the guest system must
    care for the configuration and device drivers for this device.
   </para>
  </sect2>

  <sect2>
   <title>&vgaback;</title>
   <para>
    &xen; 4.0 and newer supports VGA graphics adapter pass-through on fully
    virtualized &vmguest;s. The guest can take full control of the graphics
    adapter with high performance full 3D and video acceleration.
   </para>
   <itemizedlist>
    <title>Limitations</title>
    <listitem>
     <para>
      &vgaback; functionality is similar to &pciback; and as such also
      requires IOMMU (or Intel VT-d) support from the motherboard chipset
      and BIOS.
     </para>
    </listitem>
    <listitem>
     <para>
      Only the primary graphics adapter (the one that is used when you power
      on the computer) can be used with &vgaback;.
     </para>
    </listitem>
    <listitem>
     <para>
      &vgaback; is supported only for fully virtualized guests. Paravirtual
      guests (PV) are not supported.
     </para>
    </listitem>
    <listitem>
     <para>
      The graphics card cannot be shared between multiple &vmguest;s using
      &vgaback; &mdash; you can dedicate it to one guest only.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    To enable &vgaback;, add the following settings to your fully
    virtualized guest configuration file
   </para>
<screen>gfx_passthru=1 
pci=['yy:zz.n']</screen>
   <para>
    where <literal>yy:zz.n</literal> is the PCI controller ID of the VGA
    graphics adapter as found with <command>lspci -v</command> on &dom0;.
   </para>
  </sect2>

  <sect2>
   <title>For More Information</title>
   <para>
    There are several resources that provide interesting information about
    &pciback; in the net:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <ulink url="http://wiki.xensource.com/xenwiki/VTdHowTo"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="http://software.intel.com/en-us/articles/intel-virtualization-technology-for-directed-io-vt-d-enhancing-intel-platforms-for-efficient-virtualization-of-io-devices/"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/34434.pdf"/>
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
</chapter>
