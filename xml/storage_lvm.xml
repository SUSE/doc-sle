<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.lvm" xml:lang="en">
<!-- fs: FIXME
     This definetely does not apply to SLE 12 anymore
     Clarify whether is can be removed or needs to be replaced with something
     on udev and systemctl

  <sect1 id="lvm_activate_vgs">
   <title>Automatically Activating Non-Root LVM Volume Groups</title>

   <para>
    Activation behavior for non-root LVM volume groups is controlled by
    parameter settings in the <literal>/etc/sysconfig/lvm</literal> file.
   </para>

   <para>
    By default, non-root LVM volume groups are automatically activated on
    system restart by <filename>/etc/rc.d/boot.lvm</filename>, according to
    the setting for the <literal>LVM_VGS_ACTIVATED_ON_BOOT</literal>
    parameter in the <literal>/etc/sysconfig/lvm</literal> file. This
    parameter allows you to activate all volume groups on system restart, or
    to activate only specified non-root LVM volume groups.
   </para>

   <para>
    To activate all non-root LVM volume groups on system restart, ensure
    that the value for the <literal>LVM_VGS_ACTIVATED_ON_BOOT</literal>
    parameter in the <filename>/etc/sysconfig/lvm</filename> file is empty
    (<literal>""</literal>). This is the default setting. For almost all
    standard LVM installations, it can safely stay empty.
   </para>

<screen>
LVM_VGS_ACTIVATED_ON_BOOT=""
</screen>

   <para>
    To activate only a specified non-root LVM volume group on system
    restart, specify the volume group name as the value for the
    <literal>LVM_VGS_ACTIVATED_ON_BOOT</literal> parameter:
   </para>

<screen>
LVM_VGS_ACTIVATED_ON_BOOT="vg1"
</screen>

   <para>
    By default, newly discovered LVM volume groups are not automatically
    activated. The <literal>LVM_ACTIVATED_ON_DISCOVERED</literal> parameter
    is disabled in the <filename>/etc/sysconfig/lvm</filename> file:
   </para>

<screen>
LVM_ACTIVATED_ON_DISCOVERED="disable"
</screen>

   <para>
    You can enable the <literal>LVM_ACTIVATED_ON_DISCOVERED</literal>
    parameter to allow newly discovered LVM volume groups to be activated
    via udev rules:
   </para>

<screen>
LVM_ACTIVATED_ON_DISCOVERED="enable"
</screen>
  </sect1>
-->
 <title>LVM Configuration</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
  <abstract>
   <para>
    This chapter briefly describes the principles behind Logical Volume Manager
    (LVM) and its basic features that make it useful under many circumstances.
    The &yast; LVM configuration can be reached from the &yast; Expert
    Partitioner. This partitioning tool enables you to edit and delete existing
    partitions and create new ones that should be used with LVM.
   </para>
  </abstract>
 </info>
 <warning>
  <title>Risks</title>
  <para>
   Using LVM might be associated with increased risk, such as data loss. Risks
   also include application crashes, power failures, and faulty commands. Save
   your data before implementing LVM or reconfiguring volumes. Never work
   without a backup.
  </para>
 </warning>
 <sect1 xml:id="sec.lvm.explained">
  <title>Understanding the Logical Volume Manager</title>

  <para>
   LVM enables flexible distribution of hard disk space over several physical
   volumes (hard disks, partitions, LUNs). It was developed because the need to
   change the segmentation of hard disk space might arise only after the
   initial partitioning has already been done during installation. Because it
   is difficult to modify partitions on a running system, LVM provides a
   virtual pool (volume group or VG) of storage space from which logical
   volumes (LVs) can be created as needed. The operating system accesses these
   LVs instead of the physical partitions. Volume groups can span more than one
   disk, so that several disks or parts of them can constitute one single VG.
   In this way, LVM provides a kind of abstraction from the physical disk space
   that allows its segmentation to be changed in a much easier and safer way
   than through physical repartitioning.
  </para>

  <para>
   <xref linkend="fig.lvm.explain" xrefstyle="FigureXRef"/> compares physical
   partitioning (left) with LVM segmentation (right). On the left side, one
   single disk has been divided into three physical partitions (PART), each
   with a mount point (MP) assigned so that the operating system can access
   them. On the right side, two disks have been divided into two and three
   physical partitions each. Two LVM volume groups (VG&nbsp;1 and VG&nbsp;2)
   have been defined. VG&nbsp;1 contains two partitions from DISK&nbsp;1 and
   one from DISK&nbsp;2. VG&nbsp;2 contains the remaining two partitions from
   DISK&nbsp;2.
  </para>

  <figure xml:id="fig.lvm.explain">
   <title>Physical Partitioning versus LVM</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="lvm.svg" width="80%" format="SVG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="lvm.png" width="100%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   In LVM, the physical disk partitions that are incorporated in a volume group
   are called physical volumes (PVs). Within the volume groups in
   <xref linkend="fig.lvm.explain" xrefstyle="FigureXRef"/>, four logical
   volumes (LV&nbsp;1 through LV&nbsp;4) have been defined, which can be used
   by the operating system via the associated mount points (MP). The border
   between different logical volumes need not be aligned with any partition
   border. See the border between LV&nbsp;1 and LV&nbsp;2 in this example.
  </para>

  <para>
   LVM features:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Several hard disks or partitions can be combined in a large logical
     volume.
    </para>
   </listitem>
   <listitem>
    <para>
     Provided the configuration is suitable, an LV (such as
     <filename>/usr</filename>) can be enlarged when the free space is
     exhausted.
    </para>
   </listitem>
   <listitem>
    <para>
     Using LVM, it is possible to add hard disks or LVs in a running system.
     However, this requires hotpluggable hardware that is capable of such
     actions.
    </para>
   </listitem>
   <listitem>
    <para>
     It is possible to activate a <emphasis>striping mode</emphasis> that
     distributes the data stream of a logical volume over several physical
     volumes. If these physical volumes reside on different disks, this can
     improve the reading and writing performance like RAID&nbsp;0.
    </para>
   </listitem>
   <listitem>
    <para>
     The snapshot feature enables consistent backups (especially for servers)
     in the running system.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>LVM and RAID</title>
   <para>
    Even though LVM also supports RAID of&nbsp;0/1/4/5/6&nbsp;levels, we
    recommend to use MD RAID. However, LVM works fine with RAID&nbsp;0 and 1,
    as RAID&nbsp;0 is similar to common logical volume management (individual
    logical blocks are mapped onto blocks on the physical devices). LVM used on
    top of RAID&nbsp;1 is able to keep track of mirror synchronization and is
    fully able to manage the synchronization process. With higher RAID levels
    you need a management daemon that monitors the states of attached disks and
    can inform administrators if there is a problem in the disk array. LVM
    includes such a daemon, but in exceptional situations like a device
    failure, the daemon does not working properly.
   </para>
  </note>

  <warning os="sles" arch="zseries">
   <title>IBM &zseries;: LVM Root Filesystem</title>
   <para>
    If you configure the system with an LVM root file system, you must place
    <filename>/boot</filename> on a separate, non-LVM partition, otherwise the
    system will fail to boot. The recommended size for such a partition is 500
    MB, the recommended file system ist Ext4.
   </para>
  </warning>

  <para>
   With these features, using LVM already makes sense for heavily used home PCs
   or small servers. If you have a growing data stock, as in the case of
   databases, music archives, or user directories, LVM is especially useful. It
   allows file systems that are larger than the physical hard disk. However,
   keep in mind that working with LVM is different from working with
   conventional partitions.
  </para>

<!-- fs 2014-12-05: Sounds like this information is no longer needed
   <para>
    Starting from kernel version&nbsp;2.6, LVM version&nbsp;2 is available,
    which is downward-compatible with the previous LVM and enables the
    continued management of old volume groups. When creating new volume
    groups, decide whether to use the new format or the downward-compatible
    version. LVM&nbsp;2 does not require any kernel patches. It makes use of
    the device mapper integrated in kernel 2.6. This kernel only supports
    LVM version&nbsp;2. Therefore, when talking about LVM, this section
    always refers to LVM version&nbsp;2.
   </para>
-->

  <para>
   You can manage new or existing LVM storage objects by using the &yast;
   Partitioner. Instructions and further information about configuring LVM are
   available in the official
   <link xlink:href="http://tldp.org/HOWTO/LVM-HOWTO/"><citetitle>LVM
   HOWTO</citetitle></link>.
  </para>

  <important>
   <title>Adding Multipath Support upon an Existing LVM Configuration</title>
   <para>
    If you add multipath support after you have configured LVM, you must modify
    the <filename>/etc/lvm/lvm.conf</filename> file to scan only the multipath
    device names in the <filename>/dev/disk/by-id</filename> directory as
    described in
    <xref linkend="sec.multipath.lvm" xrefstyle="SectTitleOnPage"/>, then
    reboot the server.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="sec.lvm.vg">
  <title>Creating Volume Groups</title>

  <para>
   An LVM volume group (VG) organizes the Linux LVM partitions into a logical
   pool of space. You can carve out logical volumes from the available space in
   the group. The Linux LVM partitions in a group can be on the same or
   different disks. You can add partitions or entire disks to expand the size
   of the group. If you want to use an entire disk, it must not contain any
   partitions. If using partitions, they must not be mounted. &yast; will
   automatically change their partition type to <literal>0x8E Linux
   LVM</literal> when adding them to a VG.
  </para>

  <procedure>
   <step>
    <para>
     Launch &yast; and open the <guimenu>Partitioner</guimenu>.
    </para>
   </step>
   <step>
    <para>
     In case you need to reconfigure your existing partitioning setup, proceed
     as follows. Refer to <xref linkend="sec.yast2.i_y2_part_expert"/> for
     details. Skip this step if you only want to use unused disks or partitions
     that already exist.
    </para>
    <note>
     <title>No Support for Unpartitioned Disks</title>
     <para>
      Using an unpartitioned disk is not supported by &productname;. Thus we
      discourage you from using unpartitioned disks even though LVM can handle
      that solution.
     </para>
    </note>
    <substeps performance="required">
     <step>
      <para>
       To use an entire hard disk that already contains partitions, delete all
       partitions on that disk.
      </para>
     </step>
     <step>
      <para>
       To use a partition that is currently mounted, unmount it.
      </para>
     </step>
<!-- This seems to cause troubles, thus the step is commented out.
     <step>
      <para>
       To use unpartitioned, free space on a hard disk, create a new primary
       or logical partition on that disk. Set its type to <literal>0x8E
       Linux LVM</literal>. Do not format or mount it.
      </para>
     </step>
     -->
    </substeps>
   </step>
   <step>
    <para>
     In the left panel, select <guimenu>Volume Management</guimenu>.
    </para>
    <para>
     A list of existing Volume Groups opens in the right panel.
    </para>
   </step>
   <step>
    <para>
     At the lower left of the Volume Management page, click
     <menuchoice><guimenu>Add</guimenu> <guimenu>Volume
     Group</guimenu></menuchoice>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_lvm4_a.png" width="80%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_lvm4_a.png" width="100%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Define the volume group as follows:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Specify the <guimenu>Volume Group Name</guimenu>.
      </para>
      <para>
       If you are creating a volume group at install time, the name
       <literal>system</literal> is suggested for a volume group that will
       contain the &productname; system files.
      </para>
     </step>
     <step>
      <para>
       Specify the <guimenu>Physical Extent Size</guimenu>.
      </para>
      <para>
       The <guimenu>Physical Extent Size</guimenu> defines the size of a
       physical block in the volume group. All the disk space in a volume group
       is handled in chunks of this size. Values can be from 1 KB to 16 GB in
       powers of 2. This value is normally set to 4&nbsp;MB.
      </para>
      <para>
       In LVM1, a 4 MB physical extent allowed a maximum LV size of 256 GB
       because it supports only up to 65534 extents per LV. LVM2, which is used
       on &productname;, does not restrict the number of physical extents.
       Having many extents has no impact on I/O performance to the logical
       volume, but it slows down the LVM tools.
      </para>
      <important>
       <title>Physical Extent Sizes</title>
       <para>
        Different physical extent sizes should not be mixed in a single VG. The
        extent should not be modified after the initial setup.
       </para>
      </important>
     </step>
     <step>
      <para>
       In the <guimenu>Available Physical Volumes</guimenu> list, select the
       Linux LVM partitions that you want to make part of this volume group,
       then click <guimenu>Add</guimenu> to move them to the <guimenu>Selected
       Physical Volumes</guimenu> list.
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Finish</guimenu>.
      </para>
      <para>
       The new group appears in the <guimenu>Volume Groups</guimenu> list.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     On the Volume Management page, click <guimenu>Next</guimenu>, verify that
     the new volume group is listed, then click <guimenu>Finish</guimenu>.
    </para>
   </step>
   <step>
    <para>
     To check which physical devices are part of the volume group, open the
     &yast; Partitioner at any time in the running system and click
     <menuchoice> <guimenu>Volume Management</guimenu> <guimenu>Edit</guimenu>
     <guimenu>Physical Devices</guimenu></menuchoice>. Leave this screen with
     <guimenu>Abort</guimenu>.
    </para>
    <figure xml:id="fig.yast2.lvm.physical_volumes">
     <title>Physical Volumes in the Volume Group Named DATA</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_lvm5_a.png" width="80%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_lvm5_a.png" width="100%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.lvm.lv">
  <title>Creating Logical Volumes</title>

  <para>
   A logical volume provides a pool of space similar to what a hard disk does.
   To make this space usable, you need to define logical volumes. A logical
   volume is similar to a regular partition&mdash;you can format and mount it.
  </para>

  <para>
   Use The &yast; Partitioner to create logical volumes from an existing volume
   group. Assign at least one logical volume to each volume group. You can
   create new logical volumes as needed until all free space in the volume
   group has been exhausted. An LVM logical volume can optionally be thinly
   provisioned, allowing you to create logical volumes with sizes that overbook
   the available free space (see <xref linkend="sec.lvm.lv.thin"/> for more
   information).
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <formalpara>
     <title>Normal volume:</title>
     <para>
      (Default) The volumeâ€™s space is allocated immediately.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Thin pool:</title>
     <para>
      The logical volume is a pool of space that is reserved for use with thin
      volumes. The thin volumes can allocate their needed space from it on
      demand.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Thin volume:</title>
     <para>
      The volume is created as a sparse volume. The volume allocates needed
      space on demand from a thin pool.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>Mirrored volume:</title>
     <para>
      The volume is created with a defined count of mirrors.
     </para>
    </formalpara>
   </listitem>
  </itemizedlist>

  <procedure xml:id="pro.lvm.lv">
   <title>Setting Up a Logical Volume</title>
   <step>
    <para>
     Launch &yast; and open the <guimenu>Partitioner</guimenu>.
    </para>
   </step>
   <step>
    <para>
     In the left panel, select <guimenu>Volume Management</guimenu>. A list of
     existing Volume Groups opens in the right panel.
    </para>
   </step>
   <step>
    <para>
     Select the volume group in which you want to create the volume and choose
     <menuchoice> <guimenu>Add</guimenu> <guimenu>Logical Volume</guimenu>
     </menuchoice>.
    </para>
   </step>
   <step>
    <para>
     Provide a <guimenu>Name</guimenu> for the volume and choose
     <guimenu>Normal Volume</guimenu> (refer to
     <xref linkend="sec.lvm.lv.thin"/> for setting up thinly provisioned
     volumes). Proceed with <guimenu>Next</guimenu>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_lvm9_a.png" width="80%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_lvm9_a.png" width="100%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Specify the size of the volume and whether to use multiple stripes.
    </para>
    <para>
     Using a striped volume, the data will be distributed among several
     physical volumes. If these physical volumes reside on different hard
     disks, this generally results in a better reading and writing performance
     (like RAID&nbsp;0). The maximum number of available stripes is equal to
     the number of physical volumes. The default (<literal>1</literal> is to
     not use multiple stripes.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_lvm10_a.png" width="80%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_lvm10_a.png" width="100%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Choose a <guimenu>Role</guimenu> for the volume. Your choice here only
     affects the default values for the upcoming dialog. They can be changed in
     the next step. If in doubt, choose <guimenu>Raw Volume
     (Unformatted)</guimenu>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_lvm11_a.png" width="80%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_lvm11_a.png" width="100%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Under <guimenu>Formatting Options</guimenu>, select <guimenu>Format
     Partition</guimenu>, then select the <guimenu>File system</guimenu>. The
     content of the <guimenu>Options</guimenu> menu depends on the file system.
     Usually there is no need to change the defaults.
    </para>
    <para>
     Under <guimenu>Mounting Options</guimenu>, select <guimenu>Mount
     partition</guimenu>, then select the mount point. Click <guimenu>Fstab
     Options</guimenu> to add special mounting options for the volume.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Finish</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Next</guimenu>, verify that the changes are listed, then
     click <guimenu>Finish</guimenu>.
    </para>
   </step>
  </procedure>

  <sect2 xml:id="sec.lvm.lv.thin">
   <title>Thinly Provisioned Logical Volumes</title>
   <para>
    An LVM logical volume can optionally be thinly provisioned. Thin
    provisioning allows you to create logical volumes with sizes that overbook
    the available free space. You create a thin pool that contains unused space
    reserved for use with an arbitrary number of thin volumes. A thin volume is
    created as a sparse volume and space is allocated from a thin pool as
    needed. The thin pool can be expanded dynamically when needed for
    cost-effective allocation of storage space. Thinly provisioned volumes also
    support snapshots which can be managed with Snapper&mdash;see
    <xref linkend="cha.snapper"/> for more information.
   </para>
   <para>
    To set up a thinly provisioned logical volume, proceed as described in
    <xref linkend="pro.lvm.lv"/>. When it comes to choosing the volume type, do
    not choose <guimenu>Normal Volume</guimenu>, but rather <guimenu>Thin
    Volume</guimenu> or <guimenu>Thin Pool</guimenu>.
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>Thin pool</guimenu>
     </term>
     <listitem>
      <para>
       The logical volume is a pool of space that is reserved for use with thin
       volumes. The thin volumes can allocate their needed space from it on
       demand.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>Thin volume</guimenu>
     </term>
     <listitem>
      <para>
       The volume is created as a sparse volume. The volume allocates needed
       space on demand from a thin pool.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <important>
    <title>Thinly Provisioned Volumes in a Cluster</title>
    <para>
     To use thinly provisioned volumes in a cluster, the thin pool and the thin
     volumes that use it must be managed in a single cluster resource. This
     allows the thin volumes and thin pool to always be mounted exclusively on
     the same node.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="lvm.lv.mirror">
   <title>Creating Mirrored Volumes</title>
   <para>
    A logical volume can be created with several mirrors. LVM ensures that data
    written to an underlying physical volume is mirrored onto a different
    physical volume. Thus even though a physical volume crashes, you can still
    access the data on the logical volume. LVM also keeps a log file to manage
    the synchronization process. The log contains information about which
    volume regions are currently undergoing synchronization with mirrors. By
    default the log is stored on disk and if possible on a different disk than
    are the mirrors. But you may specify a different location for the log, for
    example volatile memory.
   </para>
   <para>
    Currently there are two types of mirror implementation available:
    "normal" (non-raid) <literal>mirror</literal> logical volumes and
    <literal>raid1</literal> logical volumes.
   </para>
   <para>
    After you create mirrored logical volumes, you can perform standard
    operations with mirrored logical volumes like activating, extending,
    and removing.
   </para>
   <sect3 xml:id="mirroring.procedure">
    <title>Setting Up Mirrored Non-raid Logical Volumes</title>
    <para>
     To create a mirrored volume use the <command>lvcreate</command> command.
     The following example creates a 500 GB logical volume with two mirrors
     called <emphasis>lv1</emphasis> which uses a volume group
     <emphasis>vg1</emphasis>.
    </para>
<screen>lvcreate -L 500G -m 2 -n lv1 vg1</screen>
    <para>
     Such a logical volume is a linear volume (without striping) that provides
     three copies of the file system. The <literal>m</literal> option specifies
     the count of mirrors. The <literal>L</literal> option specifies the size
     of the logical volumes.
    </para>
    <para>
     The logical volume is divided into regions of the 512 KB default size. If
     you need a different size of regions, use the <literal>-R</literal> option
     followed by the desired region size in megabytes. Or you can configure the
     preferred region size by editing the <literal>mirror_region_size</literal>
     option in the <filename>lvm.conf</filename> file.
    </para>
   </sect3>
   <sect3 xml:id="raid1.lvm.mirroring">
    <title>Setting Up <literal>raid1</literal> Logical Volumes</title>
    <para>
     As LVM supports RAID you can implement mirroring by using RAID1. Such
     implementation provides the following advantages compared to the non-raid
     mirrors:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       LVM maintains a fully redundant bitmap area for each mirror image, which
       increases its fault handling capabilities.
      </para>
     </listitem>
     <listitem>
      <para>
       Mirror images can be temporarily split from the array and then merged
       back.
      </para>
     </listitem>
     <listitem>
      <para>
       The array can handle transient failures.
      </para>
     </listitem>
     <listitem>
      <para>
       The LVM RAID 1 implementation supports snapshots.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     On the other hand, this type of mirroring implementation does not enable
     to create a logical volume in a clustered volume group.
    </para>
    <para>
     To create a mirror volume by using RAID, issue the command
    </para>
<screen>lvcreate --type raid1 -m 1 -L 1G -n lv1 vg1</screen>
    <para>
     where the options/parameters have the following meanings:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>--type</literal> - you need to specify
       <literal>raid1</literal>, otherwise the command uses the implicit
       segment type <literal>mirror</literal> and creates a non-raid mirror.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>-m</literal> - specifies the count of mirrors.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>-L</literal> - specifies the size of the logical volume.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>-n</literal> - by using this option you specify a name of the
       logical volume.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>vg1</literal> - is a name of the volume group used by the
       logical volume.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     LVM creates a logical volume of one extent size for each data volume in
     the array. If you have two mirrored volumes, LVM creates another two
     volumes that stores metadata.
    </para>
    <para>
     After you create a RAID logical volume, you can use the volume in the same
     way as a common logical volume. You can activate it, extend it, etc.
    </para>
   </sect3>
  </sect2>
 </sect1>
<!-- fs: FIXME
     This definetely does not apply to SLE 12 anymore
     Clarify whether is can be removed or needs to be replaced with something
     on udev and systemctl
  -->
 <sect1 xml:id="lvm_activate_vgs">
  <title>Automatically Activating Non-Root LVM Volume Groups</title>

  <para>
   Activation behavior for non-root LVM volume groups is controlled in the
   <literal>/etc/lvm/lvm.conf</literal> file and by the
   <parameter>auto_activation_volume_list</parameter> parameter. By default,
   the parameter is empty and all volumes are activated. To activate only some
   volume groups, add the names in quotes and separate them with commas, for
   example:
  </para>

<screen>auto_activation_volume_list = [ "vg1", "vg2/lvol1", "@tag1", "@*" ]</screen>

  <para>
   If you have defined a list in the
   <parameter>auto_activation_volume_list</parameter> parameter, the following
   will happen:
<!-- # If auto_activation_volume_list is defined, each LV that is to be
    # activated with the autoactivation option (-\-activate ay/-a ay)
    # is first checked against the list.  If it does not match, the LV
    # is not activated.  This list is checked as well as volume_list.
    #
    #   "vgname" and "vgname/lvname" are matched exactly.
    #   "@tag" matches any tag set in the LV or VG.
    #   "@*" matches if any tag defined on the host is also set in the LV or VG
    #
    # auto_activation_volume_list = [ "vg1", "vg2/lvol1", "@tag1", "@*" ] -->
  </para>

  <orderedlist>
   <listitem>
    <para>
     Each logical volume is first checked against this list.
    </para>
   </listitem>
   <listitem>
    <para>
     If it does not match, the logical volume will not be activated.
    </para>
   </listitem>
  </orderedlist>

  <para>
   By default, non-root LVM volume groups are automatically activated on system
   restart by Dracut. This parameter allows you to activate all volume groups
   on system restart, or to activate only specified non-root LVM volume groups.
  </para>

<!--<para>
    To activate all non-root LVM volume groups on system restart, ensure
    that the value for the <literal>LVM_VGS_ACTIVATED_ON_BOOT</literal>
    parameter in the <filename>/etc/sysconfig/lvm</filename> file is empty
    (<literal>""</literal>). This is the default setting. For almost all
    standard LVM installations, it can safely stay empty.
   </para>

<screen>
LVM_VGS_ACTIVATED_ON_BOOT=""
</screen>

   <para>
    To activate only a specified non-root LVM volume group on system
    restart, specify the volume group name as the value for the
    <literal>LVM_VGS_ACTIVATED_ON_BOOT</literal> parameter:
   </para>

<screen>
LVM_VGS_ACTIVATED_ON_BOOT="vg1"
</screen>

   <para>
    By default, newly discovered LVM volume groups are not automatically
    activated. The <literal>LVM_ACTIVATED_ON_DISCOVERED</literal> parameter
    is disabled in the <filename>/etc/sysconfig/lvm</filename> file:
   </para>

<screen>
LVM_ACTIVATED_ON_DISCOVERED="disable"
</screen>

   <para>
    You can enable the <literal>LVM_ACTIVATED_ON_DISCOVERED</literal>
    parameter to allow newly discovered LVM volume groups to be activated
    via udev rules:
   </para>

<screen>
LVM_ACTIVATED_ON_DISCOVERED="enable"
</screen>-->
 </sect1>
 <sect1 xml:id="sec.lvm.vg_resize">
  <title>Resizing an Existing Volume Group</title>

  <para>
   The space provided by a volume group can be expanded at any time in the
   running system without service interruption by adding more physical volumes.
   This will allow you to add logical volumes to the group or to expand the
   size of existing volumes as described in
   <xref linkend="sec.lvm.lv_resize"/>.
  </para>

  <para>
   It is also possible to reduce the size of the volume group by removing
   physical volumes. &yast; only allows to remove physical volumes that are
   currently unused. To find out which physical volumes are currently in use,
   run the following command. The partitions (physical volumes) listed in the
   <literal>PE Ranges</literal> column are the ones in use:
  </para>

<screen>&prompt.user;sudo pvs -o vg_name,lv_name,pv_name,seg_pe_ranges
root's password:
  VG   LV    PV         PE Ranges
             /dev/sda1
  DATA DEVEL /dev/sda5  /dev/sda5:0-3839
  DATA       /dev/sda5
  DATA LOCAL /dev/sda6  /dev/sda6:0-2559
  DATA       /dev/sda7
  DATA       /dev/sdb1
  DATA       /dev/sdc1</screen>

  <procedure>
   <step>
    <para>
     Launch &yast; and open the <guimenu>Partitioner</guimenu>.
    </para>
   </step>
   <step>
    <para>
     In the left panel, select <guimenu>Volume Management</guimenu>. A list of
     existing Volume Groups opens in the right panel.
    </para>
   </step>
   <step>
    <para>
     Select the volume group you want to change, then click
     <guimenu>Resize</guimenu>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_lvm8_a.png" width="80%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_lvm8_a.png" width="100%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Do one of the following:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <formalpara>
       <title>Add:</title>
       <para>
        Expand the size of the volume group by moving one or more physical
        volumes (LVM partitions) from the <guimenu>Available Physical
        Volumes</guimenu> list to the <guimenu>Selected Physical
        Volumes</guimenu> list.
       </para>
      </formalpara>
     </listitem>
     <listitem>
      <formalpara>
       <title>Remove:</title>
       <para>
        Reduce the size of the volume group by moving one or more physical
        volumes (LVM partitions) from the <guimenu>Selected Physical
        Volumes</guimenu> list to the <guimenu>Available Physical
        Volumes</guimenu> list.
       </para>
      </formalpara>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Click <guimenu>Finish</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Next</guimenu>, verify that the changes are listed, then
     click <guimenu>Finish</guimenu>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.lvm.lv_resize">
  <title>Resizing a Logical Volume</title>

  <para>
   In case there is unused free space available in the volume group, you can
   enlarge a logical volume to provide more usable space. You may also reduce
   the size of a volume to free space in the volume group that can be used by
   other logical volumes.
  </para>

  <note>
   <title><quote>Online</quote> Resizing</title>
   <para>
    When reducing the size of a volume, &yast; automatically resizes its file
    system, too. Whether a volume that is currently mounted can be resized
    <quote>online</quote> (that is while being mounted), depends on its file
    system. Growing the file system online is supported by Btrfs, XFS, Ext3,
    and ReiserFS.
   </para>
   <para>
    Shrinking the file system online is only supported by Btrfs. To shrink XFS,
    Ext2/3/4, and ReiserFS volumes, you need to unmount them. Shrinking volumes
    formatted with XFS is not possible, since XFS does not support file system
    shrinking.
   </para>
  </note>

  <procedure>
   <step>
    <para>
     Launch &yast; and open the <guimenu>Partitioner</guimenu>.
    </para>
   </step>
   <step>
    <para>
     In the left panel, select <guimenu>Volume Management</guimenu>. A list of
     existing Volume Groups opens in the right panel.
    </para>
   </step>
   <step>
    <para>
     Select the logical volume you want to change, then click
     <guimenu>Resize</guimenu>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_lvm12_a.png" width="80%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_lvm12_a.png" width="100%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Set the intended size by using one of the following options:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <formalpara>
       <title>Maximum Size</title>
       <para>
        Expand the size of the logical volume to use all space left in the
        volume group.
       </para>
      </formalpara>
     </listitem>
     <listitem>
      <formalpara>
       <title>Minimum Size</title>
       <para>
        Reduce the size of the logical volume to the size occupied by the data
        and the file system metadata.
       </para>
      </formalpara>
     </listitem>
     <listitem>
      <formalpara>
       <title>Custom Size</title>
       <para>
        Specify the new size for the volume. The value must be within the range
        of the minimum and maximum values listed above. Use K, M, G, T for
        Kilobytes, Megabytes, Gigabytes and Terabytes (for example
        <literal>20G</literal>).
       </para>
      </formalpara>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Click <guimenu>OK</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Next</guimenu>, verify that the change is listed, then
     click <guimenu>Finish</guimenu>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.lvm.delete">
  <title>Deleting a Volume Group or a Logical Volume</title>

  <warning>
   <title>Data Loss</title>
   <para>
    Deleting a volume group destroys all of the data in each of its member
    partitions. Deleting a logical volume destroys all data stored on the
    volume.
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     Launch &yast; and open the <guimenu>Partitioner</guimenu>.
    </para>
   </step>
   <step>
    <para>
     In the left panel, select <guimenu>Volume Management</guimenu>. A list of
     existing volume groups opens in the right panel.
    </para>
   </step>
   <step>
    <para>
     Select the volume group or the logical volume you want to remove and click
     <guimenu>Delete</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Depending on your choice warning dialogs are shown. Confirm them with
     <guimenu>Yes</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Next</guimenu>, verify that the deleted volume group is
     listed (deletion is indicated by a red colored font), then click
     <guimenu>Finish</guimenu>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.lvm.cli">
  <title>Using LVM Commands</title>

  <para>
   For information about using LVM commands, see the man pages for the commands
   described in the following table. All commands need to be executed with
   &rootuser; privileges. Either use <command>sudo</command>
   <replaceable>command</replaceable> (recommended) or execute them directly as
   &rootuser;.
  </para>

  <variablelist>
   <title>LVM Commands</title>
   <varlistentry>
    <term><command>pvcreate <replaceable>DEVICE</replaceable></command></term>
    <listitem>
     <para>
      Initializes a device (such as <filename>/dev/sdb1</filename>) for use by
      LVLM as a physical volume. If there is any file system on the specified
      device, a warning appears. Bear in mind that <command>pvcreate</command>
      checks for existing file systems only if <command>blkid</command> is
      installed (which is done by default). If <command>blkid</command> is not
      available, <command>pvcreate</command> will not produce any warning and
      you may lose your file system without any warning.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>pvdisplay <replaceable>DEVICE</replaceable></command></term>
    <listitem>
     <para>
      Displays information about the LVM physical volume, such as whether it is
      currently being used in a logical volume.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>vgcreate -c y <replaceable>VG_NAME</replaceable> <replaceable>DEV1</replaceable> [<replaceable>DEV2</replaceable>...]</command>
    </term>
    <listitem>
     <para>
      Creates a clustered volume group with one or more specified devices.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>vgcreate --activationmode <replaceable>ACTIVATION_MODE</replaceable> <replaceable>VG_NAME</replaceable></command>
    </term>
    <listitem>
     <para>
      Configures the mode of volume group activation. You can specify one of
      the following values:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        <literal>complete</literal> - only the logical volumes that are not
        affected by missing physical volumes can be activated, even though the
        particular logical volume can tolerate such a failure.
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>degraded</literal> - is the default activation mode. If there
        is a sufficient level of redundancy to activate a logical volume, the
        logical volume can be activated even though some physical volumes are
        missing.
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>partial</literal> - the LVM tries to activate the volume group
        even though some physical volumes are missing. If a non-redundant
        logical volume is missing important physical volumes, then the logical
        volume usually cannot be activated and is handled as an error target.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>vgchange -a [ey|n] <replaceable>VG_NAME</replaceable></command>
    </term>
    <listitem>
     <para>
      Activates (<literal>-a ey</literal>) or deactivates (<literal>-a
      n</literal>) a volume group and its logical volumes for input/output.
     </para>
     <para>
      When activating a volume in a cluster, ensure that you use the
      <literal>ey</literal> option. This option is used by default in the load
      script.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>vgremove <replaceable>VG_NAME</replaceable></command></term>
    <listitem>
     <para>
      Removes a volume group. Before using this command, remove the logical
      volumes, then deactivate the volume group.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>vgdisplay <replaceable>VG_NAME</replaceable></command>
    </term>
    <listitem>
     <para>
      Displays information about a specified volume group.
     </para>
     <para>
      To find the total physical extent of a volume group, enter
     </para>
<screen>vgdisplay <replaceable>vg_name</replaceable> | grep "Total PE"</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>lvcreate -L <replaceable>SIZE</replaceable> -n
     <replaceable>LV_NAME</replaceable>
     <replaceable>VG_NAME</replaceable></command>
    </term>
    <listitem>
     <para>
      Creates a logical volume of the specified size.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>lvcreate -L <replaceable>SIZE</replaceable> --thinpool
     <replaceable>POOL_NAME</replaceable>
     <replaceable>VG_NAME</replaceable></command>
    </term>
    <listitem>
     <para>
      Creates a thin pool named <literal>myPool</literal> of the specified
      size from the volume group <replaceable>vg_name</replaceable>.
     </para>
     <para>
      The following example creates a thin pool with a size of 5 GB from the
      volume group <literal>LOCAL</literal>:
     </para>
     <screen>lvcreate -L 5G --thinpool myPool LOCAL</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>lvcreate -T
     <replaceable>VG_NAME</replaceable>/<replaceable>POOL_NAME</replaceable> -V
     <replaceable>SIZE</replaceable> -n
     <replaceable>LV_NAME</replaceable></command>
    </term>
    <listitem>
     <para>
      Creates a thin logical volume within the pool
      <replaceable>pool_name</replaceable>. The following example creates a 1GB
      thin volume named <literal>myThin1</literal> from the pool
      <literal>myPool</literal> on the volume group <literal>LOCAL</literal>:
     </para>
     <screen>lvcreate -T LOCAL/myPool -V 1G -n myThin1</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>lvcreate -T
     <replaceable>VG_NAME</replaceable>/<replaceable>POOL_NAME</replaceable> -V
     <replaceable>SIZE</replaceable> -L <replaceable>SIZE</replaceable> -n
     <replaceable>LV_NAME</replaceable></command>
    </term>
    <listitem>
     <para>
      It is also possible to combine thin pool and thin logical volume
      creation in one command:
     </para>
     <screen>lvcreate -T LOCAL/myPool -V 1G -L 5G -n myThin1</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>lvcreate --activationmode
     <replaceable>ACTIVATION_MODE</replaceable>
     <replaceable>LV_NAME</replaceable></command>
    </term>
    <listitem>
     <para>
      Configures the mode of logical volume activation. You can specify one
      of the following values:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        <literal>complete</literal> - the logical volume can be activated
        only if all its physical volumes are active.
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>degraded</literal> - is the default activation mode. If
        there is a sufficient level of redundancy to activate a logical
        volume, the logical volume can be activated even though some physical
        volumes are missing.
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>partial</literal> - the LVM tries to activate the volume
        even though some physical volumes are missing. In this case part of
        the logical volume may be unavailable and it might cause data
        loss. This option is typically not used, but might be useful when
        restoring data.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      You can specify the activation mode also in
      <filename>/etc/lvm/lvm.conf</filename> by specifying one of the above
      described values of the <literal>activation_mode</literal>
      configuration option.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>lvcreate -s [-L <replaceable>SIZE</replaceable>] -n
    <replaceable>SNAP_VOLUME</replaceable>
    <replaceable>SOURCE_VOLUME_PATH</replaceable>
     <replaceable>VG_NAME</replaceable></command>
    </term>
    <listitem>
     <para>
      Creates a snapshot volume for the specified logical volume. If the size
      option (<option>-L</option> or <option>--size</option>) is not
      included, the snapshot is created as a thin snapshot.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>lvremove
     /dev/<replaceable>VG_NAME</replaceable>/<replaceable>LV_NAME</replaceable></command>
    </term>
    <listitem>
     <para>
      Removes a logical volume.
     </para>
     <para>
      Before using this command, close the logical volume by unmounting it
      with the <command>umount</command> command.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>lvremove <replaceable>SNAP_VOLUME_PATH</replaceable></command>
    </term>
    <listitem>
     <para>
      Removes a snapshot volume.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>lvconvert --merge
     <replaceable>SNAP_VOLUME_PATH</replaceable></command>
    </term>
    <listitem>
     <para>
      Reverts the logical volume to the version of the snapshot.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>vgextend <replaceable>VG_NAME</replaceable>
     <replaceable>DEVICE</replaceable></command>
    </term>
    <listitem>
     <para>
      Adds the specified device (physical volume) to an existing volume
      group.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>vgreduce <replaceable>VG_NAME</replaceable>
     <replaceable>DEVICE</replaceable></command>
    </term>
    <listitem>
     <para>
      Removes a specified physical volume from an existing volume group.
     </para>
     <para>
      Ensure that the physical volume is not currently being used by a
      logical volume. If it is, you must move the data to another physical
      volume by using the <command>pvmove</command> command.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>lvextend -L <replaceable>SIZE</replaceable>
     /dev/<replaceable>VG_NAME</replaceable>/<replaceable>LV_NAME</replaceable></command>
    </term>
    <listitem>
     <para>
      Extends the size of a specified logical volume. Afterward, you must also
      expand the file system to take advantage of the newly available
      space. See <xref linkend="cha.resize_fs"/> for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>lvreduce -L <replaceable>SIZE</replaceable>
     /dev/<replaceable>VG_NAME</replaceable>/<replaceable>LV_NAME</replaceable></command>
    </term>
    <listitem>
     <para>
      Reduces the size of a specified logical volume.
     </para>
     <para>
      Ensure that you reduce the size of the file system first before
      shrinking the volume, otherwise you risk losing data. See
      <xref linkend="cha.resize_fs"/> for details.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <command>lvrename
     /dev/<replaceable>VG_NAME</replaceable>/<replaceable>LV_NAME</replaceable>
     /dev/<replaceable>VG_NAME</replaceable>/<replaceable>NEW_LV_NAME</replaceable></command>
    </term>
    <listitem>
     <para>
      Renames an existing LVM logical volume. It does not change the volume
      group name.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <title>Bypassing udev on Volume Creation</title>
   <para>
    In case you want use LVM to manage LV device nodes and symbolic links
    instead of by using udev rules, you can achieve this by disabling
    notifications from udev with one of the following methods:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Configure <literal>activation/udev_rules = 0</literal> and
      <literal>activation/udev_sync = 0</literal> in
      <filename>/etc/lvm/lvm.conf</filename>.
     </para>
     <para>
      Note that specifying <option>--nodevsync</option> with the
      <command>lvcreate</command> command has the same effect as
      <literal>activation/udev_sync = 0</literal>; setting
      <literal>activation/udev_rules = 0</literal> is still required.
     </para>
    </listitem>
    <listitem>
     <para>
      Setting the environment variable <envar>DM_DISABLE_UDEV</envar>:
     </para>
<screen>export DM_DISABLE_UDEV=1</screen>
     <para>
      This will also disable notifications from udev. In addition, all udev
      related settings from <filename>/etc/lvm/lvm.conf</filename> will be
      ignored.
     </para>
    </listitem>
   </itemizedlist>
  </tip>

  <sect2 xml:id="sec.lvm.cli.resize">
   <title>Resizing a Logical Volume with Commands</title>
   <para>
    The <command>lvresize</command>, <command>lvextend</command>, and
    <command>lvreduce</command> commands are used to resize logical volumes.
    See the man pages for each of these commands for syntax and options
    information. To extend an LV there must be enough unallocated space
    available on the VG.
   </para>
   <para>
    The recommended way to grow or shrink a logical volume is to use the &yast;
    Partitioner. When using &yast;, the size of the file system in the volume
    will automatically be adjusted, too.
   </para>
   <para>
    LVs can be extended or shrunk manually while they are being used, but this
    may not be true for a file system on them. Extending or shrinking the LV
    does not automatically modify the size of file systems in the volume. You
    must use a different command to grow the file system afterward. For
    information about resizing file systems, see
    <xref linkend="cha.resize_fs" xrefstyle="ChapTitleOnPage"/>.
   </para>
   <para>
    Ensure that you use the right sequence when manually resizing an LV:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      If you extend an LV, you must extend the LV before you attempt to grow
      the file system.
     </para>
    </listitem>
    <listitem>
     <para>
      If you shrink an LV, you must shrink the file system before you attempt
      to shrink the LV.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    To extend the size of a logical volume:
   </para>
   <procedure>
    <step>
     <para>
      Open a terminal console.
     </para>
    </step>
    <step>
     <para>
      If the logical volume contains an Ext2 or Ext4 file system, which do not
      support online growing, dismount it. In case it contains file systems
      that are hosted for a virtual machine (such as a &xen; VM), shut down the
      VM first.
     </para>
    </step>
    <step>
     <para>
      At the terminal console prompt, enter the following command to grow the
      size of the logical volume:
     </para>
<screen>sudo lvextend -L +<replaceable>size</replaceable> /dev/<replaceable>vg_name</replaceable>/<replaceable>lv_name</replaceable></screen>
     <para>
      For <replaceable>size</replaceable>, specify the amount of space you want
      to add to the logical volume, such as 10 GB. Replace
      <filename>/dev/<replaceable>vg_name</replaceable>/<replaceable>lv_name</replaceable></filename>
      with the Linux path to the logical volume, such as
      <filename>/dev/LOCAL/DATA</filename>. For example:
     </para>
<screen>sudo lvextend -L +10GB /dev/vg1/v1<replaceable/></screen>
    </step>
    <step>
     <para>
      Adjust the size of the file system. See <xref linkend="cha.resize_fs"/>
      for details.
     </para>
    </step>
    <step>
     <para>
      In case you have dismounted the file system, mount it again.
     </para>
    </step>
   </procedure>
   <para>
    For example, to extend an LV with a (mounted and active) Btrfs on it by 10
    GB:
   </para>
<screen>sudo lvextend âˆ’L +10G /dev/LOCAL/DATA
sudo btrfs filesystem resize +10G /dev/LOCAL/DATA</screen>
   <para>
    To shrink the size of a logical volume:
   </para>
   <procedure>
    <step>
     <para>
      Open a terminal console.
     </para>
    </step>
    <step>
     <para>
      If the logical volume does not contain a Btrfs file system, dismount it.
      In case it contains file systems that are hosted for a virtual machine
      (such as a &xen; VM), shut down the VM first. Note that volumes with the
      XFS file system cannot be reduced in size.
     </para>
    </step>
    <step>
     <para>
      Adjust the size of the file system. See <xref linkend="cha.resize_fs"/>
      for details.
     </para>
    </step>
    <step>
     <para>
      At the terminal console prompt, enter the following command to shrink the
      size of the logical volume to the size of the file system:
     </para>
<screen>sudo lvreduce /dev/<replaceable>vg_name</replaceable>/<replaceable>lv_name</replaceable></screen>
    </step>
    <step>
     <para>
      In case you have unmounted the file system, mount it again.
     </para>
    </step>
   </procedure>
   <para>
    For example, to shrink an LV with a Btrfs on it by 5 GB:
   </para>
<screen>sudo btrfs filesystem resize -size 5G /dev/LOCAL/DATA
sudo lvreduce /dev/LOCAL/DATA</screen>
   <tip>
    <title>
     Resizing the Volume and the File System with a Single Command
    </title>
    <para>
     Starting with &productname; 12 SP1, <command>lvextend</command>,
     <command>lvresize</command>, and <command>lvreduce</command> support the
     option <option>--resizefs</option> which will not only change the size of
     the volume, but will also resize the file system. Therefore the examples
     for <command>lvextend</command> and <command>lvreduce</command> shown
     above can alternatively be run as follows:
    </para>
<screen>sudo lvextend --resizefs âˆ’L +10G /dev/LOCAL/DATA
sudo lvreduce  --resizefs -L -5G /dev/LOCAL/DATA</screen>
    <para>
     Note that the <option>--resizefs</option> is supported for the following
     file systems: ext2/3/4, reiserfs, Btrfs, XFS. Resizing Btrfs with this
     option is currently only available on &productname;, since it is not yet
     accepted upstream.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="sec.lvm.cli.lvmetad">
   <title>Dynamic Aggregation of LVM Metadata via <systemitem class="daemon">lvmetad</systemitem></title>
   <para>
    Most LVM commands require an accurate view of the LVM metadata stored on
    the disk devices in the system. With the current LVM design, if this
    information is not available, LVM must scan all the physical disk devices
    in the system. This requires a significant amount of I/O operations in
    systems that have many disks. In case a disk fails to respond, LVM commands
    might run into a timeout while waiting for the disk.
   </para>
   <para>
    Dynamic aggregation of LVM metadata via
    <systemitem class="daemon">lvmetad</systemitem> provides a solution for
    this problem. The purpose of the
    <systemitem class="daemon">lvmetad</systemitem> daemon is to eliminate the
    need for this scanning by dynamically aggregating metadata information each
    time the status of a device changes. These events are signaled to
    <systemitem class="daemon">lvmetad</systemitem> by udev rules. If the
    daemon is not running, LVM performs a scan as it normally would do.
   </para>
   <para>
    This feature is enabled by default. In case it is disabled on your system,
    proceed as follows to enable it:
   </para>
   <procedure>
    <step>
     <para>
      Open a terminal console.
     </para>
    </step>
    <step>
     <para>
      Stop the <systemitem class="daemon">lvmetad</systemitem> daemon:
     </para>
<screen>sudo systemctl stop lvm2-lvmetad</screen>
    </step>
    <step>
     <para>
      Edit <filename>/etc/lvm/lvm.conf</filename> and set
      <literal>use_lvmetad</literal> to <literal>1</literal>:
     </para>
<screen>use_lvmetad = 1</screen>
    </step>
    <step>
     <para>
      Restart the <systemitem class="daemon">lvmetad</systemitem> daemon:
     </para>
<screen>sudo systemctl start lvm2-lvmetad</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.lvm.cli.lvmcache">
   <title>Using LVM Cache Volumes</title>
   <para>
    LVM supports the use of fast block devices (such as an SSD device) as
    write-back or write-through caches for large slower block devices. The
    cache logical volume type uses a small and fast LV to improve the
    performance of a large and slow LV.
   </para>
   <para>
    To set up LVM caching, you need to create two logical volumes on the
    caching device. A large one is used for the caching itself, a smaller
    volume is used to store the caching metadata. These two volumes need to be
    part of the same volume group as the original volume. When these volumes
    are created, they need to be converted into a cache pool which needs to be
    attached to the original volume:
   </para>
   <procedure>
    <title>Setting up a Cached Logical Volume</title>
    <step>
     <para>
      Create the original volume (on a slow device) if not already existing.
     </para>
    </step>
    <step>
     <para>
      Add the physical volume (from a fast device) to the same volume group the
      original volume is part of and create the cache data volume on the
      physical volume.
     </para>
    </step>
    <step>
     <para>
      Create the cache metadata volume. The size should be 1/1000 of the size
      of the cache data volume, with a minimum size of 8 MB.
     </para>
    </step>
    <step>
     <para>
      Combine the cache data volume and metadata volume into a cache pool
      volume:
     </para>
<screen>lvconvert --type cache-pool --poolmetadata <replaceable>volume_group/metadata_volume</replaceable> <replaceable>volume_group/caching_volume</replaceable></screen>
    </step>
    <step>
     <para>
      Attach the cache pool to the original volume:
     </para>
<screen>lvconvert --type cache --cachepool <replaceable>volume_group/caching_volume</replaceable> <replaceable>volume_group/original_volume</replaceable></screen>
    </step>
   </procedure>
   <para>
    For more information on LVM caching, see the lvmcache(7) man page.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.lvm.tagging">
  <title>Tagging LVM2 Storage Objects</title>

  <para>
   A tag is an unordered keyword or term assigned to the metadata of a storage
   object. Tagging allows you to classify collections of LVM storage objects in
   ways that you find useful by attaching an unordered list of tags to their
   metadata.
  </para>

  <sect2 xml:id="sec.lvm.tagging.using">
   <title>Using LVM2 Tags</title>
   <para>
    After you tag the LVM2 storage objects, you can use the tags in commands to
    accomplish the following tasks:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Select LVM objects for processing according to the presence or absence of
      specific tags.
     </para>
    </listitem>
    <listitem>
     <para>
      Use tags in the configuration file to control which volume groups and
      logical volumes are activated on a server.
     </para>
    </listitem>
    <listitem>
     <para>
      Override settings in a global configuration file by specifying tags in
      the command.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    A tag can be used in place of any command line LVM object reference that
    accepts:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      a list of objects
     </para>
    </listitem>
    <listitem>
     <para>
      a single object as long as the tag expands to a single object
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Replacing the object name with a tag is not supported everywhere yet. After
    the arguments are expanded, duplicate arguments in a list are resolved by
    removing the duplicate arguments, and retaining the first instance of each
    argument.
   </para>
   <para>
    Wherever there might be ambiguity of argument type, you must prefix a tag
    with the commercial at sign (@) character, such as
    <literal>@mytag</literal>. Elsewhere, using the <quote>@</quote> prefix is
    optional.
   </para>
  </sect2>

  <sect2 xml:id="sec.lvm.tagging.requirements">
   <title>Requirements for Creating LVM2 Tags</title>
   <para>
    Consider the following requirements when using tags with LVM:
   </para>
   <variablelist>
    <varlistentry>
     <term>Supported Characters</term>
     <listitem>
      <para>
       An LVM tag word can contain the ASCII uppercase characters A to Z,
       lowercase characters a to z, numbers 0 to 9, underscore (_), plus (+),
       hyphen (-), and period (.). The word cannot begin with a hyphen. The
       maximum length is 128 characters.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Supported Storage Objects</term>
     <listitem>
      <para>
       You can tag LVM2 physical volumes, volume groups, logical volumes, and
       logical volume segments. PV tags are stored in its volume groupâ€™s
       metadata. Deleting a volume group also deletes the tags in the orphaned
       physical volume. Snapshots cannot be tagged, but their origin can be
       tagged.
      </para>
      <para>
       LVM1 objects cannot be tagged because the disk format does not support
       it.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec.lvm.tagging.syntax">
   <title>Command Line Tag Syntax</title>
   <variablelist>
    <varlistentry>
     <term><option>--addtag</option><replaceable>TAG_INFO</replaceable>
     </term>
     <listitem>
      <para>
       Add a tag to (or <emphasis>tag</emphasis>) an LVM2 storage object.
       Example:
      </para>
<screen>sudo vgchange --addtag @db1 vg1</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--deltag</option><replaceable>TAG_INFO</replaceable>
     </term>
     <listitem>
      <para>
       Remove a tag from (or <emphasis>untag</emphasis>) an LVM2 storage
       object. Example:
      </para>
<screen>sudo vgchange --deltag @db1 vg1</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--tag</option><replaceable>TAG_INFO</replaceable>
     </term>
     <listitem>
      <para>
       Specify the tag to use to narrow the list of volume groups or logical
       volumes to be activated or deactivated.
      </para>
      <para>
       Enter the following to activate the volume if it has a tag that matches
       the tag provided (example):
      </para>
<screen>sudo lvchange -ay --tag @db1 vg1/vol2</screen>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec.lvm.tagging.config">
   <title>Configuration File Syntax</title>
   <para>
    The following sections show example configurations for certain usecases.
   </para>
   <sect3 xml:id="sec.lvm.tagging.configuration.hostnames_enable">
    <title>Enabling Host Name Tags in the <filename>lvm.conf</filename> File</title>
    <para>
     Add the following code to the <filename>/etc/lvm/lvm.conf</filename> file
     to enable host tags that are defined separately on host in a
     <filename>/etc/lvm/lvm_&lt;<replaceable>hostname</replaceable>&gt;.conf</filename>
     file.
    </para>
<screen>tags {
   # Enable hostname tags
   hosttags = 1
}</screen>
    <para>
     You place the activation code in the
     <filename>/etc/lvm/lvm_&lt;<replaceable>hostname</replaceable>&gt;.conf</filename>
     file on the host. See
     <xref linkend="sec.lvm.tagging.configuration.activate" xrefstyle="HeadingOnPage"/>.
    </para>
   </sect3>
   <sect3 xml:id="sec.lvm.tagging.configuration.hostnames_define">
    <title>Defining Tags for Host Names in the lvm.conf File</title>
<screen>tags {

   tag1 { }
      # Tag does not require a match to be set.

   tag2 {
      # If no exact match, tag is not set.
      host_list = [ "hostname1", "hostname2" ]
   }
}</screen>
   </sect3>
   <sect3 xml:id="sec.lvm.tagging.configuration.activate">
    <title>Defining Activation</title>
    <para>
     You can modify the <filename>/etc/lvm/lvm.conf</filename> file to activate
     LVM logical volumes based on tags.
    </para>
    <para>
     In a text editor, add the following code to the file:
    </para>
<screen>  activation {
      volume_list = [ "vg1/lvol0", "@database" ]
  }</screen>
    <para>
     Replace <literal>@database</literal> with your tag. Use
     <literal>"@*"</literal> to match the tag against any tag set on the host.
    </para>
    <para>
     The activation command matches against <replaceable>vgname</replaceable>,
     <replaceable>vgname/lvname</replaceable>, or
     @<replaceable>tag</replaceable> set in the metadata of volume groups and
     logical volumes. A volume group or logical volume is activated only if a
     metadata tag matches. The default if there is no match, is not to
     activate.
    </para>
    <para>
     If <literal>volume_list</literal> is not present and tags are defined on
     the host, then it activates the volume group or logical volumes only if a
     host tag matches a metadata tag.
    </para>
    <para>
     If <literal>volume_list</literal> is defined, but empty, and no tags are
     defined on the host, then it does not activate.
    </para>
    <para>
     If volume_list is undefined, it imposes no limits on LV activation (all
     are allowed).
    </para>
   </sect3>
   <sect3 xml:id="sec.lvm.tagging.configuration.activate_multi">
    <title>Defining Activation in Multiple Host Name Configuration Files</title>
    <para>
     You can use the activation code in a hostâ€™s configuration file
     (<filename>/etc/lvm/lvm_&lt;<replaceable>host_tag</replaceable>&gt;.conf</filename>)
     when host tags are enabled in the <filename>lvm.conf</filename> file. For
     example, a server has two configuration files in the
     <filename>/etc/lvm/</filename> directory:
    </para>
    <simplelist>
     <member><filename>lvm.conf</filename>
     </member>
     <member><filename>lvm_&lt;<replaceable>host_tag</replaceable>&gt;.conf</filename>
     </member>
    </simplelist>
    <para>
     At start-up, load the <filename>/etc/lvm/lvm.conf</filename> file, and
     process any tag settings in the file. If any host tags were defined, it
     loads the related
     <filename>/etc/lvm/lvm_&lt;<replaceable>host_tag</replaceable>&gt;.conf</filename>
     file. When it searches for a specific configuration file entry, it
     searches the host tag file first, then the <filename>lvm.conf
     </filename>file, and stops at the first match. Within the
     <filename>lvm_&lt;<replaceable>host_tag</replaceable>&gt;.conf</filename>
     file, use the reverse order that tags were set in. This allows the file
     for the last tag set to be searched first. New tags set in the host tag
     file will trigger additional configuration file loads.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.lvm.tagging.activate_cluster">
   <title>Using Tags for a Simple Activation Control in a Cluster</title>
   <para>
    You can set up a simple host name activation control by enabling the
    <literal>hostname_tags</literal> option in the
    <filename>/etc/lvm/lvm.conf</filename> file. Use the same file on every
    machine in a cluster so that it is a global setting.
   </para>
   <procedure>
    <step>
     <para>
      In a text editor, add the following code to the
      <filename>/etc/lvm/lvm.conf</filename> file:
     </para>
<screen>tags {
   hostname_tags = 1
}</screen>
    </step>
    <step>
     <para>
      Replicate the file to all hosts in the cluster.
     </para>
    </step>
    <step>
     <para>
      From any machine in the cluster, add <literal>db1</literal> to the list
      of machines that activate <filename>vg1/lvol2</filename>:
     </para>
<screen>sudo lvchange --addtag @db1 vg1/lvol2</screen>
    </step>
    <step>
     <para>
      On the <filename>db1</filename> server, enter the following to activate
      it:
     </para>
<screen>sudo lvchange -ay vg1/vol2</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.lvm.tagging.activate_cluster_preferred">
   <title>Using Tags to Activate On Preferred Hosts in a Cluster</title>
   <para>
    The examples in this section demonstrate two methods to accomplish the
    following:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Activate volume group <filename>vg1</filename> only on the database hosts
      <filename>db1</filename> and <filename>db2</filename>.
     </para>
    </listitem>
    <listitem>
     <para>
      Activate volume group <filename>vg2</filename> only on the file server
      host <filename>fs1</filename>.
     </para>
    </listitem>
    <listitem>
     <para>
      Activate nothing initially on the file server backup host
      <filename>fsb1</filename>, but be prepared for it to take over from the
      file server host <filename>fs1</filename>.
     </para>
    </listitem>
   </itemizedlist>
   <sect3 xml:id="sec.lvm.tagging.activate_cluster_preferred.opt1">
    <title>Option 1: Centralized Admin and Static Configuration Replicated Between Hosts</title>
    <para>
     In the following solution, the single configuration file is replicated
     among multiple hosts.
    </para>
    <procedure>
     <step>
      <para>
       Add the <literal>@database</literal> tag to the metadata of volume group
       <filename>vg1</filename>. In a terminal console, enter
      </para>
<screen>sudo vgchange --addtag @database vg1</screen>
     </step>
     <step>
      <para>
       Add the <literal>@fileserver</literal> tag to the metadata of volume
       group <filename>vg2</filename>. In a terminal console, enter
      </para>
<screen>sudo vgchange --addtag @fileserver vg2</screen>
     </step>
     <step>
      <para>
       In a text editor, modify the<filename> /etc/lvm/lvm.conf</filename> file
       with the following code to define the <literal>@database</literal>,
       <literal>@fileserver</literal>, <literal>@fileserverbackup</literal>
       tags.
      </para>
<screen>tags {
   database {
      host_list = [ "db1", "db2" ]
   }
   fileserver {
      host_list = [ "fs1" ]
   }
   fileserverbackup {
      host_list = [ "fsb1" ]
   }
}

activation {
   # Activate only if host has a tag that matches a metadata tag
   volume_list = [ "@*" ]
}</screen>
     </step>
     <step>
      <para>
       Replicate the modified<filename> /etc/lvm/lvm.conf</filename> file to
       the four hosts: <filename>db1</filename>, <filename>db2</filename>,
       <filename>fs1</filename>, and <filename>fsb1</filename>.
      </para>
     </step>
     <step>
      <para>
       If the file server host goes down, <filename>vg2</filename> can be
       brought up on <filename>fsb1</filename> by entering the following
       commands in a terminal console on any node:
      </para>
<screen>sudo vgchange --addtag @fileserverbackup vg2
sudo vgchange -ay vg2</screen>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec.lvm.tagging.activate_cluster_preferred.opt2">
    <title>Option 2: Localized Admin and Configuration</title>
    <para>
     In the following solution, each host holds locally the information about
     which classes of volume to activate.
    </para>
    <procedure>
     <step>
      <para>
       Add the <literal>@database</literal> tag to the metadata of volume group
       <filename>vg1</filename>. In a terminal console, enter
      </para>
<screen>sudo vgchange --addtag @database vg1</screen>
     </step>
     <step>
      <para>
       Add the <literal>@fileserver</literal> tag to the metadata of volume
       group <filename>vg2</filename>. In a terminal console, enter
      </para>
<screen>sudo vgchange --addtag @fileserver vg2</screen>
     </step>
     <step>
      <para>
       Enable host tags in the <filename>/etc/lvm/lvm.conf</filename> file:
      </para>
      <substeps performance="required">
       <step>
        <para>
         In a text editor, modify the <filename>/etc/lvm/lvm.conf</filename>
         file with the following code to enable host tag configuration files.
        </para>
<screen>tags {
   hosttags = 1
}</screen>
       </step>
       <step>
        <para>
         Replicate the modified <filename>/etc/lvm/lvm.conf</filename> file to
         the four hosts: <filename>db1</filename>, <filename>db2</filename>,
         <filename>fs1</filename>, and <filename>fsb1</filename>.
        </para>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       On host <filename>db1</filename>, create an activation configuration
       file for the database host <filename>db1</filename>. In a text editor,
       create <filename>/etc/lvm/lvm_db1.conf</filename> file and add the
       following code:
      </para>
<screen>activation {
   volume_list = [ "@database" ]
}</screen>
     </step>
     <step>
      <para>
       On host <filename>db2</filename>, create an activation configuration
       file for the database host <filename>db2</filename>. In a text editor,
       create <filename>/etc/lvm/lvm_db2.conf</filename> file and add the
       following code:
      </para>
<screen>activation {
   volume_list = [ "@database" ]
}</screen>
     </step>
     <step>
      <para>
       On host fs1, create an activation configuration file for the file server
       host <filename>fs1</filename>. In a text editor, create
       <filename>/etc/lvm/lvm_fs1.conf</filename> file and add the following
       code:
      </para>
<screen>activation {
   volume_list = [ "@fileserver" ]
}</screen>
     </step>
     <step>
      <para>
       If the file server host <filename>fs1</filename> goes down, to bring up
       a spare file server host fsb1 as a file server:
      </para>
      <substeps performance="required">
       <step>
        <para>
         On host <filename>fsb1</filename>, create an activation configuration
         file for the host <filename>fsb1</filename>. In a text editor, create
         <filename>/etc/lvm/lvm_fsb1.conf</filename> file and add the following
         code:
        </para>
<screen>activation {
   volume_list = [ "@fileserver" ]
}</screen>
       </step>
       <step>
        <para>
         In a terminal console, enter one of the following commands:
        </para>
<screen>sudo vgchange -ay vg2
sudo vgchange -ay @fileserver</screen>
       </step>
      </substeps>
     </step>
    </procedure>
   </sect3>
  </sect2>
 </sect1>
</chapter>
