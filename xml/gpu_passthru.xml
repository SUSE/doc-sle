<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<appendix xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="app-gpu-passthru">
 <title>Configuring &gpuback; for &nvidia; cards</title>
 <sect1 xml:id="gpu-passthru-intro">
  <title>Introduction</title>

  <para>
   This article describes how to assign an &nvidia; GPU graphical card on the
   host machine to a virtualized guest.
  </para>
 </sect1>
 <sect1 xml:id="gpu-passthru-prerequisites">
  <title>Prerequisites</title>

  <itemizedlist>
   <listitem>
    <para>
     This article deals with a set of instructions based on V100/T1000 &nvidia;
     cards, and is meant for GPU compute purpose only.
    </para>
   </listitem>
   <listitem>
    <para>
     Verify that you are using an &nvidia; Tesla product&mdash;Maxwell, Pascal,
     or Volta.
    </para>
   </listitem>
   <listitem>
    <para>
     To be able to manage the host system, you need an additional display card
     on the host that you can use when configuring the &gpuback;, or a
     functional SSH environment.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="gpu-passthru-host">
  <title>Configuring the host</title>

  <sect2 xml:id="gpu-passthru-verify-host">
   <title>Verify the host environment</title>
   <procedure>
    <step>
     <para>
      Verify that the host system is SLES 12 SP3 or newer:
     </para>
<screen>
&prompt.user;cat /etc/issue
Welcome to SUSE Linux Enterprise Server 15  (x86_64) - Kernel \r (\l).
</screen>
    </step>
    <step>
     <para>
      Verify that the host supports <xref linkend="gloss-vt-acronym-vtd"/>
      technology and that it is already enabled in the BIOS:
     </para>
<screen>
&prompt.user;dmesg | grep -e "Directed I/O"
[   12.819760] DMAR: Intel(R) Virtualization Technology for Directed I/O
</screen>
     <para>
      If VT-d is not enabled in the BIOS, enable it and reboot the host.
     </para>
    </step>
    <step>
     <para>
      Verify that the host has an extra GPU or VGA card:
     </para>
<screen>
&prompt.user;lspci | grep -i "vga"
07:00.0 VGA compatible controller: Matrox Electronics Systems Ltd. \
  MGA G200e [Pilot] ServerEngines (SEP1) (rev 05)
</screen>
     <para>
      With a Tesla V100 card:
     </para>
<screen>
&prompt.user;lspci | grep -i nvidia
03:00.0 3D controller: NVIDIA Corporation GV100 [Tesla V100 PCIe] (rev a1)
</screen>
     <para>
      With a T1000 Mobile (available on Dell 5540):
     </para>
<screen>
&prompt.user;lspci | grep -i nvidia
01:00.0 3D controller: NVIDIA Corporation TU117GLM [Quadro T1000 Mobile] (rev a1)
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="gpu-passthru-enable-iommu">
   <title>Enable <xref linkend="gloss-vt-acronym-iommu"/></title>
   <para>
    <xref linkend="gloss-vt-acronym-iommu"/> is disabled by default. You need
    to enable it at boot time in the <filename>/etc/default/grub</filename>
    configuration file.
   </para>
   <procedure>
    <step>
     <para>
      For Intel-based hosts:
     </para>
<screen>GRUB_CMDLINE_LINUX="intel_iommu=on iommu=pt rd.driver.pre=vfio-pci"</screen>
     <para>
      For AMD-based hosts:
     </para>
<screen>GRUB_CMDLINE_LINUX="iommu=pt amd_iommu=on rd.driver.pre=vfio-pci"</screen>
    </step>
    <step>
     <para>
      When you save the modified <filename>/etc/default/grub</filename> file,
      re-generate the main &grub; configuration file
      <filename>/boot/grub2/grub.cfg</filename>:
     </para>
<screen>&prompt.sudo;grub2-mkconfig -o /boot/grub2/grub.cfg</screen>
    </step>
    <step>
     <para>
      Reboot the host and verify that <xref linkend="gloss-vt-acronym-iommu"/>
      is enabled:
     </para>
<screen>&prompt.user;dmesg |  grep -e DMAR -e IOMMU</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="gpu-passthru-blacklist-nouveau">
   <title>Blacklist the Nouveau driver</title>
   <para>
    Because we want to assign the &nvidia; card to a VM guest, we need to avoid
    using the card by the host opensource driver. Edit the file
    <filename>/etc/modprobe.d/50-blacklist.conf</filename> and append the
    following line to its end:
   </para>
<screen>blacklist nouveau</screen>
  </sect2>

  <sect2 xml:id="gpu-passthru-configure-vfio">
   <title>Configure <xref linkend="gloss-vt-acronym-vfio"/> and isolate the GPU used for pass-through</title>
   <procedure>
    <step>
     <para>
      Find the card vendor and model IDs. Utilize the bus number identified in
      <xref linkend="gpu-passthru-verify-host"/>, for example
      <literal>03:00.0</literal>:
     </para>
<screen>
&prompt.user;lspci -nn | grep 03:00.0
03:00.0 3D controller [0302]: NVIDIA Corporation GV100 [Tesla V100 PCIe] [10de:1db4] (rev a1)
</screen>
    </step>
    <step>
     <para>
      Create the file <filename>vfio.conf</filename> in the
      <filename>/etc/modprobe.d/</filename> directory with the following
      content:
     </para>
<screen>options vfio-pci ids=10de:1db4</screen>
     <note>
      <para>
       Double check that your card does not need an extra <option>ids=</option>
       parameter. For example, for Public Nvidia Card you need to also add the
       audio device in the list, otherwise you will not be able to use the
       card.
      </para>
     </note>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="gpu-passthru-load-vfio">
   <title>Load the <xref linkend="gloss-vt-acronym-vfio"/> driver</title>
   <para>
    There are three ways you can load the
    <xref linkend="gloss-vt-acronym-vfio"/> driver.
   </para>
   <sect3 xml:id="gpu-passthru-load-vfio-initrd">
    <title>Including the driver in the initrd file</title>
    <procedure>
     <step>
      <para>
       Create the file
       <filename>/etc/dracut.conf.d/gpu-passthrough.conf</filename> and add the
       following content:
      </para>
<screen>add_drivers+="vfio vfio_iommu_type1 vfio_pci vfio_virqfd"</screen>
     </step>
     <step>
      <para>
       Re-generate the initrd file:
      </para>
<screen>&prompt.sudo;dracut --force /boot/initrd $(uname -r)</screen>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="gpu-passthru-load-vfio-modules">
    <title>Adding the driver to the list of auto-loaded modules</title>
    <para>
     Create the file <filename>/etc/modules-load.d/vfio-pci.conf</filename> and
     add the following content:
    </para>
<screen>
pci_stub
vfio
vfio_iommu_type1
vfio_pci
kvm
kvm_intel
</screen>
   </sect3>
   <sect3 xml:id="gpu-passthru-load-vfio-manual">
    <title>Loading the driver manually</title>
    <para>
     To load the driver manually at run-time, execute the following command:
    </para>
<screen>&prompt.sudo;modprobe vfio-pci</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="gpu-passthru-disable-msr">
   <title>Disable MSR for &mswin; guests</title>
   <para>
    For &mswin; guests, we recommend disabling MSR (model-specific register) to
    avoid crash of the guest. Create the file
    <filename>/etc/modprobe.d/kvm.conf</filename> and add the following
    content:
   </para>
<screen>options kvm ignore_msrs=1</screen>
  </sect2>

  <sect2 xml:id="gpu-passthru-ovmf">
   <title>Install and enable UEFI firmware</title>
   <para>
    For proper &gpuback; functionality, the host need to boot using the UEFI
    firmware.
   </para>
   <procedure>
    <step>
     <para>
      Install the <package>qemu-ovmf</package> package that includes UEFI
      firmware images:
     </para>
<screen>&prompt.sudo;zypper install qemu-ovmf</screen>
    </step>
    <step>
     <para>
      Get the list of OVMF <literal>bin</literal> and <literal>vars</literal>
      files by filtering the results of the following command:
     </para>
<screen>&prompt.user;rpm -ql qemu-ovmf</screen>
    </step>
    <step>
     <para>
      Enable OVMF in the &libvirt; &qemu; configuration in the file
      <filename>/etc/libvirt/qemu.conf</filename> by using the list obtained
      from the previous step. It should look similar to the following:
     </para>
<screen>
nvram = [
"/usr/share/qemu/ovmf-x86_64-4m.bin:/usr/share/qemu/ovmf-x86_64-4m-vars.bin",
"/usr/share/qemu/ovmf-x86_64-4m-code.bin:/usr/share/qemu/ovmf-x86_64-4m-vars.bin",
"/usr/share/qemu/ovmf-x86_64-smm-ms-code.bin:/usr/share/qemu/ovmf-x86_64-smm-ms-vars.bin",
"/usr/share/qemu/ovmf-x86_64-smm-opensuse-code.bin:/usr/share/qemu/ovmf-x86_64-smm-opensuse-vars.bin",
"/usr/share/qemu/ovmf-x86_64-ms-4m-code.bin:/usr/share/qemu/ovmf-x86_64-ms-4m-vars.bin",
"/usr/share/qemu/ovmf-x86_64-smm-suse-code.bin:/usr/share/qemu/ovmf-x86_64-smm-suse-vars.bin",
"/usr/share/qemu/ovmf-x86_64-ms-code.bin:/usr/share/qemu/ovmf-x86_64-ms-vars.bin",
"/usr/share/qemu/ovmf-x86_64-smm-code.bin:/usr/share/qemu/ovmf-x86_64-smm-vars.bin",
"/usr/share/qemu/ovmf-x86_64-opensuse-4m-code.bin:/usr/share/qemu/ovmf-x86_64-opensuse-4m-vars.bin",
"/usr/share/qemu/ovmf-x86_64-suse-4m-code.bin:/usr/share/qemu/ovmf-x86_64-suse-4m-vars.bin",
"/usr/share/qemu/ovmf-x86_64-suse-code.bin:/usr/share/qemu/ovmf-x86_64-suse-vars.bin",
"/usr/share/qemu/ovmf-x86_64-opensuse-code.bin:/usr/share/qemu/ovmf-x86_64-opensuse-vars.bin",
"/usr/share/qemu/ovmf-x86_64-code.bin:/usr/share/qemu/ovmf-x86_64-vars.bin",
]
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="gpu-passthru-reboot">
   <title>Reboot the host machine</title>
   <para>
    For most of the changes in the above steps to take effect, you need to
    reboot the host machine:
   </para>
<screen>&prompt.sudo;shutdown -r now</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="gpu-passthru-guest">
  <title>Configuring the guest</title>
  <para>
  </para>
 </sect1>
</appendix>
