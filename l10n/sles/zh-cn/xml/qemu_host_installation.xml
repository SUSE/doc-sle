<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="qemu_host_installation.xml" version="5.0" xml:id="cha-qemu-host">
 <title>设置 KVM VM 主机服务器</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  本节介绍如何设置并使用 <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise Server</phrase></phrase> <phrase role="productnumber"><phrase os="sles;sled">15 SP4</phrase></phrase> 作为基于 QEMU-KVM 的虚拟机主机。
 </para>
 <tip>
  <title>资源</title>
  <para>
   一般情况下，虚拟 Guest 系统所需的硬件资源与将其安装在物理机上时所需的资源相同。您打算在主机系统上运行的 Guest 越多，需要添加到 VM 主机服务器的硬件资源（CPU、磁盘、内存和网络）就越多。
  </para>
 </tip>
 <sect1 xml:id="kvm-host-cpu">
  <title>CPU 的虚拟化支持</title>

  <para>
   要运行 KVM，您的 CPU 必须支持虚拟化，并且需要在 BIOS 中启用虚拟化。<filename>/proc/cpuinfo</filename> 文件包含有关 CPU 功能的信息。
  </para>

  <para os="sles;sled">
   要确定您的系统是否支持虚拟化，请参见<xref linkend="sec-kvm-requires-hardware"/>。
  </para>
 </sect1>
 <sect1 xml:id="kvm-host-soft">
  <title>所需的软件</title>

  <para>
   需在 KVM 主机上安装多个软件包。要安装所有必要的软件包，请执行以下操作：
  </para>

  <procedure>
   <step>
    <para>
     校验是否已安装 <package>yast2-vm</package> 软件包。此软件包是 YaST 的配置工具，可以简化虚拟化超级管理程序的安装过程。
    </para>
   </step>
   <step>
    <para>
     运行 <menuchoice><guimenu>YaST</guimenu><guimenu>虚拟化</guimenu><guimenu>安装超级管理程序和工具</guimenu></menuchoice>。
    </para>
    <figure os="sles;sled">
     <title>安装 KVM 超级管理程序和工具</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_hypervisors.png" width="75%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_hypervisors.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </figure>
    
   </step>
   <step>
    <para>
     选择 <guimenu>KVM 服务器</guimenu>，最好也选择 <guimenu>KVM 工具</guimenu>，然后单击<guimenu>接受</guimenu>确认。
    </para>
   </step>
   <step>
    <para>
     在安装过程中，您可以选择性地让 YaST 自动为您创建<guimenu>网桥</guimenu>。如果您不打算另外为虚拟 Guest 使用一块物理网卡，那么将 Guest 计算机连接到网络的标准方式就是使用网桥。
    </para>
    <figure>
     <title>网桥</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_netbridge.png" width="75%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_netbridge.png" width="75%"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
   <step>
    <para>
     安装所有所需的软件包（并激活新网络设置）后，尝试装载适用于您的 CPU 类型的 KVM 内核模块 — <systemitem>kvm-intel</systemitem> 或 <systemitem>kvm-amd</systemitem>：
    </para>
<screen><prompt role="root"># </prompt>modprobe kvm-intel</screen>
    <para>
     检查该模块是否已装载到内存中：
    </para>
<screen><prompt>&gt; </prompt>lsmod | grep kvm
kvm_intel              64835  6
kvm                   411041  1 kvm_intel</screen>
    <para>
     现在，KVM 主机即可为 KVM VM Guest 提供服务。有关更多信息，请参见<xref linkend="cha-qemu-running"/>。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="kvm-host-virtio">
  <title>特定于 KVM 主机的功能</title>

  <para>
   您可以让基于 KVM 的 VM Guest 充分使用 VM 主机服务器硬件的特定功能（<emphasis>半虚拟化</emphasis>），以此提高这些 Guest 的性能。本节将介绍可以通过哪些方法来使 Guest 直接访问物理主机的硬件（无需模拟层），以充分利用这些硬件。
  </para>

  <tip>
   <para>
    本节中的示例假设读者基本了解 <command>qemu-system-<replaceable>ARCH</replaceable></command> 命令行选项。有关更多信息，请参见<xref linkend="cha-qemu-running"/>。
   </para>
  </tip>

  <sect2 xml:id="kvm-virtio-scsi">
   <title>使用具有 <systemitem>virtio-scsi</systemitem> 的主机储存设备</title>
   <para>
    <systemitem>virtio-scsi</systemitem> 是 KVM 的高级储存堆栈。它取代了以前用于 SCSI 设备直通的 <systemitem>virtio-blk</systemitem> 堆栈。与 <systemitem>virtio-blk</systemitem> 相比，它具有多项优势：
   </para>
   <variablelist>
    <varlistentry>
     <term>提高了可缩放性</term>
     <listitem>
      <para>
       KVM Guest 的 PCI 控制器数量有限，导致可挂接的设备数量也受到限制。<systemitem>virtio-scsi</systemitem> 解决了这个限制，因为它可以将多个储存设备组合到单个控制器上。<systemitem>virtio-scsi</systemitem> 控制器上的每个设备以逻辑单元 (<emphasis>LUN</emphasis>) 表示。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>标准命令集</term>
     <listitem>
      <para>
       <systemitem>virtio-blk</systemitem> 使用 <systemitem>virtio-blk</systemitem> 驱动程序和虚拟机监视器均需知道的一小组命令，因此引入新命令需要同时更新该驱动程序和监视器。
      </para>
      <para>
       相比之下，<systemitem>virtio-scsi</systemitem> 并不定义命令，而是遵循行业标准 SCSI 规范为这些命令定义一个传输协议。此方法与光纤通道、ATAPI 和 USB 设备等其他技术共享。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>设备命名</term>
     <listitem>
      <para>
       <systemitem>virtio-blk</systemitem> 设备在 Guest 中显示为 <filename>/dev/vd<replaceable>X</replaceable></filename>，这与物理系统中的设备名称不同，可能导致迁移时出现问题。
      </para>
      <para>
       <systemitem>virtio-scsi</systemitem> 会确保设备名称与物理系统上的名称相同，这样便可轻松重新定位虚拟机。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>SCSI 设备直通</term>
     <listitem>
      <para>
       对于由主机上整个 LUN 提供支持的虚拟磁盘，最好让 Guest 直接向 LUN 发送 SCSI 命令（直通）。此功能在 <systemitem>virtio-blk</systemitem> 中受到限制，因为 Guest 需使用 virtio-blk 协议而不是 SCSI 命令直通，此外，此功能并不适用于 Windows Guest。<systemitem>virtio-scsi</systemitem> 原生消除了这些限制。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <sect3>
    <title><systemitem>virtio-scsi</systemitem> 的用法</title>
    <para>
     KVM 支持对 <systemitem>virtio-scsi-pci</systemitem> 设备使用 SCSI 直通功能：
    </para>
<screen><prompt role="root"># </prompt>qemu-system-x86_64 [...] \
-device virtio-scsi-pci,id=scsi</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="kvm-qemu-vnet">
   <title>使用 <systemitem>vhost-net</systemitem> 实现加速网络</title>
   <para>
    <systemitem>vhost-net</systemitem> 模块用于加速 KVM 的半虚拟化网络驱动程序。它可提供更低的延迟和更高的网络吞吐量。通过以下示例命令行启动 Guest 即可使用 <literal>vhost-net</literal> 驱动程序：
   </para>
<screen><prompt role="root"># </prompt>qemu-system-x86_64 [...] \
-netdev tap,id=guest0,vhost=on,script=no \
-net nic,model=virtio,netdev=guest0,macaddr=00:16:35:AF:94:4B</screen>
   <para>
    请注意，<literal>guest0</literal> 是 vhost 驱动的设备的标识字符串。
   </para>
  </sect2>

  <sect2 xml:id="kvm-qemu-multiqueue">
   <title>使用多队列 virtio-net 提升网络性能</title>
   <para>
    QEMU 提供了使用<emphasis>多队列</emphasis>提升网络性能的方式，来应对 VM Guest 中的虚拟 CPU 数量增加的情况。多队列 virtio-net 允许 VM Guest 的虚拟 CPU 并行传输包，因此可以提升网络性能。VM 主机服务器和 VM Guest 端都需要提供多队列支持。
   </para>
   <tip>
    <title>性能优势</title>
    <para>
     多队列 virtio-net 解决方案在以下情况下最有利：
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       网络流量包较大。
      </para>
     </listitem>
     <listitem>
      <para>
       VM Guest 上存在许多同时保持活动状态的连接，这些连接主要是在 Guest 系统之间、Guest 与主机之间，或者 Guest 与外部系统之间建立的。
      </para>
     </listitem>
     <listitem>
      <para>
       活动队列数量等于 VM Guest 中虚拟 CPU 的数量。
      </para>
     </listitem>
    </itemizedlist>
   </tip>
   <note>
    <para>
     尽管多队列 virtio-net 可以增加总网络吞吐量，但由于使用了虚拟 CPU 的计算资源，因此也会增加 CPU 消耗量。
    </para>
   </note>
   <procedure xml:id="kvm-qemu-mq-enable">
    <title>如何启用多队列 virtio-net</title>
    <para>
     下面的过程列出了使用 <command>qemu-system-ARCH</command> 启用多队列功能的重要步骤。假设在 VM 主机服务器上设置了一个具有多队列功能（自内核版本 3.8 开始支持此功能）的 tap 网络设备。
    </para>
    <step>
     <para>
      在 <command>qemu-system-ARCH</command> 中，为该 tap 设备启用多队列：
     </para>
<screen>-netdev tap,vhost=on,queues=<replaceable>2*N</replaceable></screen>
     <para>
      其中 <literal>N</literal> 代表队列对的数量。
     </para>
    </step>
    <step>
     <para>
      在 <command>qemu-system-ARCH</command> 中，为 virtio-net-pci 设备启用多队列并指定 MSI-X（消息信号式中断）矢量。
     </para>
<screen>-device virtio-net-pci,mq=on,vectors=<replaceable>2*N+2</replaceable></screen>
     <para>
      在用于计算 MSI-X 矢量数量的公式中：N 个矢量用于 TX（传输）队列，N 个矢量用于 RX（接收）队列，一个矢量用于配置目的，一个矢量用于可能的 VQ（矢量量化）控制。
     </para>
    </step>
    <step>
     <para>
      在 VM Guest 中的相关网络接口（在本示例中为 <literal>eth0</literal>）上启用多队列：
     </para>
<screen><prompt>&gt; </prompt><command>sudo</command> ethtool -L eth0 combined 2*N</screen>
    </step>
   </procedure>
   <para>
    最终的 <command>qemu-system-ARCH</command> 命令行类似于以下示例：
   </para>
<screen>qemu-system-x86_64 [...] -netdev tap,id=guest0,queues=8,vhost=on \
-device virtio-net-pci,netdev=guest0,mq=on,vectors=10</screen>
   <para>
    请注意，对于命令行的两个选项，需指定相同的网络设备 <literal>id</literal> (<literal>guest0</literal>)。
   </para>
   <para>
    在运行中的 VM Guest 内部，以 <systemitem class="username">root</systemitem> 特权指定以下命令：
   </para>
<screen><prompt>&gt; </prompt><command>sudo</command> ethtool -L eth0 combined 8</screen>
   <para>
    现在，Guest 系统网络将使用 <command>qemu-system-ARCH</command> 超级管理程序的多队列支持。
   </para>
  </sect2>

  <sect2 xml:id="kvm-vfio">
   <title>VFIO：对设备进行安全的直接访问</title>
   <para>
    将 PCI 设备直接指派到 VM Guest（PCI 直通）可以避免在性能关键型路径中进行任何模拟，从而避免性能问题。VFIO 取代了传统的 KVM PCI 直通设备指派。要使用此功能，VM 主机服务器配置必须符合<xref linkend="ann-vt-io-require"/>中所述的要求。
   </para>
   <para>
    要通过 VFIO 将 PCI 设备指派到 VM Guest，需要确定该设备属于哪个 IOMMU 组。<xref linkend="gloss-vt-acronym-iommu"/>（用于将支持直接内存访问的 I/O 总线连接到主内存的输入/输出内存管理单元）API 支持组表示法。组是可与系统中的所有其他设备相互隔离的一组设备。因此，组是 <xref linkend="vt-io-vfio"/> 使用的所有权单元。
   </para>
   <procedure>
    <title>通过 VFIO 将 PCI 设备指派到 VM Guest</title>
    <step>
     <para>
      标识要指派到 Guest 的主机 PCI 设备。
     </para>
<screen><prompt>&gt; </prompt><command>sudo</command> lspci -nn
[...]
00:10.0 Ethernet controller [0200]: Intel Corporation 82576 \
Virtual Function [8086:10ca] (rev 01)
[...]</screen>
     <para>
      记下设备 ID（在本例中为 <literal>00:10.0</literal>）和供应商 ID (<literal>8086:10ca</literal>)。
     </para>
    </step>
    <step>
     <para>
      确定此设备的 IOMMU 组：
     </para>
<screen><prompt>&gt; </prompt><command>sudo</command> readlink /sys/bus/pci/devices/0000\:00\:10.0/iommu_group
../../../kernel/iommu_groups/20</screen>
     <para>
      此设备的 IOMMU 组为 <literal>20</literal>。现在，您可以检查该设备是否属于同一个 IOMMU 组：
     </para>
<screen><prompt>&gt; </prompt><command>sudo</command> ls -l /sys/bus/pci/devices/0000\:01\:10.0/iommu_group/devices/
[...] 0000:00:1e.0 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0
[...] 0000:01:10.0 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0/0000:01:10.0
[...] 0000:01:10.1 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0/0000:01:10.1</screen>
    </step>
    <step>
     <para>
      从设备驱动程序取消绑定设备：
     </para>
<screen><prompt>&gt; </prompt><command>sudo</command> echo "0000:01:10.0" &gt; /sys/bus/pci/devices/0000\:01\:10.0/driver/unbind</screen>
    </step>
    <step>
     <para>
      使用步骤 1 中记下的供应商 ID 将设备绑定到 vfio-pci 驱动程序：
     </para>
<screen><prompt>&gt; </prompt><command>sudo</command> echo "8086 153a" &gt; /sys/bus/pci/drivers/vfio-pci/new_id</screen>
     <para>
      系统即会创建一个新设备 <filename>/dev/vfio/<replaceable>IOMMU_GROUP</replaceable></filename>，在本例中为 <filename>/dev/vfio/20</filename>。
     </para>
    </step>
    <step>
     <para>
      更改新建设备的所有权：
     </para>
<screen><prompt>&gt; </prompt><command>sudo</command> chown qemu.qemu /dev/vfio/<replaceable>DEVICE</replaceable></screen>
    </step>
    <step>
     <para>
      现在，运行为其指派了 PCI 设备的 VM Guest。
     </para>
<screen><prompt>&gt; </prompt><command>sudo</command> qemu-system-<replaceable>ARCH</replaceable> [...] -device
     vfio-pci,host=00:10.0,id=<replaceable>ID</replaceable></screen>
    </step>
   </procedure>
   <important>
    <title>不支持热插拔</title>
    <para>
     从 <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise Server</phrase></phrase> <phrase role="productnumber"><phrase os="sles;sled">15 SP4</phrase></phrase> 开始，不支持通过 VFIO 热插拔传递给 VM Guest 的 PCI 设备。
    </para>
   </important>

   <para>
    <filename>/usr/src/linux/Documentation/vfio.txt</filename> 文件中提供了有关 <xref linkend="vt-io-vfio"/> 驱动程序的更详细信息（需要安装软件包 <systemitem>kernel-source</systemitem>）。
   </para>
  </sect2>

  <sect2 xml:id="kvm-qemu-virtfs">
   <title>VirtFS：在主机与 Guest 之间共享目录</title>
   <para>
    VM Guest 通常在单独的计算空间中运行 — 它们各自都有自己的内存范围、专用的 CPU 和文件系统空间。能够共享 VM 主机服务器文件系统的某些部分，就能通过简化相互数据交换来提高虚拟化环境的灵活性。网络文件系统（例如 CIFS 和 NFS）是共享目录的传统方式，但由于它们不是专为虚拟化目的而设计，因此会受制于重大的性能和功能问题。
   </para>
   <para>
    KVM 引入了一种经过优化的新方法，称为 <emphasis>VirtFS</emphasis>（有时称为<quote>文件系统直通</quote>）。VirtFS 使用半虚拟文件系统驱动程序，可以避免将 Guest 应用程序文件系统操作转换为块设备操作，然后又将块设备操作转换为主机文件系统操作。
   </para>
   <para>
    VirtFS 通常可用于以下情况：
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      要从多个 Guest 访问共享目录，或者提供 Guest 到 Guest 的文件系统访问。
     </para>
    </listitem>
    <listitem>
     <para>
      在 Guest 引导过程中，要将虚拟磁盘替换为 Guest 的 RAM 磁盘所要连接到的根文件系统。
     </para>
    </listitem>
    <listitem>
     <para>
      要从云环境中的单个主机文件系统为不同的客户提供储存服务。
     </para>
    </listitem>
   </itemizedlist>
   <sect3 xml:id="kvm-qemu-virtfs-implement">
    <title>实施</title>
    <para>
     在 QEMU 中，可以通过定义两种类型的服务来简化 VirtFS 的实现：
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       用于在主机与 Guest 之间传输协议消息和数据的 <literal>virtio-9p-pci</literal> 设备。
      </para>
     </listitem>
     <listitem>
      <para>
       用于定义导出文件系统属性（例如文件系统类型和安全模型）的 <literal>fsdev</literal> 设备。
      </para>
     </listitem>
    </itemizedlist>
    <example xml:id="ex-qemu-virtfs-host">
     <title>使用 VirtFS 导出主机的文件系统</title>
<screen><prompt>&gt; </prompt><command>sudo</command> qemu-system-x86_64 [...] \
-fsdev local,id=exp1<co xml:id="co-virtfs-host-id"/>,path=/tmp/<co xml:id="co-virtfs-host-path"/>,security_model=mapped<co xml:id="co-virtfs-host-sec-model"/> \
-device virtio-9p-pci,fsdev=exp1<co xml:id="co-virtfs-host-fsdev"/>,mount_tag=v_tmp<co xml:id="co-virtfs-host-mnt-tag"/></screen>
     <calloutlist>
      <callout arearefs="co-virtfs-host-id">
       <para>
        要导出的文件系统的标识。
       </para>
      </callout>
      <callout arearefs="co-virtfs-host-path">
       <para>
        要导出的主机上的文件系统路径。
       </para>
      </callout>
      <callout arearefs="co-virtfs-host-sec-model">
       <para>
        要使用的安全模型 — <literal>mapped</literal> 可使 Guest 文件系统模式和权限与主机相互隔离，而 <literal>none</literal> 会调用<quote>直通</quote>安全模型，其中，对 Guest 文件进行的权限更改也会反映在主机上。
       </para>
      </callout>
      <callout arearefs="co-virtfs-host-fsdev">
       <para>
        前面使用 <literal>-fsdev id=</literal> 定义的导出的文件系统 ID。
       </para>
      </callout>
      <callout arearefs="co-virtfs-host-mnt-tag">
       <para>
        稍后要用来在 Guest 上挂载所导出文件系统的挂载标记。
       </para>
      </callout>
     </calloutlist>
     <para>
      可按如下所示在 Guest 上挂载此类导出的文件系统：
     </para>
<screen><prompt>&gt; </prompt><command>sudo</command> mount -t 9p -o trans=virtio v_tmp /mnt</screen>
     <para>
      其中，<literal>v_tmp</literal> 是前面使用 <literal>-device mount_tag=</literal> 定义的挂载标记，<literal>/mnt</literal> 是要将导出的文件系统挂载到的挂载点。
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2 xml:id="kvm-qemu-ksm">
   <title>KSM：在 Guest 之间共享内存页</title>
   <para>
    内核同页合并 (<xref linkend="gloss-vt-acronym-ksm"/>) 是 Linux 内核的一项功能，可将多个运行中进程的相同内存页合并到一个内存区域中。由于 KVM Guest 在 Linux 中以进程的形式运行，<xref linkend="gloss-vt-acronym-ksm"/> 为超级管理程序提供了内存过量使用功能，以提高内存的使用效率。因此，如果您需要在内存有限的主机上运行多个虚拟机，<xref linkend="gloss-vt-acronym-ksm"/> 可能有所帮助。
   </para>
   <para>
    <xref linkend="gloss-vt-acronym-ksm"/> 将其状态信息储存在 <filename>/sys/kernel/mm/ksm</filename> 目录下的文件中：
   </para>
<screen><prompt>&gt; </prompt>ls -1 /sys/kernel/mm/ksm
full_scans
merge_across_nodes
pages_shared
pages_sharing
pages_to_scan
pages_unshared
pages_volatile
run
sleep_millisecs</screen>
   <para>
    有关 <filename>/sys/kernel/mm/ksm/*</filename> 文件含义的详细信息，请参见 <filename>/usr/src/linux/Documentation/vm/ksm.txt</filename>（软件包 <systemitem>kernel-source</systemitem>）。
   </para>
   <para>
    要使用 <xref linkend="gloss-vt-acronym-ksm"/>，请执行以下操作。
   </para>
   <procedure>
    <step>
     <para>
      尽管 <phrase role="productname"><phrase os="sles">SLES</phrase></phrase> 在内核中包含了 <xref linkend="gloss-vt-acronym-ksm"/> 支持，但其默认处于禁用状态。要启用该支持，请运行以下命令：
     </para>
<screen><prompt role="root"># </prompt>echo 1 &gt; /sys/kernel/mm/ksm/run</screen>
    </step>
    <step>
     <para>
      现在，请在 KVM 中运行多个 VM Guest，并检查 <filename>pages_sharing</filename> 和 <filename>pages_shared</filename> 文件的内容，例如：
     </para>
<screen><prompt>&gt; </prompt>while [ 1 ]; do cat /sys/kernel/mm/ksm/pages_shared; sleep 1; done
13522
13523
13519
13518
13520
13520
13528</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
