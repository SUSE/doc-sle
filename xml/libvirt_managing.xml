<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.libvirt.managing">
 <title>Basic &vmguest; Management</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker>
   </dm:bugtracker>
  </dm:docmanager>
 </info>
 <para>
  Most management tasks, such as starting or stopping a &vmguest;, can
  either be done using the graphical application &vmm; or on the command
  line using <command>virsh</command>. Connecting to the graphical console
  via VNC is only possible from a graphical user interface.
 </para>
 <note>
  <title>Managing &vmguest;s on a Remote &vmhost;</title>
  <para>
   If started on a &vmhost;, the &libvirt; tools &vmm;,
   <command>virsh</command>, and <command>virt-viewer</command> can be used to
   manage &vmguest;s on the host. However, it is also possible to manage
   &vmguest;s on a remote &vmhost;. This requires configuring remote
   access for &libvirt; on the host. For instructions, see
   <xref linkend="cha.libvirt.connect"/>.
  </para>
  <para>
   To connect to such a remote host with &vmm;, you need to set
   up a connection as explained in
   <xref linkend="sec.libvirt.connect.connecting.vmm"/>. If connecting to a
   remote host using <command>virsh</command> or
   <command>virt-viewer</command>, you need to specify a connection URI with
   the parameter <option>-c</option> (for example, <command>virsh -c
   qemu+tls://&wsIIIname;/system</command> or <command>virsh -c
   xen+ssh://</command>). The form of connection URI depends on the
   connection type and the hypervisor&mdash;see
   <xref linkend="sec.libvirt.connect.connecting"/> for details.
  </para>
  <para>
   Examples in this chapter are all listed without a connection URI.
  </para>
 </note>
 <sect1 xml:id="sec.libvirt.managing.list">
  <title>Listing &vmguest;s</title>

  <para>
   The &vmguest; listing shows all &vmguest;s managed by &libvirt;
   on a &vmhost;.
  </para>

  <sect2 xml:id="sec.libvirt.managing.list.vmm">
   <title>Listing &vmguest;s with &vmm;</title>
   <para>
    The main window of the &vmm; lists all &vmguest;s for each &vmhost; it is
    connected to. Each &vmguest; entry contains the machine's name, its status
    (<guimenu>Running</guimenu>, <guimenu>Paused</guimenu>, or
    <guimenu>Shutoff</guimenu>) displayed as an icon and literally, and a CPU
    usage bar.
   </para>
  </sect2>

  <sect2 xml:id="sec.libvirt.managing.list.virsh">
   <title>Listing &vmguest;s with <command>virsh</command></title>
   <para>
    Use the command <command>virsh</command> <option>list</option> to get a
    list of &vmguest;s:
   </para>
   <variablelist>
    <varlistentry>
     <term>List all running guests
     </term>
     <listitem>
<screen>virsh list</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>List all running and inactive guests</term>
     <listitem>
<screen>virsh --all</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For more information and further options, see <command>virsh help
    list</command> or <command>man 1 virsh</command>.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.libvirt.managing.console">
  <title>Accessing the &vmguest; via Console</title>
  <para>
   &vmguest;s can be accessed via a VNC connection (graphical console) or, if
   supported by the guest operating system, via a serial console.
  </para>

  <sect2 xml:id="sec.libvirt.managing.console.vnc">
   <title>Opening a Graphical Console</title>
   <para>
    Opening a graphical console to a &vmguest; lets you interact with the
    machine like a physical host via a VNC connection. If accessing the VNC
    server requires authentication, you are prompted to enter a user name (if
    applicable) and a password.
   </para>
   <para>
    When you click into the VNC console, the cursor is <quote>grabbed</quote>
    and cannot be used outside the console anymore. To release it, press
    <keycombo> <keycap function="alt"/> <keycap function="control"/>
    </keycombo>.
   </para>

   <tip>
    <title>Seamless (Absolute) Cursor Movement</title>
    <para>
     To prevent the console from grabbing the cursor and to enable
     seamless cursor movement, add a tablet input device to the &vmguest;.
     See <xref linkend="sec.libvirt.config.tablet"/> for more information.
    </para>
   </tip>

   <para>
    Certain key combinations such as <keycombo> <keycap function="control"/>
    <keycap function="alt"/> <keycap function="delete"/> </keycombo> are
    interpreted by the host system and are not passed to the &vmguest;. To
    pass such key combinations to a &vmguest;, open the <guimenu>Send
    Key</guimenu> menu from the VNC window and choose the desired key
    combination entry. The <guimenu>Send Key</guimenu> menu is only available
    when using &vmm; and <command>virt-viewer</command>. With &vmm;, you can
    alternatively use the <quote>sticky key</quote> feature as explained in
    <xref linkend="tip.libvirt.inst.vmm.sticky"/>.
   </para>

   <note>
    <title>Supported VNC Viewers</title>
    <para>
     Principally all VNC viewers can connect to the console of a
     &vmguest;. However, if you are using SASL authentication and/or TLS/SSL
     connection to access the guest, the options are limited.  Common VNC
     viewers such as <command>tightvnc</command> or
     <command>tigervnc</command> support neither SASL authentication nor
     TLS/SSL. The only supported alternative to &vmm; and
     <command>virt-viewer</command> is <command>vinagre</command>.
    </para>
   </note>

   <sect3 xml:id="sec.libvirt.managing.console.vnc.vmm">
    <title>Opening a Graphical Console with &vmm;</title>
    <procedure>
     <step>
      <para>
       In the &vmm;, right-click a &vmguest; entry.
      </para>
     </step>
     <step>
      <para>
       Choose <guimenu>Open</guimenu> from the pop-up menu.
      </para>
     </step>
    </procedure>
   </sect3>

   <sect3 xml:id="sec.libvirt.managing.vnc.viewer">
    <title>
     Opening a Graphical Console with <command>virt-viewer</command>
    </title>
    <para>
     <command>virt-viewer</command> is a simple VNC viewer with added
     functionality for displaying &vmguest; consoles. For example, it can be
     started in <quote>wait</quote> mode, where it waits for a &vmguest; to
     start before it connects. It also supports automatically reconnecting to
     a &vmguest; that is rebooted.
    </para>
    <para>
     <command>virt-viewer</command> addresses &vmguest;s by name, by ID or by
     UUID. Use <command>virsh</command> <option>list --all</option> to get
     this data.
    </para>
    <para>
     To connect to a guest that is running or paused, use either the ID, UUID,
     or name. &vmguest;s that are shut off do not have an ID&mdash;you can
     only connect to them by UUID or name.
    </para>
    <variablelist>
     <varlistentry>
      <term>Connect to guest with the ID <literal>8</literal>
      </term>
      <listitem>
       <screen>virt-viewer 8</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Connect to the inactive guest named <literal>sles12</literal>; the
      connection window will open once the guest starts</term>
      <listitem>
       <screen>virt-viewer --wait sles12</screen>
       <para>
        With the <option>--wait</option> option, the connection will be
        upheld even if the &vmguest; is not running at the moment. When
        the guest starts, the viewer will be launched.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     For more information, see <command>virt-viewer</command>
     <option>--help</option> or <command>man 1 virt-viewer</command>.
    </para>
    <note>
     <title>Password Input on Remote connections with SSH</title>
     <para>
      When using <command>virt-viewer</command> to open a connection to a
      remote host via SSH, the SSH password needs to be entered twice. The
      first time for authenticating with &libvirt;, the second time for
      authenticating with the VNC server. The second password needs to be
      provided on the command line where virt-viewer was started.
     </para>
    </note>
   </sect3>
  </sect2>
  <sect2 xml:id="sec.libvirt.managing.console.serial">
   <title>Opening a Serial Console</title>
   <para>
    As an alternative to the graphical console, which requires a graphical
    environment on the client accessing the &vmguest;, virtual machines
    managed with libvirt can also be accessed from the shell via the serial
    console and <command>virsh</command>. To open a serial console to a
    &vmguest; named <quote>sles12</quote>, run the following command:
   </para>
   <screen>virsh console sles12</screen>
   <para>
    <command>virsh console</command> takes two optional flags:
    <option>--safe</option> ensures exclusive access to the console,
    <option>--force</option> disconnects any existing sessions before
    connecting. Both features need to be supported by the guest operating
    system.
   </para>
   <para>
    Being able to connect to a &vmguest; via serial console requires that the
    guest operating system supports serial console access and is properly
    supported. Refer to the guest operating system manual for more information.
   </para>

   <tip>
    <title>Enabling Serial Console Access for &suse; Linux Guests</title>
    <para>
     Serial console access in &suse; Linux is disabled by default. To enable
     it, proceed as follows:
    </para>
    <variablelist>
     <varlistentry>
      <term>&slsa; 12 / openSUSE</term>
      <listitem>
       <para>
       Launch the &yast; Boot Loader module and switch to the <guimenu>Kernel
       Parameters</guimenu> tab. Add <literal>console=ttyS0</literal> to the
       field <guimenu>Optional Kernel Command Line Parameter</guimenu>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>&slsa; 11</term>
      <listitem>
       <para>
        Launch the &yast; Boot Loader module and select the boot entry for
        which to activate serial console access. Choose
        <guimenu>Edit</guimenu> and add <literal>console=ttyS0</literal> to
        the field <guimenu>Optional Kernel Command Line Parameter</guimenu>.
        Additionally, edit
        <filename>/etc/inittab</filename> and uncomment the line with the
        following content:
       </para>
       <screen>#S0:12345:respawn:/sbin/agetty -L 9600 ttyS0 vt102</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </tip>

  </sect2>
 </sect1>
 <sect1 xml:id="sec.libvirt.managing.status">
  <title>Changing a &vmguest;'s State: Start, Stop, Pause</title>

  <para>
   Starting, stopping or pausing a &vmguest; can be done with either
   &vmm; or <command>virsh</command>. You can also configure a
   &vmguest; to be automatically started when booting the &vmhost;.
  </para>

  <para>
   When shutting down a &vmguest;, you may either shut it down
   gracefully, or force the shutdown. The latter is equivalent to pulling
   the power plug on a physical host and is only recommended if there are no
   alternatives. Forcing a shutdown may cause file system corruption and
   loss of data on the &vmguest;.
  </para>

  <tip>
   <title>Graceful Shutdown</title>
   <para>
    To be able to perform a graceful shutdown, the &vmguest; must be
    configured to support <xref linkend="gloss.vt.acpi"/>. If you have created
    the guest with the &vmm;, ACPI should be available in the &vmguest;.
   </para>
   <para>
    Depending on the guest operating system, availability of
    ACPI may not be sufficient to perform a
    graceful shutdown. It is strongly
    recommended to test shutting down and rebooting a guest before using
    it in production. &opensuse; or &sled;, for example, can require
    &pk; authorization for shutdown and reboot. Make sure this policy is
    turned off on all &vmguest;s.
   </para>
   <para>
    If ACPI was enabled during a Windows
    XP/Windows Server 2003 guest installation, turning it on in the &vmguest;
    configuration only is not sufficient. For more information, see:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <link xlink:href="https://support.microsoft.com/en-us/kb/314088"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://support.microsoft.com/en-us/kb/309283"/>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Regardless of the &vmguest;'s configuration, a graceful shutdown is always
    possible from within the guest operating system.
   </para>
  </tip>

  <sect2 xml:id="sec.libvirt.managing.status.vmm">
   <title>Changing a &vmguest;'s State with &vmm;</title>
   <para>
    Changing a &vmguest;'s state can be done either from &vmm;'s main
    window, or from a VNC window.
   </para>
   <procedure>
    <title>State Change from the &vmm; Window</title>
    <step>
     <para>
      Right-click a &vmguest; entry.
     </para>
    </step>
    <step>
     <para>
      Choose <guimenu>Run</guimenu>, <guimenu>Pause</guimenu>, or one of the
      <guimenu>Shutdown options</guimenu> from the pop-up menu.
     </para>
    </step>
   </procedure>
   <procedure>
    <title>State change from the VNC Window</title>
    <step>
     <para>
      Open a VNC Window as described in
      <xref linkend="sec.libvirt.managing.console.vnc.vmm"/>.
     </para>
    </step>
    <step>
     <para>
      Choose <guimenu>Run</guimenu>, <guimenu>Pause</guimenu>, or one of the
      <guimenu>Shut Down</guimenu> options either from the toolbar or from
      the <guimenu>Virtual Machine</guimenu> menu.
     </para>
    </step>
   </procedure>
   <sect3 xml:id="sec.libvirt.managing.status.vmm.autostart">
    <title>Automatically Starting a &vmguest;</title>
    <para>
     You can automatically start a guest when the &vmhost; boots. This feature
     is not enabled by default and needs to be enabled for each &vmguest;
     individually. There is no way to activate it globally.
    </para>
    <procedure>
     <step>
      <para>
       Double-click the &vmguest; entry in &vmm; to open its console.
      </para>
     </step>
     <step>
      <para>
       Choose <menuchoice> <guimenu>View</guimenu>
       <guimenu>Details</guimenu></menuchoice> to open the &vmguest;
       configuration window.
      </para>
     </step>
     <step>
      <para>
       Choose <guimenu>Boot Options</guimenu> and check <guimenu>Start
       virtual machine on host boot up</guimenu>.
      </para>
     </step>
     <step>
      <para>
       Save the new configuration with <guimenu>Apply</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.libvirt.managing.status.virsh">
   <title>Changing a &vmguest;'s State with <command>virsh</command></title>
   <para>
    In the following examples, the state of a &vmguest; named
    <quote>sles12</quote> is changed.
   </para>
   <variablelist>
    <varlistentry>
     <term>Start</term>
     <listitem>
<screen>virsh start sles12</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Pause</term>
     <listitem>
<screen>virsh suspend sles12</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Reboot</term>
     <listitem>
<screen>virsh reboot sles12</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Graceful shutdown</term>
     <listitem>
<screen>virsh shutdown sles12</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Force shutdown</term>
     <listitem>
<screen>virsh destroy sles12</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Turn on automatic start</term>
     <listitem>
<screen>virsh autostart sles12</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Turn off automatic start</term>
     <listitem>
<screen>virsh autostart --disable sles12</screen>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.libvirt.manage.save">
  <title>Saving and Restoring the State of a &vmguest;</title>

  <para>
   Saving a &vmguest; preserves the exact state of the guest’s memory.
   The operation is similar to <emphasis>hibernating</emphasis> a
   computer. A saved &vmguest; can be quickly restored to its previously
   saved running condition.
  </para>

  <para>
   When saved, the &vmguest; is paused, its current memory state is saved
   to disk, and then the guest is stopped. The operation does not make a
   copy of any portion of the &vmguest;’s virtual disk. The amount of
   time taken to save the virtual machine depends on the amount of memory
   allocated. When saved, a &vmguest;’s memory is returned to the pool
   of memory available on the &vmhost;.
  </para>

  <para>
   The restore operation loads a &vmguest;’s previously saved memory
   state file and starts it. The guest is not booted but instead resumed at
   the point where it was previously saved. The operation is
   similar to coming out of hibernation.
  </para>

  <para>
   The &vmguest; is saved to a state file. Make sure there is enough
   space on the partition you are going to save to. For an estimation of the
   file size in megabytes to be expected, issue the following command on the
   guest:
  </para>

<screen>free -mh | awk '/^Mem:/ {print $3}'</screen>

  <warning xml:id="adm.vm.restore">
   <title>Always Restore Saved Guests</title>
   <para>
    After using the save operation, do not boot or start the saved
    &vmguest;. Doing so would cause the machine's virtual disk and the saved
    memory state to get out of synchronization. This can result in critical
    errors when restoring the guest.
   </para>
   <para>
    To be able to work with a saved &vmguest; again, use the restore operation.
    If you used <command>virsh</command> to save a &vmguest;, you cannot
    restore it using &vmm;. In this case, make sure to restore using
    <command>virsh</command>.
   </para>
  </warning>

  <important>
   <title>
    Only for &vmguest;s with Disk Types <literal>raw</literal>,
    <literal>qcow2</literal>, <literal>qed</literal>
   </title>
   <para>
    Saving and restoring &vmguest;s is only possible if the
    &vmguest; is using a virtual disk of the type
    <literal>raw</literal> (<filename>.img</filename>),
    <emphasis>qcow2</emphasis>, or <literal>qed</literal>.
   </para>
  </important>

  <sect2 xml:id="sec.libvirt.manage.save.vmm">
   <title>Saving/Restoring with &vmm;</title>

   <procedure>
    <title>Saving a &vmguest;</title>
    <step>
     <para>
      Open a VNC connection window to a &vmguest;. Make sure the guest is
      running.
     </para>
    </step>
    <step>
     <para>
      Choose <menuchoice> <guimenu>Virtual Machine</guimenu>
      <guimenu>Shutdown</guimenu> <guimenu>Save</guimenu> </menuchoice>.
     </para>
    </step>
   </procedure>
   <procedure>
    <title>Restoring a &vmguest;</title>
    <step>
     <para>
      Open a VNC connection window to a &vmguest;. Make sure the guest is not
      running.
     </para>
    </step>
    <step>
     <para>
      Choose <menuchoice> <guimenu>Virtual Machine</guimenu>
      <guimenu>Restore</guimenu> </menuchoice>.
     </para>
     <para>
      If the &vmguest; was previously saved using &vmm;, you will not be
      offered an option to <guimenu>Run</guimenu> the guest. However, note the
      caveats on machines saved with <command>virsh</command> outlined in
      <xref linkend="adm.vm.restore"/>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.libvirt.manage.save.virsh">
   <title>Saving and Restoring with <command>virsh</command></title>
   <para>
    Save a running &vmguest; with the command <command>virsh</command>
    <option>save</option> and specify the file which it is saved to.
   </para>
   <variablelist>
    <varlistentry>
     <term>Save the guest named <literal>opensuse13</literal>
     </term>
     <listitem>
<screen>virsh save opensuse13 /virtual/saves/opensuse13.vmsav</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Save the guest with the ID <literal>37</literal>
     </term>
     <listitem>
<screen>virsh save 37 /virtual/saves/opensuse13.vmsave</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    To restore a &vmguest;, use <command>virsh</command> <option>restore</option>:
   </para>
<screen>virsh restore /virtual/saves/opensuse13.vmsave</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.libvirt.managing.snapshots">
  <title>Creating and Managing Snapshots</title>

  <para>
   &vmguest; snapshots are snapshots of the complete virtual machine
   including the state of CPU, RAM, and the content of all writable disks.
   To use virtual machine snapshots, you must have at least one
   non-removable and writable block device using the qcow2 disk image
   format.
  </para>

  <note>
   <para>
    Snapshots are supported on &kvm; &vmhost;s only.
   </para>
  </note>

  <para>
   Snapshots let you restore the state of the machine at a particular point
   in time. This is for example useful to undo a faulty configuration or the
   installation of a lot of packages. It is also helpful for testing
   purposes, as it allows you to go back to a defined state at any time.
  </para>

  <para>
   Snapshots can be taken either from running guests or from a guest currently
   not running. Taking a screenshot from a guest that is shut down ensures
   data integrity. In case you want to create a snapshot from a running
   system, be aware that the snapshot only captures the state of the disk(s),
   not the state of the memory. Therefore you need to ensure that:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     All running programs have written their data to the disk. If you are unsure,
     terminate the application and/or stop the respective service.
    </para>
   </listitem>
   <listitem>
    <para>
     Buffers have been written to disk. This can be achieved by running the
     command <command>sync</command> on the &vmguest;.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Starting a snapshot reverts the machine back to the state it was in when
   the snapshot was taken. Any changes written to the disk after that point
   in time will be lost when starting the snapshot.
  </para>
  <para>
   Starting a snapshot will restore the machine to the state (shut off or
   running) it was in when the snapshot was taken.
   After starting a snapshot that was created while the &vmguest; was shut off,
   you will need to boot it.
  </para>

  <sect2 xml:id="sec.libvirt.managing.snapshots.vmm">
   <title>Creating and Managing Snapshots with &vmm;</title>
   <para>
    To open the snapshot management view in &vmm;, open the VNC window as
    described in <xref linkend="sec.libvirt.managing.console.vnc.vmm"/>. Now
    either choose <menuchoice> <guimenu>View</guimenu>
    <guimenu>Snapshots</guimenu>
    </menuchoice> or click <inlinemediaobject>
     <textobject role="description"><phrase>Manage VM Snapshots</phrase>
     </textobject>
     <imageobject role="fo">
      <imagedata fileref="icon-snapshot.png" width="0.8em" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="icon-snapshot.png" width="1em" format="png"/>
     </imageobject>
    </inlinemediaobject> <guimenu>Manage VM Snapshots</guimenu> in the toolbar.
   </para>
   <informalfigure>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="virt_vmm_snapshots_list.png" width="75%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="virt_vmm_snapshots_list.png" width="75%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </informalfigure>
   <para>
    The list of existing snapshots for the chosen &vmguest; is displayed
    in the left-hand part of the window. The snapshot that was last started
    is marked with a green tick. The right-hand part of the window shows
    details of the snapshot currently marked in the list. These details
    include the snapshot's title and time stamp, the state of the
    &vmguest; at the time the snapshot was taken and a description.
    Snapshots of running guests also include a screenshot. The
    <guimenu>Description</guimenu> can be changed directly from this
    view. Other snapshot data cannot be changed.
   </para>
   <sect3 xml:id="sec.libvirt.managing.snapshots.vmm.add">
    <title>Creating a Snapshot</title>
    <para>
     To take a new snapshot of a &vmguest;, proceed as follows:
    </para>
    <procedure>
     <step>
      <para>
       Shut down the &vmguest; in case you want to create a snapshot from
       a guest that is not running.
      </para>
     </step>
     <step>
      <para>
       Click <inlinemediaobject>
        <textobject role="description"><phrase>Add</phrase>
        </textobject>
        <imageobject role="fo">
         <imagedata fileref="icon-add.png" width="0.8em" format="png"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="icon-add.png" width="1em" format="png"/>
        </imageobject>
       </inlinemediaobject> <guimenu>Add</guimenu> in the bottom left corner
       of the VNC window.
      </para>
      <para>
       The window <guimenu>Create Snapshot</guimenu> opens.
      </para>
     </step>
     <step>
      <para>
       Provide a <guimenu>Name</guimenu> and, optionally, a description. The
       name cannot be changed after the snapshot has been taken. To be able to
       identify the snapshot later easily, use a <quote>speaking name</quote>.
      </para>
     </step>
     <step>
      <para>
       Confirm with <guimenu>Finish</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec.libvirt.managing.snapshots.vmm.delete">
    <title>Deleting a Snapshot</title>
    <para>
     To delete a snapshot of a &vmguest;, proceed as follows:
    </para>
    <procedure>
     <step>
      <para>
       Click <inlinemediaobject>
        <textobject role="description"><phrase>Delete</phrase>
        </textobject>
        <imageobject role="fo">
         <imagedata fileref="icon-delete.png" width="0.8em" format="png"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="icon-delete.png" width="1em" format="png"/>
        </imageobject>
       </inlinemediaobject> <guimenu>Delete</guimenu> in the bottom left corner
       of the VNC window.
      </para>
     </step>
     <step>
      <para>
       Confirm the deletion with <guimenu>Yes</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec.libvirt.managing.snapshots.vmm.start">
    <title>Starting a Snapshot</title>
    <para>
     To start a snapshot, proceed as follows:
    </para>
    <procedure>
     <step>
      <para>
       Click <inlinemediaobject>
        <textobject role="description"><phrase>Play</phrase>
        </textobject>
        <imageobject role="fo">
         <imagedata fileref="icon-play.png" width="0.8em" format="png"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="icon-play.png" width="1em" format="png"/>
        </imageobject>
       </inlinemediaobject> <guimenu>Run</guimenu> in the bottom left corner of
       the VNC window.
      </para>
     </step>
     <step>
      <para>
       Confirm the start with <guimenu>Yes</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.libvirt.managing.snapshots.virsh">
   <title>Creating and Managing Snapshots with <command>virsh</command></title>
   <para>
    To list all existing snapshots for a domain
    (<replaceable>admin_server</replaceable> in the following), run the
    <literal>snapshot-list</literal> command:
   </para>
<screen>&prompt.user;virsh snapshot-list
 Name                 Creation Time             State
------------------------------------------------------------
 Basic installation incl. SMT finished 2013-09-18 09:45:29 +0200 shutoff
 Basic installation incl. SMT for CLOUD3 2013-12-11 15:11:05 +0100 shutoff
 Basic installation incl. SMT for CLOUD3-HA 2014-03-24 13:44:03 +0100 shutoff
 Basic installation incl. SMT for CLOUD4 2014-07-07 11:27:47 +0200 shutoff
 Beta1 Running        2013-07-12 12:27:28 +0200 shutoff
 Beta2 prepared       2013-07-12 17:00:44 +0200 shutoff
 Beta2 running        2013-07-29 12:14:11 +0200 shutoff
 Beta3 admin node deployed 2013-07-30 16:50:40 +0200 shutoff
 Beta3 prepared       2013-07-30 17:07:35 +0200 shutoff
 Beta3 running        2013-09-02 16:13:25 +0200 shutoff
 Cloud2 GM running    2013-12-10 15:44:58 +0100 shutoff
 CLOUD3 RC prepared   2013-12-20 15:30:19 +0100 shutoff
 CLOUD3-HA Build 680 prepared 2014-03-24 14:20:37 +0100 shutoff
 CLOUD3-HA Build 796 installed (zypper up) 2014-04-14 16:45:18 +0200 shutoff
 GMC2 post Cloud install 2013-09-18 10:53:03 +0200 shutoff
 GMC2 pre Cloud install 2013-09-18 10:31:17 +0200 shutoff
 GMC2 prepared (incl. Add-On Installation) 2013-09-17 16:22:37 +0200 shutoff
 GMC_pre prepared     2013-09-03 13:30:38 +0200 shutoff
 OS + SMT + eth[01]   2013-06-14 16:17:24 +0200 shutoff
 OS + SMT + Mirror + eth[01] 2013-07-30 15:50:16 +0200 shutoff</screen>
   <para>
    The snapshot that was last started is shown with the
    <literal>snapshot-current command:</literal>
   </para>
<screen>&prompt.user;virsh snapshot-current --name admin_server
Basic installation incl. SMT for CLOUD4
</screen>
   <para>
    Details about a particular snapshot can be obtained by running the
    <literal>snapshot-info</literal> command:
   </para>
<screen>&prompt.user;virsh snapshot-info sles "Basic installation incl. SMT for CLOUD4"
Name:           Basic installation incl. SMT for CLOUD4
Domain:         admin_server
Current:        yes
State:          shutoff
Location:       internal
Parent:         Basic installation incl. SMT for CLOUD3-HA
Children:       0
Descendants:    0
Metadata:       yes
</screen>
   <sect3 xml:id="sec.libvirt.managing.snapshots.virsh.add">
    <title>Creating a Snapshot</title>
    <para>
     To take a new snapshot of a &vmguest; currently not running, use the
     <literal>snapshot-create-as</literal> command as follows:
    </para>
<screen>virsh snapshot-create-as --domain admin_server<co xml:id="virsh.snapshot.add.domain"/> --name "Snapshot 1"<co xml:id="virsh.snapshot.add.name"/> \
--description "First snapshot"<co xml:id="virsh.snapshot.add.description"/></screen>
    <calloutlist>
     <callout arearefs="virsh.snapshot.add.domain">
      <para>
       Domain name. Mandatory.
      </para>
     </callout>
     <callout arearefs="virsh.snapshot.add.name">
      <para>
       Name of the snapshot. It is recommended to use a <quote>speaking
       name</quote>, since that makes it easier to identify the snapshot.
       Mandatory.
      </para>
     </callout>
     <callout arearefs="virsh.snapshot.add.description">
      <para>
       Description for the snapshot. Optional.
      </para>
     </callout>
    </calloutlist>
    <para>
     To take a snapshot of a running &vmguest;, you need to specify the
     <option>--live</option> parameter:
    </para>
<screen>virsh snapshot-create-as --domain admin_server --name "Snapshot 2" \
 --description "First live snapshot" --live</screen>
    <para>
     Refer to the <citetitle>SNAPSHOT COMMANDS</citetitle> section in
     <command>man 1 virsh</command> for more details.
    </para>
   </sect3>
   <sect3 xml:id="sec.libvirt.managing.snapshots.virsh.delete">
    <title>Deleting a Snapshot</title>
    <para>
     To delete a snapshot of a &vmguest;, use the
     <literal>snapshot-delete</literal> command:
    </para>
<screen>virsh snapshot-delete --domain admin_server --snapshotname "Snapshot 2"</screen>
   </sect3>
   <sect3 xml:id="sec.libvirt.managing.snapshots.virsh.start">
    <title>Starting a Snapshot</title>
    <para>
     To start a snapshot, use the <literal>snapshot-revert</literal>
     command:
    </para>
<screen>virsh snapshot-revert --domain admin_server --snapshotname "Snapshot 1"</screen>
    <para>
     To start the current snapshot (the one the &vmguest; was started
     off), it is sufficient to use <option>--current</option> rather than
     specifying the snapshot name:
    </para>
<screen>virsh snapshot-revert --domain admin_server --current</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.libvirt.managing.delete">
  <title>Deleting a &vmguest;</title>

  <para>
   By default, deleting a &vmguest; using <command>virsh</command> removes only
   its XML configuration. Since attached storage is not deleted by default, you
   can reuse it with another &vmguest;. With &vmm;, you can also delete a
   guest's storage files as well&mdash;this will completely erase the guest.
  </para>

  <sect2 xml:id="sec.libvirt.managing.delete.vmm">
   <title>Deleting a &vmguest; with &vmm;</title>
   <procedure>
    <step>
     <para>
      In the &vmm;, right-click a &vmguest; entry.
     </para>
    </step>
    <step>
     <para>
      From the context menu, choose <guimenu>Delete</guimenu>.
     </para>
    </step>
    <step>
     <para>
      A confirmation window opens. Clicking <guimenu>Delete</guimenu> will
      permanently erase the &vmguest;. The deletion is not recoverable.
     </para>
     <para>
      You can also permanently delete the guest's virtual disk by
      activating <guimenu>Delete Associated Storage Files</guimenu>. The
      deletion is not recoverable either.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.libvirt.managing.delete.virsh">
   <title>Deleting a &vmguest; with <command>virsh</command></title>
   <para>
    To delete a &vmguest;, it needs to be shut down first. It is not possible to
    delete a running guest. For information on shutting down, see
    <xref linkend="sec.libvirt.managing.status"/>.
   </para>
   <para>
    To delete a &vmguest; with <command>virsh</command>, run
    <command>virsh</command> <option>undefine</option>
    <replaceable>VM_NAME</replaceable>.
   </para>
<screen>virsh undefine sles12</screen>
   <para>
    There is no option to automatically delete the attached storage files.
    If they are managed by libvirt, delete them as described in
    <xref linkend="sec.libvirt.storage.virsh.del_volumes"/>.
   </para>
  </sect2>
 </sect1>
  <sect1 xml:id="sec.libvirt.admin.migrate">
  <title>Migrating &vmguest;s</title>

  <para>
   One of the major advantages of virtualization is that &vmguest;s are
   portable. When a &vmhost; needs to go down for maintenance, or when the
   host gets overloaded, the guests can easily be moved to another
   &vmhost;. &kvm; and &xen; even support <quote>live</quote> migrations
   during which the &vmguest; is constantly available.
  </para>

  <sect2 xml:id="libvirt.admin.live.migration.requirements">
   <title>Migration Requirements</title>
   <para>
    To successfully migrate a &vmguest; to another &vmhost;,
    the following requirements need to be met:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      It is recommended that the source and destination systems have the
      same architecture. However, it is possible to migrate between hosts
      with AMD* and Intel* architectures.
     </para>
    </listitem>
    <listitem>
     <para>
      Storage devices must be accessible from both machines (for example,
      via NFS or iSCSI) and must be configured as a storage pool on both
      machines. For more information, see <xref linkend="cha.libvirt.storage"/>.
     </para>
     <para condition="kvm4x86">
      This is also true for CD-ROM or floppy images that are connected during
      the move. However, you can disconnect them prior to the move as described
      in <xref linkend="sec.libvirt.config.cdrom.media_change"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      &libvirtd; needs to run on both &vmhost;s and you must be able
      to open a remote &libvirt; connection between the target and the
      source host (or vice versa). Refer to
      <xref linkend="sec.libvirt.connect.remote"/> for details.
     </para>
    </listitem>
    <listitem>
     <para>
      If a firewall is running on the target host, ports need to be opened
      to allow the migration. If you do not specify a port during the
      migration process, &libvirt; chooses one from the range
      49152:49215. Make sure that either this range (recommended) or a
      dedicated port of your choice is opened in the firewall on the
      <emphasis>target host</emphasis>.
     </para>
    </listitem>
    <listitem>
     <para>
      Host and target machine should be in the same subnet on the network,
      otherwise networking will not work after the migration.
     </para>
    </listitem>
    <listitem>
     <para>
      No running or paused &vmguest; with the same name must exist on the
      target host. If a shut-down machine with the same name exists, its
      configuration will be overwritten.
     </para>
    </listitem>
    <listitem>
     <para>
      All CPU models except <emphasis>host cpu</emphasis> model are
      supported when migrating &vmguest;s.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="gloss.vt.acronym.sata"/> disk device type is not
      migratable.
     </para>
    </listitem>
    <listitem>
     <para>
      File system pass-through feature is incompatible with migration.
     </para>
    </listitem>
    <listitem>
     <para>
      The &vmhost; and &vmguest; need to have proper timekeeping
      installed. See <xref linkend="sec.kvm.managing.clock"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="kvm.virtio_data_plane"/> is not supported for
      migration.
     </para>
    </listitem>
    <listitem>
     <para>
      No physical devices can be passed from host to guest. Live migration
      is currently not supported when using devices with PCI pass-through or
      <xref linkend="vt.io.sriov"/>. In case live migration needs to be
      supported, you need to use software virtualization (paravirtualization
      or full virtualization).
     </para>
    </listitem>
    <listitem>
     <para>
      Cache mode setting is an important setting for migration. See:
      <xref linkend="sec.cache.mode.live.migration"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Live migration of &vmguest;s from a host running one operating system to a
      host running a different operating system is fully supported for the
      following scenarios:
     </para>
     <itemizedlist>
      <listitem>
       <!-- fate#320425 -->
       <para>
        &slsa; 12 SP1 to &slsa; 12 SP2
       </para>
      </listitem>
      <listitem>
       <para>
        &slsa; 12 SP2 to &slsa; 12 SP2
       </para>
      </listitem>
      <listitem>
       <para>
        &slsa; 12 SP2 to &slsa; 12 SP3 (when released)
       </para>
      </listitem>
      <!--
       <listitem>
       <para>
        SLES 11 SP3 to SLES 12 SP1
       </para>
      </listitem>
      <listitem>
       <para>
        SLES 11 SP4 to SLES 12 SP1
       </para>
       </listitem>
       -->
     </itemizedlist>
     <!-- fate#320425 -->
     <para>
      Backward migration (from &slsa; 12 SP1 or SP2 to 12 or from &slsa;
      SP2 to SP1) is not supported.
     </para>
     <para>
     Live migration between &slea; 11 and &slea; 12 is not supported because
     of a different tool stack. <!-- See R/N for more details. -->
     </para>
    </listitem>
    <listitem>
     <para>
      The image directory should be located in the same path on both hosts.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec.libvirt.admin.migrate.virtmanager">
   <title>Migrating with &vmm;</title>
   <para>
    When using the &vmm; to migrate &vmguest;s, it does not matter on
    which machine it is started. You can start &vmm; on the source or the
    target host or even on a third host. In the latter case you need to be
    able to open remote connections to both the target and the source host.
   </para>
   <procedure>
    <step>
     <para>
      Start &vmm; and establish a connection to the target or the source
      host. If the &vmm; was started neither on the target nor the source
      host, connections to both hosts need to be opened.
     </para>
    </step>
    <step>
     <para>
      Right-click the &vmguest; that you want to migrate and choose
      <guimenu>Migrate</guimenu>. Make sure the guest is running or
      paused&mdash;it is not possible to migrate guests that are shut
      down.
     </para>
     <tip>
      <title>Increasing the Speed of the Migration</title>
      <para>
       To increase the speed of the migration somewhat, pause the &vmguest;.
       This is the equivalent of the former so-called
       <quote>offline migration</quote> option of &vmm;.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Choose a <guimenu>New Host</guimenu> for the &vmguest;. If the
      desired target host does not show up, make sure that you are connected to
      the host.
     </para>
     <para>
      To change the default options for conntecting to the remote host, under
      <guimenu>Connection</guimenu>, set the <guimenu>Mode</guimenu>, and the
      target host's <guimenu>Address</guimenu> (IP address or host name) and
      <guimenu>Port</guimenu>. If you specify a <guimenu>Port</guimenu>, you
      must also specify an <guimenu>Address</guimenu>.
     </para>
     <para>
      Under <guimenu>Advanced options</guimenu>, choose whether the move should
      be permanent (default) or temporary, using
      <guimenu>Temporary move</guimenu>.
     </para>
     <para>
      Additionally, there is the option <guimenu>Allow unsafe</guimenu> which
      allows migrating without disabling the cache of the &vmhost;. This can
      speed up the migration but only works when the current configuration
      allows for a consistent view of the &vmguest; storage without using
      <literal>cache=&quot;none&quot;</literal>/<literal>0_DIRECT</literal>.
     </para>
     <note>
      <title>Bandwidth Option</title>
      <para>
       In recent versions of &vmm;, the option of setting a bandwidth for the
       migration has been removed. To set a specific bandwidth, use
       <command>virsh</command> instead.
      </para>
     </note>
    </step>
    <step>
     <para>
      To perform the migration, click <guimenu>Migrate</guimenu>.
     </para>
     <para>
      When the migration is complete, the <guimenu>Migrate</guimenu> window
      closes and the &vmguest; is now listed on the new host in the
      &vmm; window. The original &vmguest; will still be available on
      the target host (in shut down state).
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.libvirt.admin.migrate.virsh">
   <title>Migrating with <command>virsh</command></title>
   <para>
    To migrate a &vmguest; with <command>virsh</command>
    <option>migrate</option>, you need to have direct or remote shell access
    to the &vmhost;, because the command needs to be run on the host. The
    migration command looks like this:
   </para>
<screen>virsh migrate [OPTIONS] <replaceable>VM_ID_or_NAME</replaceable> <replaceable>CONNECTION_URI</replaceable> [--migrateuri tcp://<replaceable>REMOTE_HOST:PORT</replaceable>]</screen>
   <para>
    The most important options are listed below. See <command>virsh help
    migrate</command> for a full list.
   </para>
   <variablelist>
    <varlistentry>
     <term><option>--live</option>
     </term>
     <listitem>
      <para>
       Does a live migration. If not specified, the guest will be paused during
       the migration (<quote>offline migration</quote>).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--suspend</option>
     </term>
     <listitem>
      <para>
       Does an offline migration and does not restart the &vmguest; on
       the target host.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--persistent</option>
     </term>
     <listitem>
      <para>
       By default a migrated &vmguest; will be migrated temporarily, so its
       configuration is automatically deleted on the target host if it is
       shut down. Use this switch to make the migration persistent.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--undefinesource</option>
     </term>
     <listitem>
      <para>
       When specified, the &vmguest; definition on the source host will
       be deleted after a successful migration (however, virtual disks
       attached to this guest will <emphasis>not</emphasis> be deleted).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The following examples use &wsIVname; as the source system and
    &wsIname; as the target system; the &vmguest;'s name is
    <literal>opensuse131</literal> with Id <literal>37</literal>.
   </para>
   <variablelist>
    <varlistentry>
     <term>Offline migration with default parameters</term>
     <listitem>
<screen>virsh migrate 37 qemu+ssh://&exampleuser_plain;@&wsIname;/system</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Transient live migration with default parameters</term>
     <listitem>
<screen>virsh migrate --live opensuse131 qemu+ssh://&exampleuser_plain;@&wsIname;/system</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Persistent live migration; delete VM definition on source</term>
     <listitem>
<screen>virsh migrate --live --persistent --undefinesource 37 \
qemu+tls://&exampleuser_plain;@&wsIname;/system</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Offline migration using port 49152</term>
     <listitem>
<screen>virsh migrate opensuse131 qemu+ssh://&exampleuser_plain;@&wsIname;/system \
--migrateuri tcp://@&wsIname;:49152</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <note>
    <title>Transient Compared to Persistent Migrations</title>
    <para>
     By default <command>virsh migrate</command> creates a temporary
     (transient) copy of the &vmguest; on the target host. A shut down
     version of the original guest description remains on the source host. A
     transient copy will be deleted from the server after it is shut down.
    </para>
    <para>
     To create a permanent copy of a guest on the target host, use
     the switch <option>--persistent</option>. A shut down version of the
     original guest description remains on the source host, too. Use the
     option <option>--undefinesource</option> together with
     <option>--persistent</option> for a <quote>real</quote> move where a
     permanent copy is created on the target host and the version on the
     source host is deleted.
    </para>
    <para>
     It is not recommended to use <option>--undefinesource</option> without
     the <option>--persistent</option> option, since this will result in the
     loss of both &vmguest; definitions when the guest is shut down on
     the target host.
    </para>
   </note>
  </sect2>

<!-- Step by step example -->

  <sect2 xml:id="sec.libvirt.migrate.stepbstep">
   <title>Step-by-Step Example</title>
   <para/>
   <sect3 xml:id="sec.migrate.stepbstep.export">
    <title>Exporting the Storage</title>
    <para>
     First you need to export the storage, to share the Guest image between
     host. This can be done by an NFS server. In the following example we
     want to share the <filename>/volume1/VM</filename> directory for all
     machines that are on the network 10.0.1.0/24. We will use a &sle;
     NFS server. As root user, edit the <filename>/etc/exports</filename>
     file and add:
    </para>
<screen>/volume1/VM 10.0.1.0/24  (rw,sync,no_root_squash)</screen>
    <para>
     You need to restart the NFS server:
    </para>
<screen>&prompt.root;systemctl restart nfsserver
&prompt.root;exportfs
/volume1/VM      10.0.1.0/24</screen>
   </sect3>
   <sect3 xml:id="sec.migrate.stepbstep.pool">
    <title>Defining the Pool on the Target Hosts</title>
    <para>
     On each host where you want to migrate the &vmguest;, the pool must
     be defined to be able to access the volume (that contains the Guest
     image). Our NFS server IP address is 10.0.1.99, its share is the
     <filename>/volume1/VM</filename> directory, and we want to get it
     mounted in the <filename>/var/lib/libvirt/images/VM</filename>
     directory. The pool name will be <emphasis>VM</emphasis>. To define
     this pool, create a <filename>VM.xml</filename> file with the following
     content:
    </para>
<screen>&lt;pool type='netfs'&gt;
  &lt;name&gt;VM&lt;/name&gt;
  &lt;source&gt;
    &lt;host name='10.0.1.99'/&gt;
    &lt;dir path='/volume1/VM'/&gt;
    &lt;format type='auto'/&gt;
  &lt;/source&gt;
  &lt;target&gt;
    &lt;path&gt;/var/lib/libvirt/images/VM&lt;/path&gt;
    &lt;permissions&gt;
      &lt;mode&gt;0755&lt;/mode&gt;
      &lt;owner&gt;-1&lt;/owner&gt;
      &lt;group&gt;-1&lt;/group&gt;
    &lt;/permissions&gt;
  &lt;/target&gt;
  &lt;/pool&gt;</screen>
    <para>
     Then load it into &libvirt; using the <command>pool-define</command>
     command:
    </para>
<screen>&prompt.root;virsh pool-define VM.xml</screen>
    <para>
     An alternative way to define this pool is to use the
     <command>virsh</command> command:
    </para>
<screen>&prompt.root;virsh pool-define-as VM --type netfs --source-host 10.0.1.99 \
     --source-path /volume1/VM --target /var/lib/libvirt/images/VM
Pool VM created</screen>
    <para>
     Then the pool can be set to start automatically at host boot (autostart
     option):
    </para>
<screen>virsh # pool-autostart VM
Pool VM marked as autostarted</screen>
    <para>
     If you want to disable the autostart:
    </para>
<screen>virsh # pool-autostart VM --disable
Pool VM unmarked as autostarted</screen>
    <para>
     Check if the pool is present:
    </para>
<screen>virsh # pool-list --all
 Name                 State      Autostart
-------------------------------------------
 default              active     yes
 VM                   active     yes

virsh # pool-info VM
Name:           VM
UUID:           42efe1b3-7eaa-4e24-a06a-ba7c9ee29741
State:          running
Persistent:     yes
Autostart:      yes
Capacity:       2,68 TiB
Allocation:     2,38 TiB
Available:      306,05 GiB</screen>
    <warning>
     <title>Pool Needs to Exist on All Target Hosts</title>
     <para>
      Remember: this pool must be defined on each host where you want to be
      able to migrate your &vmguest;.
     </para>
    </warning>
   </sect3>
   <sect3 xml:id="sec.migrate.stepbstep.volume">
    <title>Creating the Volume</title>
    <para>
     The pool has been defined&mdash;now we need a volume which will
     contain the disk image:
    </para>
<screen>virsh # vol-create-as VM sled12.qcow12 8G --format qcow2
Vol sled12.qcow12 created</screen>
    <para>
     The volume names shown will be used later to install the guest with
     virt-install.
    </para>
   </sect3>
   <sect3 xml:id="sec.migrate.stepbstep.guest">
    <title>Creating the &vmguest;</title>
    <para>
     Let's create a &sled; &vmguest; with the
     <command>virt-install</command> command. The <emphasis>VM</emphasis>
     pool will be specified with the <command>--disk</command> option,
     <emphasis>cache=none</emphasis> is recommended if you do not want to use
     the <command>--unsafe</command> option while doing the migration.
    </para>
<screen>&prompt.root;virt-install --connect qemu:///system --virt-type kvm --name \
   sled12 --memory 1024 --disk vol=VM/sled12.qcow2,cache=none --cdrom \
   /mnt/install/ISO/SLE-12-Desktop-DVD-x86_64-Build0327-Media1.iso --graphics \
   vnc --os-variant sled12
Starting install...
Creating domain...</screen>
   </sect3>
   <sect3 xml:id="sec.migrate.stepbstep.migrate">
    <title>Migrate the &vmguest;</title>
    <para>
     Everything is ready to do the migration now. Run the
     <command>migrate</command> command on the &vmhost; that is currently
     hosting the &vmguest;, and choose the destination.
    </para>
<screen>virsh # migrate --live sled12 --verbose qemu+ssh://<replaceable>IP/Hostname</replaceable>/system
Password:
Migration: [ 12 %]</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="cha.libvirt.admin.monitor">
  <title>Monitoring</title>

  <para/>

  <sect2 xml:id="cha.libvirt.admin.monitor.virt-manager">
   <title>Monitoring with &vmm;</title>
   <para>
    After starting &vmm; and connecting to the &vmhost;, a CPU usage
    graph of all the running guests is displayed.
   </para>
   <para>
    It is also possible to get information about disk and network usage with
    this tool, however, you must first activate this in
    <guimenu>Preferences</guimenu>:
   </para>
   <procedure>
    <step>
     <para>
      Run <command>virt-manager</command>.
     </para>
    </step>
    <step>
     <para>
      Select <menuchoice><guimenu>Edit</guimenu>
      <guimenu>Preferences</guimenu></menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Change the tab from <guimenu>General</guimenu> to
      <guimenu>Polling</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Activate the check boxes for the kind of activity you want to see:
      <guimenu>Poll Disk I/O</guimenu>, <guimenu>Poll Network
      I/O</guimenu>, and <guimenu>Poll Memory stats</guimenu>.
     </para>
    </step>
    <step>
     <para>
      If desired, also change the update interval using <guimenu>Update status
      every n seconds</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Close the <guimenu>Preferences</guimenu> dialog.
     </para>
    </step>
    <step>
     <para>
      Activate the graphs that should be displayed under <menuchoice>
      <guimenu>View</guimenu> <guimenu>Graph</guimenu> </menuchoice>.
     </para>
    </step>
   </procedure>
   <para>
    Afterwards, the disk and network statistics are also displayed in the
    main window of the &vmm;.
   </para>
   <para>
    More precise data is available from the VNC window. Open a VNC window as
    described in <xref linkend="sec.libvirt.managing.console.vnc"/>. Choose
    <guimenu>Details</guimenu> from the toolbar or the
    <guimenu>View</guimenu> menu. The statistics are displayed from the
    <guimenu>Performance</guimenu> entry of the left-hand tree menu.
   </para>
  </sect2>

  <sect2 xml:id="cha.libvirt.admin.monitor.virt-top">
   <title>Monitoring with <command>virt-top</command></title>
   <para>
    <command>virt-top</command> is a command line tool similar to the
    well-known process monitoring tool
    <command>top</command>. <command>virt-top</command> uses libvirt and
    therefore is capable of showing statistics for &vmguest;s running on
    different hypervisors. It is recommended to use
    <command>virt-top</command> instead of hypervisor-specific tools like
    <command>xentop</command>.
   </para>
   <para>
    By default <command>virt-top</command> shows statistics for all running
    &vmguest;s. Among the data that is displayed is the percentage of memory
    used (<literal>%MEM</literal>) and CPU (<literal>%CPU</literal>) and the
    uptime of the guest (<literal>TIME</literal>). The data is updated
    regularly (every three seconds by default). The following shows the output
    on a &vmhost; with seven  &vmguest;s, four of them inactive:
   </para>
   <screen>virt-top 13:40:19 - x86_64 8/8CPU 1283MHz 16067MB 7.6% 0.5%
7 domains, 3 active, 3 running, 0 sleeping, 0 paused, 4 inactive D:0 O:0 X:0
CPU: 6.1%  Mem: 3072 MB (3072 MB by guests)

   ID S RDRQ WRRQ RXBY TXBY %CPU %MEM    TIME   NAME
    7 R  123    1  18K  196  5.8  6.0   0:24.35 sled12_sp1
    6 R    1    0  18K    0  0.2  6.0   0:42.51 sles12_sp1
    5 R    0    0  18K    0  0.1  6.0  85:45.67 opensuse_leap
    -                                           (Ubuntu_1410)
    -                                           (debian_780)
    -                                           (fedora_21)
    -                                           (sles11sp3)</screen>
   <para>
    By default the output is sorted by ID. Use the following key combinations
    to change the sort field:
   </para>
   <simplelist>
    <member>
     <keycombo> <keycap function="shift"/> <keycap>P</keycap> </keycombo>: CPU
     usage
    </member>
    <member>
     <keycombo> <keycap function="shift"/> <keycap>M</keycap> </keycombo>:
     Total memory allocated by the guest
    </member>
    <member>
     <keycombo> <keycap function="shift"/> <keycap>T</keycap> </keycombo>: Time
    </member>
    <member>
     <keycombo> <keycap function="shift"/> <keycap>I</keycap> </keycombo>: ID
    </member>
   </simplelist>
   <para>
    To use any other field for sorting, press <keycombo> <keycap
    function="shift"/> <keycap>F</keycap> </keycombo> and select a field from
    the list. To toggle the sort order, use <keycombo> <keycap
    function="shift"/> <keycap>R</keycap> </keycombo>.
   </para>
   <para>
    <command>virt-top</command> also supports different views on the
    &vmguest;s data, which can be changed on-the-fly by pressing the following
    keys:
   </para>
   <simplelist>
    <member><keycap>0</keycap>: default view</member>
    <member><keycap>1</keycap>: show physical CPUs</member>
    <member><keycap>2</keycap>: show network interfaces</member>
    <member><keycap>3</keycap>: show virtual disks</member>
   </simplelist>
   <para>
    <command>virt-top</command> supports more hot keys to change the view on
    the data and also many command line switches that affect the behavior of
    the program. Refer to <command>man 1 virt-top</command> for details.
   </para>
  </sect2>

  <sect2 xml:id="cha.libvirt.admin.monitor.kvm_stat">
   <title>Monitoring with <command>kvm_stat</command></title>
   <para>
    <command>kvm_stat</command> can be used to trace &kvm; performance
    events. It monitors <filename>/sys/kernel/debug/kvm</filename>, so it
    needs the debugfs to be mounted. On &productname; it should be
    mounted by default. In case it is not mounted, use the following
    command:
   </para>
<screen>mount -t debugfs none /sys/kernel/debug</screen>
   <para>
    <command>kvm_stat</command> can be used in three different modes:
   </para>
<screen condition="kvm4x86">kvm_stat                    # update in 1 second intervals
kvm_stat -1                 # 1 second snapshot
kvm_stat -l &gt; kvmstats.log  # update in 1 second intervals in log format
                            # can be imported to a spreadsheet</screen>
   <example>
    <title>Typical Output of <command>kvm_stat</command></title>
<screen>kvm statistics

 efer_reload                  0       0
 exits                 11378946  218130
 fpu_reload               62144     152
 halt_exits              414866     100
 halt_wakeup             260358      50
 host_state_reload       539650     249
 hypercalls                   0       0
 insn_emulation         6227331  173067
 insn_emulation_fail          0       0
 invlpg                  227281      47
 io_exits                113148      18
 irq_exits               168474     127
 irq_injections          482804     123
 irq_window               51270      18
 largepages                   0       0
 mmio_exits                6925       0
 mmu_cache_miss           71820      19
 mmu_flooded              35420       9
 mmu_pde_zapped           64763      20
 mmu_pte_updated              0       0
 mmu_pte_write           213782      29
 mmu_recycled                 0       0
 mmu_shadow_zapped       128690      17
 mmu_unsync                  46      -1
 nmi_injections               0       0
 nmi_window                   0       0
 pf_fixed               1553821     857
 pf_guest               1018832     562
 remote_tlb_flush        174007      37
 request_irq                  0       0
 signal_exits                 0       0
 tlb_flush               394182     148</screen>
<screen condition="kvm4zSseries">?????</screen>
   </example>
   <para>
    See
    <link xlink:href="http://clalance.blogspot.com/2009/01/kvm-performance-tools.html"/>
    for further information on how to interpret these values.
   </para>
  </sect2>
 </sect1>
</chapter>
