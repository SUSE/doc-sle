<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-raid" xml:lang="en">
 <title>Software RAID configuration</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  The purpose of RAID (redundant array of independent disks) is to combine
  several hard disk partitions into one large virtual hard disk to optimize
  performance, data security, or both. Most RAID controllers use the SCSI
  protocol, because it can address a larger number of hard disks in a more
  effective way than the IDE protocol and is more suitable for parallel
  processing of commands. There are some RAID controllers that support IDE or
  SATA hard disks. Software RAID provides the advantages of RAID systems
  without the additional cost of hardware RAID controllers. However, this
  requires some CPU time and has memory requirements that make it unsuitable
  for real high performance computers.
 </para>
 <important>
  <title>RAID on cluster file systems</title>
  <para>
   Software RAID underneath clustered file systems needs to be set up using a
   cluster multi-device (Cluster MD). Refer to the
   <link xlink:href="https://documentation.suse.com/sle-ha/15-SP2/html/SLE-HA-all/cha-ha-cluster-md.html">
   <citetitle>&haguide; for the &sle; &hasi;</citetitle></link>.
  </para>
 </important>
 <para>
  &sle; offers the option of combining several hard disks into one soft RAID
  system. RAID implies several strategies for combining several hard disks in a
  RAID system, each with different goals, advantages, and characteristics.
  These variations are commonly known as <emphasis>RAID levels</emphasis>.
 </para>
 <sect1 xml:id="sec-raid-intro">
  <title>Understanding RAID levels</title>

  <para>
   This section describes common RAID levels 0, 1, 2, 3, 4, 5, and nested RAID
   levels.
  </para>

  <sect2 xml:id="sec-raid-intro-raid0">
   <title>RAID&nbsp;0</title>
   <para>
    This level improves the performance of your data access by spreading out
    blocks of each file across multiple disks. Actually, this is not a
    RAID, because it does not provide data backup, but the name
    <emphasis>RAID&nbsp;0</emphasis> for this type of system has become the
    norm. With RAID&nbsp;0, two or more hard disks are pooled together. The
    performance is very good, but the RAID system is destroyed and your data
    lost if even one hard disk fails.
   </para>
  </sect2>

  <sect2 xml:id="sec-raid-intro-raid1">
   <title>RAID&nbsp;1</title>
   <para>
    This level provides adequate security for your data, because the data is
    copied to another hard disk 1:1. This is known as <emphasis>hard disk
    mirroring</emphasis>. If a disk is destroyed, a copy of its contents is
    available on another mirrored disk. All disks except one could be damaged
    without endangering your data. However, if damage is not detected, damaged
    data might be mirrored to the correct disk and the data is corrupted that
    way. The writing performance suffers a little in the copying process
    compared to when using single disk access (10 to 20 percent slower), but
    read access is significantly faster in comparison to any one of the normal
    physical hard disks, because the data is duplicated so can be scanned in
    parallel. RAID 1 generally provides nearly twice the read transaction rate
    of single disks and almost the same write transaction rate as single disks.
   </para>
  </sect2>

  <sect2 xml:id="sec-raid-intro-raid23">
   <title>RAID&nbsp;2 and RAID&nbsp;3</title>
   <para>
    These are not typical RAID implementations. Level&nbsp;2 stripes data at
    the bit level rather than the block level. Level&nbsp;3 provides byte-level
    striping with a dedicated parity disk and cannot service simultaneous
    multiple requests. Both levels are rarely used.
   </para>
  </sect2>

  <sect2 xml:id="sec-raid-intro-raid4">
   <title>RAID&nbsp;4</title>
   <para>
    Level&nbsp;4 provides block-level striping like Level&nbsp;0 combined with
    a dedicated parity disk. If a data disk fails, the parity data is used to
    create a replacement disk. However, the parity disk might create a
    bottleneck for write access. Nevertheless, Level&nbsp;4 is sometimes used.
   </para>
  </sect2>

  <sect2 xml:id="sec-raid-intro-raid5">
   <title>RAID&nbsp;5</title>
   <para>
    RAID&nbsp;5 is an optimized compromise between Level&nbsp;0 and
    Level&nbsp;1 in terms of performance and redundancy. The hard disk space
    equals the number of disks used minus one. The data is distributed over the
    hard disks as with RAID&nbsp;0. <emphasis>Parity blocks</emphasis>, created
    on one of the partitions, are there for security reasons. They are linked
    to each other with XOR, enabling the contents to be reconstructed by the
    corresponding parity block in case of system failure. With RAID&nbsp;5, no
    more than one hard disk can fail at the same time. If one hard disk fails,
    it must be replaced when possible to avoid the risk of losing data.
   </para>
  </sect2>

  <sect2 xml:id="sec-raid-intro-raid6">
   <title>RAID&nbsp;6</title>
   <para>
    RAID&nbsp;6 is an extension of RAID&nbsp;5 that allows for
    additional fault tolerance by using a second independent distributed parity
    scheme (dual parity). Even if two of the hard disks fail during the data
    recovery process, the system continues to be operational, with no data
    loss.
   </para>
   <para>
    RAID&nbsp;6 provides for extremely high data fault tolerance by sustaining
    multiple simultaneous drive failures. It handles the loss of any two
    devices without data loss. Accordingly, it requires N+2 drives to store N
    drives worth of data. It requires a minimum of four devices.
   </para>
   <para>
    The performance for RAID&nbsp;6 is slightly lower but comparable to
    RAID&nbsp;5 in normal mode and single disk failure mode. It is very slow in
    dual disk failure mode. A RAID&nbsp;6 configuration needs a considerable
    amount of CPU time and memory for write operations.
   </para>
   <table>
    <title>Comparison of RAID 5 and RAID 6</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="3334*"/>
     <colspec colnum="2" colname="2" colwidth="3334*"/>
     <colspec colnum="3" colname="3" colwidth="3334*"/>
     <thead>
      <row>
       <entry>
        <para>
         Feature
        </para>
       </entry>
       <entry>
        <para>
         RAID 5
        </para>
       </entry>
       <entry>
        <para>
         RAID 6
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         Number of devices
        </para>
       </entry>
       <entry>
        <para>
         N+1, minimum of 3
        </para>
       </entry>
       <entry>
        <para>
         N+2, minimum of 4
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Parity
        </para>
       </entry>
       <entry>
        <para>
         Distributed, single
        </para>
       </entry>
       <entry>
        <para>
         Distributed, dual
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Performance
        </para>
       </entry>
       <entry>
        <para>
         Medium impact on write and rebuild
        </para>
       </entry>
       <entry>
        <para>
         More impact on sequential write than RAID 5
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Fault-tolerance
        </para>
       </entry>
       <entry>
        <para>
         Failure of one component device
        </para>
       </entry>
       <entry>
        <para>
         Failure of two component devices
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </sect2>

  <sect2 xml:id="sec-raid-intro-raid-nested">
   <title>Nested and complex RAID levels</title>
   <para>
    Other RAID levels have been developed, such as RAIDn, RAID&nbsp;10,
    RAID&nbsp;0+1, RAID&nbsp;30, and RAID&nbsp;50. Some are proprietary
    implementations created by hardware vendors. Examples for creating
    RAID&nbsp;10 configurations can be found in <xref linkend="cha-raid10"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-raid-yast">
  <title>Soft RAID configuration with &yast;</title>

  <para>
   The &yast; soft RAID configuration can be reached from the &yast; Expert
   Partitioner. This partitioning tool also enables you to edit and delete
   existing partitions and create new ones that should be used with soft RAID.
   These instructions apply on setting up RAID levels 0, 1, 5, and 6. Setting
   up RAID&nbsp;10 configurations is explained in <xref linkend="cha-raid10"/>.
  </para>

  <procedure>
   <step>
    <para>
     Launch &yast; and open the <guimenu>Partitioner</guimenu>.
    </para>
   </step>
   <step>
    <para>
     If necessary, create partitions that should be used with your RAID
     configuration. Do not format them and set the partition type to
     <guimenu>0xFD Linux RAID</guimenu>. When using existing partitions it is
     not necessary to change their partition type&mdash;&yast; will
     automatically do so. Refer to <xref linkend="sec-expert-partitioner"/>
     for details.
    </para>
    <para>
     It is strongly recommended to use partitions stored on different hard
     disks to decrease the risk of losing data if one is defective (RAID&nbsp;1
     and 5) and to optimize the performance of RAID&nbsp;0.
    </para>
    <para>
     For RAID&nbsp;0 at least two partitions are needed. RAID 1 requires
     exactly two partitions, while at least three partitions are required for
     RAID&nbsp;5. A RAID&nbsp;6 setup requires at least four partitions. It is
     recommended to use only partitions of the same size because each segment
     can contribute only the same amount of space as the smallest sized
     partition.
    </para>
   </step>
   <step>
    <para>
     In the left panel, select <guimenu>RAID</guimenu>.
    </para>
    <para>
     A list of existing RAID configurations opens in the right panel.
    </para>
   </step>
   <step>
    <para>
     At the lower left of the RAID page, click <guimenu>Add RAID</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Select a <guimenu>RAID Type</guimenu> and <guimenu>Add</guimenu> an
     appropriate number of partitions from the <guimenu>Available
     Devices</guimenu> dialog.
    </para>
    <para>
     You can optionally assign a <guimenu>RAID Name</guimenu> to your RAID. It
     will make it available as
     <filename>/dev/md/<replaceable>NAME</replaceable></filename>. See
     <xref linkend="sec-raid-yast-names"/> for more information.
    </para>
    <figure xml:id="fig-yast2-raid3">
     <title>Example RAID 5 configuration</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_raid3_a.png" width="80%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_raid3_a.png" width="100%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
     Proceed with <guimenu>Next</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Select the <guimenu>Chunk Size</guimenu> and, if applicable, the
     <guimenu>Parity Algorithm</guimenu>. The optimal chunk size depends on the
     type of data and the type of RAID. See
     <link xlink:href="https://raid.wiki.kernel.org/index.php/RAID_setup#Chunk_sizes"/>
     for more information. More information on parity algorithms can be found
     with <command>man 8 mdadm</command> when searching for the
     <option>--layout</option> option. If unsure, stick with the defaults.
    </para>
   </step>
   <step>
    <para>
     Choose a <guimenu>Role</guimenu> for the volume. Your choice here only
     affects the default values for the upcoming dialog. They can be changed in
     the next step. If in doubt, choose <guimenu>Raw Volume
     (Unformatted)</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Under <guimenu>Formatting Options</guimenu>, select <guimenu>Format
     Partition</guimenu>, then select the <guimenu>File system</guimenu>. The
     content of the <guimenu>Options</guimenu> menu depends on the file system.
     Usually there is no need to change the defaults.
    </para>
    <para>
     Under <guimenu>Mounting Options</guimenu>, select <guimenu>Mount
     partition</guimenu>, then select the mount point. Click <guimenu>Fstab
     Options</guimenu> to add special mounting options for the volume.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Finish</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Next</guimenu>, verify that the changes are listed, then
     click <guimenu>Finish</guimenu>.
    </para>
   </step>
  </procedure>


  <important>
   <title>RAID on disks</title>
   <para>
    While the partitioner makes it possible to create a RAID on top of disks
    instead of partitions, we do not recommend this approach for a number of
    reasons. Installing a bootloader on such RAID is not supported, so you need to
    use a separate device for booting. Tools like <package>fdisk</package>
    and <package>parted</package> do not work properly with such RAIDs, which may
    lead to incorrect diagnosis and actions by a person who is unaware of the
    RAID's particular setup.
   </para>
  </important>


  <sect2 xml:id="sec-raid-yast-names">
   <title>RAID names</title>
   <para>
    By default, software RAID devices have numeric names following the pattern
    <literal>mdN</literal>, where <literal>N</literal> is a number. As such
    they can be accessed as, for example, <filename>/dev/md127</filename> and
    are listed as <literal>md127</literal> in <filename>/proc/mdstat</filename>
    and <filename>/proc/partitions</filename>. Working with these names can be
    clumsy. &productname; offers two ways to work around this problem:
   </para>
   <variablelist>
    <varlistentry>
     <term>Providing a named link to the device</term>
     <listitem>
      <para>
       You can optionally specify a name for the RAID device when creating it
       with &yast; or on the command line with <command>mdadm --create
       '/dev/md/</command> <replaceable>NAME</replaceable>'. The device name
       will still be <literal>mdN</literal>, but a link
       <filename>/dev/md/<replaceable>NAME</replaceable></filename> will be
       created:
      </para>
<screen>&prompt.user;ls -og /dev/md
total 0
lrwxrwxrwx 1 8 Dec  9 15:11 myRAID -&gt; ../md127</screen>
      <para>
       The device will still be listed as <literal>md127</literal> under
       <filename>/proc</filename>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Providing a named device</term>
     <listitem>
      <para>
       In case a named link to the device is not sufficient for your setup, add
       the line <literal>CREATE names=yes</literal> to
       <filename>/etc/mdadm.conf</filename> by running the following command:
      </para>
<screen>&prompt.sudo;echo "CREATE names=yes" &gt;&gt; /etc/mdadm.conf</screen>
      <para>
       This will cause names like <literal>myRAID</literal> to be used as a
       <quote>real</quote> device name. The device will not only be accessible
       at <filename>/dev/myRAID</filename>, but also be listed as
       <literal>myRAID</literal> under <filename>/proc</filename>. Note that
       this will only apply to RAIDs configured after the change to the
       configuration file. Active RAIDs will continue to use the
       <literal>mdN</literal> names until they get stopped and re-assembled.
      </para>
      <warning>
       <title>Incompatible tools</title>
       <para>
        Not all tools may support named RAID devices. In case a tool expects a
        RAID device to be named <literal>mdN</literal>, it will fail to
        identify the devices.
       </para>
      </warning>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 
 <sect1 xml:id="sec-raid-counters">
  <title>Monitoring software RAIDs</title>
   <para>
    You can run <command>mdadm</command> as a daemon in the
    <literal>monitor</literal> mode to monitor your software RAID. In the
    <literal>monitor</literal> mode, <command>mdadm</command> performs regular
    checks on the array for disk failures. If there is a failure,
    <command>mdadm</command> sends an email to the administrator. To define the
    time interval of the checks, run the following command:
   </para>
<screen>mdadm --monitor --mail=root@localhost --delay=1800 /dev/md2</screen>
 <para>
  The command above turns on monitoring of the <literal>/dev/md2</literal> array
  in intervals of 1800&nbsp;seconds. In the event of a failure, an email will be
  sent to <literal>root@localhost</literal>.
 </para>
 <note>
  <title>RAID checks are enabled by default</title>
 	<para>
 	 The RAID checks are enabled by default. It may happen that the interval
 	 between each check is not long enough and you may encounter warnings. Thus,
 	 you can increase the interval by setting a higher value with the
 	 <literal>delay</literal> option.
 	</para>
 </note>
 
 </sect1>
 <sect1 xml:id="sec-raid-more">
  <title>More information</title>

  <para>
   Configuration instructions and more details for soft RAID can be found in
   the HOWTOs at:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <citetitle>The Linux RAID wiki</citetitle>:
     <link xlink:href="https://raid.wiki.kernel.org/"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <citetitle>The Software RAID HOWTO</citetitle> in the
     <filename>/usr/share/doc/packages/mdadm/Software-RAID.HOWTO.html</filename>
     file
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Linux RAID mailing lists are also available, such as
   <citetitle>linux-raid</citetitle> at
   <link xlink:href="http://marc.info/?l=linux-raid"/>.
  </para>
 </sect1>
</chapter>
