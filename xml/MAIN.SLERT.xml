<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
                 type="text/xml"
                 title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<article version="5.0" xml:lang="en"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
<!--
 o CPU set: http://www.nabble.com/cpuset- - -question-t476909.html
 -->
 <title>Quick Start</title>
 <subtitle>&productname; &productnumber;</subtitle>
 <info>
      <productnumber>&productnumber;</productnumber>
      <productname>&productname;</productname>
      <abstract>
        <para>
    &slerte; is an add-on to &slereg; that allows you to run tasks
    which require deterministic real-time processing, in a &sle;
    environment. &slerte; meets this requirement by offering several
    different options for CPU and I/O scheduling, CPU shielding and setting
    CPU affinities to processes.
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
          <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
          <dm:component>Documentation</dm:component>
          <dm:product>SUSE Linux Enterprise Real Time Extension 11 SP4 (SLERTE 11 SP4)</dm:product>
          <dm:assignee>taroth@suse.com</dm:assignee>
        </dm:bugtracker>
      </dm:docmanager>
    </info>
    <remark>dgollub: Keep this installation section ONLY if this PDF makes it on the /docs directoy on the media!</remark>
 <sect1>
  <title>Installing &slerte;</title>

  <para>
   To install &slerte; 11 SP1, start a regular &sls; 11 SP1
   installation and select &slerte; 11 SP1 as an Add-On product during
   the installation. Alternately, if &sls; is already installed, you can
   start the Add-On Product installation from YaST. However, you need to
   select the -rt kernel flavor as the default using the YaST Boot Loader
   configurator, as in this alternate case, that is not done automatically.
  </para>

  <para>
   &slerte; always needs a &sls; SP1 base, it cannot be installed in
   stand-alone mode. Refer to the &sls; &admin; manual, Section
   <quote>Installing Add-On Products</quote> at
   <link xlink:href="http://www.suse.com/doc/sles11/book_sle_deployment/?page=/documentation/sles11/book_sle_deployment/data/cha_add-ons.html"/>
   to learn more about installing add-on products.
  </para>

  <para>
   The following sections provide a brief introduction to the tools and
   possibilities of &slerte;.
  </para>
 </sect1>
 <sect1>
  <title>Managing CPU Sets with <command>cset</command></title>

  <para>
   In some circumstances, it is beneficial to be able to run specific tasks
   only on defined CPUs. For this reason, the Linux kernel provides a
   feature called cpuset. The feature cpuset provides the means to do a so
   called <quote>soft partitioning</quote> of the system. Dedicated CPUs,
   together with some predefined memory, work on several tasks.
  </para>

  <para>
   The command <command>cset</command> provides the high level functionality
   to set up and manipulate CPU Sets.
  </para>

  <para>
   <command>cset</command> consists of one super command called
   <literal>shield</literal> and the regular commands <literal>set</literal>
   and <literal>proc</literal>. The purpose of the super command
   <literal>shield</literal> is to create a common CPU shielding setup
   within one step by combining regular commands. An example for setting up
   a CPU shield is:
  </para>

<!-- shield cpu -->

<screen>cset shield --cpu=3</screen>

  <para>
   This will shield CPU3 and keep on a 4-way machine CPU0-CPU2 unshielded.
   The argument of the <option>--cpu</option> parameter accepts comma
   separated lists of CPUs including range specifications:
  </para>

<screen>cset --cpu=1,3,5-7</screen>

  <para>
   On a 8-way machine this command will shield CPU1, CPU3, CPU5, CPU6 and
   CPU7. CPU0, CPU2 and CPU4 will remain unshielded. Already existing CPU
   shields could be extended by the same command. For example adding CPU4 to
   the mentioned CPU set can be done in the following way:
  </para>

<screen>cset --cpu=1,3-7</screen>

  <para>
   CPU1, CPU3, CPU5 to CPU6 were already shielded and only CPU4 will
   additionally be shielded. Technically the command is updating the current
   CPU shield schema. If the number of shielded CPUs should be reduced and
   CPU1 should be unshielded this is done by calling:
  </para>

<screen>cset --cpu=3-7</screen>

  <para>
   Now only CPU3, CPU4, CPU5, CPU6 and CPU7 are shielded and CPU0, CPU1 and
   CPU2 are available for system usage.
  </para>

<!-- show shield CPUs  -->

  <para>
   Once the CPU shielding is set up you can display the current
   configuration by running <command>cset shield</command> without
   additional parameters:
  </para>

<!-- bg: sorry, screen too long for flyer layout -->

<screen>cset shield
cset: --&gt; shielding system active with
cset: "system" cpuset of: 0-2 cpu, with: 47
cset: "user" cpuset of:  3-7 cpu,  with: 0
</screen>

  <para>
   By default, CPU shielding consists at least of three cpusets:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>root</literal> exists always and contains all available CPUs.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>system</literal> is the cpuset of unshielded CPUs.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>user</literal> is the cpuset of shielded CPUs
    </para>
   </listitem>
  </itemizedlist>

<!-- shield process -->

  <para>
   Certain processes or groups of processes can be assigned to a shielded
   cpuset, once the CPU set is created. To start a new process in the
   shielded CPU set use the <command>--exec</command> parameter:
  </para>

<screen>cset --exec &lt;application&gt;</screen>

  <para>
   To move already running processes to the shielded CPU set use the
   <command>--shield</command> parameter which accepts a comma separated
   list and range specifications of PIDs:
  </para>

<screen>cset --shield=1,2,600-700</screen>

  <para>
   This moves processes with PID 1, 2 and from 600 to 700 to the shielded
   CPU set. If there is a gap in the range from 600 to 700, then only those
   available process will be moved to the shield without warning.
   <command>cset</command> handles threads like processes and will also
   interpret TIDs and assign them to the required CPU set.
  </para>

<!-- shield DANGER note -  stolen from "cset help shield" -->

<!-- bg: modified -->

  <warning>
   <para>
    Note that there is no checking of the processes you request to
    move into the shield with the <command>--shield</command> command. This
    means that the tool will happily move any process, even kernel threads,
    that are bound to specific CPUs with this command. You can cause a
    complete system lockup by indiscriminately specifying arbitrary PIDs to
    the <option>--shield</option> command.
   </para>
  </warning>

<!-- show shield processes -->

  <para>
   The number of currently shielded processes are shown with the same
   command that is used to show the current CPU shield setup. Additionally
   adding the <command>--verbose</command> parameter lists shielded and
   unshielded processes:
  </para>

<screen>cset shield --verbose
cset: --&gt; shielding system active with
cset: "system" cpuset of: 0-2,4-15 cpu, with:
   USER       PID  PPID S TASK NAME
      -------- ----- ----- - ---------
         root         1     0 S init [3]
[...]

cset: "user" cpuset of:    3 cpu, with: 1
   USER       PID  PPID S TASK NAME
      -------- ----- ----- - ---------
         root     10202 10170 S application
</screen>

<!-- unshield process -->

  <para>
   To remove a process (or group of processes) from the CPU shield use the
   <command>--unshield</command> parameter. The argument for the
   <command>--unshield</command> is similar to the
   <command>--shield</command> parameter, which accepts a comma separated
   list of PIDs/TIDs and range specifications:
  </para>

<screen>cset --unshield=2,650-655</screen>

  <para>
   This will unshield the process with the PID 2 and the processes in range
   of 650 and 655.
  </para>

<!-- reset CPU set -->

  <para>
   Deleting CPU sets is done with the <command>cset</command> parameter
   <option>--reset</option>. This will unshield all CPUs and migrate
   dedicated processes to all available CPUs again.
  </para>

<!-- cset help shield -->

  <para>
   For more detailed information about options and parameters of the
   subcommand <command>shield</command>, consult the help of
   <command>cset</command> by running:
  </para>

<screen>cset help shield</screen>

<!-- basic cset commands -->

  <para>
   More detailed configuration of cpusets can be done with the
   <command>cset</command> commands <option>set</option> and
   <option>proc</option>.
  </para>

<!-- cset set -->

  <para>
   The subcommand <option>set</option> is used to create, modify and destroy
   cpusets. Compared to the supercommand <option>shield</option>, the
   <option>set</option> subcommand is additionally able to assign memory
   nodes for NUMA machines.
  </para>

  <para>
   Besides assigning memory nodes, the subcommand <command>set</command> can
   be used to create cpusets in a tree-like structure, rooted at the
   <literal>root</literal> cpuset.
  </para>

<!-- cset set .... assinging CPU -->

  <para>
   To create a cpuset with the subcommand <command>set</command> you only
   need to specify the CPUs which should be used by a comma separated list
   or a range specification:
  </para>

<screen>cset set --cpu=1-7 "/one"</screen>

  <para>
   This command will create a cpuset called <quote>one</quote> with assigned
   CPUs from CPU1 to CPU7. To specify a new cpuset called <quote>two</quote>
   and that is a subset of <quote>one</quote>, proceed as follows:
  </para>

<screen>cset set --cpu=6 "/one/two"</screen>

<!-- stolen from "cset help set" -->

<!-- bg: modified -->

  <para>
   Cpusets follow certain rules. Children can only include CPUs that the
   parents already have. If you try to specify a different cpuset, the
   kernel cpuset subsystem will not let you create that cpuset. For example,
   if you create a cpuset that contains CPU3, and then attempt to create a
   child of that cpuset with a CPU other than 3, you will get an error, and
   the cpuset will not be created. The resulting error is somewhat cryptic
   and is usually <quote>Permission denied</quote>.
  </para>

<!-- cset set ... list and destroy -->

  <para>
   <remark>mdejmek: complete sentence here</remark>
   To list a table containing useful information like cpu list and memory
   list use the <option>-r</option> parameter. The <quote>-X</quote> column
   shows the exclusive state of CPU or memory. The <quote>path</quote>
   column shows the real path in the virtual file system
   <literal>cpuset</literal>.
  </para>

<screen>cpuset set -r</screen>

<!-- cset set .... assinging memory -->

  <para>
   On NUMA machines memory nodes can be assigned to a cpuset similar to
   CPUs. The <command>--mem</command> parameter of the subcommand
   <command>set</command> allows a comma separated and inclusive range
   specification of memory nodes. This example will assign MEM1, MEM3, MEM4,
   MEM5 and MEM6 to the cpuset <quote>new_set</quote>:
  </para>

<screen>cset set --mem=1,3-6 new_set</screen>

  <para>
   <remark>mdejmek: are these two parameters or one+argument?</remark>
   Additionally, with the <option>--cpu_exclusive</option> and
   <option>--mem_exclusive</option> parameters (without any additional
   arguments) set the CPUs or memory nodes exclusive to a cpuset:
  </para>

<screen>cset set --cpu_exclusive "/one"</screen>

  <para>
   <remark>dgollub: Hit tiny bug while writing this - bnc#380819 - hide this if
     not fixed?!</remark>
   The status of exclusive state of CPU or memory is shown in the "-X"
   column when running:
  </para>

<screen>cset set -r </screen>

<!-- cset help set -->

  <para>
   For more detailed information about options and parameters of the
   subcommand <option>set</option>, consult the help of
   <command>cset</command> by running:
  </para>

<screen>cset help set</screen>

<!-- cset proc .... exec -->

  <para>
   Once the cpuset is initialized, the subcommand <option>proc</option> can
   start processes on certain cpusets with the <option>--exec</option>
   parameter. The following will start the application
   <literal>fastapp</literal> within the cpuset <literal>new_set</literal>:
  </para>

<screen>cset proc --exec --set new_set fastapp</screen>

<!-- cset proc ... move -->

  <para>
   To move an already running process inside an already existing cpuset use
   the parameter <command>--move</command>, which accepts a comma separated
   list and range specifications of PIDs. The following command will move
   processes with PID 2442 and within range of 3000 to 3200 into the cpuset
   <literal>new_set</literal>:
  </para>

<screen>cset proc --move 2442,3000-3200 new_set</screen>

<!-- cset proc ... list -->

  <para>
   Listing processes running within a specific cpuset can be done by using
   the parameter <option>--list</option>.
  </para>

<screen>cset proc --list new_set</screen>

<!-- cset proc ... fromset toset -->

  <para>
   The subcommand <option>proc</option> is also able to move the entire list
   of processes within one cpuset to another cpuset by using the parameters
   <option>--fromset</option> and <option>--toset</option>. This will move
   all process assigned to <literal>old_set</literal> and assign them to
   <literal>new_set</literal>:
  </para>

<screen>cset proc --move --fromset old_set \
   --toset new_set</screen>

<!-- cset help proc -->

  <para>
   For more detailed information about options and parameters of the
   subcommand <command>proc</command>, consult the help of
   <command>cset</command> by running:
  </para>

<screen>cset help proc</screen>
 </sect1>
<!-- man chrt(1) -->
 <sect1>
  <title>Set real-time attributes of a process with <command>chrt</command></title>

  <para>
   The <command>chrt</command> command allows to manipulate the real-time
   attributes, like scheduling policy and priority, of an already running
   process or to execute a new process with specified real-time attributes.
  </para>

  <para>
   It is highly recommend for applications which should experience the full
   advantages of real-time and do not use real-time specific
   attributes by their own. To get these full real-time experiences, this
   application should be called with the <command>chrt</command> command and
   the right set of scheduler policy and priority parameters.
  </para>

  <para>
   With the following command line all running processes with their
   real-time specific attributes are shown. <literal>class</literal> shows
   the current scheduler policy and <literal>rtprio</literal> the real-time
   priority:
  </para>

<screen>ps -eo pid,tid,class,rtprio,comm
...
 1437  1437 FF      40  fastapp
</screen>

  <para>
   The above truncated example shows the <literal>fastapp</literal> process
   with PID 1437 running and with scheduler policy
   <literal>SCHED_FIFO</literal> and priority 40. Scheduler policy
   abbreviations are:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     TS - SCHED_OTHER
    </para>
   </listitem>
   <listitem>
    <para>
     FF - SCHED_FIFO
    </para>
   </listitem>
   <listitem>
    <para>
     RR - SCHED_RR
    </para>
   </listitem>
  </itemizedlist>

  <para>
   It is also possible to get the current scheduler policy and priority of
   single processes by passing to <command>chrt</command> the PID of the
   process with the parameter <command>-p</command>:
  </para>

<screen>chrt -p 1437</screen>

  <para>
   Scheduler policies have different minimum and maximum priority values.
   Minimum and maximum values for each available scheduler policy can be
   retrieved with <command>chrt</command>:
  </para>

<screen>chrt -m</screen>

  <para>
   To change the scheduler policy and the priority of a running process,
   <command>chrt</command> provides parameter <command>--fifo</command> for
   <literal>SCHED_FIFO</literal>, <command>--rr</command> for
   <literal>SCHED_RR</literal> and <command>--other</command> for
   <literal>SCHED_OTHER</literal>. This example will change the scheduler
   policy to <literal>SCHED_FIFO</literal> with priority 42 for PID 1437:
  </para>

<screen>chrt --fifo -p 42 1437</screen>

  <warning>
   <para>
    Handle changing of real-time attributes of processes with care.
    Increasing the priority of certain processes can harm the entire system,
    depending on the behavior of the process. In some cases, this could lead
    to a complete system lockup or bad influence on certain devices.
   </para>
  </warning>

<!-- man chrt(1) -->

  <para>
   For more information about chrt, consult the manual page of chrt:
  </para>

<screen>man 1 chrt</screen>
 </sect1>
<!--

  <sect1>
  <title>Executing Processes with <command>run</command></title>

  <para>
   The command <command>run</command> creates an environment
   with a special scheduler or priority while starting processes.
   The <command>run</command> command
   is a very powerful utility that can set most of the needed options either
   when starting a new process, or for a process already running. General
   options to the <command>run</command> command are:
  </para>

  <variablelist>
   <varlistentry>

    <term><option>bias=<replaceable>CPULIST</replaceable></option></term>
  <listitem>
     <para>
      Define the CPU affinity of a process. When defining a CPU list
      to a new command, the resulting process will only run on CPUs in
      this list.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

  <term><option>policy=<replaceable>policy</replaceable></option></term>
    <listitem>
     <para>
      Select one of the available schedulers for a given process. For
      &slerte;, use one of the following schedulers: <literal>SCHED_FIFO</literal>,
      <literal>SCHED_RR</literal>, or <literal>SCHED_OTHER</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

  <term><option>priority=<replaceable>priority</replaceable></option>
    </term>
    <listitem>
     <para>
      Set the priority of a process. Lower numerical values mean a lower
      priority from the scheduler.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

  <term><option>quantum=<replaceable>quantum</replaceable></option></term>
    <listitem>
     <para>
      Adjust the quantum of the schedulers <literal>SCHED_RR</literal> and
      <literal>SCHED_OTHER</literal>. This is the equivalent to the
      <literal>nice</literal> value
      generally known from <literal>SCHED_OTHER</literal>. To obtain a table
      of nice values and their respective values in milliseconds, run the
      command

  <command>run quantum=list</command>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

    <term><option>info</option></term>
    <listitem>
     <para>
      Print information about the environment the selected processes
      run in. The printed values contain the process id (Pid), the
      thread id (Tid), the CPU list to use (Bias), the CPU this
      process is running on (Actual), the selected scheduler (Policy),
      the values for priority (Pri), and quantum (Nice) as well as the
      name of the command.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For Non-Uniform Memory Access (NUMA) architectures, the

  <command>run mempolicy=MEMPOLICY_SPECIFIER</command> command can
   also be used to set memory specific options. Choose one of the
   following options:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>bind[=<replaceable>list</replaceable>]</option></term>
    <listitem>
     <para>
      Use the memory local to the CPUs in the given list. If
      <literal>list</literal> is omitted, use the CPUs given in

  <literal>bias</literal> which is required in this case.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>interleave=<replaceable>list</replaceable></option></term>
    <listitem>
     <para>
      Use memory that is attached with optimized bandwidth instead of
      optimized latency.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>preferred=<replaceable>cpu</replaceable></option></term>
    <listitem>
     <para>
      Always try to allocate memory on the given CPU and fall back to
      other nodes if the preferred node is low on memory.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>default</option></term>
    <listitem>
     <para>
      Allocate memory on the node of the CPU that triggered the
      allocation.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>nodes</option></term>
    <listitem>
     <para>
      Display the different NUMA nodes as well as the current memory
      usage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>view</option></term>
    <listitem>
     <para>
      Print the memory policy of the current process.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   All of the above commands may be used when starting new commands, but also
   to change the environment of processes already running. To change the
   environment of processes already running, several options are available
   that can be used to select single processes or groups of processes:
  </para>

  <variablelist>
   <varlistentry>

  <term><option>all</option></term>
    <listitem>
     <para>
      Select all processes running on the system. Use with care!
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

  <term><option>pid=<replaceable>list</replaceable></option></term>
    <listitem>
     <para>
      Change the environment of the processes specified by
      <replaceable>list</replaceable>. This also applies to all subthreads of the
      processes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

  <term><option>tid=<replaceable>list</replaceable></option></term>
    <listitem>
     <para>
      Change the environment for the specified TIDs. Only the listed
      threads are affected, siblings that are not listed will not be
      changed.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

  <term><option>group=<replaceable>list</replaceable></option></term>
    <listitem>
     <para>
      If process groups are set, these can be selected using this
      parameter.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

  <term><option>user=<replaceable>list</replaceable></option></term>
    <listitem>
     <para>
      Change all processes of selected users. Specify the users by account or
      numerical user id.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

  <term><option>name=<replaceable>list</replaceable></option></term>
    <listitem>
     <para>
      Update the environment for running processes whose names are contained
      in <replaceable>list</replaceable>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1>
  <title>Shielding CPUs</title>

  <para>
   In standard workstation environments, it is a good practice to distribute
   the system tasks over different CPUs to optimize performance. However, this
   also creates unpredictable latencies that should be avoided in a Real-Time
   environment, if possible. One way to solve this problem is to define
   which tasks may run on a specified CPU.
   To prevent interrupts
   from being executed on this CPU, the CPU may be shielded.
   Furthermore, if you intend to only run defined processes on
   selected CPUs, you can shield this set of CPUs from
   all processes. To run processes on those CPUs, the processes must
   set a CPU affinity to the shielded CPUs.
  </para>

  <para>
  To mark one or several CPUs as shielded, use the
   <command>shield</command> command and specify a list of CPUs.
   The numbers to use in CPULIST can be obtained from the <literal>processor
    :</literal> entry in the <filename>/proc/cpuinfo</filename> file. To
   define ranges of several CPUs, use a <quote>&mdash;</quote> between the
   numbers as in <literal>2&mdash;7</literal>.
  </para>
  <para>
   This approach is only useful on systems with at least two virtual CPUs. The
   <command>shield</command> command takes the
   following parameters:
  </para>
  <variablelist>
   <varlistentry>

  <term><option>irq=<replaceable>CPULIST</replaceable></option></term>
    <listitem>
     <para>
      CPUs in <literal>CPULIST</literal> will only run selected interrupts if
      the interrupt has an affinity to the CPU specified. When using
      this feature, you should disable the irq_balancer because it is
      not aware of CPU affinities.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

  <term><option>loc=<replaceable>CPULIST</replaceable></option></term>
    <listitem>
     <para>
      The CPUs specified should not run the local interrupt. Without
      local interrupt, the high resolution timer will not work on the
      <literal>CPULIST</literal> and as a consequence, some other
      functionality like system accounting will not work either.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

  <term><option>proc=<replaceable>CPULIST</replaceable></option></term>
    <listitem>
     <para>
      Use the CPUs in <literal>CPULIST</literal> only for
      processes that have a defined affinity to those CPUs. If several
      processes have this affinity, they may still have different
      priorities to define background and high priority
      processes. CPU affinities may be set by the administrator, or
      even by some program, if it has the capability
      <literal>CAP_SYS_NICE</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

  <term><option>all=<replaceable>CPULIST</replaceable></option></term>
    <listitem>
     <para>
      Set all of the available shielding attributes for this
      <literal>CPULIST</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

  <term><option>reset</option></term>
    <listitem>
     <para>
      Remove the shielding attributes for all CPUs.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>

  <term><option>current</option></term>
    <listitem>
     <para>
      Print the current active shielding attributes for all CPUs.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   To run a single process on a dedicated CPU, you must both shield the CPU
   from all other processes and set the CPU affinity of this process to this
   CPU. For example, if you want to run a process as the only process on
   processor <literal>#1</literal>, use the following commands:
  </para>

  <screen>shield proc=1
  run  bias=1 &lt;command&gt;
  </screen>
  </sect1>-->
<!--
 <sect1>
  <title>Using CPU Sets</title>
  <para>
   In some circumstances, it is beneficial to be able to run specific
   tasks only on defined CPUs. For this reason, the linux kernel
   provides a feature called cpuset. Cpusets provide the means to do a
   so called <quote>soft partitioning</quote> of the system. Dedicated
   CPUs, together with some predefined memory, work on several
   tasks.
  </para>
  <para>
   All systems have at least one cpuset that is called
   <filename>/</filename>. To retrieve the cpuset of a specific task
   with a certain process id <replaceable>pid</replaceable>, use the command
   <command>cat /proc/<replaceable>pid</replaceable>/cpuset</command>.
   To add, remove, or manage cpusets, a special file system with
   file system type <literal>cpuset</literal> is available. Before you
   can use this file system type, mount it to
   <filename>/dev/cpuset</filename> with the following
   commands:
  </para><screen>mkdir /dev/cpuset
mount -t cpuset none /dev/cpuset</screen>
-->
<!--
<para>
   By default, this directory will look like this:
  </para>
  <screen># ls /dev/cpuset/
cpu_exclusive  memory_migrate           memory_spread_page  notify_on_release
cpus           memory_pressure          memory_spread_slab  tasks
mem_exclusive  memory_pressure_enabled  mems</screen>
-->
<!--
  <para>
   Every cpuset has the following entries:
  </para>
  <variablelist>
   <varlistentry>
    <term>cpus</term>
    <listitem>
     <para>
      A list of CPUs available for the current cpuset. Ranges of CPUs
      are displayed with a dash between the first and the last CPU,
      else CPUs are represented by a comma separated list of CPU numbers.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mems</term>
    <listitem>
     <para>
      A list of memory nodes available to the current cpuset.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>memory_migrate</term>
    <listitem>
     <para>
      This flag determines if memory pages should be moved to the new
      configuration, in case the memory configuration of the cpuset changes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>cpu_exclusive</term>
    -->
<!-- http://lwn.net/Articles/80911/ -->
<!--
    <listitem>
     <para>
      Defines if this cpuset becomes a scheduling domain, that shares
      properties and policies.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mem_exclusive</term>
    <listitem>
     <para>
      Determines if userspace tasks in this cpuset can only get their
      memory from the memory assigned to this cpuset.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>tasks</term>
    <listitem>
     <para>
      Contains the process ids of all tasks running in this cpuset.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>notify_on_release</term>
    <listitem>
     <para>
      If this is set to 1,
      <filename>/sbin/cpuset_release_agent</filename> will be called
      when the last process leaves this cpuset. Note, that it is up to
      the administrator to create a script or binary that matches the
      local needs.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>memory_pressure</term>
    <listitem>
     <para>
     Provides the means to determine how often a cpuset is
      running short of memory. Only calculated if
      <literal>memory_pressure_enabled</literal> is enabled in the top
      cpuset.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>memory_spread_page and memory_spread_slab</term>
    <listitem>
     <para>
      Determines if file system buffers and I/O buffers are uniformly
      spread across the cpuset.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   In addition to these entries, the top cpuset also contains the
   entry <literal>memory_pressure_enabled</literal>, which must be set
   to <literal>1</literal> to use the
   <literal>memory_pressure</literal> entries in the different
   cpusets.
  </para>
  <para>
   To use cpusets, you need detailed hardware information
   for several reasons: on big machines, memory that is local to a CPU will be
   much faster than memory that is only available on a different node. If you
   want to create cpusets from several nodes, you should try to combine CPUs
   that are close together. Otherwise, task switches and memory access may slow
   down your system noticeably. </para>
  <para>
   To find out which node a CPU belongs to, use the
   <filename>/sys</filename> file system. The kernel provides
   information about available CPUs to a specific node by creating
   links in <filename>/sys/devices/system/node/nodeX/</filename>.
  </para>
  <para>
   If several CPUs are to be combined to a cpuset, check the distance
   of the CPUs from each other with the command <command>numactl
    - - hardware </command>. This command is available after installing
   the package <literal>numactl</literal>.
  </para>

  <para>
   The actual configuration and manipulation of cpusets is done by
   modifying the file system below <filename>/dev/cpuset</filename>.
   Tasks are performed in the following way:
  </para>
  <variablelist>
   <varlistentry>
    <term>Create a Cpuset</term>
    <listitem>
     <para>
      To create a cpuset with the name <literal>exampleset</literal>, just
      run <command>mkdir /dev/cpuset/exampleset</command> to create the
      respective directory. The newly created set will contain several entries that
      reflect the current status of the set. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Remove a Cpuset</term>
    <listitem>
     <para> To remove a cpuset, you only need to remove the cpuset directory.
      For example, use <command>rmdir /dev/cpuset/exampleset</command> to
      remove the previously generated cpuset named
      <literal>exampleset</literal>. In contrast to normal file systems, this
      works even if there are still entries in the directory. </para>
     <para>
      Note that you will get an error like <literal>rmdir: exampleset:
       Device or resource busy</literal>, if there are still tasks
      active in that set. To remove
      these tasks from the set, just move them to another set.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Add CPUs to a Cpuset</term>
    <listitem>
     <para>
      To add CPUs to a set, you may either specify a comma separated
      list of CPU numbers, or give a range of CPUs. For example, to
      add CPUs with the numbers <literal>2,3</literal> and
      <literal>7</literal> to <literal>exampleset</literal>, you can use
      one of the following commands: <command>/bin/echo 2,3,7 &gt;
       /dev/cpuset/exampleset/cpus</command> or
      <command>/bin/echo 2-3,7 &gt; /dev/cpuset/exampleset/cpus</command>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Add Memory to a Cpuset</term>
    <listitem>
     <para>
      You cannot move tasks to a cpuset without giving the cpuset
      access to some system memory. To do so, echo a node number
      into <filename>/dev/cpuset/exampleset/mems</filename>. If
      possible, use a node that is close to the used CPUs in this set.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Moving Tasks to Cpusets</term>
    <listitem>
     <para>
      A cpuset is just a useless structure, unless it handles some
      tasks. To add a task to
      <filename>/dev/cpuset/exampleset/</filename>, simply echo the
      task number into <filename>/dev/cpuset/exampleset/</filename>.
      The following script moves all user space processes to
      <filename>/dev/cpuset/exampleset/</filename> and leaves all
      kernel threads untouched:
     </para>
     <screen>cd /dev/cpuset/exampleset; \
for pid in $(cat ../tasks); do \
test -e /proc/$pid/exe &amp;&amp; \
echo $pid > tasks; done</screen>

     <para>
      Note, that for a clean solution, you would have to stop all processes,
      move them to the new cpuset, and let them continue afterward. Otherwise,
      the process may finish before the <emphasis>for loop</emphasis> finishes, or other processes may start
      during moving.
     </para>
     <para>
      This loop liberates all CPUs not contained in the exampleset from
      all processes. Check the result with the command
      <command>cat /dev/cpuset/tasks</command>, which then should not have any entries.
     </para>
     <para>
      Of course, you can move all tasks from a special cpuset to
      the top level set, if you intend to remove this special
      cpuset.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Automatically Remove Unused Cpusets</term>
    <listitem>
     <para> In case a cpuset is not used any longer by any process, you might
      want to clean up such unused cpusets automatically. To initialize the
      removal, you can use the <literal>notify_on_release</literal> flag. If
      this is set to <literal>1</literal>, the kernel will run
       <filename>/sbin/cpuset_release_agent</filename> when the last process
      exits. To remove an unused script, you may, for example, add the following
      script in <filename>/sbin/cpuset_release_agent</filename>: </para>
     <screen>#!/bin/sh
logger cpuset: releasing $1
rmdir /dev/cpuset/$1
     </screen>

     <para>After adding the script to your system, run
      <command>chmod 755 /sbin/cpuset_release_agent</command> to make the script
      executable.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Determine the Cpuset of a Specific Process</term>
    <listitem>
     <para>
      All processes with the process id <literal>PID</literal> have an entry
      in <filename>/proc/PID/cpuset</filename>. If you run the command
      <command>cat /proc/PID/cpuset</command> on a PID that runs in
      the cpuset <literal>exampleset</literal>, you will find the results in
      <literal>/exampleset</literal>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 -->
 <sect1>
  <title>Specifying a CPU Affinity with <command>taskset</command></title>

  <para>
   The default behavior of the kernel, is to keep a process running on the
   same CPU, if the system load is balanced over the available CPUs.
   Otherwise, the kernel tries to improve the load balancing by moving
   processes to an idling CPU. In some situations, however, it is desirable
   to set a CPU affinity for a given process. In this case, the kernel will
   not move the process away from the selected CPUs. For example, if you use
   shielding, the shielded CPUs will not run any process that does not have
   an affinity to the shielded CPUs. Another possibility is to run all low
   priority tasks on a selected CPU to remove load from the other CPUs.
  </para>

  <para>
   Note, that if a task is running inside a specific cpuset, the affinity
   mask must match at least one of the CPUs available in this set. The
   <command>taskset</command> command will not move a process outside the
   cpuset it is running in.
  </para>

  <para>
   To set or retrieve the CPU affinity of a task, a bitmask is used, that is
   represented by a hexadecimal number. If you count the bits of this
   bitmask, the lowest bit represents the first logical CPU as they are
   found in <filename>/proc/cpuinfo</filename>. For example:
  </para>

  <variablelist>
   <varlistentry>
    <term><literal>0x00000001</literal>
    </term>
    <listitem>
     <para>
      is processor #0.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>0x00000002</literal>
    </term>
    <listitem>
     <para>
      is processor #1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>0x00000003</literal>
    </term>
    <listitem>
     <para>
      is processor #0 and processor #1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>0xFFFFFFFE</literal>
    </term>
    <listitem>
     <para>
      all but the first CPU.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   If a given mask does not contain any valid CPU on the system, an error is
   returned. If taskset returns without an error, the given program has been
   scheduled to the specified list of CPUs.
  </para>

  <para>
   The command <command>taskset</command> can either be used to start a new
   process with a given CPU affinity, or to redefine the CPU affinity of a
   already running process.
  </para>

  <variablelist>
   <title>Examples</title>
   <varlistentry>
    <term><option>taskset -p <replaceable>pid</replaceable></option>
    </term>
    <listitem>
     <para>
      Retrieves the current CPU affinity of the process with PID
      <literal>pid</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>taskset -p <replaceable>mask</replaceable><replaceable>pid</replaceable></option>
    </term>
    <listitem>
     <para>
      Sets the CPU affinity of the process with PID <literal>pid</literal>
      to <literal>mask</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>taskset <replaceable>mask</replaceable><replaceable>command</replaceable></option>
    </term>
    <listitem>
     <para>
      Runs <literal>command</literal> with a CPU affinity of
      <literal>mask</literal>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For more detailed information about <command>taskset</command>, consult
   the manual page. <command>man 1 taskset</command>
  </para>
 </sect1>
 <sect1>
  <title>Changing I/O Priorities with <command>ionice</command></title>

  <para>
   Handling I/O is one of the critical issues for all high-performance
   systems. If a task has lots of CPU power available, but must wait for the
   disk, it will not work as efficiently as it could. The Linux kernel
   provides three different scheduling classes to determine the I/O handling
   for a process. All of these classes can be fine-tuned with a nice level.
  </para>

  <variablelist>
   <varlistentry>
    <term>The <emphasis>Best Effort</emphasis> Scheduler</term>
    <listitem>
     <para>
      The <emphasis>Best Effort</emphasis> scheduler is the default I/O
      scheduler, and is used for all processes that do not specify a
      different I/O scheduler class. By default, this scheduler sets its
      nice level according to the nice value of the running process.
     </para>
     <para>
      There are eight different nice levels available for this scheduler.
      The lowest priority is represented by a nice level of seven, the
      highest priority is zero.
     </para>
     <para>
      This scheduler has the scheduling class number <literal>2</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>The <emphasis>Real Time</emphasis> Scheduler</term>
    <listitem>
     <para>
      The real-time I/O class always gets the highest priority for disk
      access. The other schedulers will only be served, if no real-time
      request is present. This scheduling class may easily lock up the
      system if not implemented with care.
     </para>
     <para>
      The real-time scheduler defines nice levels just like the
      <emphasis>Best Effort</emphasis> scheduler.
     </para>
     <para>
      This scheduler has the scheduling class number <literal>1</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>The <emphasis>Idle</emphasis> Scheduler</term>
    <listitem>
     <para>
      The <emphasis>Idle</emphasis> scheduler does not define any nice
      levels. I/O is only done in this class, if no other scheduler runs an
      I/O request. This scheduler has the lowest available priority and can
      be used for processes that are not time-critical.
     </para>
     <para>
      This scheduler has the scheduling class number <literal>3</literal>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   To change I/O schedulers and nice values, use the
   <command>ionice</command> command. This provides a means to tune the
   scheduler of already running processes, or to start new processes with
   specific I/O settings.
  </para>

  <variablelist>
   <title>Examples</title>
   <varlistentry>
    <term><command>ionice -c3 -p$$</command>
    </term>
    <listitem>
     <para>
      Sets the scheduler of the current shell to <literal>Idle</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ionice</command>
    </term>
    <listitem>
     <para>
      Without additional parameters, this prints the I/O scheduler settings
      of the current shell.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ionice -c1 -p42 -n2</command>
    </term>
    <listitem>
     <para>
      Sets the scheduler of the process with process id
      <literal>42</literal> to <literal>Real Time</literal>, and its nice
      value to <literal>2</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ionice -c3 /bin/bash</command>
    </term>
    <listitem>
     <para>
      Starts the Bash shell with the <literal>Idle</literal> I/O scheduler.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For more detailed information about <command>ionice</command>, consult
   the manual page. <command>man 1 ionice</command>
  </para>
 </sect1>
 <sect1>
  <title>Changing the I/O Scheduler for Block Devices</title>

  <para>
   The Linux kernel provides several block device schedulers that can be
   selected individually for each block device. All but the
   <literal>noop</literal> scheduler perform a kind of ordering of requested
   blocks to reduce head movements on the hard disk. If you use an external
   storage system that has its own scheduler, you should disable the
   Linux internal reordering by selecting the <literal>noop</literal>
   scheduler.
  </para>

  <variablelist>
   <title>The Linux I/O Schedulers</title>
   <varlistentry>
    <term>noop</term>
    <listitem>
     <para>
      The <emphasis>noop</emphasis> scheduler is a very simple scheduler,
      that performs basic merging and sorting on I/O requests. This
      scheduler is mainly used for specialized environments that run their
      own schedulers optimized for the used hardware, such as storage
      systems or hardware RAID controllers.
     </para>
<!-- http://aplawrence.com/Linux/linux26_features.html -->
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>anticipatory</term>
    <listitem>
     <para>
      The main principle of <emphasis>anticipatory</emphasis> scheduling is,
      that after a read, the scheduler simply expects further reads from
      userspace. For this reason, after a read completes, the anticipatory
      scheduler will do nothing for a few milliseconds, giving
      userspace the possibility to ask for another read. If such a read is
      requested, it will be performed immediately. Otherwise, the scheduler
      continues with doing writes after a short timeout.
     </para>
     <para>
      The advantage of this procedure is a major reduction of seeks and
      thus, a decreased read latency. This also increases read and write
      bandwidth.
     </para>
<!-- http://www.cs.rice.edu/~ssiyer/r/antsched/antio.html -->
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>deadline</term>
    <listitem>
     <para>
      The main point of <emphasis>deadline</emphasis> scheduling is to try
      hard to answer a request before a given deadline. This results in very
      good I/O for a random single I/O in real-time environments.
     </para>
     <para>
      In principle, the <emphasis>deadline</emphasis> uses two lists with
      all requests. One is sorted by block sequences to reduce seeking
      latencies, the other is sorted by expire times for each request.
      Normally, requests are served according to the block sequence, but if
      a request reaches its deadline, the scheduler starts to work on this
      request.
     </para>
<!-- http://lwn.net/2002/0110/a/io-scheduler.php3 -->
<!-- /usr/src/linux/Documentation/block/deadline-iosched.txt -->
<!-- http://kerneltrap.org/node/431 -->
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>cfq</term>
    <listitem>
     <para>
      The <emphasis>Completely Fair Queuing</emphasis> scheduler uses a
      separate I/O queue for each process. All of these queues get a similar
      time slice for disk access. With this procedure, the
      <emphasis>CFQ</emphasis> tries to divide the bandwidth evenly between
      all requesting processes. This scheduler has a similar throughput as
      the <emphasis>anticipatory</emphasis> scheduler, but the maximum
      latency is much shorter.
     </para>
     <para>
      For the average system, this scheduler yields the best results, and
      thus, is the default I/O scheduler on &sle; systems.
     </para>
<!-- http://en.wikipedia.org/wiki/CFQ -->
<!-- http://lwn.net/Articles/114770/ -->
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   To print the current scheduler of a block device like
   <filename>/dev/sda</filename>, use the following command:
  </para>

<screen>cat /sys/block/sda/queue/scheduler
noop anticipatory deadline [cfq]</screen>

  <para>
   In this case, the scheduler for <filename>/dev/sda</filename> is set to
   <literal>cfq</literal>, the <literal>Completely Fair Queuing</literal>
   scheduler. This is the default scheduler on &slerte;.
  </para>

  <para>
   To change the schedulers, echo one of the names <literal>noop</literal>,
   <literal>anticipatory</literal>, <literal>deadline</literal>, or
   <literal>cfq</literal> into
   <filename>/sys/block/&lt;device&gt;/scheduler</filename>. For
   example, if you want to set the I/O scheduler of the device
   <filename>/dev/sda</filename> to <literal>noop</literal>, use the command
   <command>echo "noop" &gt; /sys/block/sda/scheduler</command>. To set
   other variables in the <filename>/sys</filename> file system, use a
   similar approach.
  </para>
 </sect1>
 <sect1>
  <title>Tuning the Block Device I/O Scheduler</title>

  <para>
   All schedulers, except for the <emphasis>noop</emphasis> scheduler, have
   several common parameters that may be tuned for each block device. You
   can access these parameters with <filename>sysfs</filename> in the
   <filename>/sys/block/&lt;device&gt;/queue/iosched/</filename>
   directory. The following parameters are tuneable for the respective
   scheduler:
  </para>

<!--
  http://www.linux-magazin.de/heft_abo/ausgaben/2005/04/kern_technik
  -->

  <variablelist>
   <varlistentry>
    <term>Anticipatory Scheduler</term>
    <listitem>
     <variablelist>
      <varlistentry>
       <term><option>antic_expire</option>
       </term>
       <listitem>
        <para>
         Time in milliseconds that the <emphasis>anticipatory</emphasis>
         scheduler waits for another read request close to the last read
         request performed. The <emphasis>anticipatory</emphasis> scheduler
         will not wait for upcoming read requests, if this value is set to
         zero.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>read_expire</option>
       </term>
       <listitem>
        <para>
         Deadline of a read request in milliseconds. This scheduler also
         controls the interval between expired requests. By default,
         read_expire is set to 125 milliseconds. Thus, it can take up to 250
         milliseconds until the next read request on the list is served.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>write_expire</option>
       </term>
       <listitem>
        <para>
         Similar to <option>read_expire</option> for write requests.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>read_batch_expire</option>
       </term>
       <listitem>
        <para>
         If write requests are scheduled, this is the time in milliseconds
         that reads are served before pending writes get a time slice. If
         writes are more important than reads, set this value lower than
         <option>read_expire</option>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>write_batch_expire</option>
       </term>
       <listitem>
        <para>
         Similar to <option>read_batch_expire</option> for write requests.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Deadline Scheduler</term>
    <listitem>
     <variablelist>
      <varlistentry>
       <term><option>read_expire</option>
       </term>
       <listitem>
        <para>
         The main focus of this scheduler is to limit the start latency for
         a request to a given time. Therefore, for each request, a deadline
         is calculated from the current time plus the value of
         <option>read_expire</option> in milliseconds.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>write_expire</option>
       </term>
       <listitem>
        <para>
         Similar to <option>read_expire</option> for write requests.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>fifo_batch</option>
       </term>
       <listitem>
        <para>
         If a request hits its deadline, it is necessary to move the request
         from the sorted I/O scheduler list to the dispatch queue. The
         variable <option>fifo_batch</option> controls how many requests are
         moved, depending on the cost of each request.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>front_merges</option>
       </term>
       <listitem>
        <para>
         The scheduler normally tries to find contiguous I/O requests and
         merges them. There are two kinds of merges: The new I/O request may
         be in front of the existing I/O request (front merge), or it may
         follow behind the existing request (back merge). Most merges are
         back merges. Therefore, you can disable the front merge
         functionality by setting <option>front_merges</option> to
         <literal>0</literal>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>write_starved</option>
       </term>
       <listitem>
        <para>
         In case some read or write requests hit their deadline, the
         scheduler prefers the read requests by default. To prevent write
         requests from being postponed forever, the variable
         <option>write_starved</option> controls how often read requests are
         preferred until write requests are preferred over read requests.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
<!-- http://donami.com/118 -->
<!--
    http://www.linux-magazin.de/heft_abo/ausgaben/2005/04/kern_technik
    beware, that article seems to be incorrect sometimes -->
<!-- http://lwn.net/Articles/101029/ mail from axboe, good,
    somewhat old! -->
<!-- http://lwn.net/Articles/114273/ mail from axboe, good! -->
    <term>CFQ Scheduler</term>
    <listitem>
     <variablelist>
      <varlistentry>
       <term><option>back_seek_max</option> and
        <option>back_seek_penalty</option>
       </term>
       <listitem>
        <para>
         The <emphasis>CFQ</emphasis> scheduler normally uses a strict
         ascending elevator. When needed, it also allows small backward
         seeks, but it puts some penalty on them. The maximum backward
         sector seek is defined with <option>back_seek_max</option>, and the
         multiplier for the penalty is set by
         <option>back_seek_penalty</option>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>fifo_expire_async</option> and
        <option>fifo_expire_sync</option>
       </term>
       <listitem>
        <para>
         The <option>fifo_expire_*</option> variables define the timeout in
         milliseconds for asynchronous and synchronous I/O requests.
         Typically, <option>fifo_expire_async</option> affects write and
         <option>fifo_expire_sync</option> affects both, read and write
         operations.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>quantum</option>
       </term>
       <listitem>
        <para>
         Defines the number of I/O requests to dispatch when the block
         device is idle.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><option>slice_async</option>, <option>slice_async_rq</option>,
        <option>slice_sync</option>, and <option>slice_idle</option>
       </term>
       <listitem>
        <para>
         These variables define the time slices a block device gets for
         synchronous or asynchronous operations.
        </para>
        <itemizedlist mark="bullet" spacing="normal">
         <listitem>
          <para>
           <option>slice_async</option> and <option>slice_sync</option>
           represent the length of an asynchronous or synchronous disk slice
           in milliseconds.
          </para>
         </listitem>
         <listitem>
          <para>
           <option>slice_async_rq</option> defines for how many requests an
           asynchronous disk slice lasts.
          </para>
         </listitem>
         <listitem>
          <para>
           <option>slice_idle</option> defines how long a sync slice may
           idle.
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
  </variablelist>

<!--
  TIP: versucht requests so anzuordnen, dass möglichst wenig
      kopfbewegung gemacht wird
      Storage:
      cat /sys/block/hda/queue/scheduler
  bg: should be in there ...
  -->

  <para>
   The system default Block Device I/O Scheduler could be also set by the
   kernel parameter <literal>elevator=</literal>,for example
   <literal>elevator=deadline</literal> to change the I/O Scheduler to
   <literal>deadline</literal>.
  </para>
 </sect1>
<!--
 <sect1>
  <title>Interrupt Priorities</title>

  <para>

   interrupt inheritance -> was ist denn das????
   - softirq
   /usr/src/linux/Documentation/DocBook/kernel-hacking.tmpl, line 191

  </para>
 </sect1>
 -->
 <sect1>
  <title>For More Information</title>

  <para>
   A lot of information about real-time implementations and administration
   can be found on the Internet. The following list contains several
   selected links:
<!--
   http://linuxdevices.com/articles/AT6476691775.html
   http://www.linuxdevices.com/articles/AT8073314981.html
   https://www.rtai.org/
   http://www.fsmlabs.com/
   -->
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     More detailed information about the Real-Time Linux Development and an
     introduction how to write a Real-Time application can be found in the
     Real-Time Linux Community Wiki.
     <link xlink:href="http://rt.wiki.kernel.org"/>,
     <link xlink:href="http://rt.wiki.kernel.org/index.php/HOWTO:_Build_an_RT-application"/>
    </para>
   </listitem>
   <listitem>
    <para>
     The cpuset feature of the kernel is explained in
     <filename>/usr/src/linux/Documentation/cpusets.txt</filename>. More
     detailed documentation is available from
     <link xlink:href="http://techpubs.sgi.com/library/tpl/cgi-bin/getdoc.cgi/linux/bks/SGI_Admin/books/LX_Resource_AG/sgi_html/ch04.html"/>,
     <link xlink:href="http://www.bullopensource.org/cpuset/"/>, and
     <link xlink:href="http://lwn.net/Articles/127936/"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     An overview of CPU and I/O schedulers available in Linux can be found
     at
     <link xlink:href="http://aplawrence.com/Linux/linux26_features.html"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Detailed information about the anticipatory I/O scheduler is available
     at
     <link xlink:href="http://www.cs.rice.edu/~ssiyer/r/antsched/antio.html"/>
     and <link xlink:href="http://www.cs.rice.edu/~ssiyer/r/antsched/"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     For more information about the deadline I/O scheduler, refer to
     <link xlink:href="http://lwn.net/2002/0110/a/io-scheduler.php3"/>, or
     <link xlink:href="http://kerneltrap.org/node/431"/>. In your installed
     system, find further information in
     <filename>/usr/src/linux/Documentation/block/deadline-iosched.txt</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     The CFQ I/O scheduler is covered in detail in
     <link xlink:href="http://en.wikipedia.org/wiki/CFQ"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     General information about I/O scheduling in Linux is available at
<!--
     <ulink
      url="http://www.linux-magazin.de/heft_abo/ausgaben/2005/04/kern_technik"/>,
     -->
     <link xlink:href="http://lwn.net/Articles/101029/"/>,
     <link xlink:href="http://lwn.net/Articles/114273/"/>, and
     <link xlink:href="http://donami.com/118"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     A lot of information about real-time can be found at
     <link xlink:href="http://linuxdevices.com/articles/AT6476691775.html"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
<!--
 o CPU set: http://www.nabble.com/cpuset- - -question-t476909.html
 -->
 <xi:include href="common_copyright_quick.xml"/>
</article>
