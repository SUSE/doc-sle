<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article	  [
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<!-- Converted by suse-upgrade version 1.1 -->
<article xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="article.vt.best.practices" xml:lang="en">
 <?suse-quickstart color="suse"?>
 <title>Virtualization Best Practices</title>
 <subtitle>&sle; &productname; &productnumber;</subtitle>
 <info>
  <productnumber>&productnumber;</productnumber>
  <productname>&sle;
  &productname;</productname>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker>
   </dm:bugtracker>
  </dm:docmanager>
 </info>
 <!--
     we can write it based on on scenario?
     Virtualization Capabilities:
     Consolidation (hardware1+hardware2 -> hardware)
     Isolation
     Migration (hardware1 -> hardware2)
     Disaster recovery
     Dynamic load balancing
 -->
 <sect1 xml:id="vt.best.scenario">
  <title>Virtualization Scenarios</title>
  <para>
   Virtualization offers a lot of capabilities to your environment. It can
   be used in multiple scenario. To get more details about
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_capabilities.html">Virtualization
   Capabilities</link> and
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_benefits.html">Virtualization
   Benefits</link> refer to the
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/book_virt.html">Virtualization Guide</link>.
  </para>
  <para>
   This best practice will provide you advice to help you do the right
   choice in your environment, it will recommend or discourage the usage of
   options depending on your usage. Fixing configuration issue and doing
   performance tuning will increase &vmguest; performance near to bare metal.
  </para>
  <remark>
   Bruce 20150806: There is no mention of caching considerations, of migration inhibitors, or
   basic strategies for how to do your storage or networking infrastructure,
   and only a little bit about how to map your virtualization requirements to
   host capabilities.
   I see that the doc doesn't address the issue of when to use Xen PV vs Xen FV
   vs KVM at all.
  </remark>
  <!-- Todo
       <Table Rowsep="1">
       <title>Scenario</title>
       <tgroup cols="3">
       <colspec colnum="1" colname="1" colwidth=""/>
       <colspec colnum="2" colname="2" colwidth=""/>
       <thead>
       <row>
       <entry>
       <para>Scenarios</para>
       </entry>
       <entry>
       <para>Option Recommended for</para>
       </entry>
       <entry>
       <para>Option Not recommended for</para>
       </entry>
       </row>
       </thead>
       <tbody>
       <row>
       <entry>
       <para>Consolidation</para>
       </entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Isolation</para>
       </entry>
       <entry></entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Migration</para>
       </entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Disaster Recovery</para>
       </entry>
       <entry>
       <mediaobject>
       <imageobject>
       <imagedata fileref="thumbup_green.png" width="25%" format="PNG"/>
       </imageobject>
       </mediaobject>
       </entry>
       </row>
       <row>
       <entry>
       <para>Dynamic Load Balancing</para>
       </entry>
       <entry></entry>
       <entry>
       <mediaobject>
       <imageobject>
       <imagedata fileref="thumbdown_red.png" width="25%" format="PNG"/>
       </imageobject>
       </mediaobject>
       </entry>
       </row>
       </tbody>
       </tgroup>
       </table>
  -->
 </sect1>
 <sect1 xml:id="vt.best.intro">
  <title>Before Any Modification</title>
  <sect2 xml:id="vt.best.intro.backup">
   <title>Backup First</title>
   <para>
    Playing with &vmguest; and the Host configuration can lead to data loss
    or unstable state. It's really important that you do backups of files,
    data, images etc.. before doing any change. Without backups you won't be
    able to easily restore the original state after a data loss or a
    miss-configuration.
   </para>
   <warning>
    <para>
     Backup is mandatory before doing any tests to be sure you will be able
     to roll back to a usable/stable system or configuration. Do not do any
     test or experimentation on online production system.
    </para>
   </warning>
  </sect2>
  <sect2 xml:id="vt.best.intro.testing">
   <title>Do some Tests</title>
   <para>
    The efficiency of virtualization environment depends on administration
    choice. This guide is provided as a reference for doing good choice in
    production environment. Nothing is <emphasis>graved on stone</emphasis>,
    and different infra-structure can provide different result.
    <emphasis>Pre-experimentation</emphasis> is a key point to get a
    successfully virtualization environment.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="vt.best.reco">
  <title>Recommended Usage</title>
  <sect2 xml:id="vt.best.intro.libvirt">
   <title>Prefer &libvirt; Framework</title>
   <remark>add some explanation about how to enable/disable kvm with
   qemu-system-arch</remark>
   <para>
    In SUSE Linux Enterprise it is recommended to use the &libvirt;
    framework to do any operation on hosts, containers and &vmguest;. If you
    use another tools this may not appear or be propagated in your system. For
    example creating a system image by hand with <command>qemu-img
    create data.raw 10G</command> will not be displayed in the
    <command>virt-manager</command> pool section. If you use a
    <command>qemu-system-arch</command> <remark>Bruce 20150806: explain qemu-system-arch</remark>command this will no be visible
    under &libvirt;. So you should carefully used any other management tools
    and keep in mind that using them won't be probably reflected on &libvirt; or
    virt-manager tools.
   </para>
   <note>
    <para>
     Read
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/cha_libvirt_overview.html#">&libvirt;
     overview </link> for more information.
    </para>
   </note>
  </sect2>
  <sect2 xml:id="vt.best.intro.qemu">
   <title>qemu-system-i386 VS qemu-system-x86_64</title>
   <remark>Lin 20150808: Should we add some explanation about how to enable/disable kvm
   with qemu-system-arch</remark>
   <para>
    Just as a modern 64 bit x86 PC supports running a 32 bit OS as well as a
    64 bit OS, <command>qemu-system-x86_64</command> runs 32 bit OS's
    perfectly fine, and in fact usually provides better performance to 32
    bit guests than <command>qemu-system-i386</command>, which provides a 32
    bit guest environment only. Hence we recommend using
    <command>qemu-system-x86_64</command> over
    <command>qemu-system-i386</command> for all guest types. Where
    <command>qemu-system-i386</command> is known to perform better is in
    configurations which SUSE does not support.
   </para>
  </sect2>
 </sect1>
 
 <sect1>
  <title>I/O in Virtualization</title>
  <para>
   SUSE products support various of I/O Virtualization technologies. The
   following table list all advantages and drawbacks choosing one. For more
   information about I/O in virtualization refer to <link
   xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_vt_io.html">Virtualization
   Guide, I/O in Virtualization</link>.
  </para>
  <table>
   <title>I/O Virtualization Solutions</title>
   <tgroup cols="3">
    <colspec colnum="1" colname="1" colwidth="20%"/>
    <colspec colnum="2" colname="2" colwidth=""/>
    <colspec colnum="1" colname="1" colwidth=""/>
    <thead>
     <row>
      <entry>
       <para>Technology</para>
      </entry>
      <entry>
       <para>Advantages</para>
      </entry>
      <entry>
       <para>Drawbacks</para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>Device Assignment</para><para>(pass-through)</para>
      </entry>
      <entry>
       <itemizedlist>
	<listitem>
	 <para>Access direct device under the guest</para>
	</listitem>
	<listitem>
	 <para>High performance</para>
	</listitem>
       </itemizedlist>
      </entry>
      <entry>
       <itemizedlist>
        <listitem>
	 <para>No sharing among multiple guests</para>
	</listitem>
	<listitem>
	 <para>Live migration is complex</para>
	</listitem>
	<listitem>
	 <para>PCI device limit to 8 per guest</para>
	</listitem>
	<listitem>
         <para>Limited number of slot on a server</para>
	</listitem>
       </itemizedlist>
      </entry>
     </row>
     <row>
      <entry>
       <para>Full virtualization</para><para>(IDE, SATA, SCSI)</para>
      </entry>
      <entry>
       <itemizedlist>
	<listitem>
	 <para>&vmguest; compatibility</para>
	</listitem>
	<listitem>
	 <para>Easy for live migration</para>
	</listitem>
       </itemizedlist>
      </entry>
      <entry>
       <itemizedlist>
	<listitem>
	 <para>Bad performance</para>
	</listitem>
	<listitem>
	 <para>Emulated operation</para>
	</listitem> 
       </itemizedlist>
      </entry>
     </row>
     <row>
      <entry>
       <para>Para-virtualization</para><para>(virtio-blk, virtio-net, virtio-scsi)</para>
      </entry>
      <entry>
       <itemizedlist>
	<listitem>
	 <para>Good performance</para>
	</listitem>
	<listitem>
	 <para>Efficient host communication with &vmguest; </para>
	</listitem>
       </itemizedlist>
      </entry>
      <entry>
       <itemizedlist>
	<listitem>
	 <para>Modified guest (PV drivers)</para>
	</listitem>
       </itemizedlist>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </sect1>

 <sect1 xml:id="vt.best.hostlevel">
  <title>Host-Level Configuration and Settings</title>
  <para></para>

  <sect2 xml:id="vt.best.mem">
   <title>Memory and Pages</title>
   <para>
    Linux manages memory in units called pages. On most systems the default
    page size is 4KB. Linux and the CPU need to know which pages belong to
    which processes. That information is stored in a page table. If you have
    a lot of process running, it takes more time to find where the memory is
    mapped, due to the time required to search the page table. To speed up
    the search of the table, the TLB (Translation Lookaside Buffer) was
    invented. But on a system with a lot of memory the TLB is not enough. To
    avoid any fallback to normal page table (resulting in a cache miss, which
    is time consuming), hugepages can be used. Using hugepages will reduce TLB
    overhead and TLB misses (pagewalk). A host with
    32GB (32*1014*1024 = 33554432KB) of memory and a 4KB page size
    has a TLB with: <emphasis>3355443/4 = 8388608</emphasis> entries.
    Usig a 2MB (2048KB) page size, the TLB only has <emphasis>3355443/ 2048 = 16384</emphasis> entries, greatly reducing TLB misses.
   </para>
   <sect3>
    <title>Hugepages</title>
    <para>
     Current CPU architectures support larger pages than 4KB: hugepages.
     To determine what the size of huge pages available on your system
     (could be 2MB or 1GB) check the flag in the <filename>/proc/cpuinfo</filename>.
    </para>
    <table>
     <title>DETERMINE HUGE PAGES SIZE</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="10%"/>
      <colspec colnum="2" colname="2" colwidth=""/>
      <thead>
       <row>
	<entry>
         <para>CPU flag</para>
	</entry>
	<entry>
         <para>Huge pages Size available</para>
	</entry>
       </row>
      </thead>
      <tbody>
       <row>
	<entry><para>Empty string</para></entry>
	<entry><para>No Huge pages available</para></entry>
       </row>
       <row>
	<entry><para>pse</para></entry>
	<entry><para>2MB</para></entry>
       </row>
       <row>
	<entry><para>pdpe1gb</para></entry>
	<entry><para>1GB</para></entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para>
     Using hugepages should improve performance of &vmguest; and reduce host
     memory consumption.
    </para>
    <para>
     By default the system uses THP, to get huge pages available on your system
     you need to activate it at boot time with <option>hugepages=1</option>, 
     and optionally add the hugepages size with <option>hugepagesz=2MB</option>.
    </para>
    <note>
     <title>1GB hugepages</title>
     <para>
      1GB pages can only be allocated at boot time and not freed afterwards.
     </para>
    </note>
    <para>
     In order to allocate and use Huge page table (HugeTlbPage) you need to
     mount <filename>hugetlbfs</filename> with correct permissions.
    </para>
    <procedure>
     <step>
      <para>
       Mount <option>hugetlbfs</option> to <filename>/dev/hugepages</filename>:
      </para>
      <screen># mount -t hugetlbfs hugetlbfs /dev/hugepages</screen>
     </step>
     <step>
      <para>
       To reserve memory for huge pages use <command>sysctl</command> or
       <command>echo <replaceable>NUMBER</replaceable></command>. If your
       system has a <emphasis>Hugepagesize</emphasis> of 2MB (2048KB), and you
       want to reserve 1GB (1048576KB) for your &vmguest;, you need <emphasis>1048576/2048=512</emphasis>
       pages in the pool.
      </para>
      <screen># sysctl vm.nr_hugepages=512</screen>
      <para>Or:</para>
      <screen># echo 512 > /proc/sys/vm/nr_hugepages<co
      xml:id="co.hp.nrhp"/></screen>
      <calloutlist>
       <callout arearefs="co.hp.nrhp">
	<para>
	 Current number of <emphasis>persistent</emphasis> hugepages in the
	 kernel's huge page pool.
	 <emphasis>Persistent</emphasis> huge pages will be returned to the huge
	page pool when freed by a task</para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       Add the <emphasis>memoryBacking</emphasis> element in the &vmguest; config:
      </para>
      <screen>&lt;memoryBacking&gt;
  &lt;hugepages/&gt;
&lt;/memoryBacking&gt;</screen> 
     </step>
     <step>
      <para>
       Start your &vmguest; and check on the host that it using hugepages
      </para>
      <screen># cat /proc/meminfo | grep HugePages_
HugePages_Total:<co xml:id="co.hp.total"/>    512
HugePages_Free:<co xml:id="co.hp.free"/>       92
HugePages_Rsvd:<co xml:id="co.hp.rsvd"/>        0
HugePages_Surp:<co xml:id="co.hp.surp"/>        0</screen>
      <calloutlist>
       <callout arearefs="co.hp.total">
	<para>Size of the pool of huge pages</para>
       </callout>
       <callout arearefs="co.hp.free">
	<para>Number of huge pages in the pool that are not yet allocated</para>
       </callout>
       <callout arearefs="co.hp.rsvd">
	<para>Number of huge pages for which a commitment to allocate from the pool has been made, but no allocation has yet been made</para>
       </callout>
       <callout arearefs="co.hp.surp">
	<para>number of huge pages in the pool above the value in
	<filename>/proc/sys/vm/nr_hugepages</filename>. The maximum number of surplus huge pages is controlled by
	<filename>/proc/sys/vm/nr_overcommit_hugepages</filename></para>
       </callout>
      </calloutlist>
     </step>
    </procedure>
    <note>
     <para>
      Even if Hugepages provide the best performance, they do come with some
      drawbacks. You lose features such as Memory ballooning (see <xref linkend="vt.best.virtio.balloon"/>,
      KSM (see <xref linkend="vt.best.perf.ksm"/>, and hugepages can not be
      swapped.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="vt.best.mem.thp">
    <title>Transparent Hugepages</title>
    <para>
     Transparent Hugepages provide a way to dynamically allocate HugePage with the
     <command>khugepaged</command> kernel thread instead of allocating them
     at boot time. It is enabled by default. In some situations it may be
     preferable to disable THP (like running Orable database). To disable THP,
     add the kernel parameter <option>transparent_hugepage=never</option>,
     rebuild your grub2 configuration, and reboot. Verify THP is disabled with:
    </para>
    <screen># cat /sys/kernel/mm/transparent_hugepage/enabled 
always madvise [never]</screen>
    <note>
     <para>THP is not available under &xen;</para>
    </note>
   </sect3>
   <sect3 xml:id="vt.best.mem.hot">
    <title>Memory Hotplug</title>
    <para>
     To optimize the usage of your host memory, it may be useful to plug more
     memory to a running &vmguest; when required. To support memory hotplug,
     you must first configure the <emphasis>&lt;maxMemory&gt;</emphasis> tag
     in the XML configuration:
    </para>
    <screen>&lt;maxMemory<co xml:id="co.mem.hot.max"/> slots='16'<co   xml:id="co.mem.hot.slots"/> unit='KiB'&gt;20971520<co   xml:id="co.mem.hot.size"/>&lt;/maxMemory&gt;
  &lt;memory<co   xml:id="co.mem.hot.mem"/> unit='KiB'&gt;1048576&lt;/memory&gt;
&lt;currentMemory<co xml:id="co.mem.hot.curr"/> unit='KiB'&gt;1048576&lt;/currentMemory&gt;</screen>
    <calloutlist>
     <callout arearefs="co.mem.hot.max">
      <para>Run time maximum memory allocation of the guest. </para>
     </callout>
     <callout arearefs="co.mem.hot.slots">
      <para>Number of slots available for adding memory to the guest
      </para>
     </callout>
     <callout arearefs="co.mem.hot.size">
      <para>Valid units are:</para>
      <itemizedlist>
       <listitem><para>"KB" for kilobytes (1,000 bytes)</para></listitem>
       <listitem><para>"k" or "KiB" for kibibytes (1024 bytes)</para></listitem>
       <listitem><para>"MB" for megabytes (1,000,000 bytes)</para></listitem>
       <listitem><para>"M" or "MiB" for mebibytes (1,048,576 bytes)</para></listitem>
       <listitem><para>"GB" for gigabytes (1,000,000,000 bytes)</para></listitem>
       <listitem><para>"G" or "GiB" for gibibytes (1,073,741,824 bytes)</para></listitem>
       <listitem><para>"TB" for terabytes (1,000,000,000,000 bytes)</para></listitem>
       <listitem><para>"T" or "TiB" for tebibytes (1,099,511,627,776 bytes)</para></listitem>
      </itemizedlist>
     </callout>
     <callout arearefs="co.mem.hot.mem">
      <para>Maximum allocation of memory for the guest at boot time</para>
     </callout>
     <callout arearefs="co.mem.hot.curr">
      <para>Actual allocation of memory for the guest</para>
     </callout>
    </calloutlist>
    <para>
     To hot plug a memory devices into the slots, use <command>virsh
     attach-device vm-name mem-dev.xml</command>.
    </para>
    <screen># cat mem-dev.xml
&lt;memory model='dimm'&gt;
  &lt;target&gt;
  &lt;size unit='KiB'>524287&lt;/size&gt;
  &lt;node>0&lt;/node&gt;
  &lt;/target&gt;
&lt;/memory&gt;</screen>
    <para>
     For memory device hotplug, the guest must have at least 1 NUMA cell
     defined (see <xref linkend="vt.best.numa.topo"/>).
    </para>
   </sect3>
   <sect3>
    <title>xenstore in tmpfs</title>
    <para>
     When using Xen, it is recommended to place the xenstore database
     on tmpfs. xenstore is used as a control plane by the xm/xend and
     xl/libxl toolstacks and the frontend and backend drivers servicing
     domain I/O devices. The load on xenstore increases linearly as the
     number of running domains increase. If you anticipate hosting many
     &vmguest; on a Xen host, move the xenstore database onto tmpfs to
     improve overall performance of the control plane.
     Mount the <filename>/var/lib/xenstored</filename> directory on tmpfs:
    </para>
    <screen># mount -t tmpfs tmpfs /var/lib/xenstored/</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="vt.best.perf.ksm">
   <title>KSM and Page Sharing</title>
   <para>
    Kernel Share Memory is a kernel module to increase memory density by
    merging equal anonymous pages on a system. This free memory on the
    system allow to run more &vmguest; on the same host. Running same Guest
    on a host generate a lot of common memory on the host. You can enable
    KSM with <command>echo 1 &gt; /sys/kernel/mm/ksm/run</command>. One
    advantage of using KSM in a &vmguest; is that all guest memory is backed
    by host anonymous memory, so you can share
    <emphasis>pagecache</emphasis>, <emphasis>tmpfs</emphasis> or any kind
    of memory allocated on the guest.
   </para>
   <para>
    KSM is controlled by <emphasis>sysfs</emphasis>. You can check KSM's
    values in <filename>/sys/kernel/mm/ksm/</filename>:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>pages_shared</emphasis>: how many shared pages with
      different content are being used (Read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>pages_sharing</emphasis>: how many pages are being shared
      between your kvm guests (Read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>pages_unshared</emphasis>: how many pages unique but
      repeatedly checked for merging (Read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>pages_volatile</emphasis>: how many pages changing too fast
      to be placed in a tree (Read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>full_scans</emphasis>: how many times all mergeable areas
      have been scanned (Read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>sleep_millisecs</emphasis>: how many milliseconds
      <command>ksmd</command> should sleep before next scan. a low value
      will overuse the CPU, so you will loss CPU Time for other tasks,
      setting this to a value greater than <option>1000</option> is
      recommended.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>pages_to_scan</emphasis>: how many present pages to scan
      before ksmd goes to sleep. A big value will overuse the CPU, you can
      start with <option>1000</option>, and then adjust to an appropriate
      value based on KSM result.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>merge_across_nodes</emphasis>: by default the system merge
      page across NUMA node. set this option to <option>0</option> to
      disable this behavior.
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <para>
     KSM should be used if you know that you will over-use your host system
     memory or you will run same instance of applications or &vmguest;. If
     this is not the case it's preferable to disable it. IE; in the XML
     configuration of the &vmguest; add:
    </para>
    <screen>&lt;memoryBacking&gt;
   &lt;nosharepages/&gt;
&lt;/memoryBacking&gt;</screen>
   </note>
   <warning>
    <para>
     KSM can free up some memory on the host system, but the administrator
     should also reserve enough swap to avoid system running out of memory
     in case of the amount of share memory decrease (decrease of share
     memory will result on an increasement use of the physical memory).
    </para>
   </warning>
  </sect2>
  
  <sect2 xml:id="vt.best.perf.swap">
   <title>Swapping</title>
   <para>
    <emphasis>Swap</emphasis> is mostly used by the system to store
    under-used physical memory (low usage, or not accessed since a long
    time). To prevent the system running out of memory setting up a minimum
    swap is highly recommended.
   </para>
   <sect3>
    <title>swappiness</title>
    <para>
     The <emphasis>swappiness</emphasis> setting control your system swap
     behavior. It's define how memory pages are swapped to disk. A high
     value of <emphasis>swappiness</emphasis> result in a more swapping
     system. Value available are from <option>0</option> to
     <option>100</option>. A value of <option>100</option> told the system
     to find inactive pages and put them in swap. A value of
     <option>0</option> reduce the system tendency to swap userland processes
     but do not disable swap completly (this is now the case with kernel =&gt; 3.5).
    </para>
    <para>
     To change the value and do some experiment on a live system, you
     need to do an echo of the value, and check you system memory usage (ie:
     with the <command>free</command> command):
    </para>
    <screen># echo 35 &gt; /proc/sys/vm/swappiness</screen>
    <screen># free
total       used       free     shared    buffers     cached
Mem:      24616680    4991492   19625188     167056     144340    2152408
-/+ buffers/cache:    2694744   21921936
Swap:      6171644          0    6171644</screen>
    <para>
     To get this permanently add a line in
     <filename>/etc/systcl.conf</filename>:
    </para>
    <screen>vm.swappiness = 35</screen>
    <para>
     You can also control the swap using the
     <emphasis>swap_hard_limit</emphasis> 
     element in the XML configuration of your &vmguest;. It is higly recommended to
     do some test before setting this parameter and using it in a production
     environment as the host can kill the domain if the value is too low.
    </para>
    <screen>&lt;memtune&gt;<co xml:id="co.mem.1"/>
  &lt;hard_limit unit='G'&gt;1&lt;/hard_limit&gt;<co xml:id="co.mem.hard"/>
  &lt;soft_limit unit='M'&gt;128&lt;/soft_limit&gt;<co xml:id="co.mem.soft"/>
  &lt;swap_hard_limit unit='G'&gt;2&lt;/swap_hard_limit&gt;<co xml:id="co.mem.swap"/>
&lt;/memtune&gt;</screen>
    <calloutlist>
     <callout arearefs="co.mem.1">
      <para>this element provides memory tunable parameters for the domain. 
      If this is omitted, it defaults to the OS provided defaults</para>
     </callout>
     <callout arearefs="co.mem.hard">
      <para>maximum memory the guest can use. To avoid any problem on the
      &vmguest; it is strongly recommended to do not use this parameter</para>
     </callout>
     <callout arearefs="co.mem.soft">
      <para>the memory limit to enforce during memory contention</para>
     </callout>
     <callout arearefs="co.mem.swap">
      <para>the maximum memory plus swap the &vmguest; can use</para>
     </callout>
    </calloutlist>
   </sect3>
  </sect2>

  <sect2 xml:id="vt.best.perf.io">
   <title>I/O Scheduler</title>
   <para>
    The default I/O scheduler is the Completely Fair Queuing (CFQ). The main
    aim of CFQ scheduler is to provide a fair allocation of the disk I/O
    bandwidth for all the processes which requests an I/O operation. You can
    have different I/O scheduler for different device.
   </para>
   <para>
    To get better performance in host and &vmguest; it is recommended to use
    <emphasis>noop</emphasis> in the &vmguest; (disable the I/O scheduler)
    and the <emphasis>deadline</emphasis> scheduler for a virtualization
    host.
   </para>
   <sect3>
    <title>Checking current I/O scheduler</title>
    <para>
     To check your current I/O scheduler for your disk (replace
     <replaceable>sdX</replaceable> by the disk you want to check):
    </para>
    <screen># cat /sys/block/<replaceable>sdX</replaceable>/queue/scheduler
noop [deadline] cfq</screen>
    <para>
     In our example the <emphasis>deadline</emphasis> scheduler is selected.
    </para>
   </sect3>
   <sect3>
    <title>Changing I/O Scheduler at Runtime</title>
    <para>
     You can change it at runtime with <command>echo</command>:
    </para>
    <screen># echo noop &gt; /sys/block/<replaceable>sdX</replaceable>/queue/scheduler</screen>
   </sect3>
   <sect3>
    <title>SLE11 SPX product</title>
    <para>
     To change the value at boot time for SLE11 SPX product, you need to
     modify your <filename>/boot/grub/menu.lst</filename> file, adding
     <option>elevator=deadline</option> for host and
     <option>elevator=noop</option> for &vmguest; (change will be taken into
     account at next reboot). This will be applied to all devices on your
     system.
    </para>
    <para>
     Changing the <option>elevator=</option> for the boot command line will
     apply the <option>elevator</option> to all devices on your system.
    </para>
   </sect3>
   <sect3>
    <title>SLE12 SPX product</title>
    <para>
     To change the value at boot time for SLE12 SPX product, you need to
     modify your <filename>/etc/default/grub</filename> file. Find the
     variable starting with <option>GRUB_CMDLINE_LINUX_DEFAULT</option> and
     add at the end <option>elevator=deadline</option> (or change it with
     the correct value if it is already available).
    </para>
    <para>
     Now you need to regenerate your grub2 config with:
    </para>
    <screen># grub2-mkconfig -o /boot/grub2/grub.cfg</screen>
    <para>
     If you want to have a different parameter for each disk create a
     <filename>/usr/lib/tmpfiles.d/IO_ioscheduler.conf</filename> file with:
    </para>
    <screen>w /sys/block/sda/queue/scheduler - - - - deadline
w /sys/block/sdb/queue/scheduler - - - - noop</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="vt.best.perf.cpu">
   <title>Understanding CPU</title>
   <para>
    Allocation of resources for &vmguest; is a crucial point in VM
    administration. Each &vmguest; should be <emphasis>sized</emphasis> to
    be able to run a certain amount of services, but over-allocating
    resources for &vmguest; may impact the host and all other &vmguest;s. If
    all &vmguest;s suddenly requested all their resources, the host won't be
    able to provide all of them, and this will impact the host's performance
    and will degrade all other services running on the host.
   </para>
   <para>
    CPU's Host <emphasis>components</emphasis> will be
    <emphasis>translated</emphasis> as vCPU in the &vmguest;, but even if
    you have a multi-core CPU with Hyper threading, you should understand
    that a main CPU unit and a multi-core and Hyper threading does not
    provide the same computation capabilities:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>CPU processor</emphasis>: it describes the main CPU unit, it
      can be multi-core and Hyper threaded, and most of dedicated server
      have multi CPU processor on their motherboard.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>CPU core</emphasis>: a main CPU unit can provide more than
      one core, proximity of cores speed up computation process and reduce
      energy cost.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>CPU Hyper threading</emphasis>: this implementation is used
      to improve parrallelization of computations, but this is not efficient
      as a dedicated core.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="vt.best.perf.cpuparam">
   <title>CPU Parameter</title>
   <para>
    You should avoid CPU over-commit. Unless you know exactly how many vCPU
    you require for your &vmguest; you should start with 1 vCPU per
    &vmguest;. Each vCPU should match one hardware processor or core. You
    should target a workload of 70% inside your VM (could be checked with a
    lot of tools like the <command>top</command>). If you allocate more
    processor than needed in the VM, this will add overhead, and will
    degrade cycle efficiency, the un-used vCPU will consume timer interrupts
    and will idle loop, then this will impact the &vmguest;, but also the
    host. To optimize the performance usage it's recommended to know if your
    applications are single threaded or not to avoid any over-allocation of
    vCPU.
   </para>
   <sect3>
    <title>vCPU model and Features</title>
    <para>
     <emphasis>vCPU model and features</emphasis>: CPU model and topology
     can be specified for each &vmguest;. The vCPU definition could be very
     specific excluding some CPU features, listing exact one, etc... You
     can use predefined models available in
     <filename>/usr/share/libvirt/cpu_map.xml</filename> file to exactly
     match your need. Event if could be interesting to declare a very
     specific vCPU for a &vmguest; you should keep in mind that's normalize
     vCPU model and features simplify migration among heterogeneous hosts
     (see
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_libvirt_admin_migrate.html">libvirt
     migration guide</link>). Because change the vCPU type require that the
     &vmguest; is off, which is a constraint in a production environment.
     You should also consider that multiple sockets with a single core and
     thread generally give best performance.
    </para>
    <para>To get all capabilities and topology available on your system use
    <command>virsh capabilities</command> command.</para>
    <note>
     <title>host-passthrough</title>
     <para>
      The CPU visible to the guest should be exactly the same as the host
      CPU even in the aspects that &libvirt; does not understand. Though the
      downside of this mode is that the guest environment cannot be reproduced
      on different hardware. Thus, if you hit any bugs, you are on your own.
     </para>
    </note>
    <para>
     If you want to specify a strict model of a CPU for a &vmguest; like the
     new Broadwell CPU model:
    </para>
    <screen>&lt;cpu mode='custom' match='exact'&gt;
  &lt;model&gt;Broadwell&lt;/model&gt;
&lt;/cpu&gt;</screen>
    <para>
     If you want to add the <emphasis>invtsc</emphasis> (Invariant TSC) CPU feature
     add a <emphasis>feature</emphasis> element between the
     <emphasis>&lt;cpu&gt;</emphasis> element:
    </para>
    <screen>&lt;cpu&gt;
  ....
  &lt;feature name='invtsc'/&gt;
&lt;/cpu&gt;</screen>
   </sect3>
   <sect3>
    <title>vCPU Pinning</title>
    <para>
     <emphasis>vCPU Pinning</emphasis>: it's a method to constrain vCPU
     threads to a NUMA node. The <emphasis>vcpupin</emphasis> element
     specifies which of host's physical CPUs the domain vCPU will be pinned
     to. If this is omitted, and attribute <emphasis>cpuset</emphasis> of
     element <emphasis>vcpu</emphasis> is not specified, the vCPU is pinned
     to all the physical CPUs by default.
    </para>
    <para>
     You can pin a vCPU to a specific physical CPU. vCPU will increase CPU cache hit ratio. To pin a vCPU to a specific core:
    </para>
    <note>
     <para>vCPU pinning can also be used for a non-numa node</para>
    </note>
    <screen>virsh# vcpupin <replaceable>DOMAIN_ID</replaceable> --vcpu <replaceable>vCPU_NUMBER</replaceable>
VCPU: CPU Affinity
----------------------------------
0: 0-7
virsh # vcpupin SLE12 --vcpu 0 0 --config</screen>
    <para>in The XML configuration of your domain now you should have:</para>
    <screen>&lt;cputune&gt;
   &lt;vcpupin vcpu='0' cpuset='0'/&gt;
&lt;/cputune&gt;</screen>
    <figure xml:id="fig.qemu-img.vmnuma">
     <title>NUMA vCPU placement</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="vm_numa.png" width="95%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="vm_numa.png" width="95%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <warning>
     <title>Live Migration</title>
     <para>
      Even if <emphasis>vcpupin</emphasis> can improve performance, you should
      take into consideration that live migration of a pinned &vmguest; is
      difficult, because the resource may not be available on the host, or the
      NUMA topology could be different on the destination host.
      For more recommendation about Live Migration see <link
      xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_libvirt_admin_migrate.html#libvirt_admin_live_migration_requirements">Virtualization
      Live Migration Requirements</link>
     </para>
    </warning>
   </sect3>
   <sect3>
    <title>libvirt and CPU Configuration</title>
    <para>
     For more information about vCPU configuration and tuning parameter
     refer to the
     <link xlink:href="https://libvirt.org/formatdomain.html#elementsCPU">libvirt</link>
     documentation.
    </para>
   </sect3>
  </sect2>
  <sect2>
   <title>&xen;: Moving from PV to FV</title>
   <para>
    This chapter explain how to convert a &xen; para-virtual machine to &xen;
    full virtualized machine.
   </para>
   <para>
    First you need to change to the <emphasis>-default</emphasis> kernel, if
    not already installed, you must install it while in PV mode.
   </para>
   <para>
    In case of you are using <emphasis>vda*</emphasis> naming for disk, you
    need to change this to <emphasis>hd*</emphasis> in
    <filename>/etc/fstab</filename>, <filename>/boot/grub/menu.lst</filename>
    and <filename>/boot/grub/device.map</filename>.
   </para>
   <note>
    <title>Prefer UUIDs</title>
    <para>
     You should use UUIDs or Logical Volumes within your
     <filename>/etc/fstab</filename>. Usage of UUID simplify attached network
     storage, multipathing, and virtualization.
    </para>
   </note>
   <para>
    Moving from PV to FV will lead to a missing disk driver modules from the
    initrd. The modules expected is <emphasis>xen-vbd</emphasis> (and
    <emphasis>xen-vnif</emphasis> for network). They are the PV drivers for
    fully &vmguest;. All other modules like <emphasis>ata_piix</emphasis>,
    <emphasis>ata_generic</emphasis> and <emphasis>libata</emphasis> should
    be added automatically.
   </para>
   <tip>
    <title>Adding Modules to the initrd</title>
    <para>
     On &slsa; 11, you can add modules to the
     <emphasis>INITRD_MODULES</emphasis> line in the
     <filename>/etc/sysconfig/kernel</filename> file, for example
    </para>
    <screen>INITRD_MODULES="xen-vbd xen-vnif"</screen>
    <para>
     Run <command>mkinitrd</command> to build a new initrd containing the
     modules.
    </para>
    <para>
     On &slsa; 12, open <filename>/etc/dracut.conf.d/01-dist.conf</filename>
     and add the the modules with <literal>force-drivers</literal> by adding a
     line as in the example below:
    </para>
    <screen>force-drivers+="xen-vbd xen-vnif"</screen>
    <para>
     Run <command>dracut -f</command> to build a new initrd containing the
     modules.
    </para>
   </tip>
   <para>
    You need to change few parameter in the XML configuration file which describes
    your &vmguest;:
   </para>
   <para>
    Set the OS section to something like:
   </para>
   <screen>&lt;os&gt;
  &lt;type arch='x86_64' machine='xenfv'&gt;hvm&lt;/type&gt;
  &lt;loader&gt;/usr/lib/xen/boot/hvmloader&lt;/loader&gt;
  &lt;boot dev='hd'/&gt;
&lt;/os&gt;</screen>
   <para>
    In the devices section, you need to add:
   </para>
   <screen>&lt;emulator&gt;/usr/lib/xen/bin/qemu-system-i386&lt;/emulator&gt;</screen>
   <para>
    Replace the <emphasis>xen</emphasis> disk bus with
    <emphasis>ide</emphasis>, and the <emphasis>xvda</emphasis> target device
    with <emphasis>hda</emphasis>.
   </para>
   <note>
    <title>guestfs-tools</title>
    <para>
     If you want to script this process, or work on disk images directly, you
     can use the
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_guestfs_tools.html">guestfs-tools</link>
     suite as numerous tools exist there to help to modify disk images.
    </para>
   </note>
  </sect2>
  <sect2 xml:id="vt.best.perf.numa">
   <title>NUMA affinity</title>
   <para>
    Potentially using <emphasis>NUMA</emphasis> huge impact on performance. You should consider your host topology when sizing guests. First check that your host has NUMA capabilities:
   </para>
   <screen># numactl --hardware
available: 4 nodes (0-3)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 72 73 74 75 76 77 78
79 80 81 82 83 84 85 86 87 88 89
node 0 size: 31975 MB
node 0 free: 31120 MB
node 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 90 91 92 93
94 95 96 97 98 99 100 101 102 103 104 105 106 107
node 1 size: 32316 MB
node 1 free: 31673 MB
node 2 cpus: 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 108 109 110
111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
node 2 size: 32316 MB
node 2 free: 31726 MB
node 3 cpus: 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 126 127 128
129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
node 3 size: 32314 MB
node 3 free: 31387 MB
node distances:
node   0   1   2   3 
0:  10  21  21  21 
1:  21  10  21  21 
2:  21  21  10  21 
3:  21  21  21  10</screen>
   <sect3>
    <title>Numa Node Tuning (host)</title>
    <para>
     NUMA is the acronym of Non Uniform Memory Access. A NUMA system has
     multiple physical CPUs with local memory attached.
     Moreover each CPU can access other CPU's Memory, this is what we call
     remote memory, and accessing is slower than access local memory.
    </para>
    <para>
     You can control the NUMA policy performance for domain process using the
     <emphasis>numatune</emphasis> element.
    </para>
    <screen>&lt;numatune&gt;
    &lt;memory mode="strict"<co xml:id="co.numat.mode"/> nodeset="1-4,^3"<co xml:id="co.numat.nodeset"/>/&gt;
    &lt;memnode<co xml:id="co.numat.memnode"/> cellid="0"<co xml:id="co.numat.cellid"/> mode="strict" nodeset="1"/&gt;
    &lt;memnode cellid="2" placement="strict"<co xml:id="co.numat.placement"/> mode="preferred" nodeset="2"/&gt;
&lt;/numatune&gt;</screen>
    <calloutlist>
     <callout arearefs="co.numat.mode">
      <para>
       Policies available are: <emphasis>interleave</emphasis> (round-robin
       like), <emphasis>strict</emphasis> (default) or <emphasis>preferred</emphasis>
      </para>
     </callout>
     <callout arearefs="co.numat.nodeset">
      <para>
       Specify the NUMA nodes
      </para>
     </callout>
     <callout arearefs="co.numat.memnode">
      <para>
       Specify memory allocation policies per each guest NUMA node (if this
       element is not define then this will fallback and uses the
       <emphasis>memory</emphasis> element attribute)
      </para>
     </callout>
     <callout arearefs="co.numat.cellid">
      <para>
       Addresses guest NUMA node for which the settings are applied
      </para>
     </callout>
     <callout arearefs="co.numat.placement">
      <para>
       Attribute placement can be used to indicate the memory placement mode
       for domain process, value can be <emphasis>auto</emphasis> or <emphasis>strict</emphasis>
      </para>
     </callout>
    </calloutlist>
    <warning>
     <title>Memory and CPU accross NUMA nodes</title>
     <para>
      You should avoid allocating &vmguest; memory across NUMA nodes, and
      prevent vCPUs from floating across NUMA nodes.
     </para>
    </warning>
   </sect3>
   <sect3>
    <title>NUMA Balancing</title>
    <para>
     On NUMA machines, there is a performance penalty if remote memory is
     accessed by a CPU.
     Automatic NUMA balancing scans tasks address space and unmaps pages to
     detect if pages are properly placed or if the data should be migrated to a
     memory node local to where the task is running. Every scan delay
     (<emphasis>numa_balancing_scan_delay_ms</emphasis>) the task
     scans the next scan size
     (<emphasis>numa_balancing_scan_size_mb</emphasis>) 
     number of pages in its address space. When the
     end of the address space is reached the scanner restarts from the beginning.
    </para>
    <para>
     Higher scan rates incur higher system overhead as page faults must be
     trapped and potentially data must be migrated. However, the higher the scan
     rate, the more quickly a tasks memory is migrated to a local node if the
     workload pattern changes and minimises performance impact because of remote
     memory accesses. These <command>sysctls</command> control the thresholds for scan delays and
     the number of pages scanned.
    </para>
    <screen># sysctl -a | grep numa_balancing
kernel.numa_balancing = 1<co xml:id="co.numa.balancing"/>
kernel.numa_balancing_scan_delay_ms = 1000<co xml:id="co.numa.delay"/>
kernel.numa_balancing_scan_period_max_ms = 60000<co xml:id="co.numa.pmax"/>
kernel.numa_balancing_scan_period_min_ms = 1000<co xml:id="co.numa.pmin"/>
kernel.numa_balancing_scan_size_mb = 256<co xml:id="co.numa.size"/></screen>
    <calloutlist>
     <callout arearefs="co.numa.balancing">
      <para>Enables/disables automatic page fault based NUMA memory</para>
     </callout>
     <callout arearefs="co.numa.delay">
      <para>Starting scan delay used for a task when it initially forks</para>
     </callout>
     <callout arearefs="co.numa.pmax">
      <para>Maximum time in milliseconds to scan a tasks virtual memory
      </para>
     </callout>
     <callout arearefs="co.numa.pmin">
      <para>
       Minimum time in milliseconds to scan a tasks virtual memory
      </para>
     </callout>
     <callout arearefs="co.numa.size">
      <para>
       How many megabytes worth of pages are scanned for a given scan
      </para>
     </callout>
    </calloutlist>
    <para>
     The main goal of automatic NUMA balancing is to reschedule tasks on same
     node's memory, the CPU follows the memory, or copy the memory pages to
     the same node, so the memory follows the CPU.
    </para>
    <warning>
     <title>Task Placement</title>
     <para>
      There is no rules to define the best place to run a task, because
      tasks could share memory with another one, so you should group these
      tasks on the same node to get obtain best performance. Check NUMA
      statistics <command># cat /proc/vmstat | grep numa_</command>.
     </para>
    </warning>
   </sect3>
   <sect3 xml:id="vt.best.numa.topo">
    <title>Guest NUMA Topology</title>
    <para>
     &vmguest; NUMA topology could be specified using the
     <emphasis>numa</emphasis> element in the XML configuration:
    </para>
    <screen>&lt;cpu&gt;
...
  &lt;numa&gt;
    &lt;cell<co xml:id="co.numa.cell"/> id='0'<co xml:id="co.numa.id"/> cpus='0-1'<co xml:id="co.numa.cpus"/> memory='512000' unit='KiB'/&gt;
    &lt;cell id='1' cpus='2-3' memory='256000'<co xml:id="co.numa.mem"/>
    unit='KiB'<co xml:id="co.numa.unit"/> memAccess='shared'<co
    xml:id="co.numa.memaccess"/>/&gt;
  &lt;/numa&gt;
  ...
&lt;/cpu&gt;</screen>
    <calloutlist>
     <callout arearefs="co.numa.cell">
      <para>
       Each <emphasis>cell</emphasis> element specifies a NUMA cell or a NUMA node
      </para>
     </callout>
     <callout arearefs="co.numa.id">
      <para>
       All cells should have <emphasis>id</emphasis> attribute in case
       referring to some cell is necessary in the code, otherwise the 
       cells are assigned ids in the increasing order starting from 0
      </para>
     </callout>
     <callout arearefs="co.numa.cpus">
      <para>
       Cpus specifies the CPU or range of CPUs that are part of the node
      </para>
     </callout>
     <callout arearefs="co.numa.mem">
      <para>
       The node memory in kilobytes (i.e. blocks of 1024 bytes)
      </para>
     </callout>
     <callout arearefs="co.numa.unit">
      <para>
       Attribute to define units in which memory is specified
      </para>
     </callout>
     <callout arearefs="co.numa.memaccess">
      <para>
       Optional attribute which can control whether the memory is to be
       mapped as <option>shared</option> or <option>private</option>.
       This is valid only for hugepages-backed memory. 
      </para>
     </callout>
    </calloutlist>
    <para>
     To find where the &vmguest; has allocated his pages do a: <command>cat /proc/<replaceable>PID</replaceable>/numa_maps</command> and <command>cat /sys/fs/cgroup/memory/sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>/memory.numa_stat</command>.
    </para>
    <warning>
     <title>NUMA specification</title>
     <para>The &libvirt; guest NUMA specification is currently only available for QEMU/KVM.</para>
    </warning>
   </sect3>
   <sect3>
    <title>Cpuset Memory Policy</title>
    <para>
     There is 3 memory cpuset policy mode available: interleave, bind, preferred. 
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>interleave</emphasis>: is a memory placement policy also know as round-robin. 
       This policy can provide substantial improvements for jobs that need
       to place thread local data on the corresponding node. When the interleave destination
       is not available, this will be move to another nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>bind</emphasis>: this will place memory only on one node, which means in case of insufficient memory, the allocation will fail.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>preferred</emphasis>: this policy will put a preference to allocate a
       memory to a node, but if there is not enough place for memory on this node, this will
       fallback to another node.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     You can change memory policy mode with <command>cgset</command> tool:
    </para>
    <screen># cgset -r cpuset.mems=<replaceable>NODE</replaceable> sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>/emulator</screen>
    <para>To migrate pages to a node use the <command>migratepages</command> tool:</para>
    <screen># migratepages <replaceable>PID</replaceable> <replaceable>FROM-NODE</replaceable> <replaceable>TO-NODE</replaceable></screen>
    <para>to check everything is fine <command>cat /proc/<replaceable>PID</replaceable>/status | grep Cpus</command>.</para>   
    <note>
     <title>Kernel NUMA/cpuset Memory Policy</title>
     <para>
      For more information see <link xlink:href="https://www.kernel.org/doc/Documentation/vm/numa_memory_policy.txt">Kernel
      NUMA memory policy</link> and <link xlink:href="https://www.kernel.org/doc/Documentation/cgroups/cpusets.txt">cpusets
      memory policy</link>. Check also the <link xlink:href="https://libvirt.org/formatdomain.html#elementsNUMATuning">Libvirt
      NUMA Tuning documentation.</link>
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Memory Pinning</title>
    <sect4>
     <title>non-vNUMA Guest</title>
     <para>
      On a non-vNUMA guest, pinning memory to host NUMA nodes would be done with:
     </para>
     <screen>&lt;numatune&gt;
   &lt;memory mode="strict" nodeset="0-1"/&gt;
&lt;/numatune&gt;</screen>
     <para>
      This says memory should be allocated from host <emphasis>nodes
      0</emphasis> and <emphasis>1</emphasis>. Starting the guest will fail if its memory requirements cannot be fulfilled by <emphasis>nodes 0</emphasis> and <emphasis>1</emphasis>. <command>virt-install</command> also supports this configuration with the <option>--numatune</option> option.
     </para>
    </sect4>
    <sect4>
     <title>vNUMA Guest</title>
     <para>
      If the guest has a vNUMA topology:
     </para>
     <screen>&lt;numatune&gt;
   &lt;memnode cellid="0" mode="strict" nodeset="0"/&gt;
   &lt;memnode cellid="1" mode="strict" nodeset="1"/&gt;
&lt;/numatune&gt;</screen>
     <para>
      This says vNUMA <emphasis>cell 0</emphasis> gets its memory from host <emphasis>node 0</emphasis>, and <emphasis>cell 1</emphasis> from host <emphasis>node 1</emphasis>. The amount of memory in each vNUMA cell is defined as part of the topology description (see <xref linkend="vt.best.numa.topo"/>).
     </para>
    </sect4>
    <sect4>
     <title>virt-install / virt-manager</title>
     <para>
      Configuring vNUMA and pinning guest memory to host nodes is not
      supported by <command>virt-install</command> or <command>virt-manager</command>.
     </para>
    </sect4>
   </sect3>
  </sect2>
 </sect1>

 <sect1 xml:id="vt.best.multihost">
  <title>Multi-Host Configuration and Settings</title>
  <para></para>

  <sect2 xml:id="vt.best.stor.fs">
   <title>Storage and File System</title>
   <para>
   </para>
   <!--
       <sect2 id="vt.best.stor.general">
       <title>General</title>
       <para>
       Access CD/DVD -> storage pool
       </para>
       <para>
       deleting pool
       </para>
       <para>
       Brtfs and guest image
       </para>
       <para>
       qemu direct access to host drives (-drive file=)
       </para>
       </sect2>
   -->

   <sect3 xml:id="vt.best.stor.blxvsimage">
    <title>Block devices VS Image Files</title>
    <para>A block devices is a storage device (ie:
    <filename>/dev/xvd<replaceable>X</replaceable></filename>;
    <filename>/dev/sd<replaceable>X</replaceable></filename>;
    <filename>/dev/hd<replaceable>X</replaceable></filename>).
    A disk image file is stored on the host or accessible over a network.
    </para>
    <table>
     <title>Block devices VS Disk Images</title>
     <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="10%"/>
      <colspec colnum="2" colname="2" colwidth=""/>
      <colspec colnum="1" colname="1" colwidth=""/>
      <thead>
       <row>
	<entry>
	 <para>Technology</para>
	</entry>
	<entry>
	 <para>Advantages</para>
	</entry>
	<entry>
	 <para>Drawbacks</para>
	</entry>
       </row>
      </thead>
      <tbody>
       <row>
	<entry>
	 <para>
	  Block devices
	 </para>
	</entry>
	<entry>
	 <itemizedlist>
	  <listitem><para>
	   Better performance
	  </para></listitem>
	  <listitem><para>
	   Use standard tools for administration/disk modification
	  </para></listitem>
	  <listitem><para>
	   Accessible from host (pro and con)
	  </para></listitem>
	 </itemizedlist>
	</entry>
	<entry>
	 <itemizedlist>
	  <listitem><para>
	   Live migration is complex
	  </para></listitem>
	  <listitem><para>
	   Impossible to increase capacity 
	  </para></listitem>
	 </itemizedlist>
	</entry>
       </row>
       <row>
	<entry>
	 <para>
	  Image Files
	 </para>
	</entry>
	<entry>
	 <itemizedlist>
	  <listitem><para>
	   Easier system management
	  </para></listitem>
	  <listitem><para>
	   Easily move, clone, expand, backup domains
	  </para></listitem>
	  <listitem><para>
	   Comprehensive toolkit (guestfs) for image manipulation
	  </para></listitem>
	  <listitem><para>
	   Reduce overhead through sparse files
	  </para></listitem>
	  <listitem><para>
	   Fully allocate for best performance
	  </para></listitem>
	 </itemizedlist>
	</entry>
	<entry>
	 <itemizedlist>
          <listitem><para>
	   Less performance than block devices
	  </para></listitem>
	 </itemizedlist>
	</entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </sect3>
   <sect3>
    <title>NFS storage for Images</title>
    <para>
     If your image is stored on a NFS share, you should check some server
     and client parameters to improve access to the &vmguest; image.
    </para>
    <sect4>
     <title>NFS Read/Write (client)</title>
     <para>
      Options <option>rsize</option> and <option>wsize</option>
      specify the size of the chunks of data that the
      client and server pass back and forth to each other.
      You should ensure NFS read/write sizes are sufficiently large, especially for
      large I/O. Change the <option>rsize</option> and
      <option>wsize</option> parameter in your
      <filename>/etc/fstab</filename> and increase the value to 16KB,
      this will ensure that all operation can be stuck if there is an hang.     
     </para>
     <screen>nfs_server:/exported/vm_images<co xml:id="co.nfs.server"/> /mnt/images<co xml:id="co.nfs.mnt"/> nfs<co xml:id="co.nfs.nfs"/> rw<co xml:id="co.nfs.rw"/>,hard<co xml:id="co.nfs.hard"/>,sync<co xml:id="co.nfs.sync"/>, rsize=8192<co xml:id="co.nfs.rsize"/>,wsize=8192<co xml:id="co.nfs.wsize"/> 0 0</screen>
     <calloutlist>
      <callout arearefs="co.nfs.server">
       <para>
	NFS server's host name and export path name
       </para>
      </callout>
      <callout arearefs="co.nfs.mnt">
       <para>
	Where to mount the NFS exported share
       </para>
      </callout>
      <callout arearefs="co.nfs.nfs">
       <para>
	This is an <option>nfs</option> mount point
       </para>
      </callout>
      <callout arearefs="co.nfs.rw">
       <para>
	This mount point will be accessible in read/write
       </para>
      </callout>
      <callout arearefs="co.nfs.hard">
       <para>
	Determines the recovery behavior of the NFS client after an NFS request
	times out. <option>hard</option> is the best option to avoid data corruption 
       </para>
      </callout>
      <callout arearefs="co.nfs.sync">
       <para>
	Any system call that writes data to files on that mount point causes
	that data to be flushed to the server before the system call returns
	control to user space
       </para>
      </callout>
      <callout arearefs="co.nfs.rsize">
       <para>
	Maximum number of bytes in each network READ request that the NFS
	client can receive when reading data from a file on an NFS server
       </para>
      </callout>
      <callout arearefs="co.nfs.wsize">
       <para>
	Maximum number of bytes per network WRITE request that the NFS client
	can send when writing data to a file on an NFS server. 
       </para>
      </callout>
     </calloutlist>
    </sect4>
    <sect4>
     <title>NFS Threads (server)</title>
     <para>
      Your NFS server should have enough NFS threads to handle multi-threaded
      workloads. Use <command>nfsstat</command> tool to get some RPC statistics
      on your server:
     </para>
     <screen># nfsstat -rc
Client rpc stats:
calls      retrans    authrefrsh
6401066    198          0          0</screen>
     <para>If the <emphasis>retrans</emphasis> is equal to 0 everything is
     fine, otherwise the client needs to retransmit, so increase the 
     <emphasis>USE_KERNEL_NFSD_NUMBER</emphasis> variable
     in <filename>/etc/sysconfig/nfs</filename>, and adjust accordingly
     until <emphasis>retrans</emphasis> is equal to <emphasis>0</emphasis>.
     </para>
    </sect4>
   </sect3>
  </sect2>
 </sect1>

 <sect1 xml:id="vt.best.stor">
  <title>&vmguest; Images</title>
  <para></para>
  <sect2 xml:id="vt.best.stor.imageformat">
   <title>&vmguest; Image Format</title>
    <para>
     Certain storage formats which &qemu; recognizes have their origins in
     other virtualization technologies. By recognizing these formats, &qemu;
     is able to leverage either data stores or entire guests which were
     originally targeted to run under these other virtualization
     technologies. Some of these formats are supported only in read-only
     mode, enabling either direct use of that read only data store in a
     &qemu; guest or conversion to a fully supported &qemu; storage format
     (using <command>qemu-img</command>) which could then be used in
     read/write mode. See &sle;
     <link xlink:href="https://www.suse.com/releasenotes/x86_64/SUSE-SLES/12/#fate-317891">Release
     Notes</link> to get the list of supported format.
    </para>
    <note>
     <para>
      It is recommended to convert the disk images to either
      raw or qcow2 in order to achieve good performance.
     </para>
    </note>
    <warning>
     <title>Encryption</title>
     <para>
      When you create an image, you can not use compression (<option>-c</option>) in the output file
      with the encryption option (<option>-e</option>).
     </para>
    </warning>
    <sect3>
     <title>Raw format</title>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
	This format is simple and easily exportable to all other emulators/hypervisors
       </para>
      </listitem>
      <listitem>
       <para>
	It provides best performance (least I/O overhead)
       </para>
      </listitem>
      <listitem>
       <para>
	If your file system supports holes (for example in ext2 or ext3 on
	Linux or NTFS on Windows*), then only the written sectors will
	reserve space
       </para>
      </listitem>
      <listitem>
       <para>
	The raw format permit to copy a &vmguest; image to a physical device
	(<command>dd if=<replaceable>vmguest.raw</replaceable> of=<replaceable>/dev/sda</replaceable></command>)
       </para>
      </listitem>
      <listitem>
       <para>
	It's byte-by-byte the same as what the &vmguest; sees, so this waste
	a log of space
       </para>
      </listitem>
     </itemizedlist>
    </sect3>
    <sect3>
     <title>qcow2 format</title>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
	Use it to have smaller images (useful if your file system does not supports holes, for example on
	Windows*)
       </para>
      </listitem>
      <listitem>
       <para>
	It has an optional AES encryption
       </para>
      </listitem>
      <listitem>
       <para>
	Zlib based compression option
       </para>
      </listitem>
      <listitem>
       <para>
	Support of multiple VM snapshots (internal, external)
       </para>
      </listitem>
      <listitem>
       <para>
	Improved performance and stability
       </para>
      </listitem>
      <listitem>
       <para>
	Support changing the backing file
       </para>
      </listitem>
      <listitem>
       <para>
	Support consistency checks
       </para>
      </listitem>
      <listitem>
       <para>
	Less performance than raw format
       </para>
      </listitem>
     </itemizedlist>
     <note>
      <title>12-cache-size</title>
      <para>
       qcow2 can provide the same performance in random access Read/Write as raw format,
       but it needs a well-sized cache size. By default cache size is set to
       1MB. This will give good performance up to a disk size of 8GB. If you need 
       a bigger disk size, you need to adjust the cache size. For a disk size
       of 64GB (64 *1024 = 65536), you need 65536 / 8192B = 8MB of cache
       (<option>-drive format=qcow2,12-cache-size=8M</option>).
      </para>
     </note>
     <sect4>
      <title>Cluster Size</title>
      <para>qcow2 format offer the capabilities to change the cluster
      size. Thee value must be between 512 and 2M. Smaller cluster sizes can
      improve the image file size whereas larger cluster sizes
      generally provide better performance.</para>
     </sect4>
     <sect4>
      <title>Pre allocation</title>
      <para>
       An image with preallocated metadata is initially larger but can
       improve performance when the image needs to grow.
      </para>
     </sect4>
     <sect4>
      <title>Lazy Refcounts</title>
      <para>Reference count updates are postponed with the goal of
      avoiding metadata I/O and improving performance. This is
      particularly interesting with <option>cache=writethrough</option> which does not
      batch metadata updates, but in case of host crash, the
      reference count tables must be rebuilt, this is done automatically
      at the next open with <command>qemu-img check -r all</command>,
      but this take some time...</para>
     </sect4>
    </sect3>
    <sect3>
     <title>qed format</title>
     <para>
      qed is the next generation qcow (Qemu Copy On Write).
     </para>
     <itemizedlist>
      <listitem>
       <para>
	Strong data integrity because of simple design 
       </para>
      </listitem>
      <listitem>
       <para>
	Retains sparseness over non-sparse channels (e.g. HTTP) 
       </para>
      </listitem>
      <listitem>
       <para>
	Support changing the backing file
       </para>
      </listitem>
      <listitem>
       <para>
	Support consistency checks
       </para>
      </listitem>
      <listitem>
       <para> 
	Fully asynchronous I/O path 
       </para>
      </listitem>  
      <listitem>
       <para>
	Does not support internal snapshot
       </para>
      </listitem>
      <listitem>
       <para>
	Relies on the host file system and cannot be stored on a logical volume directly
       </para>
      </listitem>
     </itemizedlist>
    </sect3>
    <sect3>
     <title>VMDK format</title>
     <para>
      VMware 3, 4, or 6 image format, for exchanging images with that product.
     </para>
    </sect3>
    <sect3>
     <title>Image Information</title>
     <para>
      Use <command>qemu-img info <replaceable>vmguest.img</replaceable></command> to get images's information like:
      the format, the virtual size, the physical size, snapshots if available.
     </para>
    </sect3>
    <sect3>
     <title>qemu-img Reference</title>
     <para>
      Refer to <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html#cha.qemu.guest_inst.qemu-img.create">SLE12
      qemu-img documentation</link> for more information on <command>qemu-img</command> tool and examples.
     </para>
    </sect3>
    <sect3 xml:id="vt.best.overlay">
     <title>Overlay Storage Image</title>
     <para>
      The qcow2 and qed formats provide a way to create a base-image, but also a way
      to create available overlay disk images on top of the
      base image. Backing file is useful to be able to revert to a know
      state and discard the overlay.
     </para>
     <para>
      To create an overlay image:
     </para>
     <screen># qemu-img create -o<co xml:id="co.1.minoro"/>backing_file=vmguest.raw<co xml:id="co.1.backingfile"/>,backing_fmt=raw<co xml:id="co.1.backingfmt"/>\
     -f<co xml:id="co.1.minorf"/> qcow2 vmguest.cow<co xml:id="co.1.imagename"/></screen>
     <calloutlist>
      <callout arearefs="co.1.minoro">
       <para>use <option>-o ?</option> for an overview of available options</para>
      </callout>
      <callout arearefs="co.1.backingfile">
       <para>the backing file name</para>
      </callout>
      <callout arearefs="co.1.backingfmt">
       <para>specify the file format for the backing file</para>
      </callout>
      <callout arearefs="co.1.minorf">
       <para>specify the image format for the &vmguest;</para>
      </callout>
      <callout arearefs="co.1.imagename">
       <para>image name of the &vmguest;, it will only record the
       differences from the backing_file</para>
      </callout>
     </calloutlist>
     <para>
      Now you can start your &vmguest;, use it do some modification
      etc.... the backing image will be untouched and all changes to the
      storage will be recorded in the overlay image file. The backing file will
      never be modified unless you use the <option>commit</option> monitor command (or
      <command>qemu-img commit</command>).
     </para>
     <warning>
      <title>Backing Image Path</title>
      <para>
       You should not change the path to the backing image, or you need to
       adjust it. The path is stored is the overlay image file. If you
       want to update the path, you should do a symbolic link from
       original path to the new path and then use the
      <command>qemu-img</command> <option>rebase</option> option.</para>
      <screen># ln -sf /var/lib/images/vmguest.raw  /var/lib/images/SLE12/vmguest.raw</screen>
      <screen># qemu-img rebase<co xml:id="co.2.rebase"/>-u<co xml:id="co.2.unsafe"/> -b<co xml:id="co.2.minorb"/> /var/lib/images/vmguest.raw /var/lib/images/SLE12/vmguest.cow<co xml:id="co.2.image"/></screen>
      <calloutlist>
       <callout arearefs="co.2.rebase">
	<para>changes the backing file image</para>
       </callout>
       <callout arearefs="co.2.unsafe">
        <para>use the unsafe mode (see note below)</para>
       </callout>
       <callout arearefs="co.2.minorb">
	<para>specify the backing file image to use</para>
       </callout>
       <callout arearefs="co.2.image">
        <para>specify the image that will be affected</para>
       </callout>
      </calloutlist>
      <para>There are two different modes in which <option>rebase</option> can operate:</para>
      <itemizedlist>
       <listitem>
	<para>
	 <emphasis>Safe</emphasis>: this is the default mode and performs a real rebase
	 operation. The safe mode is an expensive operation.
	</para>
       </listitem>
       <listitem>
	<para>
	 <emphasis>Unsafe</emphasis>: the unsafe mode
	 (<option>-u</option>) only change the backing file name and
	 format of file name without any checks on the file
	 contents. You should use this mode to rename or moving
	 backing file.
	</para>
       </listitem>
      </itemizedlist>
     </warning>
     <para>
      A common usage is to initiate a new guest with the backing file. Let's
      assume we have a <filename>sle12_base.img</filename> &vmguest;
      ready to use (fresh installation without any modification). This will
      be our backing file.
      Now you need to test a new package, on an updated system and on a
      system with a different kernel.
      We can use <filename>sle12_base.img</filename> to instantiate new &sle; &vmguest; by 
      creating a qcow2 overlay files, pointing to this backing file
      (<filename>sle12_base.img</filename>).
     </para>
     <para>
      In our example we will use <filename>sle12_updated.qcow2</filename>
      for the updated system, and
      <filename>sle12_kernel.qcow2</filename>
      for the system with a different kernel.
     </para>
     <para>To create the two thin provisioned system use the
     <command>qemu-img</command> command
     line with <option>-b</option> option:</para>
     <screen># qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
     /var/lib/libvirt/sle12_updated.qcow2
     Formatting 'sle12_updated.qcow2', fmt=qcow2 size=17179869184
     backing_file='sle12_base.img' encryption=off cluster_size=65536 
     lazy_refcounts=off nocow=off</screen>
     <screen># qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
     /var/lib/libvirt/sle12_kernel.qcow2
     Formatting 'sle12_kernel.qcow2', fmt=qcow2 size=17179869184
     backing_file='vmguest-sle12_base.img' encryption=off cluster_size=65536 
     lazy_refcounts=off nocow=off</screen>
     <para>
      The images are now usable, and you can do your test without touching the
      initial <filename>sle12_base.img</filename> backing file, all
      change will be stored in the new images. Moreover, you can also use
      these new images as a backing file, and create a new overlay.
     </para>
     <screen># qemu-img create -b sle12_kernel.qcow2 -f qcow2 sle12_kernel_TEST.qcow2</screen>
     <para>
      <command>qemu-img info</command> with the option
      <option>--backing-chain</option> will return all information of the
      entire backing chain recursively:
     </para>
     <screen># qemu-img info --backing-chain<co xml:id="co.3.backingchain"/> \
/var/lib/libvirt/images/sle12_kernel_TEST.qcow2
image: sle12_kernel_TEST.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: sle12_kernel.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle12_kernel.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: SLE12.qcow2
Format specific information:
compat: 1.1
lazy refcounts: false

image: sle12_base.img
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 16G
cluster_size: 65536
Format specific information:
compat: 1.1
lazy refcounts: true</screen>
     <calloutlist>
      <callout arearefs="co.3.backingchain">
       <para>will enumerate information about backing files in a disk image chain</para>
      </callout>
     </calloutlist>
     <figure xml:id="fig.qemu-img.overlay">
      <title>Understanding Image Overlay </title>
      <mediaobject>
       <imageobject role="fo">
	<imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
	<imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </sect3>
   </sect2>

   <!--
       <sect2 id="vt.best.stor.diskio">
       <title>Disk IO Modes</title>
       <table>
       <title>Notation Conventions</title>
       <tgroup cols="2">
       <colspec colnum="1" colname="1"/>
       <colspec colnum="2" colname="2"/>
       <thead>
       <row>
       <entry>
       <para>Mode</para>
       </entry>
       <entry>
       <para>Description</para>
       </entry>
       </row>
       </thead>
       <tbody>
       <row>
       <entry>
       <para>Disk IO Modes
       </para>
       </entry>
       <entry>
       <para>Native</para>
       </entry>
       </row>
       <row>
       <entry>
       <para>kernel asynchronous IO</para>
       </entry>
       <entry>
       <para>threads</para>
       </entry>
       </row>
       <row>
       <entry>
       <para>host user-mode based threads</para>
       </entry>
       <entry>
       <para>default, 'threads' mode in SLES</para>
       </entry>
       </row>
       </tbody>
       </tgroup>
       </table>
       </sect2>
   -->


  <sect2>
   <title>Open a &vmguest; Image</title>
   <para>
    You should use <emphasis>guestfs-tools</emphasis> to access to the
    file system image of your &vmguest;. If you do not have this tool installed
    you can mount them with other linux tools, but you should avoid accessing
    un-trusted or unknown &vmguest;'s image system because this can lead to
    security issue (read <link
    xlink:href="https://www.berrange.com/posts/2013/02/20/a-reminder-why-you-should-never-mount-guest-disk-images-on-the-host-os/">D.Berrang's
    post</link> for more information).
   </para>
   <sect3>
    <title>Raw Image</title>
    <sect4>
     <title>Mounting a Raw Image</title>
     <procedure>
      <step>
       <para>
	To be able to mount the image, first you need to find a free loop
	device:
       </para>
       <screen># losetup -f<co xml:id="co.losetup.find"/>
/dev/loop1</screen>
       <calloutlist>
	<callout arearefs="co.losetup.find">
	 <para>Find first unused device</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>
	Associate the image with the loop device:
       </para>
       <screen># losetup /dev/loop1 SLE12.raw</screen>
      </step>
      <step>
       <para>
	Check everything is fine:
       </para>
       <screen># losetup -l<co xml:id="co.losetup.list"/>
NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop1         0      0         0  0    /var/lib/libvirt/images/SLE12.raw</screen>
       <calloutlist>
	<callout arearefs="co.losetup.list">
	 <para>List info about all or specified</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>
	Check image's partitions with <command>kpartx</command>:
       </para>
       <screen># kpartx -a<co xml:id="co.kpartx.a"/> -v<co xml:id="co.kpartx.v"/> /dev/loop1
add map loop1p1 (254:1): 0 29358080 linear /dev/loop1 2048</screen>
       <calloutlist>
        <callout arearefs="co.kpartx.a">
	 <para>Add partition devmappings</para>
	</callout>
        <callout arearefs="co.kpartx.v">
	 <para>Verbose mode</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>
	You can now mount the image partition(s):
       </para>
       <screen># mkdir /mnt/sle12mount
# mount /dev/mapper/loop1p1 /mnt/sle12mount</screen>
      </step>
     </procedure>
     <note>
      <title>Raw image with LVM</title>
      <para>If your raw image contains an LVM volume group you should use some
      LVM tools to be able to mount the partition. Refer to <xref linkend="sect4.lvm.found"/></para>
     </note>
    </sect4>
    <sect4>
     <title>Umount a Raw Image</title>
     <procedure>
      <step>
       <para>Umount all mounted partitions</para>
       <screen># umount /mnt/sle12mount</screen>
      </step>
      <step xml:id="step.umount.raw">
       <para>Delete partition devmappings with <command>kpartx</command> and <option>-d</option> option</para>
       <screen># kpartx -d /dev/loop1</screen>
      </step>
      <step>
       <para>Detach the devices with <command>losetup</command></para>
       <screen># losetup -d /dev/loop1</screen>
      </step>
     </procedure>
    </sect4>
   </sect3>
   <sect3>
    <title>Qcow2 image</title>
    <sect4>
     <title>Mount a Qcow2 Image</title>
     <para>
      This procedure describe the step by step process you should follow to be
      able to mount a qcow2 image.
     </para>
     <procedure>
      <step>
       <para>First you need to probe the <emphasis>nbd</emphasis>
       modules (network block devices).
       </para>
       <screen># modprobe nbd max_part=16<co xml:id="co.nbd.maxpart"/>
# dmesg | grep nbd
[89155.142425] nbd: registered device at major 43</screen>
       <calloutlist>
	<callout arearefs="co.nbd.maxpart">
	 <para>Number of partitions per device</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>Check that the <filename>/dev/nbd0</filename> is not used, and
       connect the &vmguest; image (ie: SLE12.qcow2) to the NBD device witht the  
       <command>qemu-nbd</command> command:</para>
       <screen># qemu-nbd -c<co xml:id="co.qemunbd.minusc"/> /dev/nbd0<co xml:id="co.qemunbd.device"/> SLE12.qcow2<co xml:id="co.qemunbd.image"/></screen>
       <calloutlist>
	<callout arearefs="co.qemunbd.minusc">
	 <para>Connect <filename>SLE12.qcow2</filename> to the local NBD device <filename>/dev/nbd0</filename></para>
	</callout>
	<callout arearefs="co.qemunbd.device">
	 <para>NBD device to use</para>
	</callout>
	<callout arearefs="co.qemunbd.image">
	 <para>&vmguest; image to use</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>
	Inform the operating system about partition table changes with 
	<command>partprobe</command>
       </para>
       <screen># partprobe /dev/nbd0 -s<co xml:id="co.partprobe.sum"/>
/dev/nbd0: msdos partitions 1 2
# dmesg | grep nbd0 | tail -1
[89699.082206]  nbd0: p1 p2</screen>
       <calloutlist>
	<callout arearefs="co.partprobe.sum">
	 <para>Print a summary of contents</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>
	2 partitions are available in the SLE12.qcow2 image:
	<filename>/dev/nbd0p1</filename> and <filename>/dev/nbd0p2</filename>.
	Before mount this partition you need to check that there is no LVM volume
	with <command>vgscan</command>.
       </para>
       <screen># vgscan 
No volume groups found</screen>
      </step>
      <step>
       <para>
	No LVM volume has been found, so you can mount the partition with
	<command>mount</command>. Refer to <xref linkend="sect4.lvm.found"/> to see how to handle LVM volume.
       </para>
       <screen># mkdir /mnt/nbd0p2
# mount /dev/nbd0p1 /mnt/nbd0p2</screen>
       <para>You can now access your &vmguest; file system.</para>
      </step>
     </procedure>
    </sect4>
    <sect4>
     <title>Umount a Qcow2 image</title>
     <procedure>
      <title>Cleanup</title>
      <step>
       <para>
	To clean up everything, <command>umount</command> the partition
       </para>
       <screen># umount /mnt/nbd0p2</screen>
      </step>
      <step xml:id="step.umount.qcow2">
       <para>
	Disconnect the image from the <filename>/dev/nbd0</filename> device
       </para>
       <screen># qemu-nbd -d<co xml:id="co.qemunbd.minusd"/> /dev/nbd0</screen>
       <calloutlist>
	<callout arearefs="co.qemunbd.minusd">
	 <para>Disconnect the specified device</para>
	</callout>
       </calloutlist>
      </step>
     </procedure>
     <note>
      <title>Free NBD Devices</title>
      <para>It is pretty easy to detect if an NBD devices is free. Run the
      following command:
      </para>
      <screen># lsof /dev/nbd0
COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF  NODE NAME
qemu-nbd 15149 root   10u   BLK   43,0      0t0 47347 /dev/nbd0</screen>
      <para>It it produces a line, the nbd device is busy. This can also
      be confirmed by the presence of the
      <filename>/sys/devices/virtual/block/nbd0/pid</filename> file.</para>
     </note>
    </sect4>
   </sect3>
   <sect3 xml:id="sect4.lvm.found">
    <title>Image with LVM</title>
    <para>
     In case of a LVM volume group has been found,
     <command>vgscan</command> will return nothing, until you have passed
     the <option>-v</option> (verbose) parameter.
    </para>
    <screen># vgscan -v
connect() failed on local socket: No such file or directory
Internal cluster locking initialisation failed.
WARNING: Falling back to local file-based locking.
Volume Groups with the clustered attribute will be inaccessible.
Wiping cache of LVM-capable devices
Wiping internal VG cache
Reading all physical volumes.  This may take a while...
Finding all volume groups
Finding volume group "system"
Found volume group "system" using metadata type lvm2</screen>
    <para>
     The <emphasis>system</emphasis> LVM volume group has been found on the
     system. You can get more info in this volume with <command>vgdisplay
     <replaceable>VOLUMEGROUPNAME</replaceable></command> (in our case
     <replaceable>VOLUMEGROUPNAME</replaceable> is <emphasis>system</emphasis>).
     You should activate this volume group to expose LVM partitions as devices
     so the system can mount them. Use <command>vgchange</command>:
    </para>
    <screen># vgchange -ay<co xml:id="co.lvm.a"/> -v<co xml:id="co.lvm.v"/>
connect() failed on local socket: No such file or directory
Internal cluster locking initialisation failed.
WARNING: Falling back to local file-based locking.
Volume Groups with the clustered attribute will be inaccessible.
Finding all volume groups
Finding volume group "system"
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/home
Creating system-home
Loading system-home table (254:0)
Resuming system-home (254:0)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/root
Creating system-root
Loading system-root table (254:1)
Resuming system-root (254:1)
Found volume group "system"
activation/volume_list configuration setting not defined: Checking only
host tags for system/swap
Creating system-swap
Loading system-swap table (254:2)
Resuming system-swap (254:2)
Activated 3 logical volumes in volume group system
    3 logical volume(s) in volume group "system" now active</screen>
    <calloutlist>
     <callout arearefs="co.lvm.a">
      <para>Activate the volume group</para>
     </callout>
     <callout arearefs="co.lvm.v">
      <para>Verbose mode, without this parameter the output is quiet</para>
     </callout>
    </calloutlist>
    <para>All partition in the volume group will be listed in
    <filename>/dev/mapper</filename> directory. You can simply mount them now.</para>
    <screen># ls /dev/mapper/system-*
/dev/mapper/system-home  /dev/mapper/system-root /dev/mapper/system-swap

# mkdir /mnt/system-root
# mount  /dev/mapper/system-root /mnt/system-root

# ls /mnt/system-root/
bin   dev  home  lib64       mnt  proc        root  sbin     srv  tmp  var
boot  etc  lib   lost+found  opt  read-write  run   selinux  sys  usr</screen>
    <para>To clean up:</para>
    <procedure>
     <step><para>Umount all partition (with
     <command>umount</command>)</para>
     <screen># umount /mnt/system-root</screen>
     </step>
     <step>
      <para>Un-activate the LVM volume group (with <command>vgchange -an
      <replaceable>VOLUMEGROUPNAME</replaceable></command>)</para>
      <screen># vgchange -an -v system
connect() failed on local socket: No such file or directory
Internal cluster locking initialisation failed.
WARNING: Falling back to local file-based locking.
Volume Groups with the clustered attribute will be inaccessible.
Using volume group(s) on command line
Finding volume group "system"
Found volume group "system"
Removing system-home (254:0)
Found volume group "system"
Removing system-root (254:1)
Found volume group "system"
Removing system-swap (254:2)
Deactivated 3 logical volumes in volume group system
0 logical volume(s) in volume group "system" now active</screen>
     </step>
     <step>
      <itemizedlist>
       <listitem>
	<para>
	 In case of Qcow2 format, end the procedure from 
	 <xref linkend="step.umount.qcow2"/> (<command>qemu-nbd -d
	/dev/nbd0</command>)      </para>
       </listitem>
       <listitem>
	<para>
	 In Case of Raw format, end the procedure from 
	 <xref linkend="step.umount.raw"/> (<command>kpartx -d
	 /dev/loop1</command>; <command>losetup -d /dev/loop1</command>) 
	</para>
       </listitem>
      </itemizedlist>
     </step>
    </procedure>
    <warning>
     <title>Check Cleanup is OK</title>
     <para>
      You should double check that your cleanup procedure is ok using
      system command like <command>losetup</command> or
      <command>qemu-nbd</command>, <command>mount</command>,
      <command>vgscan</command>. If this is not the case you may have trouble
      using the &vmguest; because the system image will be used in different place.
     </para>
    </warning>
   </sect3>
  </sect2>

  <sect2>
   <title>File System Sharing</title>
   <para>
    You can access an host directory in the &vmguest; using the <emphasis>file system</emphasis>
    element. In the following example we will share the
    <filename>/data/shared</filename> directory and mount it under the
    &vmguest;.
    Note that <emphasis>accessmode</emphasis> parameter only works with
    <emphasis>type='mount'</emphasis> for the QEMU/KVM drive. All other
    <emphasis>type</emphasis> are mostly available for LXC driver.
   </para>
   <screen>&lt;filesystem type='mount'<co xml:id="co.fs.mount"/> accessmode='mapped'<co xml:id="co.fs.mode"/>&gt;
   &lt;source dir='/data/shared'<co xml:id="co.fs.sourcedir"/>&gt;
   &lt;target dir='shared'<co xml:id="co.fs.targetdir"/>/&gt;
&lt;/filesystem&gt;</screen>
   <calloutlist>
    <callout arearefs="co.fs.mount">
     <para>A host directory to mount &vmguest;</para>
    </callout>
    <callout arearefs="co.fs.mode">
     <para>Access mode (the security mode) set to <emphasis>mapped</emphasis> will give access
     with the permissions of the hypervisor. Use
     <emphasis>passthrough</emphasis> to access this share with the 
     permissions of the user inside the &vmguest;</para>
    </callout>
    <callout arearefs="co.fs.sourcedir">
     <para>Path to share with the &vmguest;</para>
    </callout>
    <callout arearefs="co.fs.targetdir">
     <para>Name or label of the path for the mount command</para>
    </callout>
   </calloutlist>
   <para>
    Under the &vmguest; now you need to mount the <emphasis>target
    dir='shared'</emphasis>:       
   </para>
   <screen># mkdir /opt/mnt_shared
# mount shared -t 9p /opt/mnt_shared -o trans=virtio
# mount | grep shared shared on /opt/mnt_shared type 9p (rw,relatime,sync,dirsync,trans=virtio)</screen>
   <para>
    Read the <link xlink:href="https://libvirt.org/formatdomain.html#elementsFilesystems">&libvirt;
    File System </link> and <link xlink:href="http://wiki.qemu.org/Documentation/9psetup">&qemu; 9psetup</link> for more information.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="vt.best.vm.perf">
  <title>Configurations and Settings Performed within the &vmguest;</title>
  <para></para>
  <sect2 xml:id="vt.best.perf.virtio">
   <title>Virtio Driver</title>
   <para>
    To increase &vmguest; performance it's recommended to use PV drivers,
    the host implementation is in user space, so no driver is needed in the
    host. Virtio is a virtualization standard, so the guest's device driver
    is aware that it is running in a virtual environment. Note that virtio
    used in &kvm; is different, but architecturally similar to the &xen;
    paravirtualized device drivers (like
    <link xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link> in
    a Windows* guest).
   </para>
   <note>
    <title>I/O in Virtualization</title>
    <para>
     To have a better understanding on this topic refer to
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_vt_io.html">I/O
     Virtualization</link> section in the official Virtualization guide.
    </para>
   </note>
   <sect3>
    <title>virtio blk</title>
    <para>
     <emphasis>virtio_blk</emphasis> is the virtio block device for disk.
    </para>
    <warning>
     <title>&qemu; option</title>
     <para>
      The <option>-hd[abcd]</option> for virtio disk won't work, you must use
      <option>-drive</option> instead.
     </para>
    </warning>
    <warning>
     <title>Disk Name Change</title>
     <para>
      For a Linux guest, the disk will show up as <option>/dev/vd[a-z][1-9]</option>, if you
      migrate from a non-virtio disk you need to change
      <option>root=</option> in GRUB config, and regenerate the
      <filename>initrd</filename> file or the system won't be able to boot.
     </para>
    </warning>
    <para>
     Example of a virtio disk definition:
    </para>
    <screen>&lt;disk type='....' device='disk'&gt;
    ....
    &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</screen>
    <para>
     This is preferable to remove every disk block in the XML configuration
     containing <emphasis>&lt;address .*&gt;</emphasis> because it will be
     re-generated automatically.
    </para>
   </sect3>
   <sect3>
    <title>virtio net</title>
    <para>
     <emphasis>virtio_net</emphasis> is the virtio network device. The
     kernel modules should be insmoded automatically in the guest at boot
     time. You need to start the service to get network available.
    </para>
    <screen>&lt;interface type='network'&gt;
    ...
    &lt;model type='virtio' /&gt;
&lt;/interface&gt;</screen>
   </sect3>
   <sect3 xml:id="vt.best.virtio.balloon">
    <title>virtio balloon</title>
    <para>
     The virtio balloon is used for host memory over-commits for guests.
     For linux guests, Balloon driver runs in guest kernel, for windows guests,
     Balloon driver is in VMDP package.
     <emphasis>virtio_balloon</emphasis> is a PV driver to give or take
     memory from a &vmguest;. 
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>Inflate balloon</emphasis>: Return memory from guest to host kernel(for kvm) or
       to hypervisor(for xen)
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Deflate balloon</emphasis>: Guest will have more available memory
      </para>
     </listitem>
    </itemizedlist>
    <para>
     It's controlled by
     <emphasis>currentMemory</emphasis> and <emphasis>memory</emphasis>
     option.
    </para>
    <screen>&lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
    &lt;currentMemory unit='KiB'&gt;1048576&lt;/currentMemory&gt;
    [...]
    &lt;devices&gt;
    &lt;memballoon model='virtio'/&gt;
    &lt;/devices&gt;</screen>
    <para>
     You can also use <command>virsh</command> to change it:
    </para>
    <screen># virsh setmem <replaceable>DOMAIN_ID</replaceable> <replaceable>MEMORY in KB</replaceable></screen>
   </sect3>
   <sect3>
    <title>Checking virtio Presence</title>
    <para>
     You can check the virtio block pci with:
    </para>
    <screen># find /sys/devices/ -name virtio*
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2</screen>
    <para>
     To find the block device <filename>vdX</filename> associated:
    </para>
    <screen># find /sys/devices/ -name virtio* -print  -exec ls {}/block 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2
vda</screen>
    <para>
     Get more information on the virtio block:
    </para>
    <screen># udevadm info -p /sys/devices/pci0000:00/0000:00:08.0/virtio2
P: /devices/pci0000:00/0000:00:08.0/virtio2
E: DEVPATH=/devices/pci0000:00/0000:00:08.0/virtio2
E: DRIVER=virtio_blk
E: MODALIAS=virtio:d00000002v00001AF4
E: SUBSYSTEM=virtio</screen>
    <para>
     Check all virtio driver used:
    </para>
    <screen># find /sys/devices/ -name virtio* -print  -exec ls -l {}/driver 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
lrwxrwxrwx 1 root root 0 Jun 17 15:48 /sys/devices/pci0000:00/0000:00:06.0/virtio0/driver -&gt; ../../../../bus/virtio/drivers/virtio_console
/sys/devices/pci0000:00/0000:00:07.0/virtio1
lrwxrwxrwx 1 root root 0 Jun 17 15:47 /sys/devices/pci0000:00/0000:00:07.0/virtio1/driver -&gt; ../../../../bus/virtio/drivers/virtio_balloon
/sys/devices/pci0000:00/0000:00:08.0/virtio2
lrwxrwxrwx 1 root root 0 Jun 17 14:35 /sys/devices/pci0000:00/0000:00:08.0/virtio2/driver -&gt; ../../../../bus/virtio/drivers/virtio_blk</screen>
   </sect3>
   <!--
       <para>
       Currently performance is much better when using a host kernel configured with <emphasis>CONFIG_HIGH_RES_TIMERS</emphasis>. Another option is use HPET/RTC and <option>-clock=</option> &qemu; option.
       </para>
   -->
   <sect3>
    <title>Find Device driver options</title>
    <para>Virtio devices and other drivers have some options, to list all of
    them use the <option>help</option> parameter of the<command>qemu-system-ARCH</command> command.</para>
    <screen># qemu-system-x86_64 -device virtio-net,help
virtio-net-pci.ioeventfd=on/off
virtio-net-pci.vectors=uint32
virtio-net-pci.indirect_desc=on/off
virtio-net-pci.event_idx=on/off
virtio-net-pci.any_layout=on/off
.....</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="vt.best.perf.cirrus">
   <title>Cirrus Video Driver</title>
   <para>
    To get 16bit color, high compatibility and better performance it's
    recommended to use the <emphasis>cirrus</emphasis> video driver.
   </para>
   <note>
    <title>&libvirt;</title>
    <para>
     &libvirt; ignore the vram value because video size was been hardcoded in
     &qemu;.
    </para>
   </note>
   <screen>&lt;video&gt;
   &lt;model type='cirrus' vram='9216' heads='1'/&gt;
&lt;/video&gt;</screen>
  </sect2>

  <sect2 xml:id="vt.best.entropy">
   <title>Better Entropy</title>
   <para>
    Virtio RNG is a paravirtualized device that is exposed as a hardware RNG
    device to the guest. On the host side, it can be wired up to one of
    several sources of entropy, including a real hardware RNG device as well
    as the host's <filename>/dev/random</filename> if hardware support does not exist.
    The Linux kernel contains the guest driver for the device since version 2.6.26.
   </para>
   <para>
    The system entropy is collected from various non-deterministic hardware
    events and is mainly used by cryptographic applications.
    The virtual random number generator device (paravirtualized device) allows the host to pass through
    entropy to &vmguest; operating systems. This result in a better entropy in the &vmguest;.
   </para>
   <para>To use the <filename>hwrng</filename> add the device in the XML configuration:</para>
   <screen>&lt;devices&gt;
   &lt;rng model='virtio'&gt;
   &lt;backend model='random'&gt;/dev/random&lt;/backend&gt;
   &lt;/rng&gt;
&lt;/devices&gt;</screen>
   <para>
    The host now should used <filename>/dev/random</filename>:
   </para>
   <screen># lsof /dev/random
qemu-syst 4926 qemu    6r   CHR    1,8      0t0 8199 /dev/random</screen>
   <para>
    On the &vmguest; the source of entropy can be check with <command>cat
    /sys/devices/virtual/misc/hw_random/rng_available</command> and the the
    current device used for entropy with:
   </para>
   <screen># cat /sys/devices/virtual/misc/hw_random/rng_current
virtio_rng.0</screen>
   <para>
    You should install the <emphasis>rn-tools</emphasis> package on the &vmguest;,
    enable the service, and start it. Under SLE12 do:
   </para>
   <screen># zypper in rng-tools
# systemctl enable rng-tools
# systemctl start rng-tools</screen>
  </sect2>

  <sect2 xml:id="vt.best.perf.disable">
   <title>Disable Unused Tools and Devices</title>
   <para>
    You should minimize software and service available on the hosts.
    Moreover it's not recommended to mix different <emphasis>Virtualization
    technologies</emphasis>, like KVM and Containers, on the same host, this
    will reduce resource, will increase security risk and software update
    queue. This lead to reduce the overall host availability, will degrade
    performance even if each resource for both technologies are well sized.
   </para>
   <para>
    Most of Operating System default installation configuration are not
    optimized for VM usage. You should only install what you really need,
    and remove all other components in &vmguest;.
   </para>
   <para>
    Windows* Guest:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Disable the screen saver
     </para>
    </listitem>
    <listitem>
     <para>
      Remove all graphical effects
     </para>
    </listitem>
    <listitem>
     <para>
      Disable indexing of hard disk disk if not necessary
     </para>
    </listitem>
    <listitem>
     <para>
      Check the list of started services and disable the one you do not need
     </para>
    </listitem>
    <listitem>
     <para>
      Check and remove all devices not necessary
     </para>
    </listitem>
    <listitem>
     <para>
      Disable system update if not needed or configure it to avoid any delay
      while rebooting or shutting down the host
     </para>
    </listitem>
    <listitem>
     <para>
      Check the Firewall rules
     </para>
    </listitem>
    <listitem>
     <para>
      Schedule appropriately backups and anti-virus programs
     </para>
    </listitem>
    <listitem>
     <para>
      Install
      <link xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link>
      para-virtualized driver for best performance
     </para>
    </listitem>
    <listitem>
     <para>
      Check the operating System recommendation like in
      <link xlink:href="http://windows.microsoft.com/en-us/windows/optimize-windows-better-performance#optimize-windows-better-performance=windows-7">Microsoft
      Windows* 7 better performance</link> web page
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Linux Guest:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Remove Xorg start if not needed
     </para>
    </listitem>
    <listitem>
     <para>
      Check the list of started services and adjust accordingly
     </para>
    </listitem>
    <listitem>
     <para>
      Operating system needs specific kernel parameters for best
      performance, check the OS recommendations
     </para>
    </listitem>
    <listitem>
     <para>
      Restrict installation of software to a minimal fingerprint
     </para>
    </listitem>
    <listitem>
     <para>
      Optimize the scheduling of predictable tasks (system update, hard
      drive disk checking etc...)
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="vt.best.perf.mtype">
   <title>Updating the Guest Machine Type</title>
   <para>
    &qemu; machine types define some details of the architecture which are
    particularly relevant in the context of migration and save/restore,
    where all the details of the virtual machine ABI need to be carefully
    accounted for. As changes or improvements to &qemu; are made or certain
    types of fixes are done, new machine types are created which include
    those changes. Though the older machine types used for supported
    versions of &qemu; are still valid to use, the user would do well to try
    to move to the latest machine type supported by the current release, so
    as to be able to take advantage of all the changes represented in that
    machine type.
   </para>
   <para>
    Changing the guest's machine type for a Linux guest will mostly be
    transparent, whereas for Windows* guests, it is probably a good idea to
    take a snapshot or backup of the guest in case Windows* has issues with
    the changes it detects and subsequently the user decides to revert to
    the original machine type the guest was created with.
   </para>
   <note>
    <title>Changing the Machine Type</title>
    <para>
     Refer to the Virtualization guide section
     <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html#sec.libvirt.config.mahcinetype.virsh">Change
     Machine type</link> for documentation.
    </para>
   </note>
  </sect2>
 </sect1>
 
 <sect1 xml:id="vt.best.vm.setup.config">
  <title>&vmguest; Specific Configurations and Settings</title>
  <sect2 xml:id="vt.best.acpi">
   <title>ACPI Testing</title>
   <para>
    The capabilities to change a &vmguest; state heavily depends on the
    operating system. It's really important to test this feature before any
    use of your &vmguest; in production. For example most Linux OS disable
    this capability by default, so this require that you enable this
    operation (mostly through Policy Kit).
   </para>
   <para>
    ACPI must be enabled in the guest for a graceful shutdown to work. To
    check if ACPI is enabled, run:
   </para>
   <screen># virsh dumpxml <replaceable>VMNAME</replaceable> | grep acpi</screen>
   <para>
    If nothing is printed, ACPI is not enabled for your machine. Use
    <command>virsh edit</command> to add the following XML under
    &lt;domain&gt;:
   </para>
   <screen>&lt;features&gt;
   &lt;acpi/&gt;
&lt;/features&gt;</screen>
   <para>
    If ACPI was enabled during a Windows* Server 20XX guest installation,
    turning it on in the &vmguest; configuration alone is not sufficient.
    See the following articles for more information:
   </para>
   <simplelist>
    <member><link xlink:href="http://support.microsoft.com/kb/314088/EN-US/"/>
    </member>
    <member><link xlink:href="http://support.microsoft.com/?kbid=309283"/>
    </member>
   </simplelist>
   <para>
    A graceful shutdown is of course always possible from within the guest
    operating system, regardless of the &vmguest;'s configuration.
   </para>
  </sect2>
  <sect2 xml:id="vt.best.guest.kbd">
   <title>Keyboard Layout</title>
   <para>
    Even if it is possible to specify the keyboard layout from a
    <command>qemu-system-ARCH</command> command it's recommended to do this
    configuration in the &libvirt; XML file. To change the keyboard layout
    while connecting to a remote &vmguest; using vnc you should edit the
    &vmguest; XML configuration file. The XML is located at
    <filename>/etc/libvirt/<replaceable>HYPERVISOR</replaceable></filename>.
    For example to add an "en-us" keymap add in the <emphasis>&lt;devices&gt;</emphasis> section:
   </para>
   <screen>&lt;graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/&gt;</screen>
   <para>
    Check the vncdisplay configuration and connect to your &vmguest;:
   </para>
   <screen># virsh vncdisplay sles12 127.0.0.1:0</screen>
  </sect2>
  <sect2 xml:id="vt.best.guest.spice_default_url">
   <title>Spice default listen URL</title>
   <para>
    If no network interface other than <emphasis>lo</emphasis> is assigned
    an IPv4 address on the host, the default address the spice server will
    listen on will not work. An error like the following one would happen:
   </para>
   <screen># virsh start sles12
error: Failed to start domain sles12
error: internal error: process exited while connecting to monitor: ((null):26929): Spice-Warning **: reds.c:2330:reds_init_socket: getaddrinfo(127.0.0.1,5900): Address family for hostname not supported
2015-08-12T11:21:14.221634Z qemu-system-x86_64: failed to initialize spice server</screen>
   <para>In order to change this you can change the default
   <emphasis>spice_listen</emphasis> value in <filename>/etc/libvirt/qemu.conf</filename> by
   the local ipv6 address <emphasis>::1</emphasis>. The spice server listening address
   can also be changed on a per &vmguest; basis, use <command>virsh edit</command> to add
   the listen XML attribute to the <emphasis>graphics type='spice'</emphasis> element:
   </para>
   <screen>&lt;graphics type='spice' listen='::1' autoport='yes'/&gt;></screen>
  </sect2>
  <sect2>
   <title>XML to QEMU command line</title>
   <para>
    Sometimes it could be useful to get the QEMU command line to launch the
    &vmguest; from the XML file. Use <command>virsh</command> with
   <option>domxml</option>.</para>
   <screen># virsh domxml-to-native<co xml:id="co.domxml.native"/> qemu-argv<co xml:id="co.domxml.argv"/> SLE12.xml<co xml:id="co.domxml.file"/></screen>
   <calloutlist>
    <callout arearefs="co.domxml.native">
     <para>Convert the file xml in domain XML format to the native guest configuration</para>
    </callout>
    <callout arearefs="co.domxml.argv">
     <para>For QEMU/KVM hypervisor, the format argument must be qemu-argv</para>
    </callout>
    <callout arearefs="co.domxml.file">
     <para>Domain XML file to use</para>
    </callout>
   </calloutlist>
   <screen># virsh domxml-to-native qemu-argv /etc/libvirt/qemu/SLE12.xml 
   LC_ALL=C PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
   QEMU_AUDIO_DRV=none /usr/bin/qemu-system-x86_64 -name SLE12 -machine \
   pc-i440fx-2.3,accel=kvm,usb=off -cpu SandyBridge -m 4048 -realtime \
   mlock=off -smp 4,sockets=4,cores=1,threads=1 -uuid 8616d00f-5f05-4244-97cc-86aeaed8aea7 \
   -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/SLE12.monitor,server,nowait \
   -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew \
   -global kvm-pit.lost_tick_policy=discard -no-hpet \
   -no-shutdown -global PIIX4_PM.disable_s3=1 -global PIIX4_PM.disable_s4=1 \
   -boot strict=on -device ich9-usb-ehci1,id=usb,bus=pci.0,addr=0x4.0x7 \
   -device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,multifunction=on,addr=0x4 \
   -device ich9-usb-uhci2,masterbus=usb.0,firstport=2,bus=pci.0,addr=0x4.0x1 \
   -device ich9-usb-uhci3,masterbus=usb.0,firstport=4,bus=pci.0,addr=0x4.0x2 \
   -drive file=/var/lib/libvirt/images/SLE12.qcow2,if=none,id=drive-virtio-disk0,format=qcow2,cache=none \
   -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=2 \
   -drive if=none,id=drive-ide0-0-1,readonly=on,format=raw  \
   -device ide-cd,bus=ide.0,unit=1,drive=drive-ide0-0-1,id=ide0-0-1 -netdev tap,id=hostnet0  \
   -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:28:04:a9,bus=pci.0,addr=0x3,bootindex=1 \
   -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 \
   -vnc 127.0.0.1:0 -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 \
   -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on</screen>
  </sect2>
  <sect2>
   <title>Add device to an XML configuration</title>
   <para>
    If you want to create a new &vmguest; based on an XML file, and you
    are not familiar with XML configuration syntax, you can specify the QEMU
    command line using the special tag <emphasis>qemu:commandline</emphasis>.
    For example if you want to add a virtio-balloon-pci, add this block at the
    end of the XML configuration file (before the &lt;/domain&gt; tag).
   </para>
   <screen>&lt;qemu:commandline&gt;
    &lt;qemu:arg value='-device'/&gt;
    &lt;qemu:arg value='virtio-balloon-pci,id=balloon0'/&gt;
&lt;/qemu:commandline&gt;</screen>
  </sect2>
 </sect1>
 <!-- TODO
      <sect2 id="vt.best.guest.clock">
      <title>Clock Setting</title>
      <para>
      
      clock setting (common source)
      kvm clock and ntp   
      </para>
      Use Time Stamp Counter (TSC) as 
      clocksource (if host CPU supports TSC)

kvm-clock
hpet
acpi_pm
jiffies
tsc
</sect2>

<sect2 id="vt.guest.guest.bio">
<title>Bio-based</title>
<para>
bio-based driver on slow devices
</para>
</sect2>
 -->


 



 <!--
     <sect1 id="vt.best.snapsav">
     <title>Saving, Migrating and Snapshoting</title>
     <para>
     Migration requirements/Recomendations
     save VM and start/boot (memory invalid)
     snapshot naming importance
     avoid qemu-img snapshot
     cache mode in live migration
     guestfs and live system
     </para>
     </sect1>


<sect1 id="vt.best.security">
<title>Security consideration</title>
<para>
Connection to guest: security policy
Authentication for libvirtd and VNC need to be configured separately
&qemu; Guest Agent
The VNC TLS (set at start)
</para>
</sect1>

<sect1 id="vt.best.pcipass">
<title>pcpipass</title>
<para>
Pci device (not online!, managed/unmanaged)
howto check SR-IOV capabilities
</para>
</sect1>
 -->
 <!--
     <sect1 xml:id="vt.best.net">
     <title>Network Tips</title>
     <para>
     </para>

<sect2 xml:id="vt.best.net.vnic">


<title>Virtual NICs</title>
<para>
virtio-net (KVM) : multi-queue option
vhost-net (KVM) : Default vNIC, best performance
netbk (Xen) : kernel threads vs tasklets
</para>
</sect2>

<sect2 xml:id="vt.best.net.enic">
<title>Emulated NICs</title>
<para>
e1000: Default and preferred emulated NIC
rtl8139
</para>
</sect2>

<sect2 xml:id="vt.best.net.sharednic">
<title>Shared Physical NICs</title>
<para>
SR-IOV: macvtap
Physicial NICs : PCI pass-through
</para>
</sect2>

<sect2 xml:id="vt.best.general">
<title>Network General</title>
<para>
use multi-network to avoid congestion
admin, storage, migration ...
use arp-filter to prevent arp flux

same MTU in all devices to avoid fragmentation
yast to configure bridge
Network MAC address
bridge configuration in bridge.conf file
PCI pass-through Vfio to improve performance
</para>
</sect2>
</sect1>
 -->
 <!--
     <sect1 xml:id="vt.best.debug">
     <title>Troubleshooting/Debugging</title>
     <para>
     </para>

<sect2 xml:id="vt.best.debug.xen">
<title>Xen</title>
<para>
</para>
<sect3 xml:id="vt.best.debug.xen.log">
<title>Xen Log Files</title>
<para>
libxl logs:
<filename>/var/log/xen/*</filename>
qemu-dm-domain.log, xl-domain.log
bootloader.log, vm-install, xen-hotplug
Process specific logs, often requiring debug log levels to be useful
Some logs require 'set -x' to be added to /etc/xen/scripts/*

libvirt logs:
<filename>/var/log/libvirt/libxl</filename>
libxl-driver.log
domain.log
</para>
</sect3>
<sect3 xml:id="vt.best.debug.xen.hypervisor">
<title>Daemon and Hypervisor Logs</title>
<para>
View systemd journal for specific units/daemons: <command>journalctl
<command>journalctl [\-\-follow] unit xencommons.service</command>
<command>journalctl /usr/sbin/xenwatchdogd</command>
xl dmesg
Xen hypervisor logs
</para>
</sect3>

<sect3 xml:id="vt.best.debug.xen.loglevel">
<title>Increasing Logging Levels</title>
<para>
Log levels are increased through xen parameters:
</para>
<screen>loglvl=all</screen>
<para>
Increased logging for Xen hypervisor
</para>
<screen>guest_loglvl=all</screen>
<para>
Increased logging for guest domain actions Grub2 config:

Edit <filename>/etc/default/grub</filename>, then recreate <filename>grub.cfg</filename>:
GRUB_CMDLINE_XEN_DEFAULT=loglvl=all guest_loglvl=all
<command>grub2-mkconfig -o /boot/grub/grub.cfg</command>
</para>
</sect3>
</sect2>

<sect2 xml:id="vt.best.debug.support">
<title>Support</title>
<para></para>
<sect3 xml:id="vt.best.debug.support.config">
<title>Supportconfig and Virtualization</title>
<para>
Core files:
basic-environment.txt
Reports detected virtualization hypervisor
Under some hypervisors (xen), subsequent general checks might be incomplete

Hypervisor specific files:
kvm.txt, xen.txt
Both logs contain general information:
RPM version/verification of important packages
Kernel, hardware, network details
</para>
</sect3>

<sect3 xml:id="vt.best.debug.support.kvm">
<title>kvm.txt</title>
<para>
libvirt details
General libvirt details
Libvirt daemon status
KVM statistics
virsh version, capabilities, nodeinfo, etc...

Domain list and configurations
Conf and log files
<filename>/etc/libvirt/libvirtd.conf</filename>
Last 500 lines from <filename>/var/log/libvirt/qemu/domain.log</filename>
</para>
</sect3>

<sect3 xml:id="vt.best.debug.support.xen">
<title>xen.txt</title>
<para>
Daemon status
xencommons, xendomains and xen-watchdog daemons
grub/grub2 configuration (for xen.gz parameters)

libvirt details
Domain list and configurations

xl details
Domain list and configurations
Conf and Log files
<filename>/etc/xen/xl.conf</filename>, <filename>/etc/libvirt/libvirtd.conf</filename>
Last 500 lines from <filename>/var/log/xen/*</filename>,
<filename>/var/log/libvirt/libxl/*</filename> 
Output of <command>xl dmesg</command> and <command>xl info</command>
</para>
</sect3>
</sect2>

<sect2 xml:id="vt.best.debug.advanced">
<title>Advanced Debugging Options</title>
<para>
Serial console
</para>
<screen>GRUB_CMDLINE_XEN_DEFAULT=loglvl=all guest_loglvl=all console=com1 com1=115200,8n1</screen>
<screen>GRUB_CMDLINE_LINUX_DEFAULT=console=ttyS0,115200</screen>
<para>
Debug keys
<command>xl debug keys h; xl dmesg</command>
<command>xl debug keys q; xl dmesg</command>
Additional Xen debug tools:
<command>xenstore-{ls,read,rm,watch,write}, xentrace, xentop</command>

Capturing Guest Logs
Capturing guest logs during triggered problem:
Connect to domain:
<command>virsh console domname</command>
Execute problem command
Capturing domain boot messages
</para>
<screen>xl create -c VM config file</screen>
<screen>virsh create VM config file \-\-console</screen>
</sect2>
<sect2 xml:id="vt.best.trouble">
<title>Troubleshooting Installations</title>
<para>
virt-manager and virt-install logs:
Found in <filename>~/.cache/virt-manager</filename>

Debugging virt-manager:
<command>virt-manager \-\-no-fork</command>
Sends messages directly to screen and log file
</para>
<screen>LIBVIRT_DEBUG=1 virt-manager \-\-no-fork</screen>
<para>
See libvirt messages in <filename>/var/log/messages</filename>

Use <command>xl</command> to rule out libvirt layer
</para>
</sect2>
<sect2 xml:id="vt.best.trouble.libvirt"> 
<title>Troubleshooting Libvirt</title>
<para>
Client side troubleshooting
</para>
<screen>LIBVIRT_DEBUG=1
1: debug, 2: info, 3: warning, 4: error</screen>
<para>
Server side troubleshooting
<filename>/etc/libvirt/libvirtd.conf</filename> (restart libvirtd after changes)
log_level = 1
log_output = 1:file:/var/log/libvirtd.log
log_filters = 1:qemu 3:remote
</para>
</sect2>

<sect2 xml:id="vt.best.trouble.kernel">
<title>Kernel Cores</title>
<para>
Host cores -vs- guest domain cores
Host cores are enabled through Kdump YaST module
For Xen dom0 cores, 'crashkernel=size@offset' should be added as a Xen hypervisor parameter

Guest cores require:
on_crash[action]on_crash tag
Possible coredump actions are:
coredump-restart     Dump core, then restart the VM
coredump-destroy    Dump core, then terminate the VM
Crashes are written to:
<filename>/var/lib/libvirt/{libxl,qemu}/dump</filename>
<filename>/var/lib/xen/dump</filename>  (if using xl).
</para>
</sect2>


<sect2 xml;id="vt.best.debug.other">
<title>Other</title>
<para>
VGA trouble debug
</para>
</sect2>
</sect1>
 -->
 <sect1>
  <title>Hypervisors VS Containers</title>
  <para/>
  <table>
   <title>Hypervisors VS Containers</title>
   <tgroup cols="3">
    <colspec colnum="1" colwidth="20%"/>
    <colspec colnum="2" colwidth="30%"/>
    <colspec colnum="3" colwidth="30%"/>
    <thead>
     <row>
      <entry><para>Features</para></entry>
      <entry><para>Hypervisors</para></entry>
      <entry><para>Containers</para></entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry><para>Technologies</para></entry>
      <entry><para>Emulation of a physical computing environment</para></entry>
      <entry><para>Use kernel host</para></entry>
     </row>
     <row>
      <entry><para>System layer level</para></entry>
      <entry><para>Managed by a virtualization layer (Hypervisor)</para></entry>
      <entry><para>Rely on kernel namespaces and cgroups</para></entry>
     </row>
     <row>
      <entry><para>Level (layer)</para></entry>
      <entry><para>Hardware level</para></entry>
      <entry><para>Software level</para></entry>
     </row>
     <row>
      <entry><para>Virtualization mode available</para></entry>
      <entry><para>FV or PV</para></entry>
      <entry><para>None, only userland</para></entry>
     </row>
     <row>
      <entry><para>Security</para></entry>
      <entry><para>Strong</para></entry>
      <entry><warning><para>Security is Very low</para></warning></entry>
     </row>
     <row>
      <entry><para>Confinement</para></entry>
      <entry><para>Full isolation</para></entry>
      <entry><warning><para>Host kernel (OS must be compatible with kernel version)</para></warning></entry>
     </row>
     <row>
      <entry><para>Operating System</para></entry>
      <entry><para>Any operating System</para></entry>
      <entry><para>Only Linux (must be "kernel" compatible)</para></entry>
     </row>
     <row>
      <entry><para>Type of System</para></entry>
      <entry><para>Full OS needed</para></entry>
      <entry><para>Scope is an instance of Linux</para></entry>
     </row>
     <row>
      <entry><para>Boot time</para></entry>
      <entry><para>slow to start (OS delay)</para></entry>
      <entry><para>Really quick start</para></entry>
     </row>
     <row>
      <entry><para>Overhead</para></entry>
      <entry><para>High</para></entry>
      <entry><para>Very low</para></entry>
     </row>
     <row>
      <entry><para>Efficiency</para></entry>
      <entry><para>Depends on OS</para></entry>
      <entry><para>Very efficient</para></entry>
     </row>
     <row>
      <entry><para>Sharing with host</para></entry>
      <entry><warning><para>Complex because of isolation</para></warning></entry>
      <entry><para>Sharing is easy (host see everything; containers
      sees its own)</para></entry>
     </row>
     <row>
      <entry><para>Migration</para></entry>
      <entry><para>Support migration (live mode)</para></entry>
      <entry><warning><para>not possible</para></warning></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <sect2>
   <title>Getting the Best of the Two Worlds</title>
   <para>
    Even if the above table seems to indicate that running a single application
    in a very secure way is not possible, starting with &sls; 12 SP 1
    <command>virt-sandbox</command> will allow running a single application
    in a &kvm; guest. <command>virt-sandbox</command> bootstraps any command
    within a linux kernel with a minimal root file system.
   </para>
   <para>
    The guest root file system can either be the root file system mounted read-only
    or a disk image. The following steps will show how to setup a sandbox with
    qcow2 disk image as root file system.
   </para>
   <procedure>
    <step>
     <para>Create the disk image using <command>qemu-img</command></para>
     <screen># qemu-img create -f qcow2 rootfs.qcow2 6G</screen>
    </step>
    <step>
     <para>Format the disk image</para>
     <screen># modprobe nbd<co xml:id="co.vsmkfs.modprobe"/>
# /usr/bin/qemu-nbd --format qcow2 -n -c /dev/nbd0 $PWD/test-base.qcow2<co xml:id="co.vsmkfs.qemu-nbd"/>
# mkfs.ext3 /dev/nbd0<co xml:id="co.vsmkfs.do"/></screen>
     <calloutlist>
      <callout arearefs="co.vsmkfs.modprobe">
       <para>
        Make sur the nbd module is loaded: it is not loaded by default and
       will only be used to format the qcow image.</para>
      </callout>
      <callout arearefs="co.vsmkfs.qemu-nbd">
       <para>
        Create an nbd device for the qcow2 image. This device will then behave like
        any other block device. The example uses <replaceable>/dev/nbd0</replaceable>
        but any other free NBD device will work.
       </para>
      </callout>
      <callout arearefs="co.vsmkfs.do">
       <para>
        Format the disk image directly. Note that no partition table has been
        created: virt-sandbox considers the image to be a partition, not a disk.
       </para>
       <para>
        The partition formats which can be used are restricted: the linux kernel
        bootstrapping the sandbox need to have the corresponding features builtin.
        The ext4 module is also available at the sandbox start up time.
       </para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>
      Now populate the newly formatted image.
     </para>
     <screen># guestmount -a base.qcow2 -m /dev/sda:/ /mnt<co
     xml:id="co.vsfs.mount"/>

# zypper --root /mnt ar cd:///?devices=/dev/dvd SLES12_DVD
# zypper --root /mnt in -t pattern Minimal<co xml:id="co.vsfs.install"/>

# guestunmount /mnt<co xml:id="co.vsfs.unmount"/></screen>
     <calloutlist>
      <callout arearefs="co.vsfs.mount">
       <para>
        Mount the qcow2 image using the <command>guestfs</command> tools.
       </para>
      </callout>
      <callout arearefs="co.vsfs.install">
       <para>
        Use zypper with the <emphasis>--root</emphasis> parameter to add a &sls;
        repository and install the Minimal pattern in the disk image. Any additional
        package or configuration change should be done in this step.
       </para>
       <note>
        <title>Using backing chains</title>
        <para>
         In order to share the root file system between several sandboxes, create
         qcow2 images with a common disk image as backing chain as described in the
         <link xlink:href="#vt.best.overlay">Overlay Storage Image section</link>.
        </para>
       </note> 
      </callout>
      <callout arearefs="co.vsfs.unmount">
       <para>Unmount the qcow2 image.</para>
      </callout>
     </calloutlist>
    </step>
    <step>
     <para>
      Run the sandbox, using <command>virt-sandbox</command>. This command has many
      interesting options, read its man page to discover them all. It can either be
      running as root or as an unprivileged user.
     </para>
     <screen># virt-sandbox -n <replaceable>name</replaceable> \
     -m host-image:/=$PWD/rootfs.qcow2 \ <co xml:id="co.vs.rootfs"/>
     -m host-bind:/srv/www=/guests/www \ <co xml:id="co.vs.bind"/>
     -m ram:/tmp=100MiB \
     -m ram:/run=100MiB \ <co xml:id="co.vs.tmpfs"/>
     -N source=default,address=192.168.122.12/24 \ <co xml:id="co.vs.net"/>
     -- \
     <replaceable>/bin/sh</replaceable></screen>
     <calloutlist>
      <callout arearefs="co.vs.rootfs">
       <para>
        Mount the created disk image as the root file system. Note, that without any
        image being mounted as <filename>/</filename>, the host root file system is
        read-only mounted as the guest one.
       </para>
       <para>
        The host-image mount is not reserved for the root file system, it can be used
        to mount any disk image anywhere in the guest.
       </para>
      </callout>
      <callout arearefs="co.vs.bind">
       <para>
        The host-bind mount is pretty convenient to share files and folders between the
        host and the guest. In this example the host folder <filename>/guests/www</filename>
        is mounted as <filename>/srv/www</filename> in the sandbox.
       </para>
      </callout>
      <callout arearefs="co.vs.tmpfs">
       <para>
        The ram mounts are defining <emphasis>tmpfs</emphasis> mounts in the sandbox.
       </para>
      </callout>
      <callout arearefs="co.vs.net">
       <para>
        The network uses a network defined in libvirt. When running as unprivileged user, the
        source can be omitted, and the &kvm; user networking feature will be used. Using this
        option requires <emphasis>dhcp-client</emphasis> and <emphasis>iproute2</emphasis>, which
        is the case with &sls; Minimal pattern.
       </para>
      </callout>
     </calloutlist>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1>
  <title>External References</title>
  <para>
  </para>
  <itemizedlist>
   <listitem><para>
    <link xlink:href="kernel.org/doc/ols/2009/ols2009-pages-19-28.pdf">Increasing memory density using KSM</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="http://www.linux-kvm.org/page/KSM">linux-kvm.org KSM</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="https://www.kernel.org/doc/Documentation/vm/ksm.txt">KSM's kernel documentation</link></para>
   </listitem>
   <listitem>
    <para><link xlink:href="https://lwn.net/Articles/329123/">ksm - dynamic page sharing driver for linux v4</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="http://www.espenbraastad.no/post/memory-ballooning/">Memory Ballooning</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="http://wiki.libvirt.org/page/Virtio">libvirt virtio</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="https://www.kernel.org/doc/Documentation/block/cfq-iosched.txt">CFQ's kernel documentation</link></para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="https://www.kernel.org/doc/Documentation/sysctl/kernel.txt">Documentation for sysctl</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/525459/">LWN Random Number</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://wiki.mikejung.biz/KVM_/_Xen">KVM Xen tweaks</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="http://events.linuxfoundation.org/sites/events/files/slides/CloudOpen2013_Khoa_Huynh_v3.pdf">Dr. Khoa
     Huynh, IBM Linux Technology Center</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="https://www.kernel.org/doc/Documentation/kernel-parameters.txt">Kernel Parameters</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="http://lwn.net/Articles/374424/">Huge pages
     Administration (Mel Gorman)</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt">kernel hugetlbpage</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href=""/>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</article>
