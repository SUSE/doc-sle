<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="libvirt_configuration_virsh.xml" version="5.0" xml:id="cha-libvirt-config-virsh">
  <title>使用 <command>virsh</command> 配置虚拟机</title>
  <info>
    <abstract>
      <para>
        您也可以在命令行上使用 <command>virsh</command> 来配置虚拟机 (VM)，以此替代虚拟机管理器。使用 <command>virsh</command> 可以控制 VM 的状态、编辑 VM 的配置，甚至将 VM 迁移到另一台主机。下列章节介绍如何使用 <command>virsh</command> 来管理 VM。
      </para>
    </abstract>
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
      <dm:bugtracker/>
      <dm:translation>yes</dm:translation>
    </dm:docmanager>
  </info>
  <sect1 xml:id="sec-libvirt-config-editing-virsh">
    <title>编辑 VM 配置</title>

    <para>
      VM 配置存储在 <filename>/etc/libvirt/qemu/</filename> 中的某个 XML 文件内，其内容如下所示：
    </para>

    <example>
      <title>示例 XML 配置文件</title>
<screen>
&lt;domain type='kvm'&gt;
  &lt;name&gt;sles15&lt;/name&gt;
  &lt;uuid&gt;ab953e2f-9d16-4955-bb43-1178230ee625&lt;/uuid&gt;
  &lt;memory unit='KiB'&gt;2097152&lt;/memory&gt;
  &lt;currentMemory unit='KiB'&gt;2097152&lt;/currentMemory&gt;
  &lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;
  &lt;os&gt;
    &lt;type arch='x86_64' machine='pc-q35-2.0'&gt;hvm&lt;/type&gt;
  &lt;/os&gt;
  &lt;features&gt;...&lt;/features&gt;
  &lt;cpu mode='custom' match='exact' check='partial'&gt;
    &lt;model fallback='allow'&gt;Skylake-Client-IBRS&lt;/model&gt;
  &lt;/cpu&gt;
  &lt;clock&gt;...&lt;/clock&gt;
  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;
  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;
  &lt;on_crash&gt;destroy&lt;/on_crash&gt;
  &lt;pm&gt;
    &lt;suspend-to-mem enabled='no'/&gt;
    &lt;suspend-to-disk enabled='no'/&gt;
  &lt;/pm&gt;
  &lt;devices&gt;
    &lt;emulator&gt;/usr/bin/qemu-system-x86_64&lt;/emulator&gt;
    &lt;disk type='file' device='disk'&gt;...&lt;/disk&gt;
  &lt;/devices&gt;
  ...
&lt;/domain&gt;
</screen>
    </example>

    <para>
      要编辑 VM Guest 的配置，请检查它是否处于脱机状态：
    </para>

<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh list --inactive</command></screen>

    <para>
      如果您的 VM Guest 在此列表中，则表明您可以放心地编辑其配置：
    </para>

<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh edit <replaceable>NAME_OF_VM_GUEST</replaceable></command>
    </screen>

    <para>
      在保存更改之前，<command>virsh</command> 会根据 RelaxNG 纲要验证您的输入。
    </para>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-machinetype-virsh">
    <title>更改计算机类型</title>

    <para>
      使用 <command>virt-install</command> 工具安装时，VM Guest 的计算机类型默认为 <emphasis>pc-q35</emphasis>。计算机类型存储在 VM Guest 配置文件中的 <tag>type</tag> 元素内：
    </para>

<screen>&lt;type arch='x86_64' machine='pc-q35-2.3'&gt;hvm&lt;/type&gt;</screen>

    <para>
      以下过程示范了如何将此值更改为 <literal>q35</literal> 计算机类型。值 <literal>q35</literal> 表示一种 Intel* 芯片组，其中包括 <xref linkend="gloss-vt-acronym-pcie"/>，最多支持 12 个 USB 端口，并支持 <xref linkend="gloss-vt-acronym-sata"/> 和 <xref linkend="gloss-vt-acronym-iommu"/>。
      
    </para>

    <procedure>
      <title>更改计算机类型</title>
      <step>
        <para>
          检查您的 VM Guest 是否处于非活动状态：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh list --inactive</command>
Id    Name                           State
----------------------------------------------------
-     sles15                         shut off</screen>
      </step>
      <step>
        <para>
          编辑此 VM Guest 的配置：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh edit sles15</command></screen>
      </step>
      <step>
        <para>
          请将 <tag class="attribute">machine</tag> 属性的值替换为 <tag class="attvalue">pc-q35-2.0</tag>：
        </para>
<screen>&lt;type arch='x86_64' machine='pc-q35-2.0'&gt;hvm&lt;/type&gt;</screen>
      </step>
      <step>
        <para>
          重启动 VM Guest：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh start sles15</command></screen>
      </step>
      <step>
        <para>
          检查计算机类型是否已更改。登录到 VM Guest 并运行以下命令：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>dmidecode | grep Product</command>
Product Name: Standard PC (Q35 + ICH9, 2009)</screen>
      </step>
    </procedure>

    <tip>
      <title>计算机类型更新建议</title>
      <para>
        每当升级主机系统上的 QEMU 版本时（例如，将 VM 主机服务器升级到新服务包时），请将 VM Guest 的计算机类型升级到最新的可用版本。要进行检查，请在 VM 主机服务器上使用 <command>qemu-system-x86_64 -M help</command> 命令。
      </para>
      <para>
        默认计算机类型（例如 <literal>pc-i440fx</literal>）会定期更新。如果您的 VM Guest 仍在 <literal>pc-i440fx-1.<replaceable>X</replaceable></literal> 计算机类型上运行，我们强烈建议更新到 <literal>pc-i440fx-2.<replaceable>X</replaceable></literal>。这样就可以利用计算机定义中最近的更新和更正，并确保将来可以更好地兼容。
      </para>
    </tip>
  </sect1>
  <sect1 xml:id="sec-libvirt-hypervisor-features-virsh">
    <title>配置超级管理程序功能</title>

    <para>
      <command>libvirt</command> 可自动启用一组默认的超级管理程序功能（这些功能在大多数情况下已够用），同时还允许按需启用和禁用功能。例如，Xen 不支持默认启用 PCI 直通。必须使用<literal>passthrough</literal>设置来启用此功能。可以使用 <command>virsh</command> 来配置超级管理程序功能。查看 VM Guest 配置文件中的 <tag>&lt;features&gt;</tag> 元素，并根据需要调整 VM Guest 功能。仍以 Xen 直通为例：
    </para>

<screen><prompt>&gt; </prompt><command>sudo</command> virsh edit sle15sp1
 &lt;features&gt;
    &lt;xen&gt;
      &lt;passthrough/&gt;
    &lt;/xen&gt;
 &lt;/features&gt;
</screen>

    <para>
      保存更改并重启动 VM Guest。
    </para>

    <para>
      有关详细信息，请参见 <link xlink:href="https://libvirt.org/formatdomain.html#elementsFeatures"/> 上 libvirt 的《<citetitle>Domain XML format</citetitle>》手册中的“<citetitle>Hypervisor features</citetitle>”一节。
    </para>
  </sect1>
  <sect1 xml:id="libvirt-cpu-virsh">
    <title>配置 CPU</title>

    <para>
      可以使用 <command>virsh</command> 来配置提供给 VM Guest 的虚拟 CPU 的许多属性。可以更改分配给 VM Guest 的当前和最大 CPU 数量，以及 CPU 型号及其功能集。以下小节介绍如何更改 VM Guest 的常用 CPU 设置。
    </para>

    <sect2 xml:id="sec-libvirt-cpu-num-virsh">
      <title>配置 CPU 数量</title>
      <para>
        分配的 CPU 数量存储在 <filename>/etc/libvirt/qemu/</filename> 下 VM Guest XML 配置文件中的 <tag class="attribute">vcpu</tag> 元素内：
      </para>
<screen>&lt;vcpu placement='static'&gt;1&lt;/vcpu&gt;</screen>
      <para>
        在此示例中，只为 VM Guest 分配了一个 CPU。下面的过程说明如何更改分配给 VM Guest 的 CPU 数量：
      </para>
      <procedure>
        <step>
          <para>
            检查您的 VM Guest 是否处于非活动状态：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh list --inactive</command>
Id    Name                           State
----------------------------------------------------
-     sles15                         shut off</screen>
        </step>
        <step>
          <para>
            编辑现有 VM Guest 的配置：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh edit sles15</command></screen>
        </step>
        <step>
          <para>
            更改分配的 CPU 数量：
          </para>
<screen>&lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;</screen>
        </step>
        <step>
          <para>
            重启动 VM Guest：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh start sles15</command></screen>
        </step>
        <step>
          <para>
            检查 VM 中的 CPU 数量是否已更改。
          </para>
<screen>
<prompt>&gt; </prompt><command>sudo</command> <command>virsh vcpuinfo sled15</command>
VCPU:           0
CPU:            N/A
State:          N/A
CPU time        N/A
CPU Affinity:   yy

VCPU:           1
CPU:            N/A
State:          N/A
CPU time        N/A
CPU Affinity:   yy
</screen>
        </step>
      </procedure>
      <para>
        还可以在 VM Guest 正在运行时更改 CPU 数量。可以热插接 CPU，只要不超过 VM Guest 启动时配置的最大数量即可。同样，可以热拔除 CPU，只要不达到下限 1 即可。以下示例说明如何将活动 CPU 计数从 2 个更改为预定义的最大计数 4 个。
      </para>
      <procedure>
        <step>
          <para>
            检查当前的在线 vcpu 计数：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh vcpucount sles15 | grep live</command>
maximum      live           4
current      live           2
</screen>
        </step>
        <step>
          <para>
            将当前或活动的 CPU 数量更改为 4 个：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh setvcpus sles15 --count 4 --live</command></screen>
        </step>
        <step>
          <para>
            检查当前的在线 vcpu 计数现在是否为 4 个：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh vcpucount sles15 | grep live</command>
maximum      live           4
current      live           4
</screen>
        </step>
      </procedure>
      <important>
        <title>超过 255 个 CPU</title>
        <para>
          在 KVM 中可以定义 CPU 数量超过 255 个的 VM Guest。但是，需要完成额外的配置才能启动和运行 VM Guest。需要微调 <literal>ioapic</literal> 功能，并且需要将 IOMMU 设备添加到 VM Guest。下面是适用于 288 个 CPU 的示例配置。
        </para>
<screen>
&lt;domain&gt;
 &lt;vcpu placement='static'&gt;288&lt;/vcpu&gt;
 &lt;features&gt;
  &lt;ioapic driver='qemu'/&gt;
 &lt;/features&gt;
 &lt;devices&gt;
  &lt;iommu model='intel'&gt;
   &lt;driver intremap='on' eim='on'/&gt;
  &lt;/iommu&gt;
 &lt;/devices&gt;
&lt;/domain&gt;
</screen>
      </important>
    </sect2>

    <sect2 xml:id="sec-libvirt-cpu-model-virsh">
      <title>配置 CPU 型号</title>
      <para>
        向 VM Guest 公开的 CPU 型号往往会影响该 VM Guest 中运行的工作负载。默认 CPU 型号派生自一种名为 <literal>host-model</literal> 的 CPU 模式。
      </para>
<screen>&lt;cpu mode='host-model'/&gt;</screen>
      <para>
        启动 CPU 模式为 <literal>host-model</literal> 的 VM Guest 时，<systemitem class="library">libvirt</systemitem> 会将其主机 CPU 型号复制到 VM Guest 定义中。可以在 <command>virsh
        capabilities</command> 的输出中观察复制到 VM Guest 定义的主机 CPU 型号和功能。
      </para>
      <para>
        另一种有趣的 CPU 模式是 <literal>host-passthrough</literal>。
      </para>
<screen>&lt;cpu mode='host-passthrough'/&gt;</screen>
      <para>
        启动 CPU 模式为 <literal>host-passthrough</literal> 的 VM Guest 时，将为该 VM Guest 提供与 VM 主机服务器 CPU 完全相同的 CPU。当 <systemitem class="library">libvirt</systemitem> 的简化 <literal>host-model</literal> CPU 不能提供 VM Guest 工作负载所需的 CPU 功能时，该型号可能很有用。<literal>host-passthrough</literal> CPU 模式存在迁移能力下降的劣势。采用 <literal>host-passthrough</literal> CPU 模式的 VM Guest 只能迁移到具有相同硬件的 VM 主机服务器。
      </para>
      <para>
        使用 <literal>host-passthrough</literal> CPU 模式时，仍可以禁用不需要的功能。以下配置将为 VM Guest 提供与主机 CPU 完全相同的 CPU，但会禁用 <literal>vmx</literal> 功能。
      </para>
<screen>
&lt;cpu mode='host-passthrough'&gt;
  &lt;feature policy='disable' name='vmx'/&gt;
  &lt;/cpu&gt;
</screen>
      <para>
        <literal>custom</literal> CPU 模式是另一种常用模式，用于定义可在群集中不同主机之间迁移的规范化 CPU。例如，在主机包含 Nehalem、IvyBridge 和 SandyBridge CPU 的群集中，可以使用包含 Nehalem CPU 型号的 <literal>custom</literal> CPU 模式来配置 VM Guest。
      </para>
<screen>
&lt;cpu mode='custom' match='exact'&gt;
  &lt;model fallback='allow'&gt;Nehalem&lt;/model&gt;
  &lt;feature policy='require' name='vme'/&gt;
  &lt;feature policy='require' name='ds'/&gt;
  &lt;feature policy='require' name='acpi'/&gt;
  &lt;feature policy='require' name='ss'/&gt;
  &lt;feature policy='require' name='ht'/&gt;
  &lt;feature policy='require' name='tm'/&gt;
  &lt;feature policy='require' name='pbe'/&gt;
  &lt;feature policy='require' name='dtes64'/&gt;
  &lt;feature policy='require' name='monitor'/&gt;
  &lt;feature policy='require' name='ds_cpl'/&gt;
  &lt;feature policy='require' name='vmx'/&gt;
  &lt;feature policy='require' name='est'/&gt;
  &lt;feature policy='require' name='tm2'/&gt;
  &lt;feature policy='require' name='xtpr'/&gt;
  &lt;feature policy='require' name='pdcm'/&gt;
  &lt;feature policy='require' name='dca'/&gt;
  &lt;feature policy='require' name='rdtscp'/&gt;
  &lt;feature policy='require' name='invtsc'/&gt;
  &lt;/cpu&gt;
</screen>
      <para>
        有关 <systemitem class="library">libvirt</systemitem> 的 CPU 型号和拓扑选项的详细信息，请参见 <link xlink:href="https://libvirt.org/formatdomain.html#cpu-model-and-topology"/> 上的 <citetitle>CPU model and topology</citetitle> 文档。
      </para>
    </sect2>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-boot-menu-virsh">
    <title>更改引导选项</title>

    <para>
      可以在 <tag>os</tag> 元素中找到 VM Guest 的引导菜单，如以下示例所示：
    </para>

<screen>&lt;os&gt;
  &lt;type&gt;hvm&lt;/type&gt;
  &lt;loader&gt;readonly='yes' secure='no' type='rom'/&gt;/usr/lib/xen/boot/hvmloader&lt;/loader&gt;
  &lt;nvram template='/usr/share/OVMF/OVMF_VARS.fd'/&gt;/var/lib/libvirt/nvram/guest_VARS.fd&lt;/nvram&gt;
  &lt;boot dev='hd'/&gt;
  &lt;boot dev='cdrom'/&gt;
  &lt;bootmenu enable='yes' timeout='3000'/&gt;
  &lt;smbios mode='sysinfo'/&gt;
  &lt;bios useserial='yes' rebootTimeout='0'/&gt;
  &lt;/os&gt;</screen>

    <para>
      此示例中显示了两个设备：<tag class="attvalue">hd</tag> 和 <tag class="attvalue">cdrom</tag>。配置还反映了实际引导顺序，在示例中，<tag class="attvalue">hd</tag> 在 <tag class="attvalue">cdrom</tag> 之前引导。
    </para>

    <sect2 xml:id="sec-libvirt-config-bootorder-virsh">
      <title>更改引导顺序</title>
      <para>
        VM Guest 的引导顺序通过 XML 配置文件中的设备顺序来表示。由于设备可以互换，因此可以更改 VM Guest 的引导顺序。
      </para>
      <procedure>
        <step>
          <para>
            打开 VM Guest 的 XML 配置。
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh edit sles15</command></screen>
        </step>
        <step>
          <para>
            更改可引导设备的顺序。
          </para>
<screen>
...
&lt;boot dev='cdrom'/&gt;
&lt;boot dev='hd'/&gt;
...
      </screen>
        </step>
        <step>
          <para>
            通过查看 VM Guest 的 BIOS 中的引导菜单来检查引导顺序是否已更改。
          </para>
        </step>
      </procedure>
    </sect2>

    <sect2 xml:id="sec-libvirt-config-directkernel-virsh">
      <title>使用直接内核引导</title>
      <para>
        使用直接内核引导可以从主机上存储的内核和 initrd 引导。在 <tag>kernel</tag> 和 <tag>initrd</tag> 元素中设置这两个文件的路径：
      </para>
<screen>&lt;os&gt;
    ...
  &lt;kernel&gt;/root/f8-i386-vmlinuz&lt;/kernel&gt;
  &lt;initrd&gt;/root/f8-i386-initrd&lt;/initrd&gt;
    ...
&lt;os&gt;</screen>
      <para>
        要启用直接内核引导，请执行以下操作：
      </para>
      <procedure>
        <step>
          <para>
            打开 VM Guest 的 XML 配置：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh edit sles15</command></screen>
        </step>
        <step>
          <para>
            在 <tag>os</tag> 元素内部，添加一个 <tag>kernel</tag> 元素以及主机上内核文件的路径：
          </para>
<screen>...
&lt;kernel&gt;/root/f8-i386-vmlinuz&lt;/kernel&gt;
...</screen>
        </step>
        <step>
          <para>
            添加 <tag>initrd</tag> 元素以及主机上 initrd 文件的路径：
          </para>
<screen>...
&lt;initrd&gt;/root/f8-i386-initrd&lt;/initrd&gt;
...</screen>
        </step>
        <step>
          <para>
            启动 VM 以从新内核引导：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh start sles15</command></screen>
        </step>
      </procedure>
    </sect2>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-memory-virsh">
    <title>配置内存分配</title>

    <para>
      您还可以使用 <command>virsh</command> 来配置分配给 VM Guest 的内存量。该配置存储在 <tag>memory</tag> 元素中，它定义了引导时为 VM Guest 分配的最大内存。可选的 <tag>currentMemory</tag> 元素定义分配给 VM Guest 的实际内存。<tag>currentMemory</tag> 可以小于 <tag>memory</tag>，这样，就可以在 VM Guest 正在运行时增加（或<emphasis>扩大</emphasis>）内存。如果省略 <tag>currentMemory</tag>，则其默认值与 <tag>memory</tag> 元素的值相同。
    </para>

    <para>
      可以通过编辑 VM Guest 配置来调整内存设置，但请注意，更改只会在下次引导后生效。以下步骤说明如何将 VM Guest 更改为使用 4G 内存引导，但随后可以扩展到 8G：
    </para>

    <procedure>
      <step>
        <para>
          打开 VM Guest 的 XML 配置：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh edit sles15</command></screen>
      </step>
      <step>
        <para>
          搜索 <tag>memory</tag> 元素并设置为 8G：
        </para>
<screen>...
&lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
...</screen>
      </step>
      <step>
        <para>
          如果 <tag>currentMemory</tag> 元素不存在，请将其添加到 <tag>memory</tag> 元素下面，或将其值更改为 4G：
        </para>
<screen>
[...]
&lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
&lt;currentMemory unit='KiB'&gt;4194304&lt;/currentMemory&gt;
[...]</screen>
      </step>
    </procedure>

    <para>
      当 VM Guest 正在运行时，可以使用 <command>setmem</command> 子命令更改内存分配。以下示例显示如何将内存分配增加到 8G：
    </para>

    <procedure>
      <step>
        <para>
          检查 VM Guest 的现有内存设置：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh dominfo sles15 | grep memory</command>
Max memory:     8388608 KiB
Used memory:    4194608 KiB
</screen>
      </step>
      <step>
        <para>
          将使用的内存更改为 8G：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh setmem sles15 8388608</command></screen>
      </step>
      <step>
        <para>
          检查已更新的内存设置：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh dominfo sles15 | grep memory</command>
Max memory:     8388608 KiB
Used memory:    8388608 KiB
</screen>
      </step>
    </procedure>

    <important>
      <title>大内存 VM Guest</title>
      <para>
        内存要求为 4 TB 或以上的 VM Guest 必须使用 <literal>host-passthrough</literal> CPU 模式，或者在使用 <literal>host-model</literal> 或 <literal>custom</literal> CPU 模式时显式指定虚拟 CPU 地址大小。默认的虚拟 CPU 地址大小可能不足以满足 4 TB 或以上的内存配置。以下示例说明了在使用 <literal>host-model</literal> CPU 模式时如何使用 VM 主机服务器的物理 CPU 地址大小。
      </para>
<screen>
[...]
&lt;cpu mode='host-model' check='partial'&gt;
&lt;maxphysaddr mode='passthrough'&gt;
&lt;/cpu&gt;
[...]</screen>
      <para>
        有关指定虚拟 CPU 地址大小的详细信息，请参见 <link xlink:href="https://libvirt.org/formatdomain.html#cpu-model-and-topology"/> 上的 <citetitle>CPU model and
        topology</citetitle> 文档中的 <literal>maxphysaddr</literal> 选项。
      </para>
    </important>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-pci-virsh">
    <title>添加 PCI 设备</title>

    <para>
      要使用 <command>virsh</command> 将 PCI 设备指派到 VM Guest，请执行以下步骤：
    </para>

    <procedure>
      <step>
        <para>
          标识要指派到 VM Guest 的主机 PCI 设备。在下面的示例中，我们要将一块 DEC 网卡指派到 Guest：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>lspci -nn</command>
[...]
<emphasis role="bold">03:07.0</emphasis> Ethernet controller [0200]: Digital Equipment Corporation DECchip \
21140 [FasterNet] [1011:0009] (rev 22)
[...]</screen>
        <para>
          请记下设备 ID（在本例中为 <literal>03:07.0</literal>）。
        </para>
      </step>
      <step>
        <para>
          使用 <command>virsh
          nodedev-dumpxml <replaceable>ID</replaceable></command> 收集有关设备的详细信息。要获取 <replaceable>ID</replaceable>，请将设备 ID (<literal>03:07.0</literal>) 中的冒号和句点替换为下划线。使用<quote>pci_0000_</quote>作为结果的前缀：<literal>pci_0000_03_07_0</literal>。
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh nodedev-dumpxml pci_0000_03_07_0</command>
&lt;device&gt;
  &lt;name&gt;pci_0000_03_07_0&lt;/name&gt;
  &lt;path&gt;/sys/devices/pci0000:00/0000:00:14.4/0000:03:07.0&lt;/path&gt;
  &lt;parent&gt;pci_0000_00_14_4&lt;/parent&gt;
  &lt;driver&gt;
    &lt;name&gt;tulip&lt;/name&gt;
  &lt;/driver&gt;
  &lt;capability type='pci'&gt;
    <emphasis role="bold">&lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;3&lt;/bus&gt;
    &lt;slot&gt;7&lt;/slot&gt;
    &lt;function&gt;0&lt;/function&gt;</emphasis>
    &lt;product id='0x0009'&gt;DECchip 21140 [FasterNet]&lt;/product&gt;
    &lt;vendor id='0x1011'&gt;Digital Equipment Corporation&lt;/vendor&gt;
    &lt;numa node='0'/&gt;
  &lt;/capability&gt;
&lt;/device&gt;</screen>
        <para>
          记下域、总线和功能的值（请查看上面以粗体列显的 XML 代码）。
        </para>
      </step>
      <step>
        <para>
          从主机系统上分离设备，然后将其挂接到 VM Guest：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh nodedev-detach pci_0000_03_07_0</command>
  Device pci_0000_03_07_0 detached</screen>
        <tip>
          <title>多功能 PCI 设备</title>
          <para>
            使用不支持 FLR（功能级重置）或 PM（电源管理）重置的多功能 PCI 设备时，需要从 VM 主机服务器分离其所有功能。出于安全原因，整个设备都必须重置。如果 VM 主机服务器或其他 VM Guest 仍在使用设备的某个功能，<systemitem>libvirt</systemitem> 将拒绝指派该设备。
          </para>
        </tip>
      </step>
      <step>
        <para>
          将域、总线、插槽和功能值从十进制转换为十六进制。在本示例中，域 = 0，总线 = 3，插槽 = 7，功能 = 0。确保按正确顺序插入值：
        </para>
<screen><prompt>&gt; </prompt><command>printf "&lt;address domain='0x%x' bus='0x%x' slot='0x%x' function='0x%x'/&gt;\n" 0 3 7 0</command></screen>
        <para>
          这会返回以下结果：
        </para>
<screen>&lt;address domain='0x0' bus='0x3' slot='0x7' function='0x0'/&gt;</screen>
      </step>
      <step>
        <para>
          在您的域上运行 <command>virsh edit</command>，并使用上一步的结果在 <literal>&lt;devices&gt;</literal> 部分添加以下设备项：
        </para>
<screen>&lt;hostdev mode='subsystem' type='pci' managed='yes'&gt;
  &lt;source&gt;
    &lt;address domain='0x0' bus='0x03' slot='0x07' function='0x0'/&gt;
  &lt;/source&gt;
&lt;/hostdev&gt;</screen>
        <tip xml:id="tip-libvirt-config-pci-virsh-managed">
          <title><literal>managed</literal>与<literal>unmanaged</literal>模式的比较</title>
          <para>
            <systemitem>libvirt</systemitem> 可识别 PCI 设备的两种处理模式：<literal>managed</literal>或<literal>unmanaged</literal>。在受管模式下，<systemitem>libvirt</systemitem> 将处理从现有驱动程序取消绑定设备（如果需要）、重置设备、在启动域之前将设备绑定到 <systemitem>vfio-pci</systemitem> 等事项的所有细节。对于受管设备，当终止域或者从域中去除设备时，<systemitem>libvirt</systemitem> 会将设备从 <systemitem>vfio-pci</systemitem> 取消绑定，然后将其重新绑定到原始驱动程序。如果设备不受管，则用户必须确保在将设备指派到域之前以及在设备不再由域使用之后，所有这些设备管理方面的操作都已完成。
          </para>
        </tip>
        <para>
          在上面的示例中，<literal>managed='yes'</literal> 选项表示设备是受管的。要将设备切换为不受管模式，请在上面的列表中设置 <literal>managed='no'</literal>。如果这样做，需使用 <command>virsh nodedev-detach</command> 和 <command>virsh
          nodedev-reattach</command> 命令处理相关的驱动程序。在启动 VM Guest 之前，需运行 <command>virsh
          nodedev-detach pci_0000_03_07_0</command> 以从主机分离设备。如果 VM Guest 未运行，您可以运行 <command>virsh nodedev-reattach pci_0000_03_07_0</command>，使设备可供主机使用。
        </para>
      </step>
      <step>
        <para>
          关闭 VM Guest，并禁用 SELinux（如果它正在主机上运行）。
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>setsebool -P virt_use_sysfs 1</command></screen>
      </step>
      <step>
        <para>
          启动 VM Guest 以使指派的 PCI 设备可用：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh start sles15</command></screen>
      </step>
    </procedure>

    <important>
      <title>SLES11 SP4 KVM Guest</title>
      <para>
        在包含 SLES11 SP4 KVM Guest 的新型 QEMU 计算机（pc-i440fx-2.0 或更高版本）上，默认不会在 Guest 中装载 <systemitem class="resource">acpiphp</systemitem> 模块。必须装载此模块才能启用磁盘和网络设备热插拔功能。要手动装载该模块，请使用 <command>modprobe acpiphp</command> 命令。也可以通过在 <filename>/etc/modprobe.conf.local</filename> 文件中添加 <literal>install
        acpiphp /bin/true</literal> 来自动装载该模块。
      </para>
    </important>

    <important>
      <title>使用 QEMU Q35 计算机类型的 KVM Guest</title>
      <para>
        使用 QEMU Q35 计算机类型的 KVM Guest 采用 PCI 拓扑，其中包含一个 <literal>pcie-root</literal> 控制器和七个 <literal>pcie-root-port</literal> 控制器。<literal>pcie-root</literal> 控制器不支持热插拔。每个 <literal>pcie-root-port</literal> 控制器支持热插拔一个 PCIe 设备。PCI 控制器无法热插拔，因此，如果要热插拔的 PCIe 设备超过七个，请相应地做好规划并添加更多 <literal>pcie-root-port</literal> 控制器。可以添加一个 <literal>pcie-to-pci-bridge</literal> 控制器来支持热插拔旧式 PCI 设备。有关不同 QEMU 计算机类型的 PCI 拓扑的详细信息，请参见 <link xlink:href="https://libvirt.org/pci-hotplug.html"/>。
      </para>
    </important>

    <sect2 xml:id="tip-libvirt-config-zpci">
      <title>IBM Z 的 PCI 直通</title>
      <para>
        为了支持 IBM Z，QEMU 扩展了 PCI 表示形式，现在它允许用户配置额外的属性。<literal>&lt;zpci/&gt;</literal> <systemitem class="library">libvirt</systemitem> 规范中额外添加了两个属性 — <option>uid</option> 和 <option>fid</option>。<option>uid</option> 表示用户定义的标识符，<option>fid</option> 表示 PCI 功能标识符。这些属性是可选的，如果不指定，系统将使用不冲突的值自动生成这些属性。
      </para>
      <para>
        要在域规范中包含 zPCI 属性，请使用以下示例定义：
      </para>
<screen>
&lt;controller type='pci' index='0' model='pci-root'/&gt;
&lt;controller type='pci' index='1' model='pci-bridge'&gt;
  &lt;model name='pci-bridge'/&gt;
  &lt;target chassisNr='1'/&gt;
  &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x0'&gt;
    &lt;zpci uid='0x0001' fid='0x00000000'/&gt;
  &lt;/address&gt;
&lt;/controller&gt;
&lt;interface type='bridge'&gt;
  &lt;source bridge='virbr0'/&gt;
  &lt;model type='virtio'/&gt;
  &lt;address type='pci' domain='0x0000' bus='0x01' slot='0x01' function='0x0'&gt;
    &lt;zpci uid='0x0007' fid='0x00000003'/&gt;
  &lt;/address&gt;
&lt;/interface&gt;
</screen>
    </sect2>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-usb-virsh">
    <title>添加 USB 设备</title>

    <para>
      要使用 <command>virsh</command> 将 USB 设备指派到 VM Guest，请执行以下步骤：
    </para>

    <procedure>
      <step>
        <para>
          标识要指派到 VM Guest 的主机 USB 设备：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>lsusb</command>
[...]
Bus 001 Device 003: ID <emphasis role="bold">0557:2221</emphasis> ATEN International Co., Ltd Winbond Hermon
[...]</screen>
        <para>
          记下供应商 ID 和产品 ID。在本示例中，供应商 ID 为 <literal>0557</literal>，产品 ID 为 <literal>2221</literal>。
        </para>
      </step>
      <step>
        <para>
          在您的域上运行 <command>virsh edit</command>，并使用上一步的值在 <literal>&lt;devices&gt;</literal> 部分添加以下设备项：
        </para>
<screen>&lt;hostdev mode='subsystem' type='usb'&gt;
  &lt;source startupPolicy='optional'&gt;
   <emphasis role="bold">&lt;vendor id='0557'/&gt;
   &lt;product id='2221'/&gt;</emphasis>
  &lt;/source&gt;
&lt;/hostdev&gt;</screen>
        <tip>
          <title>供应商/产品或设备地址</title>
          <para>
            如果不使用 <tag class="emptytag">vendor</tag> 和 <tag class="emptytag">product</tag> ID 定义主机设备，您可以根据<xref linkend="sec-libvirt-config-pci-virsh"/>中所述的适用于主机 PCI 设备的操作使用 <tag class="emptytag">address</tag> 元素。
          </para>
        </tip>
      </step>
      <step>
        <para>
          关闭 VM Guest，并禁用 SELinux（如果它正在主机上运行）：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>setsebool -P virt_use_sysfs 1</command></screen>
      </step>
      <step>
        <para>
          启动 VM Guest 以使指派的 PCI 设备可用：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh start sles15</command></screen>
      </step>
    </procedure>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-io">
    <title>添加 SR-IOV 设备</title>

    <para>
      支持单根 I/O 虚拟化 (<xref linkend="vt-io-sriov"/>) 的 <xref linkend="gloss-vt-acronym-pcie"/> 设备能够复制其资源，因此它们看上去像是多个设备。可将其中的每个<quote>伪设备</quote>指派到 VM Guest。
    </para>

    <para>
      <xref linkend="vt-io-sriov"/> 是外围部件互连专业组 (PCI-SIG) 联盟制定的行业规范。其中介绍了物理功能 (PF) 和虚拟功能 (VF)。PF 是用于管理和配置设备的完整 <xref linkend="gloss-vt-acronym-pcie"/> 功能。PF 还可以移动数据。VF 在配置和管理方面的作用有所欠缺 — 它们只能移动数据，提供的配置功能有限。由于 VF 不包括所有的 <xref linkend="gloss-vt-acronym-pcie"/> 功能，主机操作系统或<xref linkend="gloss-vt-hypervisor"/>必须支持 <xref linkend="vt-io-sriov"/> 才能访问和初始化 VF。理论上 VF 的最大数量为每台设备 256 个（因此，对于双端口以太网卡，最大数量为 512 个）。在实际环境中，此最大数量要少得多，因为每个 VF 都会消耗资源。
    </para>

    <sect2 xml:id="sec-libvirt-config-io-requirements">
      <title>要求</title>
      <para>
        要使用 <xref linkend="vt-io-sriov"/>，必须符合以下要求：
      </para>
      <itemizedlist mark="bullet" spacing="normal">
        <listitem>
          <para>
            支持 <xref linkend="vt-io-sriov"/> 的网卡（从 <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise Server</phrase></phrase>
            <phrase role="productnumber"><phrase os="sles;sled">15</phrase></phrase> 开始，只有网卡支持 <xref linkend="vt-io-sriov"/>）
          </para>
        </listitem>
        <listitem>
          <para>
            支持硬件虚拟化的 AMD64/Intel 64 主机（AMD-V 或 Intel VT-x）<phrase os="sles;sled">，有关详细信息，请参见<xref linkend="sec-kvm-requires-hardware"/></phrase>
          </para>
        </listitem>
        <listitem>
          <para>
            支持设备指派的芯片组（AMD-Vi 或 Intel <xref linkend="gloss-vt-acronym-vtd"/>）
          </para>
        </listitem>
        <listitem>
          <para>
            <systemitem class="library">libvirt</systemitem> 0.9.10 或更高版本
          </para>
        </listitem>
        <listitem>
          <para>
            主机系统上必须装载并配置 <xref linkend="vt-io-sriov"/> 驱动程序
          </para>
        </listitem>
        <listitem>
          <para>
            符合<xref linkend="ann-vt-io-require"/>中所列要求的主机配置
          </para>
        </listitem>
        <listitem>
          <para>
            要指派到 VM Guest 的 VF 的 PCI 地址列表
          </para>
        </listitem>
      </itemizedlist>
      <tip>
        <title>检查设备是否支持 SR-IOV</title>
        <para>
          可以通过运行 <command>lspci</command> 从设备的 PCI 描述符中获取有关该设备是否支持 SR-IOV 的信息。支持 <xref linkend="vt-io-sriov"/> 的设备会报告类似如下的功能：
        </para>
<screen>Capabilities: [160 v1] Single Root I/O Virtualization (<xref linkend="vt-io-sriov"/>)</screen>
      </tip>
      <note>
        <title>在创建 VM Guest 时添加 SR-IOV 设备</title>
        <para>
          您必须已按照<xref linkend="sec-libvirt-config-io-config"/>中所述配置 VM 主机服务器，才可在最初设置 VM Guest 时向其添加 SR-IOV 设备。
        </para>
      </note>
    </sect2>

    <sect2 xml:id="sec-libvirt-config-io-config">
      <title>装载和配置 SR-IOV 主机驱动程序</title>
      <para>
        要访问和初始化 VF，需在主机系统上装载一个支持 SR-IOV 的驱动程序。
      </para>
      <procedure>
        <step>
          <para>
            在装载驱动程序之前，请运行 <command>lspci</command> 来确保可正常检测到网卡。以下示例显示了双端口 Intel 82576NS 网卡的 <command>lspci</command> 输出：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>/sbin/lspci | grep 82576</command>
01:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)</screen>
          <para>
            如果未检测到网卡，可能是因为未在 BIOS/EFI 中启用硬件虚拟化支持。要检查是否已启用硬件虚拟化支持，请查看主机 BIOS 中的设置。
          </para>
        </step>
        <step>
          <para>
            运行 <command>lsmod</command> 来检查是否已装载 <xref linkend="vt-io-sriov"/> 驱动程序。在以下示例中，用于检查是否装载了 Intel 82576NS 网卡的 igb 驱动程序的命令返回了一条结果。这表示已装载该驱动程序。如果该命令未返回任何结果，则表示未装载该驱动程序。
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>/sbin/lsmod | egrep "^igb "</command>
igb                   185649  0</screen>
        </step>
        <step>
          <para>
            如果已装载驱动程序，请跳过以下步骤。如果尚未装载 <xref linkend="vt-io-sriov"/> 驱动程序，需要先去除非 <xref linkend="vt-io-sriov"/> 驱动程序，然后再装载新驱动程序。使用 <command>rmmod</command> 卸载驱动程序。下面的示例会卸载 Intel 82576NS 网卡的非 <xref linkend="vt-io-sriov"/> 驱动程序：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>/sbin/rmmod igbvf</command></screen>
        </step>
        <step>
          <para>
            随后使用 <command>modprobe</command> 命令装载 <xref linkend="vt-io-sriov"/> 驱动程序 — 必须指定 VF 参数 (<literal>max_vfs</literal>)：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>/sbin/modprobe igb max_vfs=8</command></screen>
        </step>
      </procedure>
      <remark>Unsure if the following procedure is really needed.</remark>
      <para>
        或者，您也可以通过 SYSFS 装载驱动程序：
      </para>
      <procedure>
        <step>
          <para>
            通过列出以太网设备确定物理 NIC 的 PCI ID：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>lspci | grep Eth</command>
06:00.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:00.1 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)</screen>
        </step>
        <step>
          <para>
            要启用 VF，请向 <literal>sriov_numvfs</literal> 参数回送需要装载的 VF 数量：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>echo 1 &gt; /sys/bus/pci/devices/0000:06:00.1/sriov_numvfs</command></screen>
        </step>
        <step>
          <para>
            校验是否已装载 VF NIC：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>lspci | grep Eth</command>
06:00.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:00.1 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)
06:08.0 Ethernet controller: Emulex Corporation OneConnect NIC (Skyhawk) (rev 10)</screen>
        </step>
        <step>
          <para>
            获取可用 VF 的最大数量：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>lspci -vvv -s 06:00.1 | grep 'Initial VFs'</command>
                       Initial VFs: 32, Total VFs: 32, Number of VFs: 0,
Function Dependency Link: 01</screen>
        </step>
        <step>
          <para>
            创建 <filename>/etc/systemd/system/before.service</filename> 文件，用于在引导时通过 SYSFS 装载 VF：
          </para>
<screen>[Unit]
Before=
[Service]
Type=oneshot
RemainAfterExit=true
ExecStart=/bin/bash -c "echo 1 &gt; /sys/bus/pci/devices/0000:06:00.1/sriov_numvfs"
# beware, executable is run directly, not through a shell, check the man pages
# systemd.service and systemd.unit for full syntax
[Install]
# target in which to start the service
WantedBy=multi-user.target
#WantedBy=graphical.target</screen>
        </step>
        <step>
          <para>
            在启动 VM 之前，需要创建指向 <filename>/etc/init.d/after.local</filename> 脚本（用于分离 NIC）的另一个服务文件 (<filename>after-local.service</filename>)。否则 VM 将无法启动：
          </para>
<screen>[Unit]
Description=/etc/init.d/after.local Compatibility
After=libvirtd.service
Requires=libvirtd.service
[Service]
Type=oneshot
ExecStart=/etc/init.d/after.local
RemainAfterExit=true

[Install]
WantedBy=multi-user.target</screen>
        </step>
        <step>
          <para>
            将此文件复制到 <filename>/etc/systemd/system</filename>。
          </para>
<screen>#! /bin/sh
# ...
virsh nodedev-detach pci_0000_06_08_0</screen>
          <para>
            将此文件另存为 <filename>/etc/init.d/after.local</filename>。
          </para>
        </step>
        <step>
          <para>
            重引导计算机，然后根据本过程的第一步所述，重新运行 <command>lspci</command> 命令检查是否已装载 SR-IOV 驱动程序。如果已成功装载 SR-IOV 驱动程序，您应该会看到额外的 VF 行：
          </para>
<screen>01:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
01:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
01:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
[...]
04:00.0 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:00.1 Ethernet controller: Intel Corporation 82576NS Gigabit Network Connection (rev 01)
04:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
04:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
04:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
[...]</screen>
        </step>
      </procedure>
    </sect2>

    <sect2 xml:id="sec-libvirt-config-io-attach">
      <title>将 VF 网络设备添加到 VM Guest</title>
      <para>
        在 VM 主机服务器上正确设置 <xref linkend="vt-io-sriov"/> 硬件后，便可将 VF 添加到 VM Guest。为此，需要先收集特定的数据。
      </para>
      <procedure>
        <title>将 VF 网络设备添加到现有 VM Guest</title>
        <para>
          下面的过程使用的是示例数据。请将其替换为您设置中的相应数据。
        </para>
        <step>
          <para>
            使用 <command>virsh nodedev-list</command> 命令获取您要指派的 VF 的 PCI 地址及其对应的 PF。<xref linkend="sec-libvirt-config-io-config"/>中所示的 <command>lspci</command> 输出中的数字值（例如 <literal>01:00.0</literal> 或 <literal>04:00.1</literal>）已经过转换：添加了前缀 <literal>pci_0000_</literal> 并将冒号和句点替换为下划线。因此，<command>lspci</command> 列出的 PCI ID <literal>04:00.0</literal> 会被 virsh 列为 <literal>pci_0000_04_00_0</literal>。下面的示例列出了 Intel 82576NS 网卡的第二个端口的 PCI ID：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh nodedev-list | grep 0000_04_</command>
<emphasis role="bold">pci_0000_04_00_0</emphasis>
<emphasis role="bold">pci_0000_04_00_1</emphasis>
pci_0000_04_10_0
pci_0000_04_10_1
pci_0000_04_10_2
pci_0000_04_10_3
pci_0000_04_10_4
pci_0000_04_10_5
pci_0000_04_10_6
pci_0000_04_10_7
pci_0000_04_11_0
pci_0000_04_11_1
pci_0000_04_11_2
pci_0000_04_11_3
pci_0000_04_11_4
pci_0000_04_11_5</screen>
          <para>
            前两个项表示 <emphasis role="bold">PF</emphasis>，其他项表示 VF。
          </para>
        </step>
        <step>
          <para>
            对您要添加的 VF 的 PCI ID 运行以下 <command>virsh nodedev-dumpxml</command> 命令：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh nodedev-dumpxml pci_0000_04_10_0</command>
&lt;device&gt;
  &lt;name&gt;pci_0000_04_10_0&lt;/name&gt;
  &lt;parent&gt;pci_0000_00_02_0&lt;/parent&gt;
  &lt;capability type='pci'&gt;
    <emphasis role="bold">&lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;4&lt;/bus&gt;
    &lt;slot&gt;16&lt;/slot&gt;
    &lt;function&gt;0&lt;/function&gt;</emphasis>
    &lt;product id='0x10ca'&gt;82576 Virtual Function&lt;/product&gt;
    &lt;vendor id='0x8086'&gt;Intel Corporation&lt;/vendor&gt;
    &lt;capability type='phys_function'&gt;
      &lt;address domain='0x0000' bus='0x04' slot='0x00' function='0x0'/&gt;
    &lt;/capability&gt;
  &lt;/capability&gt;
&lt;/device&gt;</screen>
          <para>
            下一步需要以下数据：
          </para>
          <itemizedlist>
            <listitem>
              <para>
                <literal>&lt;domain&gt;0&lt;/domain&gt;</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>&lt;bus&gt;4&lt;/bus&gt;</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>&lt;slot&gt;16&lt;/slot&gt;</literal>
              </para>
            </listitem>
            <listitem>
              <para>
                <literal>&lt;function&gt;0&lt;/function&gt;</literal>
              </para>
            </listitem>
          </itemizedlist>
        </step>
        <step>
          <para>
            创建一个临时 XML 文件（例如 <filename>/tmp/vf-interface.xml</filename>），其中包含将 VF 网络设备添加到现有 VM Guest 所需的数据。该文件至少需包含如下所示的内容：
          </para>
<screen>&lt;interface type='hostdev'&gt;<co xml:id="sriov-iface"/>
 &lt;source&gt;
  &lt;address type='pci' domain='0' bus='11' slot='16' function='0'2/&gt;<co xml:id="sriov-data"/>
 &lt;/source&gt;
&lt;/interface&gt;</screen>
          <calloutlist>
            <callout arearefs="sriov-iface">
              <para>
                VF 的 MAC 地址不固定；每次重引导主机后，MAC 地址都会改变。使用 <tag class="attribute">hostdev</tag> 以<quote>传统</quote>方式添加网络设备时，每次重引导主机后都需要重新配置 VM Guest 的网络设备，因为 MAC 地址会改变。为避免出现这种问题，<systemitem class="library">libvirt</systemitem> 引入了 <tag class="attvalue">hostdev</tag> 值用于在指派设备<emphasis>之前</emphasis>设置网络特定的数据。
              </para>
            </callout>
            <callout arearefs="sriov-data">
              <para>
                请在此处指定上一步中获取的数据。
              </para>
            </callout>
          </calloutlist>
        </step>
        <step>
          <para>
            如果设备已挂接到主机，则无法将它挂接到 VM Guest。要使该设备可供 Guest 使用，请先将它从主机分离：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh nodedev-detach pci_0000_04_10_0</command></screen>
        </step>
        <step>
          <para>
            将 VF 接口添加到现有 VM Guest：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh attach-device <replaceable>GUEST</replaceable> /tmp/vf-interface.xml --<replaceable>OPTION</replaceable></command></screen>
          <para>
            需将 <replaceable>GUEST</replaceable> 替换为 VM Guest 的域名、ID 或 UUID。--<replaceable>OPTION</replaceable> 可为下列其中一项：
          </para>
          <variablelist>
            <varlistentry>
              <term><option>--persistent</option></term>
              <listitem>
                <para>
                  此选项始终将设备添加到域的永久性 XML 中。如果域正在运行，则会热插入设备。
                </para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term><option>--config</option></term>
              <listitem>
                <para>
                  此选项只影响永久性 XML，即使域正在运行也是如此。设备在下次引导后才会显示在 VM Guest 中。
                </para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term><option>--live</option></term>
              <listitem>
                <para>
                  此选项只影响正在运行的域。如果域处于非活动状态，则操作将会失败。设备不会永久保留在 XML 中，因此下次引导后将在 VM Guest 中可用。
                </para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term><option>--current</option></term>
              <listitem>
                <para>
                  此选项影响域的当前状态。如果域处于非活动状态，设备将添加到永久性 XML 中，因此将在下次引导后可用。如果域处于活动状态，则会热插入设备，但不会将其添加到永久性 XML 中。
                </para>
              </listitem>
            </varlistentry>
          </variablelist>
        </step>
        <step>
          <para>
            要分离 VF 接口，请使用 <command>virsh
            detach-device</command> 命令，该命令也接受上面所列的选项。
          </para>
        </step>
      </procedure>
    </sect2>

    <sect2 xml:id="libvirt-config-io-pool">
      <title>动态分配池中的 VF</title>
      <para>
        如果您按照<xref linkend="sec-libvirt-config-io-attach"/>中所述以静态方式在 VM Guest 的配置中定义了 VF 的 PCI 地址，此类 Guest 将很难迁移到另一台主机。该主机必须在 PCI 总线上的相同位置具有相同的硬件，否则每次启动之前都必须修改 VM Guest 配置。
      </para>
      <para>
        另一种方法是使用一个包含 <xref linkend="vt-io-sriov"/> 设备所有 VF 的设备池创建 <systemitem class="library">libvirt</systemitem> 网络。之后，VM Guest 将引用此网络，每次 VM Guest 启动时，系统都会向它动态分配单个 VF。当 VM Guest 停止时，该 VF 将返回到池中，可供其他 Guest 使用。
      </para>
      <sect3 xml:id="libvirt-config-io-pool-host">
        <title>在 VM 主机服务器上使用 VF 池定义网络</title>
        <para>
          以下网络定义示例为 <xref linkend="vt-io-sriov"/> 设备创建了一个包含所有 VF 的池，该设备的物理功能 (PF) 位于主机中的网络接口 <literal>eth0</literal> 上：
        </para>
<screen>&lt;network&gt;
  &lt;name&gt;passthrough&lt;/name&gt;
    &lt;forward mode='hostdev' managed='yes'&gt;
      &lt;pf dev='eth0'/&gt;
    &lt;/forward&gt;
  &lt;/network&gt;</screen>
        <para>
          要在主机上使用此网络，请将上述代码保存到文件（例如 <filename>/tmp/passthrough.xml</filename>）中，然后执行以下命令。请记得将 <literal>eth0</literal> 替换为 <xref linkend="vt-io-sriov"/> 设备的 PF 的实际网络接口名称：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh net-define /tmp/passthrough.xml</command>
<prompt>&gt; </prompt><command>sudo</command> <command>virsh net-autostart passthrough</command>
<prompt>&gt; </prompt><command>sudo</command> <command>virsh net-start passthrough</command></screen>
      </sect3>
      <sect3 xml:id="libvirt-config-io-pool-guest">
        <title>将 VM Guest 配置为使用池中的 VF</title>
        <para>
          下面的 VM Guest 设备接口定义示例使用<xref linkend="libvirt-config-io-pool-host"/>中为 <xref linkend="vt-io-sriov"/> 设备创建的池中的某个 VF。Guest 首次启动时，<systemitem class="library">libvirt</systemitem> 会自动派生与该 PF 关联的所有 VF 的列表。
        </para>
<screen>&lt;interface type='network'&gt;
  &lt;source network='passthrough'&gt;
&lt;/interface&gt;</screen>
        <para>
          在第一个使用以 VF 池定义的网络的 VM Guest 启动后，校验关联的 VF 列表。为此，请在主机上运行 <command>virsh net-dumpxml passthrough</command>。
        </para>
<screen>&lt;network connections='1'&gt;
  &lt;name&gt;passthrough&lt;/name&gt;
  &lt;uuid&gt;a6a26429-d483-d4ed-3465-4436ac786437&lt;/uuid&gt;
  &lt;forward mode='hostdev' managed='yes'&gt;
    &lt;pf dev='eth0'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x1'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x3'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x5'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x10' function='0x7'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x1'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x3'/&gt;
    &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x11' function='0x5'/&gt;
  &lt;/forward&gt;
  &lt;/network&gt;</screen>
      </sect3>
    </sect2>
  </sect1>
  <sect1 xml:id="libvirt-config-listing-host-devs">
    <title>列出挂接的设备</title>

    <para>
      尽管 <command>virsh</command> 中没有任何机制可列出 VM 主机服务器中已挂接到其 VM Guest 的所有设备，但您可以通过运行以下命令列出已挂接到特定 VM Guest 的所有设备：
    </para>

<screen>virsh dumpxml <replaceable>VMGUEST_NAME</replaceable> | xpath -e /domain/devices/hostdev</screen>

    <para>
      例如：
    </para>

<screen>
<prompt>&gt; </prompt><command>sudo</command> virsh dumpxml sles12 | -e xpath /domain/devices/hostdev
Found 2 nodes:
-- NODE --
&lt;hostdev mode="subsystem" type="pci" managed="yes"&gt;
  &lt;driver name="xen" /&gt;
  &lt;source&gt;
    &lt;address domain="0x0000" bus="0x0a" slot="0x10" function="0x1" /&gt;
  &lt;/source&gt;
  &lt;address type="pci" domain="0x0000" bus="0x00" slot="0x0a" function="0x0" /&gt;
  &lt;/hostdev&gt;
-- NODE --
&lt;hostdev mode="subsystem" type="pci" managed="yes"&gt;
  &lt;driver name="xen" /&gt;
  &lt;source&gt;
    &lt;address domain="0x0000" bus="0x0a" slot="0x10" function="0x2" /&gt;
  &lt;/source&gt;
  &lt;address type="pci" domain="0x0000" bus="0x00" slot="0x0b" function="0x0" /&gt;
&lt;/hostdev&gt;
</screen>

    <tip xml:id="libvirt-config-listing-host-devs-sriov">
      <title>列出通过 <literal>&lt;interface type='hostdev'&gt;</literal> 挂接的 SR-IOV 设备</title>
      <para>
        对于通过 <literal>&lt;interface type='hostdev'&gt;</literal> 挂接到 VM 主机服务器的 SR-IOV 设备，需要使用不同的 XPath 查询：
      </para>
<screen>virsh dumpxml <replaceable>VMGUEST_NAME</replaceable> | xpath -e /domain/devices/interface/@type</screen>
    </tip>
  </sect1>
  <sect1 xml:id="libvirt-config-storage-virsh">
    <title>配置存储设备</title>

    <para>
      存储设备在 <tag>disk</tag> 元素中定义。一般的 <tag>disk</tag> 元素支持多个属性。下面是两个最重要的属性：
    </para>

    <itemizedlist>
      <listitem>
        <para>
          <tag class="attribute">type</tag> 属性描述虚拟磁盘设备的来源。有效值为 <tag class="attvalue">file</tag>、<tag class="attvalue">block</tag>、<tag class="attvalue">dir</tag>、<tag class="attvalue">network</tag> 或 <tag class="attvalue">volume</tag>。
        </para>
      </listitem>
      <listitem>
        <para>
          <tag class="attribute">device</tag> 属性显示如何向 VM Guest 操作系统公开磁盘。例如，可能的值可能包括 <tag class="attvalue">floppy</tag>、<tag class="attvalue">disk</tag>、<tag class="attvalue">cdrom</tag> 等。
        </para>
      </listitem>
    </itemizedlist>

    <para>
      下面是最重要的子元素：
    </para>

    <itemizedlist>
      <listitem>
        <para>
          <tag>driver</tag>包含驱动程序和总线。VM Guest 使用驱动程序和总线来操作新磁盘设备。
        </para>
      </listitem>
      <listitem>
        <para>
          <tag>target</tag> 元素包含新磁盘显示在 VM Guest 中时所用的设备名称。它还包含可选的总线属性，该属性定义用于操作新磁盘的总线的类型。
        </para>
      </listitem>
    </itemizedlist>

    <para>
      下面的过程说明如何将存储设备添加到 VM Guest：
    </para>

    <procedure>
      <step>
        <para>
          编辑现有 VM Guest 的配置：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh edit sles15</command></screen>
      </step>
      <step>
        <para>
          在 <tag>disk</tag> 元素内部，连同属性 <tag class="attvalue">type</tag> 和 <tag class="attvalue">device</tag> 一起添加 <tag>disk</tag> 元素：
        </para>
<screen>&lt;disk type='file' device='disk'&gt;</screen>
      </step>
      <step>
        <para>
          指定 <tag>driver</tag> 元素并使用默认值：
        </para>
<screen>&lt;driver name='qemu' type='qcow2'/&gt;</screen>
      </step>
      <step>
        <para>
          创建一个磁盘映像作为新虚拟磁盘设备的来源：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>qemu-img create -f qcow2 /var/lib/libvirt/images/sles15.qcow2 32G</command></screen>
      </step>
      <step>
        <para>
          添加磁盘来源的路径：
        </para>
<screen>&lt;source file='/var/lib/libvirt/images/sles15.qcow2'/&gt;</screen>
      </step>
      <step>
        <para>
          定义 VM Guest 中的目标设备名以及用于操作磁盘的总线：
        </para>
<screen>&lt;target dev='vda' bus='virtio'/&gt;</screen>
      </step>
      <step>
        <para>
          重启动您的 VM：
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh start sles15</command></screen>
      </step>
    </procedure>

    <para>
      现在，新存储设备在 VM Guest 操作系统中应该可供使用。
    </para>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-controllers-virsh">
    <title>配置控制器设备</title>

    <para>
      <command>libvirt</command> 根据 VM Guest 使用的虚拟设备类型自动管理控制器。如果 VM Guest 包含 PCI 和 SCSI 设备，libvirt 将自动创建并管理 PCI 和 SCSI 控制器。<command>libvirt</command> 还可为特定于超级管理程序的控制器（例如 KVM VM Guest 的 <literal>virtio-serial</literal> 控制器，或 Xen VM Guest 的 <literal>xenbus</literal> 控制器）建模。尽管默认控制器及其配置在一般情况下都可满足需求，但在某些用例中，需要手动调整控制器或其属性。例如，virtio-serial 控制器可能需要更多端口，或者 xenbus 控制器可能需要更多内存或更多虚拟中断。
    </para>

    <para>
      Xenbus 控制器的独特之处在于，它充当着所有 Xen 半虚拟设备的控制器。如果 VM Guest 包含许多磁盘和/或网络设备，则控制器可能需要更多内存。Xen 的 <literal>max_grant_frames</literal> 属性设置要将多少授权帧或共享内存块分配给每个 VM Guest 的 <literal>xenbus</literal> 控制器。
    </para>

    <para>
      默认值 32 在大多数情况下已够用，但包含大量 I/O 设备的 VM Guest 以及 I/O 密集型工作负载可能会由于授权帧耗尽而发生性能问题。<command>xen-diag</command> 可以检查 dom0 与 VM Guest 的当前和最大 <literal>max_grant_frames</literal> 值。VM Guest 必须正在运行：
    </para>

<screen><prompt>&gt; </prompt><command>sudo</command> virsh list
 Id   Name             State
--------------------------------
 0    Domain-0         running
 3    sle15sp1         running

 <prompt>&gt; </prompt><command>sudo</command> xen-diag gnttab_query_size 0
domid=0: nr_frames=1, max_nr_frames=256

<prompt>&gt; </prompt><command>sudo</command> xen-diag gnttab_query_size 3
domid=3: nr_frames=3, max_nr_frames=32
</screen>

    <para>
      <literal>sle15sp1</literal> Guest 仅使用了 32 个帧中的 3 个。如果您发现了性能问题并且有日志项指出帧数不足，请使用 <command>virsh</command> 增大该值。查看 Guest 配置文件中的 <literal>&lt;controller type='xenbus'&gt;</literal> 行，并添加 <literal>maxGrantFrames</literal> 控制元素：
    </para>

<screen><prompt>&gt; </prompt><command>sudo</command> virsh edit sle15sp1
 &lt;controller type='xenbus' index='0' maxGrantFrames='40'/&gt;
</screen>

    <para>
      保存更改并重启动 Guest。现在，xen-diag 命令应该会显示您的更改：
    </para>

<screen><prompt>&gt; </prompt><command>sudo</command> xen-diag gnttab_query_size 3
domid=3: nr_frames=3, max_nr_frames=40
</screen>

    <para>
      与 maxGrantFrames 类似，xenbus 控制器也支持 <option>maxEventChannels</option>。事件通道类似于半虚拟中断，它们与授权帧共同构成半虚拟驱动程序的数据传输机制。它们还用于处理器间的中断。包含大量 vCPU 和/或许多半虚拟设备的 VM Guest 可能需要增大最大默认值 1023。可以像更改 maxGrantFrames 那样更改 maxEventChannels：
    </para>

<screen><prompt>&gt; </prompt><command>sudo</command> virsh edit sle15sp1
 &lt;controller type='xenbus' index='0' maxGrantFrames='128' maxEventChannels='2047'/&gt;
</screen>

    <para>
      有关详细信息，请参见 <link xlink:href="https://libvirt.org/formatdomain.html#elementsControllers"/> 上 libvirt 的《<citetitle>Domain XML format</citetitle>》手册中的“<citetitle>Controllers</citetitle>”一节。
    </para>
  </sect1>
  <sect1 xml:id="libvirt-video-virsh">
    <title>配置视频设备</title>

    <para>
      使用虚拟机管理器时，只能定义视频设备型号。只能在 XML 配置中更改分配的 VRAM 量或 2D/3D 加速。
    </para>

    <sect2 xml:id="libvirt-video-vram-virsh">
      <title>更改分配的 VRAM 量</title>
      <procedure>
        <step>
          <para>
            编辑现有 VM Guest 的配置：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh edit sles15</command></screen>
        </step>
        <step>
          <para>
            更改分配的 VRAM 大小：
          </para>
<screen>&lt;video&gt;
&lt;model type='vga' vram='65535' heads='1'&gt;
...
&lt;/model&gt;
&lt;/video&gt;</screen>
        </step>
        <step>
          <para>
            通过查看虚拟机管理器中的数量来检查 VM 中的 VRAM 量是否已更改。
          </para>
        </step>
      </procedure>
    </sect2>

    <sect2 xml:id="libvirt-video-accel-virsh">
      <title>更改 2D/3D 加速状态</title>
      <procedure>
        <step>
          <para>
            编辑现有 VM Guest 的配置：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>virsh edit sles15</command></screen>
        </step>
        <step>
          <para>
            要启用/禁用 2D/3D 加速，请相应地更改 <literal>accel3d</literal> 和 <literal>accel2d</literal> 的值：
          </para>
<screen>
&lt;video&gt;
 &lt;model&gt;
  &lt;acceleration accel3d='yes' accel2d='no'&gt;
 &lt;/model&gt;
&lt;/video&gt;</screen>
        </step>
      </procedure>
      <tip>
        <title>启用 2D/3D 加速</title>
        <para>
          只有 <literal>virtio</literal> 和 <literal>vbox</literal> 视频设备支持 2D/3D 加速。无法在其他视频设备上启用此功能。
        </para>
      </tip>
    </sect2>
  </sect1>
  <sect1 xml:id="virsh-network-devices">
    <title>配置网络设备</title>

    <para>
      本节介绍如何使用 <command>virsh</command> 配置虚拟网络设备的特定方面。
    </para>

    <para>
      <link xlink:href="https://libvirt.org/formatdomain.html#elementsDriverBackendOptions"/> 中提供了有关 <systemitem class="library">libvirt</systemitem> 网络接口规范的更多细节。
    </para>

    <sect2 xml:id="virsh-multiqueue">
      <title>使用多队列 virtio-net 提升网络性能</title>
      <para>
        多队列 virtio-net 功能允许 VM Guest 的虚拟 CPU 并行传输包，因此可以提升网络性能。有关更多一般信息，请参见<xref linkend="kvm-qemu-multiqueue"/>。
      </para>
      <para>
        要为特定的 VM Guest 启用多队列 virtio-net，请遵照<xref linkend="sec-libvirt-config-editing-virsh"/>中所述编辑其 XML 配置，并按如下所示修改其网络接口：
      </para>
<screen>
&lt;interface type='network'&gt;
 [...]
 &lt;model type='virtio'/&gt;
 &lt;driver name='vhost' queues='<replaceable>NUMBER_OF_QUEUES</replaceable>'/&gt;
&lt;/interface&gt;
</screen>
    </sect2>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-direct">
    <title>使用 macvtap 共享 VM 主机服务器网络接口</title>

    <para>
      使用 Macvtap 可将 VM Guest 虚拟接口直接挂接到主机网络接口。基于 macvtap 的接口扩展了 VM 主机服务器网络接口，它在相同以太网段上有自己的 MAC 地址。通常，使用此功能是为了使 VM Guest 和 VM 主机服务器都直接显示在 VM 主机服务器连接的交换机上。
    </para>

    <note>
      <title>Macvtap 不能与 Linux 网桥搭配使用</title>
      <para>
        Macvtap 不能与已连接到 Linux 网桥的网络接口搭配使用。在尝试创建 macvtap 接口之前，请去除网桥中的接口。
      </para>
    </note>

    <note>
      <title>使用 macvtap 在 VM Guest 与 VM 主机服务器之间通讯</title>
      <para>
        使用 macvtap 时，一个 VM Guest 可与其他多个 VM Guest 通讯，并可与网络上的其他外部主机通讯。但是，该 VM Guest 无法与用于运行它的 VM 主机服务器通讯。这是规定的 macvtap 行为，原因与 VM 主机服务器物理以太网挂接到 macvtap 网桥的方式有关。从 VM Guest 进入该网桥并转发到物理接口的流量无法回弹到 VM 主机服务器的 IP 堆栈。同样，来自 VM 主机服务器 IP 堆栈并发送到物理接口的流量无法回弹到 macvtap 网桥以转发到 VM Guest。
      </para>
    </note>

    <para>
      libvirt 通过指定接口类型 <literal>direct</literal> 支持基于 macvtap 的虚拟网络接口。例如：
    </para>

<screen>&lt;interface type='direct'&gt;
   &lt;mac address='aa:bb:cc:dd:ee:ff'/&gt;
   &lt;source dev='eth0' mode='bridge'/&gt;
   &lt;model type='virtio'/&gt;
   &lt;/interface&gt;</screen>

    <para>
      可以使用 <literal>mode</literal> 属性控制 macvtap 设备的操作模式。以下列表显示了该属性的可能值以及每个值的说明：
    </para>

    <itemizedlist mark="bullet" spacing="normal">
      <listitem>
        <para>
          <literal>vepa</literal>：将所有 VM Guest 包发送到外部网桥。如果包的目标是某个 VM Guest，而该 VM Guest 所在的 VM 主机服务器与包的来源服务器相同，那么这些包将由支持 VEPA 的网桥（现今的网桥通常都不支持 VEPA）发回到该 VM 主机服务器。
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>bridge</literal>：将其目标与来源为同一 VM 主机服务器的包直接递送到目标 macvtap 设备。来源和目标设备需处于 <literal>bridge</literal> 模式才能直接递送。如果其中一个设备处于 <literal>vepa</literal> 模式，则需要使用支持 VEPA 的网桥。
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>private</literal>：将所有包发送到外部网桥；如果通过外部路由器或网关发送所有包，并且设备会将其发回到 VM 主机服务器，则将所有包递送到同一 VM 主机服务器上的目标 VM Guest。如果来源或目标设备处于 private 模式，将遵循此过程。
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>passthrough</literal>：可为网络接口提供更强大能力的一种特殊模式。将所有包转发到接口，并允许 virtio VM Guest 更改 MAC 地址或设置混杂模式，以桥接该接口或在该接口上创建 VLAN 接口。请注意，在 <literal>passthrough</literal> 模式下，网络接口不可共享。将某个接口指派到 VM Guest 会使其与 VM 主机服务器断开连接。出于此原因，在 <literal>passthrough</literal> 模式下经常会将 SR-IOV 虚拟功能指派到 VM Guest。
        </para>
      </listitem>
    </itemizedlist>
  </sect1>
  <sect1 xml:id="sec-libvirt-config-disable-virtio-mellon">
    <title>禁用内存气球设备</title>

    <para>
      内存气球已成为 KVM 的默认选项。设备将显式添加到 VM Guest，因此您无需在 VM Guest 的 XML 配置中添加此元素。如果您出于任何原因想要在 VM Guest 中禁用内存气球，需按如下所示设置 <literal>model='none'</literal>：
    </para>

<screen>&lt;devices&gt;
   &lt;memballoon model='none'/&gt;
&lt;/device&gt;</screen>
  </sect1>
  <sect1 xml:id="virsh-video-dual-head">
    <title>配置多个监视器（双头）</title>

    <para>
      <systemitem class="library">libvirt</systemitem> 支持使用双头配置在多个监视器上显示 VM Guest 的视频输出。
    </para>

    <important>
      <title>不受 Xen 支持</title>
      <para>
        Xen 超级管理程序不支持双头配置。
      </para>
    </important>

    <procedure>
      <title>配置双头</title>
      <step>
        <para>
          当虚拟机正在运行时，校验 <package>xf86-video-qxl</package> 软件包是否已安装在 VM Guest 中：
        </para>
<screen><prompt>&gt; </prompt>rpm -q xf86-video-qxl</screen>
      </step>
      <step>
        <para>
          关闭 VM Guest，并按照<xref linkend="sec-libvirt-config-editing-virsh"/>中所述开始编辑其 XML 配置。
        </para>
      </step>
      <step>
        <para>
          校验虚拟显卡的型号是否为<quote>qxl</quote>：
        </para>
<screen>
&lt;video&gt;
 &lt;model type='qxl' ... /&gt;
</screen>
      </step>
      <step>
        <para>
          将显卡型号规格中的 <option>heads</option> 参数从默认值 <literal>1</literal> 增大为 <literal>2</literal>，例如：
        </para>
<screen>
&lt;video&gt;
 &lt;model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='2' primary='yes'/&gt;
 &lt;alias name='video0'/&gt;
 &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x0'/&gt;
&lt;/video&gt;
</screen>
      </step>
      <step>
        <para>
          将虚拟机配置为使用 Spice 显示器而不是 VNC：
        </para>
<screen>
&lt;graphics type='spice' port='5916' autoport='yes' listen='0.0.0.0'&gt;
 &lt;listen type='address' address='0.0.0.0'/&gt;
&lt;/graphics&gt;
</screen>
      </step>
      <step>
        <para>
          启动虚拟机并使用 <command>virt-viewer</command> 连接到其显示器，例如：
        </para>
<screen><prompt>&gt; </prompt>virt-viewer --connect qemu+ssh://<replaceable>USER@VM_HOST</replaceable>/system</screen>
      </step>
      <step>
        <para>
          在 VM 列表中，选择您已修改了其配置的 VM，并单击<guimenu>连接</guimenu>确认。
        </para>
      </step>
      <step>
        <para>
          在 VM Guest 中装载图形子系统 (Xorg) 后，选择<menuchoice><guimenu>视图</guimenu><guimenu>显示器</guimenu><guimenu>显示器 2</guimenu></menuchoice> 打开一个新窗口，其中会显示第二个监视器的输出。
        </para>
      </step>
    </procedure>
  </sect1>
  <sect1 xml:id="virsh-kvm-zseries-crypto">
    <title>将 IBM Z 上的加密适配器直通到 KVM Guest</title>

    <sect2 xml:id="virsh-kvm-zseries-crypto-intro">
      <title>简介</title>
      <para>
        IBM Z 计算机附带加密硬件以及一些实用的功能，例如生成随机数、生成数字签名或加密。KVM 允许将这些加密适配器作为直通设备专门用于 Guest。这意味着，超级管理程序无法观察 Guest 与设备之间的通讯。
      </para>
    </sect2>

    <sect2 xml:id="virsh-kvm-zseries-crypto-cover">
      <title>本章内容</title>
      <para>
        本章介绍如何将 IBM Z 主机上的加密适配器和域专用于 KVM Guest。该过程包括以下基本步骤：
      </para>
      <itemizedlist>
        <listitem>
          <para>
            对主机上的默认驱动程序屏蔽加密适配器和域。
          </para>
        </listitem>
        <listitem>
          <para>
            装载 <literal>vfio-ap</literal> 驱动程序。
          </para>
        </listitem>
        <listitem>
          <para>
            将加密适配器和域指派到 <literal>vfio-ap</literal> 驱动程序。
          </para>
        </listitem>
        <listitem>
          <para>
            将 Guest 配置为使用加密适配器。
          </para>
        </listitem>
      </itemizedlist>
    </sect2>

    <sect2 xml:id="virsh-kvm-zseries-crypto-reqs">
      <title>要求</title>
      <itemizedlist>
        <listitem>
          <para>
            QEMU/<systemitem class="library">libvirt</systemitem> 虚拟化环境需已正确安装且正常运行。
          </para>
        </listitem>
        <listitem>
          <para>
            用于运行内核的 <literal>vfio_ap</literal> 和 <literal>vfio_mdev</literal> 模块需在主机操作系统上可用。
          </para>
        </listitem>
      </itemizedlist>
    </sect2>

    <sect2 xml:id="virsh-kvm-zseries-crypto-proc">
      <title>将加密适配器专用于 KVM 主机</title>
      <procedure>
        <step>
          <para>
            校验是否已在主机上装载 <literal>vfio_ap</literal> 和 <literal>vfio_mdev</literal> 内核模块：
          </para>
<screen><prompt>&gt; </prompt>lsmod | grep vfio_</screen>
          <para>
            如有任何一个模块未列出，请手动装载，例如：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> modprobe vfio_mdev</screen>
        </step>
        <step>
          <para>
            在主机上创建一个新的 MDEV 设备，并校验是否已添加该设备：
          </para>
<screen>
uuid=$(uuidgen)
$ echo ${uuid} | sudo tee /sys/devices/vfio_ap/matrix/mdev_supported_types/vfio_ap-passthrough/create
dmesg | tail
[...]
[272197.818811] iommu: Adding device 24f952b3-03d1-4df2-9967-0d5f7d63d5f2 to group 0
[272197.818815] vfio_mdev 24f952b3-03d1-4df2-9967-0d5f7d63d5f2: MDEV: group_id = 0
</screen>
        </step>
        <step>
          <para>
            识别主机逻辑分区中您要专用于 KVM Guest 的设备：
          </para>
<screen><prompt>&gt; </prompt>ls -l /sys/bus/ap/devices/
[...]
lrwxrwxrwx 1 root root 0 Nov 23 03:29 00.0016 -&gt; ../../../devices/ap/card00/00.0016/
lrwxrwxrwx 1 root root 0 Nov 23 03:29 card00 -&gt; ../../../devices/ap/card00/
</screen>
          <para>
            在此示例中，该设备是卡 <literal>0</literal> 队列 <literal>16</literal>。为了与硬件管理控制台 (HMC) 配置相匹配，需要将十六进制数 <literal>16</literal> 转换为十进制数 <literal>22</literal>。
          </para>
        </step>
        <step>
          <para>
            使用以下命令对 <literal>zcrypt</literal> 屏蔽适配器：
          </para>
<screen>
<prompt>&gt; </prompt>lszcrypt
CARD.DOMAIN TYPE MODE STATUS REQUEST_CNT
-------------------------------------------------
00 CEX5C CCA-Coproc online 5
00.0016 CEX5C CCA-Coproc online 5
</screen>
          <para>
            屏蔽适配器：
          </para>
<screen>
<prompt>&gt; </prompt>cat /sys/bus/ap/apmask
0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
echo -0x0 | sudo tee /sys/bus/ap/apmask
0x7fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
</screen>
          <para>
            屏蔽域：
          </para>
<screen>
<prompt>&gt; </prompt>cat /sys/bus/ap/aqmask
0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
echo -0x0 | sudo tee /sys/bus/ap/aqmask
0xfffffdffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
</screen>
        </step>
        <step>
          <para>
            将适配器 0 和域 16（十进制数 22）指派到 <literal>vfio-ap</literal>：
          </para>
<screen>
<prompt>&gt; </prompt><command>sudo</command> echo +0x0 &gt; /sys/devices/vfio_ap/matrix/${uuid}/assign_adapter
<prompt>&gt; </prompt>echo +0x16 | sudo tee /sys/devices/vfio_ap/matrix/${uuid}/assign_domain
<prompt>&gt; </prompt>echo +0x16 | sudo tee /sys/devices/vfio_ap/matrix/${uuid}/assign_control_domain
</screen>
        </step>
        <step>
          <para>
            校验配置的矩阵：
          </para>
<screen>
<prompt>&gt; </prompt>cat /sys/devices/vfio_ap/matrix/${uuid}/matrix
00.0016
</screen>
        </step>
        <step>
          <para>
            创建一个新 VM（参见<xref linkend="cha-kvm-inst"/>）并等待它初始化，或使用现有的 VM。对于这两种情况，请确保 VM 已关闭。
          </para>
        </step>
        <step>
          <para>
            将 VM 的配置更改为使用 MDEV 设备：
          </para>
<screen>
<prompt>&gt; </prompt><command>sudo</command> virsh edit <replaceable>VM_NAME</replaceable>
[...]
&lt;hostdev mode='subsystem' type='mdev' model='vfio-ap'&gt;
 &lt;source&gt;
  &lt;address uuid='24f952b3-03d1-4df2-9967-0d5f7d63d5f2'/&gt;
 &lt;/source&gt;
&lt;/hostdev&gt;
[...]
</screen>
        </step>
        <step>
          <para>
            重启动 VM：
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> virsh reboot <replaceable>VM_NAME</replaceable></screen>
        </step>
        <step>
          <para>
            登录到 Guest 并校验该适配器是否存在：
          </para>
<screen>
<prompt>&gt; </prompt>lszcrypt
CARD.DOMAIN TYPE MODE STATUS REQUEST_CNT
-------------------------------------------------
00 CEX5C CCA-Coproc online 1
00.0016 CEX5C CCA-Coproc online 1
</screen>
        </step>
      </procedure>
    </sect2>

    <sect2 xml:id="virsh-kvm-zseries-crypto-moreinfo">
      <title>更多资料</title>
      <itemizedlist>
        <listitem>
          <para>
            <xref linkend="cha-vt-installation"/>中详细介绍了虚拟化组件的安装。
          </para>
        </listitem>
        <listitem>
          <para>
            <link xlink:href="https://www.kernel.org/doc/Documentation/s390/vfio-ap.txt"/> 中详细介绍了 <literal>vfio_ap</literal> 体系结构。
          </para>
        </listitem>
        <listitem>
          <para>
            <link xlink:href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1787405"/> 中提供了一般概述和详细过程。
          </para>
        </listitem>
        <listitem>
          <para>
            <link xlink:href="https://www.kernel.org/doc/html/latest/driver-api/vfio-mediated-device.html"/> 中详细介绍了 VFIO 调解设备 (MDEV) 的体系结构。
          </para>
        </listitem>
      </itemizedlist>
    </sect2>
  </sect1>
</chapter>
