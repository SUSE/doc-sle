<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.raid10" xml:lang="en">
 <title>Creating Software RAID&nbsp;10 Devices</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker>
          </dm:bugtracker>
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
    </info>
    <para>
  This section describes how to set up nested and complex RAID&nbsp;10
  devices. A RAID&nbsp;10 device consists of nested RAID 1 (mirroring)
  and RAID 0 (striping) arrays. Nested RAIDs can either be set up as striped
  mirrors (RAID&nbsp;1+0) or as mirrored stripes (RAID&nbsp;0+1). A
  complex RAID&nbsp;10 setup also combines mirrors and stripes and
  additional data security by supporting a higher data redundancy level.
 </para>
 <sect1 xml:id="sec.raid10.nest">
  <title>Creating Nested RAID 10 Devices with <command>mdadm</command></title>

  <para>
   A nested RAID device consists of a RAID array that uses another RAID
   array as its basic element, instead of using physical disks. The goal of
   this configuration is to improve the performance and fault tolerance of
   the RAID. Setting up nested RAID levels is not supported by &yast;,
   but can be done by using the <command>mdadm</command> command line tool.
  </para>

  <para>
   Based on the order of nesting, two different nested RAIDs can be set up.
   This document uses the following terminology:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <formalpara>
     <title>RAID 1+0:</title>
     <para>
      RAID 1 (mirror) arrays are built first, then combined to form a RAID 0
      (stripe) array.
     </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>RAID 0+1:</title>
     <para>
      RAID 0 (stripe) arrays are built first, then combined to form a RAID 1
      (mirror) array.
     </para>
    </formalpara>
   </listitem>
  </itemizedlist>

  <para>
   The following table describes the advantages and disadvantages of RAID 10
   nesting as 1+0 versus 0+1. It assumes that the storage objects you use
   reside on different disks, each with a dedicated I/O capability.
  </para>

  <table>
   <title>Nested RAID Levels</title>
   <tgroup cols="3">
    <colspec colnum="1" colname="1" colwidth="20*"/>
    <colspec colnum="2" colname="2" colwidth="30*"/>
    <colspec colnum="3" colname="3" colwidth="60*"/>
    <thead>
     <row>
      <entry>
       <para>
        RAID Level
       </para>
      </entry>
      <entry>
       <para>
        Description
       </para>
      </entry>
      <entry>
       <para>
        Performance and Fault Tolerance
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        10 (1+0)
       </para>
      </entry>
      <entry>
       <para>
        RAID 0 (stripe) built with RAID 1 (mirror) arrays
       </para>
      </entry>
      <entry>
       <para>
        RAID 1+0 provides high levels of I/O performance, data redundancy,
        and disk fault tolerance. Because each member device in the RAID 0
        is mirrored individually, multiple disk failures can be tolerated
        and data remains available as long as the disks that fail are in
        different mirrors.
       </para>
       <para>
        You can optionally configure a spare for each underlying mirrored
        array, or configure a spare to serve a spare group that serves all
        mirrors.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        10 (0+1)
       </para>
      </entry>
      <entry>
       <para>
        RAID 1 (mirror) built with RAID 0 (stripe) arrays
       </para>
      </entry>
      <entry>
       <para>
        RAID 0+1 provides high levels of I/O performance and data
        redundancy, but slightly less fault tolerance than a 1+0. If
        multiple disks fail on one side of the mirror, then the other mirror
        is available. However, if disks are lost concurrently on both sides
        of the mirror, all data is lost.
       </para>
       <para>
        This solution offers less disk fault tolerance than a 1+0 solution,
        but if you need to perform maintenance or maintain the mirror on a
        different site, you can take an entire side of the mirror offline
        and still have a fully functional storage device. Also, if you lose
        the connection between the two sites, either site operates
        independently of the other. That is not true if you stripe the
        mirrored segments, because the mirrors are managed at a lower level.
       </para>
       <para>
        If a device fails, the mirror on that side fails because RAID 1 is
        not fault-tolerant. Create a new RAID 0 to replace the failed side,
        then resynchronize the mirrors.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <sect2 xml:id="sec.raid10.nest.10">
   <title>Creating Nested RAID 10 (1+0) with mdadm</title>
   <para>
    A nested RAID 1+0 is built by creating two or more RAID 1 (mirror)
    devices, then using them as component devices in a RAID 0.
   </para>
   <important>
    <title>Multipathing</title>
    <para>
     If you need to manage multiple connections to the devices, you must
     configure multipath I/O before configuring the RAID devices. For
     information, see
     <xref linkend="cha.multipath" xrefstyle="ChapTitleOnPage"/>.
    </para>
   </important>
   <para>
    The procedure in this section uses the device names shown in the
    following table. Ensure that you modify the device names with the names
    of your own devices.
   </para>
   <table>
    <title>Scenario for Creating a RAID 10 (1+0) by Nesting</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="3334*"/>
     <colspec colnum="2" colname="2" colwidth="3334*"/>
     <colspec colnum="3" colname="3" colwidth="3334*"/>
     <thead>
      <row>
       <entry>
        <para>
         Raw Devices
        </para>
       </entry>
       <entry>
        <para>
         RAID 1 (mirror)
        </para>
       </entry>
       <entry>
        <para>
         RAID 1+0 (striped mirrors)
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <simplelist>
         <member><filename>/dev/sdb1</filename>
         </member>
         <member><filename>/dev/sdc1</filename>
         </member>
        </simplelist>
       </entry>
       <entry>
        <para>
         <filename>/dev/md0</filename>
        </para>
       </entry>
       <entry morerows="1">
        <para/>
        <para>
         <filename>/dev/md2</filename>
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <simplelist>
         <member><filename>/dev/sdd1</filename>
         </member>
         <member><filename>/dev/sde1</filename>
         </member>
        </simplelist>
       </entry>
       <entry>
        <para>
         <filename>/dev/md1</filename>
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <procedure>
    <step>
     <para>
      Open a terminal console.
     </para>
    </step>
    <step>
     <para>
      If necessary, create four 0xFD Linux RAID partitions of equal size
      using a disk partitioner such as parted.
     </para>
    </step>
    <step>
     <para>
      Create two software RAID&nbsp;1 devices, using two different
      devices for each device. At the command prompt, enter these two
      commands:
     </para>
<screen>sudo mdadm --create /dev/md0 --run --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1
sudo mdadm --create /dev/md1 --run --level=1 --raid-devices=2 /dev/sdd1 /dev/sde1</screen>
    </step>
    <step>
     <para>
      Create the nested RAID 1+0 device. At the command prompt, enter the
      following command using the software RAID 1 devices you created in the
      previous step:
     </para>
<screen>sudo mdadm --create /dev/md2 --run --level=0 --chunk=64 \
--raid-devices=2 /dev/md0 /dev/md1</screen>
     <para>
      The default chunk size is 64 KB.
     </para>
    </step>
    <step>
     <para>
      Create a file system on the RAID 1+0 device
      <filename>/dev/md2</filename>, for example an XFS file system:
     </para>
<screen>sudo mkfs.xfs /dev/md2</screen>
     <para>
      Modify the command to use a different file system.
     </para>
    </step>
    <step>
     <para>
      Edit the <filename>/etc/mdadm.conf</filename> file or create it, if it
      does not exist (for example by running <command>sudo vi
      /etc/mdadm.conf</command>). Add the following lines (if the file
      already exists, the first line probably already exists).
     </para>
<screen>DEVICE containers partitions
ARRAY /dev/md0 UUID=<replaceable>UUID</replaceable>
ARRAY /dev/md1 UUID=<replaceable>UUID</replaceable>
ARRAY /dev/md2 UUID=<replaceable>UUID</replaceable></screen>
     <para>
      The UUID of each device can be retrieved with the following command:
     </para>
<screen>sudo mdadm -D /dev/<replaceable>DEVICE</replaceable> | grep UUID</screen>
    </step>
    <step>
     <para>
      Edit the <filename>/etc/fstab</filename> file to add an entry for the
      RAID 1+0 device <filename>/dev/md2</filename>. The following example
      shows an entry for a RAID device with the XFS file system and
      <filename>/data</filename> as a mount point.
     </para>
<screen>/dev/md2 /data xfs defaults 1 2</screen>
    </step>
    <step>
     <para>
      Mount the RAID device:
     </para>
<screen>sudo mount /data</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.raid10.nest.01">
   <title>Creating Nested RAID 10 (0+1) with mdadm</title>
   <para>
    A nested RAID 0+1 is built by creating two to four RAID 0 (striping)
    devices, then mirroring them as component devices in a RAID 1.
   </para>
   <important>
    <title>Multipathing</title>
    <para>
     If you need to manage multiple connections to the devices, you must
     configure multipath I/O before configuring the RAID devices. For
     information, see
     <xref linkend="cha.multipath" xrefstyle="ChapTitleOnPage"/>.
    </para>
   </important>
   <para>
    In this configuration, spare devices cannot be specified for the
    underlying RAID&nbsp;0 devices because RAID&nbsp;0 cannot tolerate
    a device loss. If a device fails on one side of the mirror, you must
    create a replacement RAID&nbsp;0 device, than add it into the mirror.
   </para>
   <para>
    The procedure in this section uses the device names shown in the
    following table. Ensure that you modify the device names with the names
    of your own devices.
   </para>
   <table>
    <title>Scenario for Creating a RAID 10 (0+1) by Nesting</title>
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="3334*"/>
     <colspec colnum="2" colname="2" colwidth="3334*"/>
     <colspec colnum="3" colname="3" colwidth="3334*"/>
     <thead>
      <row>
       <entry>
        <para>
         Raw Devices
        </para>
       </entry>
       <entry>
        <para>
         RAID 0 (stripe)
        </para>
       </entry>
       <entry>
        <para>
         RAID 0+1 (mirrored stripes)
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <simplelist>
         <member><filename>/dev/sdb1</filename>
         </member>
         <member><filename>/dev/sdc1</filename>
         </member>
        </simplelist>
       </entry>
       <entry>
        <para>
         <filename>/dev/md0</filename>
        </para>
       </entry>
       <entry morerows="1">
        <para/>
        <para>
         <filename>/dev/md2</filename>
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <simplelist>
         <member><filename>/dev/sdd1</filename>
         </member>
         <member><filename>/dev/sde1</filename>
         </member>
        </simplelist>
       </entry>
       <entry>
        <para>
         <filename>/dev/md1</filename>
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <procedure>
    <step>
     <para>
      Open a terminal console.
     </para>
    </step>
    <step>
     <para>
      If necessary, create four 0xFD Linux RAID partitions of equal size
      using a disk partitioner such as parted.
     </para>
    </step>
    <step>
     <para>
      Create two software RAID&nbsp;0 devices, using two different
      devices for each RAID 0 device. At the command prompt, enter these two
      commands:
     </para>
<screen>sudo mdadm --create /dev/md0 --run --level=0 --chunk=64 \
--raid-devices=2 /dev/sdb1 /dev/sdc1
sudo mdadm --create /dev/md1 --run --level=0 --chunk=64 \
--raid-devices=2 /dev/sdd1 /dev/sde1</screen>
     <para>
      The default chunk size is 64 KB.
     </para>
    </step>
    <step>
     <para>
      Create the nested RAID 0+1 device. At the command prompt, enter the
      following command using the software RAID&nbsp;0 devices you
      created in the previous step:
     </para>
<screen>sudo mdadm --create /dev/md2 --run --level=1 --raid-devices=2 /dev/md0 /dev/md1</screen>
    </step>
    <step>
     <para>
      Create a file system on the RAID 1+0 device
      <filename>/dev/md2</filename>, for example an XFS file system:
     </para>
<screen>sudo mkfs.xfs /dev/md2</screen>
     <para>
      Modify the command to use a different file system.
     </para>
    </step>
    <step>
     <para>
      Edit the <filename>/etc/mdadm.conf</filename> file or create it, if it
      does not exist (for example by running <command>sudo vi
      /etc/mdadm.conf</command>). Add the following lines (if the file
      exists, the first line probably already exists, too).
     </para>
<screen>DEVICE containers partitions
ARRAY /dev/md0 UUID=<replaceable>UUID</replaceable>
ARRAY /dev/md1 UUID=<replaceable>UUID</replaceable>
ARRAY /dev/md2 UUID=<replaceable>UUID</replaceable></screen>
     <para>
      The UUID of each device can be retrieved with the following command:
     </para>
<screen>sudo mdadm -D /dev/<replaceable>DEVICE</replaceable> | grep UUID</screen>
    </step>
    <step>
     <para>
      Edit the <filename>/etc/fstab</filename> file to add an entry for the
      RAID 1+0 device <filename>/dev/md2</filename>. The following example
      shows an entry for a RAID device with the XFS file system and
      <filename>/data</filename> as a mount point.
     </para>
<screen>/dev/md2 /data xfs defaults 1 2</screen>
    </step>
    <step>
     <para>
      Mount the RAID device:
     </para>
<screen>sudo mount /data</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.raid10.complex">
  <title>Creating a Complex RAID 10</title>

  <para>
   &yast; (and <command>mdadm</command> with the
   <option>--level=10</option> option) creates a single complex software
   RAID&nbsp;10 that combines features of both RAID 0 (striping) and RAID
   1 (mirroring). Multiple copies of all data blocks are arranged on
   multiple drives following a striping discipline. Component devices should
   be the same size.
  </para>

  <para>
   The complex RAID&nbsp;10 is similar in purpose to a nested
   RAID&nbsp;10 (1+0), but differs in the following ways:
  </para>

  <table>
   <title>Complex RAID 10 compared to Nested RAID 10</title>
   <tgroup cols="3">
    <colspec colnum="1" colname="1" colwidth="3334*"/>
    <colspec colnum="2" colname="2" colwidth="3334*"/>
    <colspec colnum="3" colname="3" colwidth="3334*"/>
    <thead>
     <row>
      <entry>
       <para>
        Feature
       </para>
      </entry>
      <entry>
       <para>
        Complex RAID&nbsp;10
       </para>
      </entry>
      <entry>
       <para>
        Nested RAID&nbsp;10 (1+0)
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        Number of devices
       </para>
      </entry>
      <entry>
       <para>
        Allows an even or odd number of component devices
       </para>
      </entry>
      <entry>
       <para>
        Requires an even number of component devices
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Component devices
       </para>
      </entry>
      <entry>
       <para>
        Managed as a single RAID device
       </para>
      </entry>
      <entry>
       <para>
        Manage as a nested RAID device
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Striping
       </para>
      </entry>
      <entry>
       <para>
        Striping occurs in the near or far layout on component devices.
       </para>
       <para>
        The far layout provides sequential read throughput that scales by
        number of drives, rather than number of RAID 1 pairs.
       </para>
      </entry>
      <entry>
       <para>
        Striping occurs consecutively across component devices
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Multiple copies of data
       </para>
      </entry>
      <entry>
       <para>
        Two or more copies, up to the number of devices in the array
       </para>
      </entry>
      <entry>
       <para>
        Copies on each mirrored segment
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Hot spare devices
       </para>
      </entry>
      <entry>
       <para>
        A single spare can service all component devices
       </para>
      </entry>
      <entry>
       <para>
        Configure a spare for each underlying mirrored array, or configure a
        spare to serve a spare group that serves all mirrors.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <sect2 xml:id="sec.raid10.complex.replicas">
   <title>Number of Devices and Replicas in the Complex RAID&nbsp;10</title>
   <para>
    When configuring a complex RAID&nbsp;10 array, you must specify the
    number of replicas of each data block that are required. The default
    number of replicas is two, but the value can be two to the number of
    devices in the array.
   </para>
   <para>
    You must use at least as many component devices as the number of
    replicas you specify. However, the number of component devices in a
    RAID&nbsp;10 array does not need to be a multiple of the number of
    replicas of each data block. The effective storage size is the number of
    devices divided by the number of replicas.
   </para>
   <para>
    For example, if you specify two replicas for an array created with five
    component devices, a copy of each block is stored on two different
    devices. The effective storage size for one copy of all data is 5/2 or
    2.5 times the size of a component device.
   </para>
  </sect2>

  <sect2 xml:id="sec.raid10.complex.layout">
   <title>Layout</title>
   <para>
    The complex RAID&nbsp;10 setup supports three different layouts which
    define how the data blocks are arranged on the disks. The available
    layouts are near (default), far and offset. They have different
    performance characteristics, so it is important to choose the right
    layout for your workload.
   </para>
   <sect3 xml:id="sec.raid10.complex.layout.near">
    <title>Near Layout</title>
    <para>
     With the near layout, copies of a block of data are striped near each
     other on different component devices. That is, multiple copies of one
     data block are at similar offsets in different devices. Near is the
     default layout for RAID&nbsp;10. For example, if you use an odd
     number of component devices and two copies of data, some copies are
     perhaps one chunk further into the device.
    </para>
    <para>
     The near layout for the complex RAID&nbsp;10 yields read and write
     performance similar to RAID&nbsp;0 over half the number of drives.
    </para>
    <para>
     Near layout with an even number of disks and two replicas:
    </para>
<screen>sda1 sdb1 sdc1 sde1
  0    0    1    1
  2    2    3    3
  4    4    5    5
  6    6    7    7
  8    8    9    9</screen>
    <para>
     Near layout with an odd number of disks and two replicas:
    </para>
<screen>sda1 sdb1 sdc1 sde1 sdf1
  0    0    1    1    2
  2    3    3    4    4
  5    5    6    6    7
  7    8    8    9    9
  10   10   11   11   12</screen>
   </sect3>
   <sect3 xml:id="sec.raid10.complex.layout.far">
    <title>Far Layout</title>
    <para>
     The far layout stripes data over the early part of all drives, then
     stripes a second copy of the data over the later part of all drives,
     making sure that all copies of a block are on different drives. The
     second set of values starts halfway through the component drives.
    </para>
    <para>
     With a far layout, the read performance of the complex RAID&nbsp;10
     is similar to a RAID&nbsp;0 over the full number of drives, but
     write performance is substantially slower than a RAID&nbsp;0 because
     there is more seeking of the drive heads. It is best used for
     read-intensive operations such as for read-only file servers.
    </para>
    <para>
     The speed of the RAID&nbsp;10 for writing is similar to other
     mirrored RAID types, like RAID&nbsp;1 and RAID&nbsp;10 using near
     layout, as the elevator of the file system schedules the writes in a
     more optimal way than raw writing. Using RAID&nbsp;10 in the far
     layout is well suited for mirrored writing applications.
    </para>
    <para>
     Far layout with an even number of disks and two replicas:
    </para>
<screen>sda1 sdb1 sdc1 sde1
  0    1    2    3
  4    5    6    7
  . . .
  3    0    1    2
  7    4    5    6</screen>
    <para role="intro">
     Far layout with an odd number of disks and two replicas:
    </para>
<screen>sda1 sdb1 sdc1 sde1 sdf1
  0    1    2    3    4
  5    6    7    8    9
  . . .
  4    0    1    2    3
  9    5    6    7    8</screen>
   </sect3>
   <sect3 xml:id="sec.raid10.complex.layout.offset">
    <title>Offset Layout</title>
    <para>
     The offset layout duplicates stripes so that the multiple copies of a
     given chunk are laid out on consecutive drives and at consecutive
     offsets. Effectively, each stripe is duplicated and the copies are
     offset by one device. This should give similar read characteristics to
     a far layout if a suitably large chunk size is used, but without as
     much seeking for writes.
    </para>
    <para>
     Offset layout with an even number of disks and two replicas:
    </para>
<screen>sda1 sdb1 sdc1 sde1
  0    1    2    3
  3    0    1    2
  4    5    6    7
  7    4    5    6
  8    9   10   11
 11    8    9   10</screen>
    <para>
     Offset layout with an odd number of disks and two replicas:
    </para>
<screen>sda1 sdb1 sdc1 sde1 sdf1
  0    1    2    3    4
  4    0    1    2    3
  5    6    7    8    9
  9    5    6    7    8
 10   11   12   13   14
 14   10   11   12   13</screen>
   </sect3>
   <sect3 xml:id="sec.raid10.complex.layout.parameter">
    <title>Specifying the number of Replicas and the Layout with &yast; and mdadm</title>
    <para>
     The number of replicas and the layout is specified as <guimenu>Parity
     Algorithm</guimenu> in &yast; or with the <option>--layout</option>
     parameter for mdadm. The following values are accepted:
    </para>
    <variablelist>
     <varlistentry>
      <term><literal>n<replaceable>N</replaceable></literal>
      </term>
      <listitem>
       <para>
        Specify <literal>n</literal> for near layout and replace
        <replaceable>N</replaceable> with the number of replicas.
        <literal>n2</literal> is the default that is used when not
        configuring layout and the number of replicas.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>f<replaceable>N</replaceable></literal>
      </term>
      <listitem>
       <para>
        Specify <literal>f</literal> for far layout and replace
        <replaceable>N</replaceable> with the number of replicas.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>o<replaceable>N</replaceable></literal>
      </term>
      <listitem>
       <para>
        Specify <literal>o</literal> for offset layout and replace
        <replaceable>N</replaceable> with the number of replicas.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <note>
     <title>Number of Replicas</title>
     <para>
      &yast; automatically offers a selection of all possible values for
      the <guimenu>Parity Algorithm</guimenu> parameter.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.raid10.complex.yast">
   <title>Creating a Complex RAID&nbsp;10 with the &yast; Partitioner</title>
   <procedure>
    <step>
     <para>
      Launch &yast; and open the Partitioner.
     </para>
    </step>
    <step>
     <para>
      If necessary, create partitions that should be used with your RAID
      configuration. Do not format them and set the partition type to
      <guimenu>0xFD Linux RAID</guimenu>. When using existing partitions it
      is not necessary to change their partition type&mdash;&yast;
      will automatically do so. Refer to
      <xref linkend="sec.yast2.i_y2_part_expert"/> for details.
     </para>
     <para>
      For RAID&nbsp;10 at least four partitions are needed. It is
      strongly recommended to use partitions stored on different hard disks
      to decrease the risk of losing data if one is defective. It is
      recommended to use only partitions of the same size because each
      segment can contribute only the same amount of space as the smallest
      sized partition.
     </para>
    </step>
    <step>
     <para>
      In the left panel, select <guimenu>RAID</guimenu>.
     </para>
     <para>
      A list of existing RAID configurations opens in the right panel.
     </para>
    </step>
    <step>
     <para>
      At the lower left of the RAID page, click <guimenu>Add RAID</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Under <guimenu>RAID Type</guimenu>, select <guimenu>RAID 10 (Mirroring
      and Striping)</guimenu>.
     </para>
     <para>
      You can optionally assign a <guimenu>RAID Name</guimenu> to your RAID.
      It will make it available as
      <filename>/dev/md/<replaceable>name</replaceable></filename>. See
      <xref linkend="sec.raid.yast.names"/> for more information.
     </para>
    </step>
    <step>
     <para>
      In the <guimenu>Available Devices</guimenu> list, select the desired
      partitions, then click <guimenu>Add</guimenu> to move them to the
      <guimenu>Selected Devices</guimenu> list.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="raid10_a.png" width="80%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="raid10_a.png" width="100%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      (Optional) Click <guimenu>Classify</guimenu> to specify the preferred
      order of the disks in the RAID array.
     </para>
     <para>
      For RAID types such as RAID&nbsp;10, where the order of added disks
      matters, you can specify the order in which the devices will be used.
      This will ensure that one half of the array resides on one disk
      subsystem and the other half of the array resides on a different disk
      subsystem. For example, if one disk subsystem fails, the system keeps
      running from the second disk subsystem.
     </para>
     <substeps performance="required">
      <step>
       <para>
        Select each disk in turn and click one of the <guimenu>Class
        X</guimenu> buttons, where X is the letter you want to assign to the
        disk. Available classes are A, B, C, D and E but for many cases fewer
        classes are needed (only A and B, for example). Assign all available
        RAID disks this way.
       </para>
       <para>
        You can press the <keycap function="control"/> or <keycap
        function="shift"/> key to select multiple devices. You can also
        right-click a selected device and choose the appropriate class from
        the context menu.
       </para>
      </step>
      <step>
       <para>
        Specify the order of the devices by selecting one of the sorting
        options:
       </para>
       <formalpara>
        <title><guimenu>Sorted</guimenu>:</title>
        <para>
         Sorts all devices of class A before all devices of class B and so
         on. For example: <literal>AABBCC</literal>.
        </para>
       </formalpara>
       <formalpara>
        <title><guimenu>Interleaved</guimenu>:</title>
        <para>
         Sorts devices by the first device of class A, then first device of
         class B, then all the following classes with assigned devices. Then
         the second device of class A, the second device of class B, and so
         on follows. All devices without a class are sorted to the end of
         the devices list. For example: <literal>ABCABC</literal>.
        </para>
       </formalpara>
       <formalpara>
        <title>Pattern File:</title>
        <para>
         Select an existing file that contains multiple lines, where each is
         a regular expression and a class name (<literal>"sda.*
         A"</literal>). All devices that match the regular expression are
         assigned to the specified class for that line. The regular
         expression is matched against the kernel name
         (<filename>/dev/sda1</filename>), the udev path name
         (<filename>/dev/disk/by-path/pci-0000:00:1f.2-scsi-0:0:0:0-part1</filename>)
         and then the udev ID
         (<filename>dev/disk/by-id/ata-ST3500418AS_9VMN8X8L-part1</filename>).
         The first match made determines the class if a deviceâ€™s name
         matches more than one regular expression.
        </para>
       </formalpara>
      </step>
      <step>
       <para>
        At the bottom of the dialog, click <guimenu>OK</guimenu> to
        confirm the order.
       </para>
       <informalfigure>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="raid10_classify_a.png" width="80%" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="raid10_classify_a.png" width="100%" format="PNG"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Click <guimenu>Next</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Under <guimenu>RAID Options</guimenu>, specify the <guimenu>Chunk
      Size</guimenu> and <guimenu>Parity Algorithm</guimenu>, then click
      <guimenu>Next</guimenu>.
     </para>
     <para>
      For a RAID 10, the parity options are n (near), f (far), and o
      (offset). The number indicates the number of replicas of each data
      block that are required. Two is the default. For information, see
      <xref linkend="sec.raid10.complex.layout" xrefstyle="SectTitleOnPage"/>.
     </para>
    </step>
    <step>
     <para>
      Add a file system and mount options to the RAID device, then click
      <guimenu>Finish</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Next</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Verify the changes to be made, then click <guimenu>Finish</guimenu> to
      create the RAID.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.raid10.complex.yast.mdadm">
   <title>Creating a Complex RAID 10 with mdadm</title>
   <para>
    The procedure in this section uses the device names shown in the
    following table. Ensure that you modify the device names with the names
    of your own devices.
   </para>
   <table>
    <title>Scenario for Creating a RAID&nbsp;10 Using mdadm</title>
    <tgroup cols="2">
     <colspec colnum="1" colname="1" colwidth="50*"/>
     <colspec colnum="2" colname="2" colwidth="50*"/>
     <thead>
      <row>
       <entry>
        <para>
         Raw Devices
        </para>
       </entry>
       <entry>
        <para>
         RAID&nbsp;10
        </para>
       </entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         <filename>/dev/sdf1</filename>
        </para>
        <para>
         <filename>/dev/sdg1</filename>
        </para>
        <para>
         <filename>/dev/sdh1</filename>
        </para>
        <para>
         <filename>/dev/sdi1</filename>
        </para>
       </entry>
       <entry>
        <para>
         <filename>/dev/md3</filename>
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <procedure>
    <step>
     <para>
      Open a terminal console.
     </para>
    </step>
    <step>
     <para>
      If necessary, create at least four 0xFD Linux RAID partitions of equal
      size using a disk partitioner such as parted.
     </para>
    </step>
    <step>
     <para>
      Create a RAID&nbsp;10 by entering the following command.
     </para>
<screen>mdadm --create /dev/md3 --run --level=10 --chunk=32 --raid-devices=4 \
/dev/sdf1 /dev/sdg1 /dev/sdh1 /dev/sdi1</screen>
     <para>
      Make sure to adjust the value for <option>--raid-devices</option> and
      the list of partitions according to your setup.
     </para>
     <para>
      The command creates an array with near layout and two replicas. To
      change any of the two values, use the <option>--layout</option> as
      described in <xref linkend="sec.raid10.complex.layout.parameter"/>.
     </para>
    </step>
    <step>
     <para>
      Create a file system on the RAID&nbsp;10 device
      <filename>/dev/md3</filename>, for example an XFS file system:
     </para>
<screen>sudo mkfs.xfs /dev/md3</screen>
     <para>
      Modify the command to use a different file system.
     </para>
    </step>
    <step>
     <para>
      Edit the <filename>/etc/mdadm.conf</filename> file or create it, if it
      does not exist (for example by running <command>sudo vi
      /etc/mdadm.conf</command>). Add the following lines (if the file
      exists, the first line probably already exists, too) .
     </para>
<screen>DEVICE containers partitions
ARRAY /dev/md3 UUID=<replaceable>UUID</replaceable></screen>
     <para>
      The UUID of the device can be retrieved with the following command:
     </para>
<screen>sudo mdadm -D /dev/md3 | grep UUID</screen>
    </step>
    <step>
     <para>
      Edit the <filename>/etc/fstab</filename> file to add an entry for the
      RAID&nbsp;10 device <filename>/dev/md3</filename>. The following
      example shows an entry for a RAID device with the XFS file system and
      <filename>/data</filename> as a mount point.
     </para>
<screen>/dev/md3 /data xfs defaults 1 2</screen>
    </step>
    <step>
     <para>
      Mount the RAID device:
     </para>
<screen>sudo mount /data</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
